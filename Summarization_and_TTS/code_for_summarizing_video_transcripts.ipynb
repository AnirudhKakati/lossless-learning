{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dffac96-2bb2-4e56-ab1f-ccbd28c366c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datasets\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "884549f4-5ab1-4898-afd5-b39b45dd8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = [a for a in os.listdir() if \".json\" in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0701dc6-ac0d-470a-8305-adb81dea02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_list = [a for a in json_list if \"fetched_videos_foundational_mathematics_youtube_videos_with_transcripts.json\" in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ef1e4d-3d58-4ca5-80d7-d6d1585d790d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 136 transcripts.\n"
     ]
    }
   ],
   "source": [
    "file_name = []\n",
    "data = []\n",
    "\n",
    "for file in json_list:\n",
    "    file_name.append(file)\n",
    "    output_file = '/projects/anra7539/projects/big_data/'+file\n",
    "    data.append(pd.read_json(output_file))\n",
    "\n",
    "shape = sum([d.shape[0] for d in data])\n",
    "\n",
    "print(f\"Total {shape} transcripts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea72bc-23ad-4509-bc09-4ac514f5e716",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcad5a15-ff9f-4766-8694-2c5bf1d5d51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5ca3a2020245c8abf1b125b23810e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "device = \"cuda\"\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(name,\n",
    "                                                          load_in_8bit = True,\n",
    "                                                          trust_remote_code = True,\n",
    "                                             device_map = device,\n",
    "                                             cache_dir='/scratch/alpine/anra7539')\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(name, truncation_side = \"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee938d2c-66a9-4c27-aa04-e84970ef1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization(transcript, prompt):\n",
    "    with torch.no_grad():\n",
    "        input_text = f'''{prompt}\\n\\nTranscript: {transcript}\\nSummary:'''\n",
    "        input_tokens = tokenizer(input_text, return_tensors=\"pt\", truncation=True, \n",
    "                                 max_length=128000).to(device)\n",
    "        outputs = model.generate(**input_tokens, \n",
    "                                 max_new_tokens=500, \n",
    "                                 temperature=0.1, \n",
    "                                 top_p=0.9, \n",
    "                                 repetition_penalty=1.1, \n",
    "                                 pad_token_id=tokenizer.eos_token_id, do_sample=True)        \n",
    "        \n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        summary = full_response.split(\"Summary:\")[1].strip()\n",
    "        \n",
    "        last_period_idx = summary.rfind(\".\")\n",
    "        if last_period_idx != -1:\n",
    "            summary = summary[:last_period_idx + 1].strip()\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e30b406-98c0-4b20-ab62-fe18b6da0cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''You are an expert technical writer.\n",
    "Read the transcript of a technical talk and write a detailed summary in first-person voice, as if you are explaining the ideas.\n",
    "Do NOT mention or refer to the speaker, presenter, or author.\n",
    "Avoid all phrases like \"the speaker says,\" \"in this talk,\" or \"Hannes explains.\"\n",
    "Your goal is to internalize the knowledge from the transcript and re-express it clearly and concisely, in your own words, as though you were explaining it to a peer.\n",
    "Write 25–35 sentences.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54db87cb-1f8d-4d67-b0c1-41ced0d67645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/136 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "100%|██████████| 136/136 [53:14<00:00, 23.49s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(file_name)):\n",
    "    output_file = f'/projects/anra7539/projects/big_data/transcript_summaries/summary_{file_name[i]}'\n",
    "    \n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r') as f:\n",
    "            try:\n",
    "                existing_data = [json.loads(line) for line in f]\n",
    "            except json.JSONDecodeError:\n",
    "                existing_data = []\n",
    "    else:\n",
    "        existing_data = []\n",
    "\n",
    "    processed_indices = {item['index'] for item in existing_data}\n",
    "\n",
    "    with open(output_file, 'a') as f:\n",
    "        for j in tqdm(range(len(data[i]))):\n",
    "            if j in processed_indices:\n",
    "                continue \n",
    "    \n",
    "            summary = summarization(data[i].Transcript[j], prompt)\n",
    "            \n",
    "            result = {\n",
    "                \"index\": j,\n",
    "                \"Domain\": data[i].Domain[j],\t\n",
    "                \"Sub Domain\": data[i]['Sub Domain'][j],\n",
    "                \"Topic\": data[i].Topic[j],\n",
    "                \"Video Title\": data[i]['Video Title'][j],\n",
    "                \"URL\": data[i].URL[j],\n",
    "                \"Thumbnail\": data[i].Thumbnail[j],\n",
    "                \"ID\": data[i].ID[j],\n",
    "                \"Publish Time\": data[i]['Publish Time'][j],\n",
    "                \"Channel\": data[i].Channel[j],\n",
    "                \"Channel ID\": data[i]['Channel ID'][j],\n",
    "                \"Transcript\": data[i].Transcript[j],\n",
    "                \"Summary\": summary\n",
    "            }\n",
    "    \n",
    "            f.write(json.dumps(result) + \"\\n\")\n",
    "            f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d104e9-231e-4f9c-ae6a-8c1c45ac13ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
