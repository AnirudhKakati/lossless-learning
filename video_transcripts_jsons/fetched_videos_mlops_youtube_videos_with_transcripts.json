[
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Model Deployment using TensorFlow Serving",
        "Video Title":"Deploying production ML models with TensorFlow Serving overview",
        "URL":"https:\/\/www.youtube.com\/watch?v=1d4BzR_7Nbc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/1d4BzR_7Nbc\/hqdefault.jpg",
        "ID":"1d4BzR_7Nbc",
        "Publish Time":"2022-07-21T13:00:00Z",
        "Channel":"TensorFlow",
        "Channel ID":"UC0rqucBdTuFTjJiefW5t-IQ",
        "Transcript":"WEI WEI: Hi, there. Thanks for watching our new video series of deploying production ML models with TensorFlow Serving. My name is Wei, and I'm a developer advocate at Google. In this video series, we're going to take a deep dive into TensorFlow Serving and go beyond the simple paradigm of starting a TF Serving Docker image and then running inferences. We will talk about how to customize TF Serving, how to tune the performance, how to do A\/B testing and monitoring, and so much more. If you want to learn more about deploying production ML models, you are in the right place. Suppose you have come up with a brilliant ML model that achieves a new state of the art. Now, you want to deploy it into production to solve the business problems. What do you do? As you may know, a production ML system requires so much more than just the model. It requires a comprehensive set of tools, including configuration, data collection, and verification, machine resource management, feature extraction, process management, serving infrastructure, monitoring, and et cetera. Too many people are surprised ML code is just one tiny piece in the entire system. There are many, many aspects in any production ML system. In this video series, we are going to focus only on the serving infrastructure part, which is a critical part in ML ops. So what is serving anyway? Let's say you have a model developed by a research scientist. You're ready to feed the production data to it and make business decisions based on the output of your model. In essence, you have a client app, which could be your website or your mobile apps. They send client requests with input data to your server. The server accepts the data, optionally does some preprocessing, runs inferences on your machine learning model to produce an output, optionally postprocess the output, and lastly, sends the result back to your client app. The part of where your server runs inferences is called serving. And TensorFlow Serving is an inference framework that helps you serve your production ML models with low latency and high throughput. TensorFlow Serving makes it easy to deploy new algorithms and experiments while keeping the same server architecture and APIs. With TensorFlow Serving, you can run multiple models in a single process, or you can run multiple versions of a model loaded over time. It can support both REST and gRPC and has out-of-the-box support for GPU. TensorFlow Serving also seamlessly works with Docker and Kubernetes to scale with demand. TF Serving is also part of TensorFlow Extended, which is a full suite of TensorFlow components to tackle the entire machine learning pipeline. Now let's take a quick look at the overall architecture of TF Serving. At a high level, TensorFlow saves the model as a kind of servable managed by the loader. Inside a TF Serving core, the Dynamic Manager determines when the loader should load the servable, in which case, the ServableHandle will be used to perform predictions based on incoming requests from the clients. You don't necessarily need to understand all the internal details to use TF Serving effectively. But this will be useful if you want to create a new servable to serve non-TensorFlow models, which we will discuss in a future episode. So how do we actually use TF Serving? The general workflow consists of four steps. You train your model with TensorFlow, exporting your model using SavedModel formats. Then you can start a TF Serving model server. And lastly, you make requests to the Serving server via REST or gRPC. Let's see it in action. You can use model.fit method to train your model. Once your training is done, you can use model.save method to export the model as a saved model. Next step is to download a TF Serving image by using Docker pool. We also clone the TF Serving repository so that we can use a sample saved model for demonstration. Now we can start the TF Serving server by running Docker run. We're using a simple model, 1\/2 plus 2, which halves the inputs and then adds two. Once the model server is started, we can query the model results by sending a POST request to TF Serving. We use curl to fill the input of 1, 2, and 5 to the model, and then we get 2.5, 3, and 4.5 as a return. So that's it. We have successfully deployed and run our first model using TensorFlow Serving. Another way to do this is to directly use TensorFlow model server without Docker. You can install the TensorFlow model server binary via apt cat, and start it directly. Then you can send a POST request to it like we did before. If you have a compute-intensive model to serve, you may want to use accelerators like GPU. All you need to do is to use the GPU Docker image and the GPU version of your saved model. And TensorFlow Serving will then use GPU to run inference for better performance. One more tip before we wrap up today-- if you have trouble sending requests to TensorFlow Serving, you can turn on web host logging to see the specific error for each incoming request. You just need to set TF_CPP_VMODULE environment variable when you start Docker, and TF Serving will be showing the errors for each troubling request. Here, it shows my input dimensions are wrong. So I can go back and change my request. This is very helpful for debugging your code. So to summarize, deploying machine learning models into production is a very important matter, and TF Serving is a high-performance serving infrastructure that makes it easy for you. We have given you a quick walkthrough of using TF Serving. If you want to learn more about TF Serving at a high level, please check out these links. In our next episode, we're going to learn how to send requests to TF Serving using Python and C++. Thank you. [MUSIC PLAYING]"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Model Deployment using TensorFlow Serving",
        "Video Title":"tf serving tutorial | tensorflow serving tutorial | Deep Learning Tutorial 48 (Tensorflow, Python)",
        "URL":"https:\/\/www.youtube.com\/watch?v=P-5sMcpTE0g",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/P-5sMcpTE0g\/hqdefault.jpg",
        "ID":"P-5sMcpTE0g",
        "Publish Time":"2021-08-05T12:30:11Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"Are you using flask or fast API to serve your machine learning model? Google's tensorflow team has developed this tool called tf serving which is little better than flask and fast APi. It allows you to solve model in a better way and it also allows you to do your model version management in little better way. So in this video we'll look into some theory and then we'll practically see how this tool works let's begin! Let's say you're building an email classification model where you're specifying whether email is spam or not spam a typical data science workflow would be you will collect data, do data cleaning feature engineering and you will train a model. Let's say this is a tensorflow model you will then export it to a file you can just say model.save and that will export the model to a file on hard disk. Then you can write a fast api or flask based server. This is the proper approach people write these servers which will load the save model. As you can see in this line and when clients make http call such as this predict function it will call this function where you use the loaded model. Now let's say this model is running fine in production you get new data, you train a new model and you are ready to deploy the next version which is version 2 to your beta users. So what happens here is version 1 is production and then version 2 is ready to be deployed to beta users. Now imagine how you would have to change your fast API code in this case. You would somehow detect in your predict function that the given user is a beta user and then you can call beta model. So see here I'm loading model one and two into two different variables and I can so the request based on what type of user that is. And this is one approach maybe you can have a different server altogether just for beta users. But you can already see the complexity here here you have to do if else maybe you have five different versions which you want to serve to different type of users. Overall you get an idea the version management is little tedious. TF serving makes this version management easy and model serving becomes very easy here you have to write all this code we will see in tf serving you don't have to write any code you just run one command and your server is ready. So I will show you practically how it looks but let me mention one another benefit of tf serving- it is batch inference . You might have let's say thousands of incoming requests for inference. In tf serving you can match those requests and solve those requests to a model in batches the benefit is better hardware resource utilization. You can have a timeout parameter say 5 second is a time timeout so and your batch size is 100. In 5 seconds let's say you only receive 52 requests then it will badge only 52 because you don't want these requests to be waiting till you receive 100 requests, you know. So batching thing also works now let me just show you directly how this whole thing works. In one of my videos in deep learning tutorial playlist I built a text classification model using BERT so here is the video if you want to see the model building process but I'm just going to open that same notebook here and you can see you're classifying email as spam and non-spam using BERT and tensorflow once the model is built and ready you can export it to a file using dot save method. So here I have called dot save three times basically just to show you three different versions and you can see these three different versions are saved here so you see saved model directory here these are essentially the same models. But in real life you would have different models you know version one and version two would have some differences. But here just for the tutorial purpose I save the same model so if you go to individual model you'll see couple of files you know like the assets and variables and so on you you don't have to worry about what these files are you can just directly load this model and start using it. The first step here would be to install tensorflow serving the most convenient way to install tensorflow serving is docker so you can just pull the docker image by running this docker pull command. There are by the way other ways like if you're using Ubuntu you can do apt-get and things like that but here I will just use docker okay so in my git bash I can just run docker poll tensorflow server and it will pull the latest image I already have the latest image so it's not doing much but if you run docker desktop I have windows and on windows I have already installed docker desktop and if you look at docker desktop you see I I already have tensorflow serving image on my computer you can use git clone this this will just download you know some sample models for you but we are going to use our own model so you can follow these commands just for your own learning. But here I'm going to just lower the model which I saved so I have saved again in df serving I have all these saved models okay so now I'm going to open windows powershell so you open windows powershell I already have it open here. I'm just going to call it clear and okay it will look like this and once you have this open you need to first load the docker container. so how to load that? Okay, I have created a github page where I have given all the commands and I'm going to put the link of this in video description below but the way you load docker is by calling this docker run minus v so I will just gradually type in those commands so you get an idea so you will say docker run okay. See there are a couple of command line options this is not a docker tutorial so I'm not gonna go into each option in detail but minus v is an important one so here you supply your host directory. So I want to use this directory here right so I will just say control c okay and control v. so I want this directory to be mapped to some directory inside my docker container so I'll just give the same name you know tf serving so this directory from my host maps to this directory in my docker image then I will loop port. So let's say I want 8605 port to be exposed as 8605 so 8605 on my host system is mapped to h605 on my on my what on my docker container image and then I will do entry point now if I don't do entry point directly what's gonna happen is docker will have its default entry point so the image that we loaded the default entry point will directly run the tf serving command. But we don't want to do that so I will just do entry point. Entry point is bin bash. Bin bash will take you to command prompt basically okay and then you give the name of your image so what is the name of your image well it is tensorflow slash surving when you do that you will enter into you see you have entered into now your docker image and within the docker image since you mapped this 48 serving. See 48 tf serving? You see that here and if you do let me just clear it. If you do ls. See if you do ls minus you see all these directories you see saved models which is good. Now the way you run tensorflow serving is by running this command this will run your tensorflow serving and here you need to supply your rest API port. So my rest API port where i'll be making my you know http calls is eight six zero five and you can have any any port pretty much but we decided h605 you need to give model name let's see my my any anything you can give x y z my model name is email model and what is my model base path? So my model base path is nothing but this directory. Okay when you run this command see in one line we okay what is what is it saying there is some error happening okay we forgot to give the name of the model file so it is actually actually saved models right you need to give saved models actually. Okay so that was the reason i was getting an error. So now this means my model server is ready see just by writing one line of code I I created my server now how does this server work well for that you need to run postman so install postman it's a popular tool which is used to make http request you know what before I run postman let me just show you in a browser itself so 8605 is my port then you do v1 is just a fixed thing okay v1 then you do models then you do email model so when I do this it is saying I have version 3 available which means by default when I ran that command it looked into save model directory and whatever is the highest number version 3 it says that is the model which is available. Now I will use postman to make actual requests so in the postman what you can do is you can say email model colon predict. okay so I have same url basically see this part is kind of fixed. I was confused initially by v1 but ignore that v1 is always v1. It's not the actual version okay then email model and then colon predict in the body so by default you'll be here you have to come to body and in the body click on row raw and this is the format that it expects. Okay so let me reduce the font size a little bit okay so here you will say instances this is a fixed format okay don't ask me why it is like that that's a format that tf serving expects and I'm giving two emails this one is not a spam email this one is a spam email and when I say send see it is sending the request and it got this prediction back. It got this prediction back from the model server that we just started. So here if the value is less than 0.5 it is it is not a spam if it is more than 0.5 it is spam. So you can see is clearly see second one is spam that's why value is more than 0.5 this one is not a spam hence the value is less than 0.5 if you want to call this by version number. You can simply say slash versions slash 3 column predict and you get the same response. But we already saw this has only version two of three available if i do version two and send see what happens. It says where this model is not found even version one is not found so what if I want to make all three versions available? I want to make all three versions available okay? For that you have to use model config file how do I do that okay let me exit ctrl c it will exit. Okay I can do that by changing the command a little bit so here [Music] I will say model config file is equal to so I need to supply model config file so I already have this config called model.config.a and if you look at that file what it says is model version policy all which means in this directory. Whatever version you see make them all survivable instances. So I'm going to run this see successfully loaded version 2 version 1 version 3. All threes will be survivable so now when I say see version one predict it works version two predict it works I get the same output by the way because my models are same but in real life scenario version one, version two, version three. This they will give a little different output. version three works. Version four what's gonna happen obviously you don't have version four guys many times you have a production version let's say one is a production version then you build version two and you want to deploy that only to beta users and you don't want this call to be happening through version. Maybe it will be better if I can do something like okay production you know like production versus beta do you think that would be better? Well that's also supported and in order to do that you have to use version labels. So I have the same exit file but I added this section. I'm saying my version one is my production version two is my beta and I'm going to run my model server now with that particular file so it's a different config file basically c. okay so request to oh I see okay you need to if you get you get this okay let me just show you okay if if you give this command it gives this error and to tackle this error you need to give this particular option here. If you supply this particular command line option then you don't get this error. Okay so now my you know tf serving is ready whenever you see this. It means it is ready and it can serve using labels so let's see so first of all you can obviously call using version numbers. So let's verify that first so here I will supply version number one. You know it works just okay but now I want to use labels so you will just say instead of version labels and lesser beta. See my beta works you can also do production these are the two labels I have this works just okay and in your client code what you'll be doing is you know let's say you are making this code in Javascript. In Javascript you will have all these urls or beta whatever and based on beta or production user you can switch your users so this is almost like a b testing type of scenario where you are testing you know new version for beta users. If you look at documentation model config file has few other options as well. For example you can serve two entirely different models. You can have a dog and cat classifier here and truck and car cat classifier here. And just by running one command you can have your server which can do different type of inferences. So just go through this documentation. I did not cover all the options we talked about batching. So matching configuration can be this you just pass this batch parameters file and in the file you can say okay batch 128 request with certain timeout and it will help you utilize your hardware resources in a most appropriate way. That's all i had for this tutorial. I have a Github page where I have given my notebook you can export the models i did not upload exported models because they were very big you have all this config files so see this is config.c and so on I highly encourage that you practice whatever you learned in this video because just by watching video you're not going to learn anything. Trust me you need to practice. So just install df serving practice whatever you learned in this video and I hope you can get a good understanding of how this thing works. If you like this video please give it a thumbs up and share it with your friends. All the useful links are in video description below. Thank you. Thanks for watching."
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Model Deployment using TensorFlow Serving",
        "Video Title":"Model Deployment on Production || Tensorflow Serving Tutorial",
        "URL":"https:\/\/www.youtube.com\/watch?v=aSLQI4dJA-w",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/aSLQI4dJA-w\/hqdefault.jpg",
        "ID":"aSLQI4dJA-w",
        "Publish Time":"2021-10-04T16:05:26Z",
        "Channel":"Developers Hutt",
        "Channel ID":"UCVPZdqRRRCK3GEOYOL-r73A",
        "Transcript":"we have developed several models from scratch on this channel but that's the only part of the whole machine learning system the next most important part is the deployment of those models in production so that it can solve for what it trained for there are a lot of videos and blogs which shows deployment of machine learning model using flask and some other python libraries but in real world that's not the optimal way to serve to serve your models there are some highly optimized tools for deployment of machine learning models in production so this is the beginning of a new playlist on this channel where we will use some production grade tools to deploy machine learning models and in this video we will use tensorflow serving tool to see how we can deploy an image classifier so before jumping straight to deployment process let's understand the workflow first when we say model deployment it means we have a system in which we can push the input and get the output so tensorflow serving provides us this part it has every necessary component inside it for hosting a model so that you don't need to worry about any type of code or management you just have to send the input and get the output by making api calls once you deploy it tensorflow serving supports both cpu and gpu deployment but in this video we will going with the cpu deployment only but the process for gpu deployment is almost identical as the cpu so there is no separate video required for that so here are the steps for a successful deployment first you need to prepare and train or load a pre-trained model get the details about input and output layers freeze the parameters and save the graph file into a saved model format now get the tensorflow serving image from docker hub then type magic command to host your saved model and for this tutorial you will need linux which is optional you can have windows but there are some docker issues in that docker installed and tensorflow now let's get started choose or make your working directory and inside it make another directory with the same name as you want for your model now open up the jupyter notebook and import tensorflow i am using pre-trained model but you can train your model if you don't want a pre-trained one once your model gets loaded you can use summary to see information about each layers in the model but we need shape of input and output layer so that we can reshape our data during the time of inference once you get that iterate through each of the layers and freeze the weights then use kerat.save function to save the frozen model in saved model format now this is the model which we want to deploy on tensorflow serving and hence we have the model part ready now pull the tensorflow serving image from docker hub and one thing to remember your model should have to follow a directory structure which looks something like this your model name your model version and inside version directory you have to put your model and here you can see the model is inside our model name directory so select all the files and put it inside the directory name 1 which means the version 1 and now you can see the perfect structure as we wanted now run the docker image on port 8501 for http request and 8500 for grpc request then mount the model directory inside the container using mount and give the absolute source path to the model directory and the target path is where it located inside the container now set the environment variable for model name as the same name of the model directory and the set tag as image name and here you can see the grpc is hosted on this address and the rest api on this address now the hosting part is done let's see how we can communicate with it first take an image from internet and save it to your working directory which we will use to send to model to get the prediction make a file to send the inputs via http request import request to send the request to server import json to encode and decode data numpy for array manipulation opencv for reading image tensorflow and time now read the image and opencv read it in pgr format so you have to convert it into rgb resize it according to the input layer of the hosted model and expand the dimension to set the batch now the full url to communicate is your local host port version models model name and the use case now encode the image data to json format and write a header to identify that the sending data is in json format now use the post function to send the data on the server and get the response which is your prediction now print the prediction to see what it has given us it is a whole bunch of numbers but what does that mean actually here are 1000 floating point values which represents the probability for each class now let's decode the prediction into a readable format so keras provides us a function to decode the prediction for models trained on imagenet and hence this is the result and here you can see the classes and the probability and the highest probability is our correct answer that's all to the hosting and interacting with your model on the production server in later videos of this series we will see more tools to deploy and manage multiple modules on the both cpu and gpu servers simultaneously so stay tuned for that and also don't forget to stay updated with the content on instagram link in the description and as always thanks for watching you"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Model Deployment using TensorFlow Serving",
        "Video Title":"Ease ML deployments with TensorFlow Serving  - Cambridge ML Summit \u201819",
        "URL":"https:\/\/www.youtube.com\/watch?v=4mqFDwIdKh0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/4mqFDwIdKh0\/hqdefault.jpg",
        "ID":"4mqFDwIdKh0",
        "Publish Time":"2019-11-01T23:07:40Z",
        "Channel":"Google for Developers",
        "Channel ID":"UC_x5XG1OV2P6uZZ5FSM9Ttw",
        "Transcript":"HANNES HAPKE: A quick show of hands, who is deploying machine learning models today? OK. Who is using TF Serving for that? OK. And who's using Flask? OK. I'm curious about that difference there. OK. I hope this is interesting for you. I want to show you how easy it is to deploy machine learning models because the bad news is that most machine learning models won't get deployed. And there are a million reasons for that. And one of the key reasons is that a lot of people think that machine learning deployments of the models is tedious, is cumbersome. But I want to show you in the next 25 minutes, that it's not the case. That it's actually pretty easy and straightforward. Hi, my name is Hannes, and I'm the head of engineering at a company called Caravel, where we focus on conversational machine learning models. And in this small company, we continuously deploy machine learning models. So I have to automate most of this stuff. and I have to focus on easy deployments so that we can get this out with a small team. So what I want to show you in the next 20 minutes is sort of, like, how we do this and how we set up our deployment environments, so that the models get out to the people who use them. So let's face it, you do all the hard work. You train all the machine learning models. But then you want to get the models out so that everybody else can use them. And the deployment of the machine learning models is what is between your hard work and the greater world. So I want to show you how easy you can do this, but a lot of data scientists ask the question, why do I really need to worry about machine learning deployments? Well, so we have these two fields. We have data science and machine learning on one side. And then we have DevOps on the other side. And there's currently a new discipline emerging, which is called machine learning infrastructure. But while this is sort of emerging and job descriptions are being created and formed, I think it's up to the data scientist to push out the models. The DevOps folks are really good at scaling model, at scaling microservices and providing network access to these services. But they're not the experts on the TensorFlow ecosystem, or they're not familiar with the different standards on how to serialize models. So it comes down to the data scientists to push for the deployments. And what I want to show you in this presentation here is how you can deploy these models. And then you can either take it up to the DevOps folks in your company or your organization to show them how that can be deployed, or you can deploy them yourselves in the various environments you're using. But before we get into the details of how to use TensorFlow Serving, I want to show you an inefficient way of deploying machine learning models. This is a very simple setup of a Flask API. In this moment, we create an endpoint called classify. We take some form data. In this moment, we're having a form field called review. If we preprocessing that through some function-- don't worry about this for right now-- but then we do the model prediction on it. We use a Keras model here. We call the predict method on it. And whatever we get back from that method, we JSON-ify it. And then send it back as a response to this HTTP request. A lot people do that. A lot of blogs point out that this is the way to do it. But I think this is a highly inefficient way of doing this. And here's the reason why. Everything you do in this moment is highly inconsistent. You can create another model where your API standards or the API structure is completely different. Your payloads might be completely different. And so when you bring in new people to the team, they see one model and they'll see one pattern. And they'll see another model and a completely different pattern. Nothing is really standardized in that moment. If you use a Flask setup, then it's up to you. You define how you manage different versions of the model. And again, you run into these issues of different setups for different models. So it's highly inconsistent. And then, you're using your precious hardware highly inefficient. What you want to do at the end of the day is you want to use mini-batching. And I will get to that later-- what that means and how you can use this. And again, it's highly inefficient for large models. So these days, where all the talk about transformers, we talk about [INAUDIBLE],, XL Net and models in the gigabyte range. And in that moment, it's really inefficient to just load in the Flask environment, and then maybe your web server [INAUDIBLE] spins out multiple environments and everything gets replicated, and you're just-- your memory explodes in that moment. So here's the rescue to this madness. This would be my Flask setup. It works, but over time it gets really difficult to maintain. So the rescue to that is TensorFlow Serving. TensorFlow Serving is part of the TensorFlow Extended Ecosystem. As you know, in the TensorFlow ecosystem, there are a bunch of other tools around the actual model framework, which you can use to validate data. You can validate models. You analyze the models, et cetera, et cetera. One of the tools is TensorFlow Serving. It has been battle-tested at Google for many years, and I think it was open source around two years ago-- and it's highly scalable. And it allows you to deployment models easily for up to 2 gigabytes. And the reason why we're getting to this 2 gigabyte limit is just that the protobuf file format, which is used internally, has a 32-bit limit. But there are ways around it. So let me show you how you can quickly deploy machine learning models. But before we get to that point, you have to export your model. So let's assume you have trained your model, you have done all of the validation. You're ready. You're good to go. You just want to get this out. So here's the five-minute deployment. You export your Keras model. In this moment, it doesn't need to be a Keras model. It can be a TensorFlow estimator or it can be a graph. But basically what you do is, in the TensorFlow 2.0 world, you call a function called TF safe model. And then, this will export your model into a very predefined data structure. So basically your model gets converted into a Protobuf file. And then, you're going to see this folder structure and file structure. So your export will be time stamped. And the timestamp will be the epoch of that moment when you exported it. You're going to have a folder with assets, which could be your dictionaries if you have like hash tables, or you have a lookup of vocabulary. You have your actual Protobuf file, which is defining your network. And then you have a variable folder, which is containing all the variable information of the different weights and things like that. So every time that you export a model, you get a new folder. The time stamp is changing every time. So once you do that, in the remaining four minutes of your TensorFlow starting setup, you pull the Docker image. And you basically start up the Docker run command with a bunch of parameters. First of all, you expose two ports. We get to that in a second, which services they are representing. You mount your directory where you, say, exported your folder to, your model to. You tell TensorFlow Serving which model it's supposed to load. In this case, the model is called my_model. And then you just run the container called TensorFlow Serving. In case you want to do the inferences on a GPU, the only thing what you have to do is you swap out the container image name. [INAUDIBLE] All right. With that, you're going to see a bunch of output. This is an example output in what we run at Caravel. We've run a bunch of embeddings and language models. So in this case, it's a highly trained model. And it's really big. And so it takes a long time to load it. I think it's somewhere-- 18 seconds or something. But what you want to look out for are these red lines there. They basically tell you, hey, everything works successfully. And you have two endpoints. We haven't programmed a single line of code. And you have two fully functioning APIs ready for inferences right there-- five minutes. So in this moment, we have to link the models to the Docker container. We did this through mounting folders on the host machine. And often you don't want to do this. You want to put them somewhere in the cloud, when you have a GCP bucket or a S3 bucket. So you can easily load this out of the box. So in this case, the only thing you have to change is your start up command of TensorFlow Serving. So in this case, you don't mount the path. We just point towards the S3 bucket at this moment. You have to provide the credentials, either inside of the container or you load them as an additional Docker run command. And you're good to go. Again, you haven't programmed a single line of code-- works out of the box. Sometimes you want to load different models, different models at the same time through the same instance. And the way you can do this is you define a model configuration file. And this is basically the file I'm showing you on this slide. You can define the different models with a base path. That's the path where you can find all the different versions for the different models. And then you pass this additional configuration file to your TensorFlow Serving command. And you're good to go. The only one thing I want to highlight here is I would not use this in production. Because if you run multiple models within the same container, maybe one model is more often used than the other one and you want to scale them at different rates. So in this moment, it might make more sense to sort of separate them by containers. And then you can scale them differently. Sometimes it's really useful to specify specific versions of your model. So remember we exported the model at the beginning and so you get the time stamps? Well, you can specify these timestamps in your configuration file. And you can tell TensorFlow Serving which models to load and which ones to serve at the time. The standard behavior of TensorFlow Serving is that it will always serve you the latest model. So if you put a new model in that S3 bucket or GCP bucket, the server will automatically detect it and swap out the models. So again, you don't have to restart the service. You don't have to program in a new line of code. You don't have to deploy the container again. Everything happens under the hood automatically. And sometimes, you just want to serve two versions at the same time. And you can do this through the configuration here I'm showing you on this slide. And then, sometimes you don't want to remember the version numbers. You just want to give it labels. And you can define, for example, a stable model or a testing model. And under the hood in your configuration file, you can swap out the version numbers. But your clients, or the people using your inference servers, just point at the keys or at the labels. And then you can swap out the versions under the hood. So everything stays the same. Again, you just change your configuration file, load it into your container, and you're good to go. So now with everything being set up, let's talk about how you can actually infer your models. And you have two options there. As we saw at the beginning, when the server came up, you have two ports. You have 8,500, and you have 8501. The first port is for the RPC protocol. And the second port is for a standard called REST. And here I'm going to show you a couple of examples how you can infer on the REST side. So the REST protocol's pretty common. You basically have a couple-- you have URLs, you make Get or Post requests or the standard HTTP request, you submit some data, and then you get a response back. In this case, you submit your data to a very predefined URL structure, which is your HOST machine. Your port, you define in this case will be 8501. And you give it the model name. In our case, it's like my_model. If you want to specify a particular version, you see how you can do this. You just append it to the URL and you're good to go-- nothing crazy. Once we have this, we can write a few lines of code. But this is pretty standard code. This is, like, the standard Python implementation with a request package. So we define the path here. We create the inference data, which is basically-- in this case, I have a little review model where I want to figure out what's the sentiment of this text. And we pass a dictionary with a key called instances and the list of the different samples to the model server. In this case, I just make one request at a time. And then the response contains the score for each label in this case. So that's going to be, then, returned from the API. If you want to use the RPC protocol, the set up is a little bit more involved. And it's a little bit more code than what I would like to squeeze on one slide. So I won't show you the code for that. But the TensorFlow Serving documentation is really good. And you can pick up a lot of examples. But what I want to stress is sort of the workflow. So in this case, you would convert your samples into a Protobuf format. And in this case, you would define your data structures [INAUDIBLE] the input field. So you are basically saying this input is a string, this input is a float, et cetera. And then you convert them into base64. And after you created, like, a RPC stub, you have all these RPC methods. And then you can infer the data from the RPC servers. And often the question, again, is hey, what should I use? Should I use REST or RPC? And my standard answer to that question is if you want to go quickly, implement, get something off the ground, use REST. It's easy to debug. You can use Curl. You can use your browser. You're good to go in a couple of minutes. But if you want the efficient way, I would recommend the RPC implementation. Because that format was developed by Google. And it's used to reduce the network traffic. Because you basically limit your payloads, and it also becomes highly redundant, or sort of secure in terms of the transmission. And the wonderful thing in terms of model inferences is that the inference is more efficient and faster. So if you submit on the REST side, the server has to figure out what's the data structure. Because under the hood what it's actually doing is it's converting all your data samples into TF example records. And then it's inferring them under the hood. So if you submit them through JSON, basically the REST standard, then the server has to figure out is it a string or is it an integer, things like this. And this is going to take time. So for example, in the case of [INAUDIBLE],, we have 768 fields in that list. Every time, the server has to figure out, now is it a string or is it a float? So in this case, if you used RPC, it's much quicker. A lot of people ask about A\/B testing of models. And while TensorFlow Serving is not supporting this on the server side, very easily you can implement a client side A\/B testing. Remember earlier I showed you how you can define the different model inference URLs? Well, you can just swap out the version numbers. And here's a little function. Feel free to use it. So we just define the path here, which then gives us the URL. And then, with another little line of code, we can swap out the version number based on the percentage we define on the threshold. And if you pass in a non-value to your version, TensorFlow Serving automatically defaults to the default model. And if you specify a version number, then it will use that for 10% of the cases here. Sometimes it's really helpful on the client side to know some of the meta information of the models. And this is often being used in some information of, like, hey, which model am I inferring here? What's the version number? What are the inputs? Can you tell me a little bit more information about the model itself? And TensorFlow Serving can provide you these information out of the box. So we have a little function here, which is inferring very similar to what we have seen before. But instead of making a post request, we make a get request to the very similar API. And we append \/metadata to the UL structure, and you're good to go. And what you see from there is a very lengthy data structure, which tells you all the information about the models. And what is actually showing is something called the model signature. It tells you all the inputs, the data formats, the names, and the outputs. And again, you get the names and the data structures. And you can use that in various forms and ways. And often, it's really good, especially when you have telemetry systems and you want to send some feedback from your client to another system. You want to know which model just inferred and gave you the prediction. So a lot of people say, oh, I can do this with Flask, maybe not the last one or, a-- but the next thing I want to show you, you can definitely not do with Flask, or you have to code a lot. So here's something which you get out of the box with TensorFlow Serving, which is an amazing feature. And I think it's highly underrated, and it's hard to find in the documentation. You can do something called minibatching, which is a wonderful thing with TensorFlow Serving. So in the initial case of Flask, we saw that if we made an HTTP request, you made the model prediction, and then it sent the response back. But in this, basically, request response loop, you basically locked up your hardware for that one prediction. Well, we use CPUs and GPUs efficiently. So we can bundle together a bunch of samples, infer them at the same time, and then send out the results to the different clients again. And that way, we can use our precious hardware more efficient. So let me show you how easy it is to do that with TensorFlow Serving. But in this case, you basically get a bunch of requests. Instead of inferring them one by one, you either wait for a limited amount of time, or once your batch is full, you infer. And then you're good to go, and you send it back to the clients. The configuration is super simple. You define a text file with the information I show you here in this slide. You define your batch size. You define your timeout, how long do you want to wait. And you have a couple other options, like, batch threads and batch queues, or how many cues you want to use, and things like this. But the bare minimum definition is something what you see on this slide. And then, again, you don't have to code. You just load that additional configuration file. You mount the directory where you can find that file. You tell TensorFlow Serving that you want to enable batching. And then you pass in the parameter file, and you're good to go. One thing I want to highlight here is if you have different business cases or, for example, if you want to do batch processes, you want to have a long time out, and you want to maximize your batches. But if you have real time applications, you want to reduce your time out. And you want to be really tight on how many milli or microseconds you want to provide or you want to wait. So for most applications, you actually want to be somewhere in between. So that requires some trial and error to see how big is your batch size. You can watch this on your GPU with the memory usage. And then you can see what the latency is on your internal application. So I would advise you to just trial and error with the parameters there. There are a couple more optimizations I want to point out. And again, it's fairly easy. You just have the additional parameters you pass into your [INAUDIBLE] command. And you have a couple of parameters here. And I want to point out the first one specifically. Initially, I showed you the slide of how to load remote models. So if you have a bunch of versions in the remote bucket, by default, TensorFlow Serving goes to the bucket every two seconds and checks is there a new model available or not. It does that polling. So over time, that can occur a lot of charges. Because you basically do a list request, and you submit a bunch of data back with all of the model information. So in this case, I would change the polling time. It's going to save you some money. And then you can, if you don't want to use all the cores for TensorFlow, you have a couple options there. But yeah, feel free to play around with it-- configurations. And again, you just pass them in as additional arguments. And then, as a last optimization I want to point out, with machine learning models becoming bigger and bigger, these days we talk about quantitization. And we talk about pruning. You can use TensorRT together with TensorFlow Serving. And you can use their nvidia tool by converting your model with the saved_model_cli tool, which is a wonderful tool also to inspect your machine learning models in case somebody else developed the model and there's no documentation. You can inspect the model, and it shows you the entire signature, what I showed you earlier with the metadata. You can also see this in your command line with the saved_model_cli tool. You convert this model. And then you start TensorFlow Serving with a runtime argument for nvidia. You point the model towards the optimized model. And you have to use their GPU container, because it's nvidia. So they want to use their GPUs for that. With that, a quick shout out to the folks in the room who might have some PyTorch models. You can use ONNX. ONNX is an exchange format for machine learning models. And they have their own runtime engine which you can use in various environments like browsers, [INAUDIBLE] devices, and servers, as well. Quick note on what's sort of the future of the model deployments-- one thing you can use today is TensorFlow Transform. Anybody use TF Transform? It's a wonderful package. I did drink the Kool-Aid on the TF Transform. So I have to be careful. Because the wonderful thing is, especially with TensorFlow Serving, you can convert your pre-processing steps as a graph. And you can connect them with your model itself. So what I do these days, I deploy them as a bundle. That avoids sort of the misalignment of my pre-processing functions and my inference models and my model definitions. And in that case, I can offload the entire pre-processing to the model server. That's usually a really performing machine. And my clients don't have to worry about tokenization and converting to [INAUDIBLE] and things like this. The wonderful thing about the TensorFlow ecosystem is that the Protobuf files, or the folder structure I showed you earlier of the exporter models, you can use them today to deploy your models to etch devices in the browsers, for example. The only thing is, what you have to do is you have to convert these exported models to something what JavaScript can understand. But everything is based on this one exported file, what I showed you earlier. There's a wonderful development happening around Kubeflow Pipelines. So if you build entire machine learning pipelines, and you want to do some evolved A\/B testing, or you want to do some model routing, I highly recommend Kubeflow. There's a wonderful package, as part of the Kubeflow program. It's called Istio. Istio is only used for routing [INAUDIBLE] traffic between containers. But you can use it in the setup form models, as well. So you can phase models in and out. You can do some A\/B testing. It's a wonderful set up. So I highly recommend that. And then what's going to have a big change on the way we probably deploy machine learning models in the future is federated learning. So instead of the clients submit the data back to the server, we make the inferences on the client side and then sort of build these mesh networks and we put them together. So I'm looking forward to see a couple of changes there in the future. Just a quick shout out-- my friend Catherine and I, we're working on a compilation of teaching data scientists all the things you need to do before you train and after you train. And if you want to learn more details about how to deploy machine learning models, I highly recommend this publication. We're trying to get an early release out for TensorFlow world. And with that, thank you. And I'm looking forward to your questions. [APPLAUSE] [MUSIC PLAYING]"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Flask and FastAPI for Model Serving",
        "Video Title":"Deploying ML Models as APIs using Flask and FastAPI",
        "URL":"https:\/\/www.youtube.com\/watch?v=O3z95pRKW6M",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/O3z95pRKW6M\/hqdefault.jpg",
        "ID":"O3z95pRKW6M",
        "Publish Time":"2021-06-05T23:20:04Z",
        "Channel":"DSC MAIT",
        "Channel ID":"UC-7pVvDglahkJCIYvYxq8Jw",
        "Transcript":"hello everyone so i welcome you to the dsc made webinar on deploying ml models using apis as apis using fast api and flask uh just confirm me if you are able to hear me and view the screen properly just type in the chat box only [Music] okay so let's start this session firstly let me introduce myself so i am costa gupta third year i teammate and i i am a machine learning mentor in dsc made so i usually code in python javascript and any other language which comes to my project and i was working as a machine learning writer in april machine learning plus my internship just got finished today and let's talk about this topic deploying machine learning models as apis using fast flask and fast api so let us first discuss why we want to deploy our applications what is the need of deploying the applications so if you look at this image this jupiter notebook in the background so what you are seeing is a jupiter notebook which most of the data scientists use to perform the daily tasks like training the model and cleaning the data set fetching the data and all of the tasks are performed in jupiter notebooks but do you think that any recruiter or any end-to-end user would like to run the notebook from the end and then use your model suppose you built a machine learning model for salary prediction you predicted a salary based on number of factors but do you think that a person will get your notebook and then run the full notebook and then predict the salaries it's never going to happen neither the recruiter nor the end-to-end user will do this for example you can take the example of gpg3 gt3 is a language model with 175 billion parameters it takes months to train the model i have the access key but i i am not training the gpt3 on my machine because i simply can't do i don't have the infrastructure neither i can train a huge model like this so what gpt 3 and open air people have done is train the model deployed in our in a cloud platform and then provided access so we will talk about this later so the second point i want to mention is taking action based on results so suppose you built a salary predictor right you provided the information and you want a fee a job in data science field as an intern and these are the various parameters like the experience and the work you have done so based on that if you are generating a list of all the companies which offer these experiences so you can directly match the user with these entries so these type of actions you cannot take in a jupyter notebook or a normal python file you need to make that model applicable to the users only making it relevant to the users like i said in the starting also you cannot share this notebook with the users and then say them just in the more just run all the cells and you will get the result so that's why we are deploying the models so that how is the deployment not deploying the models so let's talk about this model what is this model so for this uh webinar we will be studying the classification model like i am not going to explain the model and the technicalities because i am assuming a bit you know about machine learning stuff so what i am going to explain you what is the model we are dealing so the model is classification model for hip-hop and rock its it's popular like you will get the data set on kegel also so what what the data contain data contain the criteria for the music like if you listen to music music don't have the title and the artist name and the label it also has the technicalities like acousticness danceability energy livelies tempo valence so these are the various factors that decide on music so we we got the data set of these uh features of music and then we built a classification model for this the model was built using decision tree algorithm so it's a basic algorithm suppose you suppose consider the case you wanted to attend this session but you had a test so you will attend it or not that's a one case now if you don't want to attend what you will do so that type of classification like you do in your mind is just same as the decision tree algorithm so that was a brief about what algorithm was used in this model so we will discuss this we will use this model as in deployment for flask and fast api so do you have any questions till this point let me know in the chat or you can unmute yourself also just let me know yes no so i will proceed further okay cool so next we will talk about deploying machine learning models using i said apis so what the hell is api so api you will get this term a lot api is one of the best use cases of technology and if you're a python geek you must have used a lot of apis apis are around many apis you can use so what is the api and why we use it so suppose uh you went to a restaurant and you gave a order to the waiter and the waiter send that order to the kitchen staff that this this xyz table requires this much food so what the waiter will do convey the message from your end to the uh restaurant staff so that yeah so that information carrier waiter is the api here suppose the restaurant staff is the server and you are the user so api is serving the data from the server to your set so you can you must be thinking you can do this directly only you can run the model you can directly ask the user but what what benefit an api will give you is that suppose a waiter is serving in a restaurant he is not he's not going to only serve you there are many customers there are many tables so the waiter will fetch the data fetch the food from the restaurant staff and provide all the clients all the users with the food so same thing is the api you can write the api in any language ruby node anything python but you can use the api to fetch the data in any language javascript python php any language you can use that api to get the data the core purpose of api is to be an intermediate between the server and the clients so what like i explained in the layman example waiters in the restaurant vedas is getting the food data from the kitchen staff as the server and then providing all the clients all the customers in the restaurant as the food items the main benefit you will get about creating apis and a model because you can use that api model to to embed in any application like you are working as a team and you went to a hacket and you want to develop a machine learning model case so one team can work on back-end part creating the api part of the model and one team can work on making the android app to use that api created by the back end to fetch the data as a client in the android app i will give i will show both the examples website and android app as i've created both of them so let's talk about how to make this api how to create this api so in like we're talking about in python so we will talk about two frameworks flask and fast api there is another uh framework called django which is a full-fledged mvp making uh framework we will not talk about it as it it has a huge scope and it's limited to this webinar so let's talk about flask what is a what is flask flask is microwave framework you can create like if you belong to or if you know a little bit about web development so you must know node.js and express they are used to create a backend of the website in the same way flash scan fast api are used to create the back end of the websites using python so it is similar to node.js and express and almost most of the similarities are same i have used both so i can say this it is used for backend development and it is popular because it's very easy to use it's a minimalistic package you will not find much code in the flask because it doesn't have any it doesn't depend on too many libraries it's only a minimalistic package but the flask 2.2 came and it eliminated some of the points i will discuss that while we will see the code and that's all about it you can build web applications apis may be websites i build websites also many are build websites also so you can build anything related to web using flask so why there was a need of fast api why the fast api came so the main purpose of fast api is to eliminate the speed factor and the usability of the users so suppose if i'm building a api and a model using flask i will not get features like form validation now what i mean by form validation is that when i am providing the uh parameters to the flask to make for the api call to predict the results like my model requires danceability energy like i mentioned in the first half we want to pass that information to the model so that model can predict and return the results but suppose uh like danceability takes a float value and you send a random string flask will give the error and break the application but what fast api will do is that it it will lead to form validation it will check that the value is getting float or not that's inbuilt you don't need to create an extra effort for this but in flask you can do the same thing but it will require a lot of code and a lot of validation at your end so these kind of problems are solved by fast api second thing is that it's a modern modern framework modern framework it just came in the previous years and it it has gained a lot of popularity also it automatically create talks with gui that's the main and the most like feature of from my site you can create docs for the apis directly using the fast api so suppose if i want to know the usage of the endpoints like most famous api in these times is the kovin api you you all have access that api to create some reminders for the notification alerts that the vaccine is available or not so that documentation is automatically generated using fast api you cannot do this thing in flask maybe there there are some extensions but as per i know fast api can do this thing at one command you don't need to explicitly tell any command that create the docs for this you just access an endpoint slash docs and you will get the all the end points api uh endpoint of the api as the docs so this type of thing using the past api uses swagger and open ei specifications for making this uh gui interface i will show you the interface also it is faster than flask definitely it is faster than flash because fast api relies on uv con and flask relies on g unicorn so i won't get into the technicals of both but just try to understand that fast api can make asynchronous request and flash cannot do so i'm just giving you the overview because it's it's a bit technical to explain and you won't get this act first and like i said flask uses fast api uses flash swagger and open eye to do these things so you have your model now let's talk about how to create these codes in flask or fast api so when you have the model ready like you have done all the pre-processing steps all the validation cross validations and opencv search you have the final model so like in this example i have made a created an example to demonstrate how to prepare how to pickle the model how to save the model so that we can use it further in an api so to do that you just do a name of the model and this is the basic python command to create a file or to access a file just uh the changes here is pickle dot dump pickle dot dump serialize the model see uh serialized model and then save it to the disk so that we can access it further so at this point if anyone has any doubt you can tell in the chat section or unmute yourself if you want to ask [Music] okay so i'm assuming that everyone got the point so let's see this in action let's do the let's see the code how this how these things are implemented in a real coding so let me just switch to the vs code [Music] just confirm if you are able to view the vs code or not in the chat box okay cool so let's talk about how to deploy the machine learning models finally using flask so what let me zoom in a bit so that it's clearly visible okay so what we will do is import like the usual import statements import flask from flask and flask also has predefined request and json if i will talk about this later important to unpickle the model like like i said before i have already saved the model this is the model dot uh pickle file it is a decision tree file so first thing is first we need to initialize a flask cap now initializing a flask app is pretty simple just uh write any name here and equal to class name okay so here i am loading the model how the mode how the model is loaded is pretty simple like the how we read the files in python the only difference is dot load so that the model can be loaded here now like in web development in node.js or express we create routes so what are routes are basically the endpoints of the website suppose you hit the endpoint of coving govern api i don't remember the end point of that but when you hit the endpoint you get the results so endpoint is basically the how should i code this so just understand this this is we are creating routes for the flask so this only slash route is for the home page like first time we are visiting our website it's a first time first index page that's the route for the slash root so what we will do is create a function for the this route and pass it to this decorator now decorators are the functions that like a decorator is used to register a view of the function decorators are used to extend the functionality of the pre-built functions so the route function is already defined in the flask app in the flask library so we are just extending that functionality to our uh custom functions so we are using like we have created an index dot index page function and i have returned a json json format for this so that everybody can clearly see the messages so that's uh this message returns now if this uh i have made a api for this so next up we will be creating is the predict like as i said this is the route for the route and this is for the predict so what predict will do is if i am going to the local or slash product it will route me to this this logic so what i need to provide in flask in flask i need to provide the queries the argument for the request now what is query so if i go to the google like if you go to the google and go to the endpoint query or queue question mark and then equal to you type the text and you get the results so that's how the google queries work so we need to create similar type of query in our application so we have created the predict and then query to fetch the data so the fetching of data is done by done by getting the argument from the request and i have imported the request from the flask and like the like we can use directly dot get method of the query to get the uh get the parameters of our of our end point so let me run this application and then i will be able to get a better like [Music] so i'm just activating the [Music] environment for this so let's run python class [Music] to see this query more easily i have installed the thunder client you can use postman also but thunder client has released a lot of tests to run the postman and then do all the processing thunder client is way too better for this so i've already clicked the flash query so like i said this is the local host where the website where the server is currently hosted you can see one the local host and this is the predict and point where which we have created here this is the this is the predict endpoint we have created to get the data so now we have we are hitting this endpoint and getting all the parameters uh using this x uh question mark and equal to all the parameters we have to pass like the consciousness equal to this float value and i have converted like what parameters you will get from this we will it will be in string format you need to type that into float so float of query dot get acousticism so as like a consciousness i have got all the parameters for this so [Music] for this and like in the query parameters i can simply write these parameters and type their values so this is made more easy in thunderclan but if you go to it it will be same it will be same in postman but if you go to the google chrome and then access this link you will not get this type of form you need to manually type this whole this whole url and then you will get the result so just see that just see this in action if i [Music] okay so if i click on send it will send a query to this flask demo this endpoint and i have that got the prediction as rock because these parameters belong to the rock type of music so you can change them if if it gets anything no so that so that's how you can deploy a ml app in flask so that's pretty easy because you can just create the endpoint of this fetch the parameters from the query and how you will predict the model i skipped that part so as you have loaded the model and model variable using pickle you can use model dot predict function of the sql onto directly directly give all the parameters of the model to the predict function and then get the results like in my case it was all these parameters and in my case it needed a one-dimensional area so i had to give it an arrow of error and then tool is and then the first result i need to get so this just sonify function is used to just return this dictionary as in a json the json is pretty similar to the dictionaries in python it carries a key value appears in it it's pretty easy to interpret this as as i said in the api part you can call the api in any language javascript or anything so it will be easy for the front-end and the back-end part back-end uh developers to fetch this data and process it more easily using this jsonify function they will get the json as it is so as you can see we can send many requests to this and if i change i don't think that the hip-hop hip-hop has a different parameters parameter list so this was all about flask so let me know if anyone has any issues with this or anything to ask in this code or something related to flask or shall i move to the fast api okay so i'm moving to fast api so like i said in the flask you need to create endpoints and route for the api in the same way we will create a route for the fast api but here is but the only difference here is that we we can directly specify the method we want to get like the get method here and the post method here but in flask you need to pass this explicitly and here like the method is equal to post and get you need to specify this directly now the benefit of getting this as a post i will show you in a bit so it's it's almost the same you have created the fast api app you have loaded the model and just an endpoint of the index so that the front page doesn't return anything wrong and the predict function but the main difference here like i said is the form validation you can validate the data you are sending to the api so fast api it depends on identical is a data validation library for the python so what identity model does is if i uh open up the music class which i have created for this so the music class crea uh inherits from the base model of the identity by identic will automatically verify whether the field provided is of the same type as provided in the model also so you can skip all of this this is the additional uh additional data i have provided so that i can get the description i you can write the description and i here i will define the consciousness and all the parameters as float because all of them are float type parameters and that will be more convenient for us so the next thing we will do is just directly pass this class this created class in the data in a variable called data so now we can directly access this data as data dot dictionary like we access the data in python in the dictionary's key value page we can directly access received as a constitution received of danceability you can directly access this that has key value pairs that more that is more easy than this flask app then this flashcap then getting the query dot get method and then getting the parameter we can directly get this as dictionaries in fast gpa and the rest is also same passing the all the parameters to the model dot predict and then to list and then directly returning this prediction you can also return this as jsonifer you can also import that data import that function from fast api dot prediction in running this as uv con so if if we make a request to this if you request make request to fast api so now the fast api let me run the fast gpa first [Music] so the fast api is running on localhost 4000 port 4000 port and directly predict so now here we are not using the query parameter we are passing the data to the body itself so that we can use the post method we are passing is that is json and all the parameters i think this is rock because the temp is 101 so let us send this data just notice here you will get a request from this like this you got you got the request from this and the prediction was hip-hop so you can this thing as uh good in this fast uk so what was the benefit of making this fast jpi like you were getting the same response in flask at the prediction but what was the use of making this fast api so let me show you this in browser like the docs generation [Music] [Music] so as you can see here i have got to the endpoint of 4000 now if i type slash docs you will get an automatic generated docs for fast api it's automatically done you have seen the code there was nothing here so nothing uh mentioned in the code to create the docs it's automatically created by fast api so if you go to this post method you can directly test out this applic uh this api here also so if i click try it out and then execute it will direct it will give the results here also so that's the main benefit of using fast api over flask flask you can also create this open api specification using flash but it will take a lot of effort so instead you can directly hop to the fast api so i created so as you have also created a [Music] get method for index you will also get result for this this is the home page of the api so in this way you can create multiple endpoints for an api and then fetch the data for this [Music] so let's see what else we have [Music] so let me show you the applications of this uh like the railroad application which i have created so all the code for this like the fast api demo and the flask and this music file is already present in the dsc made organization repo you can directly access it here is the wrapper for this [Music] all the code is present and the model file updo you can test out if you want to try out so let me show you an application of this so i have created this wait so what i was talking about how to how to make use of this application apis in your applications so what you can do is create an uh android app the most basic way so let me show you an android app which fetches this data only like the one when i show you in the in this code so the android app is hosted on apptize.io you can also visit this link to access this app so if you open up the app it's an tv in my lap you can search more about it how to make python apps using tv or you can suggest me if you want and tutorial for this so this is the android app which have created for the api so if you type all the parameters of the music like you can type here also let me put all of the zeros [Music] so basically this app will make a request to our hosted api and then predict the music gen using the data provided in the app like when i click predict you will get the prediction as rocks so this api call was made to the hero code so let me show you how what is the any type of api using hairpoo [Music] so the heroku requires you to make a proc file so the proc file is basically an interface between the app and your account so the profile for flask usually contains the uh this keyword web and the g unicorn g unicorn is the server which the eriku supports if you deploy this flask app as it is on the hero it will not run because it's not running parallely on the so it cannot make multiple requests at the same time for all the users so for that we will using gcon junicon and then the name of the file and the name of the app which is created in the uh fast shape flask like here we created the app so when you put this proc file and the requirement.txt for the program in a github repo you can directly connect that github repo to your heroku account and then simply click on deploy like here i have already deployed the application [Music] you can click directly on deploy so let us check the logs of this so that we can get the result for this like this you you see this we have made a request predict for a cost is 0 0 0 0 all the values were 0 except tempo which is 38 and then we got a result here so if i again click raw predict you will get another request another predict function for this and you will get the result here so this kind of thing can do with the api another example i want to show you is the website you can also integrate any type of websites using the api you can do multiple things using you can make a maybe a desktop application for this you can get android apps websites you have ample of options for the api so here also i can type all the parameters for this i'm randomly selecting the parameters [Music] and as soon as you click predict you will get a request here that there was a predict request for this sorry this one predict and all the parameters mentioned which are getting into a model and then getting a prediction so as you can see this was a hip-hop so you can just play with your api and then fetch the results to get better and better results you can do a lot of multiple stuffs using apis and all so that was all from my side if you have any questions anything any suggestion or anything you want to ask please feel free to and mute your mic or maybe share in the chat box [Music] could you please just explain it once again like which part using api i did not get it uh like the code part or the usage part code code but okay sure so let's start with the flask api flask part again so let me just create another another endpoint for this tutorial so closing this and [Music] i'm activating the environment for this so that my dependencies are not mismatched so the server is up and running so suppose you want to create an api to return something so what you will do is uh instantiate a flask app the flask app can be instead initialized using the flask directly flash function and passing the name this name is required to run the application directory this name so when you use the decorator for this like i am creating the route so suppose i am creating the route vsc and in this route this route is basically a decorator decorators are used to extend the functionalities of the current functions so if i'm defining a function jsc [Music] psc and returning a simple thing like return as this is a dictionary i am returning but it will be rendered as a json by the browsers because the format is all almost same so let's visit this link for the flask when you will hit the endpoint dsc you will get this result hello like the one which we created here so the endpoint are basically the the points where we can make the connection with the server you can understand this also so what we are doing here is that creating the endpoint predict or you can get any type of input like i created here dc and then getting the data from this endpoint if i give a data here like name is equal to yes but it will not give any type of result because i have not fetched this data from the endpoint so for getting this data from the endpoint you can directly use query equal to request dot args and then access that data using the query dot get method so query is the name of the object time created you have you can create bsc also any name you see now i will use the dsc dot at esc.get and uh parameter i have created his name name hello name and so what is there so let's directly write [Music] [Music] maybe so instead of returning let's return this [Music] as you can see we have what the parameter contained was returned directly by the website so that's that was i was talking about how to create an api under endpoint [Music] so okay [Music] oh double quotes okay so any more doubts anything you want to ask [Music] [Music] okay then [Music] the android app part again so what exactly you want to see like the code part or the prediction part okay so you want to see the code for the android app [Music] okay just wait a second [Music] so the android app is hosted in this vml depository of my profile you can visit that so the android app is created using keyway key vmd the file this is the code for the android app but it's not similar to what you saw here or what you make in tinker or gui tinker or pi game that's not the same because to create an android app using python you need to use kvl tv and kvmd so if you have the knowledge of kvmd then only i think you can understand this because it's a different lecture and different part so the android app is like the android app will take more time for this session because it's android app explanation requires another slides so what i can show is how the prediction is happening so if you see this predict function [Music] i am getting the values for the all the parameters from the app like this is the way you get values from the kiwi apps and then i'm just making the request a url request like this is the website for the hosted web app and then using the predict endpoint for this and passing all the parameters for this and then making the request for this so the kiwi md part is actually not covered in this tutorial [Music] okay any more questions or doubts regarding anything you want to ask in this session or anything [Music] so we'll wait for two minutes more"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Flask and FastAPI for Model Serving",
        "Video Title":"Flask and FastAPI - ML Model Deployment",
        "URL":"https:\/\/www.youtube.com\/watch?v=o92S8p2qb9g",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/o92S8p2qb9g\/hqdefault.jpg",
        "ID":"o92S8p2qb9g",
        "Publish Time":"2023-05-20T18:26:30Z",
        "Channel":"Siddharth vij",
        "Channel ID":"UCH3wT6uqux0ybV89fo-G-QQ",
        "Transcript":"hi everyone hope everybody is doing well so today we are going to talk about machine learning model deployments based on flask and fast API so these two are two different approaches which have been used for ML deployments Class has been there for very long time and fast API is a new thing in the town now let's get started with a high level overview of what these two things are to show a demo of these two things and to talk about these two things class can fast API I have used the same use case that I used in my last video on machine learning model deployments using Azure ml the use case which I used is also available on Google not Google taggle and the use case was about taxi demand prediction so that means if we provide the location and time to the model it would tell us what is the demand percentage for the cabs in that area at that specific point of time while training that model we created multiple artifacts which are listed here the first is encoded then there is a k means then there is a actual model which is used for prediction all these three artifacts were created in that tackle use case during the training time and now I am going to deploy this model using flask and fast API so let's talk about all these files one by one the first thing that we are going to speak about is flask which is the simplest thing out of these two it has been there for a long time so I'm not going to spend much time on it because there are multiple videos already on YouTube to talk about it so flask is based on wsti server so what we do is it is a python based framework flask now if a python based framework has to connect to a web server it should Connect using something called as wsgi Gateway now when we start flask API it basically starts in development mode so let me just restart last API to show you the same again in the command you can see flask app that is the name of the file that I have given here so that is the reason you see flask hyphen a flask so flask cap is the name of this file and you can see this warning it is the development server do not use it in the production use production wsdi server instead right but we are using it only for development and testing purposes so it should be fine so we have two routes one is the development or one is the base route on the base route we are just printing out a message which says it is a default class Gap the second is the inference root which is the predict root in this we are getting the data which is latitude longitude and the date time and we are sending out the demand percentage in the return statement now this return statement is also using the score file let me open The Score file now this core file is very much similar to the same scoring file which I used in my last video on machine learning model deployment using Azure ml there are few changes here and there for a zero ml deployment the scoring file would have a init function and all these three items would be a part of that function but here we don't need to do that we can live with these Global variables which are loading the model artifacts and then the function which is just doing the inference or the prediction based on the input Json which we are passing the file is very much similar to the file that I used in the last video and the file along with all the with all the code is also available on GitHub the link I will put in the description as well for reference let's come in let's come back to flask so that was all about the code and this is the command that I used to start this class cap or to start this here in the terminal now it gives me a URL where I can test so it is running on localhost at 5000 port so what I have done is I will just open it up so as you can see here I have these three inputs one is longitude latitude and pickup date time which is nothing but location and date time so when I send this on the predict wrote on 5000 I get the demand percentage as something so this is coming out from this class server the base note can also be tested the similar way you can see that this is a flash Gap default page so this is the base rule which you can see here this is the flask map default page so this was about flask now let's move on to fast API so fast API is the new thing in the term this is based on asgi interface now what is the difference between flask and fast API fast API is not using wsdi it is using Asta which is an asynchronous interface which is comparatively better than the server which was used before which is flask right in terms of supporting asynchronous request inherently within the framework itself now let's look at the file it is more similar to the file that we used for flask there are a few changes here and there one is ubicot so I'm importing this Library this is the library that supports Asda interface which is getting used here apart from that we do see another file which is TDP inputs now what is this function TDP inputs coming from so this is coming from a file called as TDP input your fast API uses a library we can use a library called as by identic which will validate the input parameters in the Json which are passed on for prediction so I am passing these three parameters I am telling it that they should all be string and Pi identic would inherently take care of this and would throw any errors if there are any issues with the input Json object passed on to predict now this TDP input I am importing here and the function TDP inputs which is nothing but this I'm sorry the class I am importing here and if you see here when you run this fast API app the data would come in from the postman as Json object into this this validation of these three items would happen here longitude latitude and date time and then we would come to this piece this print statements are just for debugging you can remove them and just paste it them for debugging there's no need of them so the data comes here from this class into Data variable and here I'm converting that into dictionary and passing it into score file which is returning in a demand percentage this is running on the port 8000 and the command to run this fast EPA server is given below first DBA app is the name of the file app is the name which is given on the top so if you see here app is equal to fast API so that is the same thing which is given in this is a function reload there's a parameter reload so that if there are any changes in this file we don't need to restart it manually the restart of the web the restart of the server would happen automatically now let's test the faster period so all I need to do is change the port number to 8000 with the same inputs latitude longitude and the date time and we should get 31 percentage hopefully yes we are able to get the demand percentage now this is another thing which is interesting with respect to past API this is something that you might have seen in case of flask as well in case of flask we have a UI called a Swagger through Swagger you can pull in all the details on the UI here we have something which is called as docs so if you open the port if you just type in Docs you would come to this kind of an interface which is similar to the Swagger now here you can test the same thing which you tested using the Postman so let me do same thing here so I'm doing the post I am trying to predict now the data would be same as this one right we are passing the longitude latitude and date time and we are expecting that we would get the demand percentage so this is a quick way to test rather than opening Postman and we are getting the output as demand percentage 88 percent now let us go back to the code yeah so I think that's all pretty much that that's pretty much all about these two different ways to deploy flask and fast CP thank you so much for joining have a great day ahead bye foreign"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Flask and FastAPI for Model Serving",
        "Video Title":"FastAPI Tutorial | FastAPI vs Flask",
        "URL":"https:\/\/www.youtube.com\/watch?v=Wr1JjhTt1Xg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Wr1JjhTt1Xg\/hqdefault.jpg",
        "ID":"Wr1JjhTt1Xg",
        "Publish Time":"2021-08-18T12:30:11Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"Have you ever wondered when you place an order on Amazon.com what happens behind the scene? What you see on the website, Amazon website is basically called the UI code when you click on place order button it sends an http request to a back-end and in the back end there is a back-end server running, a web server running which serves those requests. Now that web server can be returned in a javascript framework called node.js or if you're using python you can use a framework called flask. Nowadays a new web from framework has come up which is called fast API and this video is a beginner video. For fast API in this video we will install fast API and we'll play with it a little bit to see what benefit it has to offer over flask. Let's install fast API on your computer. First I'm assuming you have Python installed which comes with pip so you will run pip install fast api and it is as simple as that. You also need to install something called uvicon so after fast API is installed you will run pip install uvicon. This is the server that you'll be using to run your fast API server so pip install first api and pip install uvicon. These are the two commands that you run to complete your installation. Now I will write my first fla first api server. I have opened pyjam you can use any editor sublime notepad plus plus vs code, any editor that you're comfortable with. and here I'm going to go into zen mode. I will be meditating, all right? I will install the fast API first by doing this and then I will create a fast API instance so you're just creating an instance of this class you can call it app or anything that you feel comfortable with then you will write your first endpoint. So I will explain what that endpoint means. Let's say I'm writing a simple function called hello and that hello endpoint will return me some fixed string. So I'm writing a python function it's an async function okay and I will say return for example welcome so my code is done so you see like only five lines of code and I have written my first endpoint. Now to run the server you will use UV con command you will say uecon and then the name of the file the name of this file, by the way, let me show you is main dot pi. Okay. So that's the file so I will use ubicon main. That's a file name colon app so you are not using main.pi you're just saying main and then colon app whatever is a variable name here and you will do hyphen hi for reload. I will explain what that reload option is but when you run it it says application startup complete and your server is available at this port. This is the host and port, okay? So we'll go to now browser and try to run this link. I'm using chrome here and when I run this link I will say hello see I get this thing back if so hello is essentially my entry point, okay? When you are working on less Amazon placing an order you know you might be having URLs like post order so post order is a typical name of the end point which will post an order you know or place an order so these are all called endpoint it could be post order it could be get, order it can be anything it's up to you it's not like fixed list of strings you can define it to be anything so here I'm using hello and hello is returning me welcome I can change my string and say welcome to fast API tutorial and I have to just save. I don't need to stop my server and rerun it again, see? I did not stop and rerun it and it automatically reloaded my new code and that that was the purpose of this option so now when I go back to my browser and refresh it you see you can see welcome to fast API tutorial. So my core changes I'm doing are dynamically getting reflected into my execution now let's make it more interesting. Let's have it such that it takes a parameter where I can say okay the world and it will say welcome to first API tutorial how do you do that well here you can supply that parameter so here I'm saying name okay and I'm using this bracket this is more like a Python format string kind of syntax where you are supplying name. Here and you can use the same word as a variable name here and now that name that someone is supplying you in a URL is available here in this variable and you can just say this I will use Python's format string and again, you don't need to reload the server it's automatically loading those changes on its own. Now when I refresh it it's all right so let me refresh it so it says see the world you can give any name like say tom and that this string will be passed into name variable and it will come back in the message. Here if we are using get method in http rest protocol there are a couple of other methods as well. So let's go over them one by one get is usually used to create data for example you are looking for iPhone case on amazon website when you make that query the website is making a get request to get you all the iPhone covers there could be another request called post which is used to create data. So when you place an order on Amazon website let's say you are issuing a post query so this get post etc these are the rest API protocol endpoints the third one is put which will be used to update data so an existing order you want to update it and the fourth one delete obviously if you want to delete any data like deleting an order you will use this endpoint. Now there are other endpoints as well but these four are the popular ones. Now while working on this tutorial let's say you got hungry and you went to grubhub to order some food and let's say you're looking for Indian food and this is showing you all the Indian restaurant and it might show you all the Indian kind of recipes. So let's say you are a backend engineer working in Grubhub and you want to write an endpoint where you can say something like get item so let me just show you here you can maybe say let's say get items and Indian and it will return you the indian food items which are available. Okay so I'm going to just change this entry point and I will say get items and this is the name of my cuisine and that comes here as a variable. This function name can be anything you it doesn't have to be get items. And let's say you know I'm retrieving my item records from a database but I don't have a database here so I will just use a simple dictionary where I'm saying okay indian cuisine means these many recipes okay I love samosa by the way American means these these recipes and so on and here you can just use food items dot get cuisine. So if someone is applying Indian cuisine it will return this one so let's let's try this out really quickly. We are very hungry! Okay this is hello hello doesn't work I will say get items indian get items Indian is not working actually it was reloading I did not wait enough. So when I refresh this I'm getting see Indian item samosa and dosa. All right. Let's try some Italian food ah Ravioli pizza. Okay how about Mexican well. We don't have Mexican so then this get will return null but this is not ideal. Actually you want to give user a message that in my website the only supported cuisines are Indian, American, and Italian. Now let's talk about the benefit of fast API over flask. So the first benefit is that in flask if you have to do this kind of data validation then you will do something like if cuisine in rf cuisine not in let's say Indian so you will have to write a code like this where I have just said food items dot keys which will be Indian, American and italian and you will say if cuisine not in this then return this message that only valid supported cuisines are Indian you know American and Italian now see this is a very simple function you might have a big function with a lot of validation and when you are using framework like flask you have to do all this validation yourself which is not ideal. What if the framework itself gives you some validation. So that's what fast fast API can do it for you it can give this validation for free and I will explain how. So I'm just going to remove this code here and I have imported nm in python and NM is used if you have fixed category of things and I'm going to create a class which will have all three cuisines are specified as inum. So see these are my available cuisines and now here you can just use python's type hint and say available cuisines so you're saying that cuisine has to be from these are these available cuisine now this is a type in python is not a statically typed language. So if you run the code just like that it's not going to do anything but fast API understands now that the user or the backend engineer is expecting cuisine to be one of these three and if you don't supply the cuisine which is you know part of this list let's see what happens. So here once again if I supply Indian things are going to just work as fine but when I say Mexican it will give me an error that it is not valid. The valid values are these three so see you don't have to write that validation code here and this is benefit number one that fast API offers inbuilt data validation it is a huge benefit it can make your code really compact because you don't have to do all this validation yourself. It can also reduce bugs so you see you see a very nice error here. I quickly wrote a second function. Let's say on your Grubhub website you have some coupon codes you know and one two three are coupon code IDs and these are the relevant amount in percentages of for those coupon codes and you can have another endpoint called get coupon and code and now code you expect it to be only integer so using type in again you are saying it has to be integer. Okay and let's see if it is not integer so I go back here. I will say get coupon and when I say 1 you know I get 10 percent when I say 2 I get 20 percent but if I do abc it will again tell me that value is not valid integer value is not a valid integer. It has to be integer so again the data validation in fast API is super awesome. The second big benefit of fast API is the enable documentation so you can do slash docs and it will generate the documentation for you. See I didn't have to write anything this could be very useful for front-end engineer who is using your back-end. Bcause that way they will know what you are expecting in your API. For example, in get cuisine you are expecting only Indian, American, Chinese. So if you click on this try it out button see it is showing you that these are the three varied cuisines and you can also send a test request so if I say Indian and if I execute you know see I'm getting very delicious samosa and dosa bag which I can eat and increase my programming productivity you know American execute hot dog, apple pie. So this is giving you a nice test pad to test out your APIs you can also use a different kind of documentation so fast API. Again, if you do slash docs you get this documentation if you do re doc. You know redoc that's another way of generating documentation. So see here it's some generates like response and response samples as well. So you can just explore it. It's pretty useful this is the official website of fast api and if you click on tutorials they have amazing quality tutorials. I have not seen tutorials of such a great quality. Very simple easy to understand on any other technology frameworks. Okay so I highly recommend you go through all these tutorials because I just covered the basics of fast api. I compared it with flask but there are so many things you know so many options available. For example- form data. Form data is a usual thing when you're doing UI coding in your javascript and you can directly import that form class here in the past api back-end and you can do various things with it. You know you get some ready-made functionality so explore these options and you're going to absolutely love the documentation of this portal. While coding we covered the two big benefits of fast API which was inbuilt data validation and the second was inbuilt documentation support. There are few other benefits as well for example the benefit number three is that the fast API as the name suggests is actually very fast the way it is return it gives you the best performance so your server will run almost at the speed of you know node.js server. So the performance runtime performance is beautiful so that's benefit number three. Benefit number four is the code that you're writing is very compact and as a developer, it will take you very less time to write fast API code so it's a compact code. The code development is very fast and there are very few bugs so just to quickly summarize four benefits- inbuilt data validation and build documentation support. The runtime performance is pretty good. It is very fast and the development time is also very less and there are less bugs. I hope you like this video. If you did, please give it a thumbs up and if you have any question post in the comment box below."
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Flask and FastAPI for Model Serving",
        "Video Title":"Deploy ML models with FastAPI, Docker, and Heroku | Tutorial",
        "URL":"https:\/\/www.youtube.com\/watch?v=h5wLuVDr0oc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/h5wLuVDr0oc\/hqdefault.jpg",
        "ID":"h5wLuVDr0oc",
        "Publish Time":"2022-07-30T12:00:22Z",
        "Channel":"AssemblyAI",
        "Channel ID":"UCtatfZMf-8EkIwASXM4ts0A",
        "Transcript":"hi everyone i'm patrick and in this tutorial we learn how to deploy machine learning models with fast api and docker and then have a production ready app so you can use this template to deploy the container everywhere you want in this video we go ahead and deploy to heroku because there's a free tier and you can follow along also this approach should work in the same way for any machine learning or deep learning framework you want to use so if it's psychic learn or tensorflow or pytorch the approach is the same and it's also not too difficult so let's get started let's quickly test the final app so in this video we built a language detection model and as you can see here we have the live url at heroku app.com predict and we send a post request with this text and if we click on send we get this response with languages english if i for example switch this with the german version and click on send then the language should be german so this is correct and fun fact on the side at assembly ai we also have a automatic language detection feature in our api so this is actually a real world project so let's see if we also can develop an accurate model and deploy this so first let's build and train the model in a notebook so here i'm in a google collab and the focus of the video is not how we develop this specific model but rather how we go from notebook to production app in a moment so let's go over this very quickly the data set is available on keggle also this notebook and all the rest of the code is available on github i put the link in the description so here we have our imports then we load the data set and analyze this so we have lots of different texts and the corresponding language this is our x and y then the first step we apply is a label encoder so this transforms the labels y so basically what it does is that for each of these texts these classes it assigns a number starting from zero and whenever we have a preprocessing step that transforms our data we have to remember this and later also use this in our code so for example this is another pre-processing step here we apply regular expressions to remove these special characters so we also have to use this in the code later then we have the typical train test split and now we build our model so it consists of two parts in this case the first one is a count vectorizer here we fit this and then transform this and then we have the second step which is a naive bayes model and then we can use model predict with the test data and then here we have some metrics and for example print the accuracy so we can see this is 98 so pretty good now one best practice we can apply here if it's possible is to combine all steps into only one step this is much less error prone and then we only have to save one model instead of two here so with sklearn we can do this with the pipeline with tensorflow for example we often have a sequential model where we can put in all the layers in this sequential model so with the pipeline we can now combine the vectorizer and the naive bayes and then we can fit the pipeline and then we can use this pipeline to predict the test set so now we only have to apply this one step and if we compare the accuracy then we see this is the exact same result so now when we are done with training the model we have to save and download this and with sk learn we can do this with pickle dump with tensorflow and pie charge there's also a very simple api to save your model one thing i recommend to do here is to also save a model version so you can keep track of the current version you have so we use this syntax major minor and patch version so this is our first miner model version and one other thing i want to mention here is that now in this case this will be one pickle file so we can actually go ahead now and click on download here but if you use tensorflow or pytorch then often it will save this in one whole directory and we cannot download a whole directory so there's no download button so one trick you can apply is to sip a folder with this command so exclamation mark sip and then you give it the name of the zip file and the name of the folder and then you can download the zip file so now we have this and um finally let's test this one more time so here we have the text and if we run this then we see this is italian and we can also see that y is only this number eight here so here we actually apply the label encoder classes to get the actual um language so italian so i printed this in the top so here are the label encoder classes so we also have to get this for our code and yeah now we're done so now we can use this model and build our app now let's create the fast api app and for this in the root directory let's create an inner directory app and in here we will put all the code so we have one main.pi file this is where we will put the fast api endpoints and i go over this in a moment and then we have a inner model directory and here i start the downloaded pickle file the trained model and then another file that i called model.pine basically this is a helper file that does the model prediction so here i hard coded the model version then i also use path and the path of the current file to get the base directory and this is because the folder structure inside the docker container can be a little bit different and i want to make sure that we can find this pickle file here so then we can open this and make sure that this version corresponds to the file name and then we can say pickle load and loaded our model then here we have the classes so this is from the label encoder the classes in the same order and then we only need one helper function in this case predict pipeline that gets the text so this is a string and then here we do all the same pre-processing steps that we did in the notebook and then we can say model predict we because we now have only one step with the pipeline object so then we get the prediction and this is a number so then we have to access the index of the classes and then we can return the language so this is the model.pi and now let's go to the main.pi so here we import fast api and base model from pedentic i show you what this is doing in a moment then we also import predict pipeline and the version as model version and be careful to start your path with app so app.model.model for this file and then it also finds it in the docker container then we create the app and in fast api it's super simple to create your endpoint so it's very similar to flask we define a function and then decorate this with app.get or app.post so i often like to have a endpoint for a health check so here we simply return health check okay and here also the model version for additional information for example you can also return the api version here and then we have one predict endpoint this is a post endpoint and in here we simply call predict pipeline and put in the text and then we return the language and this is a dictionary and now to make sure that we pass the correct data types to this api when we send the data so the input we want to have should be a string and fast api works with tie pins so for this we can define a class text in that inherits from base model and this should only have one field this is a text this should be a string and now when we send the data and this is not a string then fast api can detect this for us automatically and then raise an error or show an error in the api and this is super cool a super cool feature of fast api so this is why we use this base model and in a similar way we do this for the output so for the response we say prediction out which inherits from base model and here this should be one field language and this should also be a string and then in the code here we can access payload text and we have to make sure that we have one field language and this is basically all we need for this simple api and now we can dockerize it now to dockerize this of course you have to have docker installed on your machine and then in the root directory we need to create a file that is called docker file and then it's also a good practice to have a dot docker ignore this is similar to a dot git ignore here i simply copy paste this from github you find this in the description and this ignores certain files inside the container and now for the docker file we can go to the official fast api docs there's one section fast api in containers so i recommend reading through this because there are different ways of doing it one way of doing it is to use the official docker image with g unicorn uv corn you can also find this here on github so it says docker image with uv corn managed by g unicorn for high performance fast api web apps and in order to use this we can copy this code so this goes in the docker file and this is basically all we need to do and this uses this base image then it copies the requirements txt inside the container then it runs pip install requirements and then it copies the app directory also in the container and that's why also we have to have this folder named app and then we need one file that is called requirements.txt and here we put in all the dependencies so in our case since we already used the base image it already includes fast api uv corn and g unicorn so the only missing dependency in our project is scikit-learn for example if you use tensorflow then you can put in tensorflow here or pytorch and for example it's also important to mention or worth mentioning that you can use tensorflow cpu oftentimes because you don't need the full version and then your container will be much smaller so then it's also good practice to pin the version so for this we can go back to the notebook and then in here we can for example say import sk learn and then sk learn dot underscore version this should give you the current version and then we can copy this so it's 1 0 2 so it follows the same pattern major minor patch so let's go back and say equal equal and then this and now this is all that we need so now we can build the image now to build the docker image we can open a terminal and then i actually noticed one small change that we have to do so as you can see in the docker file we have the app requirements txt so basically an app directory is the new root directory and we have an inner app directory so we have to say copy from app to app slash app and now save this and now we can say docker build and then minus t and give it a name here i call this language detection app and then a dot for the current folder and then hit enter and now this will build the docker image and now this was already done so if you do this for the first time then this might take a few seconds or minutes so now we have this and now we can run the container by saying docker run and then we map the port from the docker container to our machine by using 8080 so this is port 80 and then the name language detection app so now this is running and starting and now you should see that the uv converters are starting and it's listening to zero zero zero port 80 so basically this is our localhost port 80 so now we can find this and then go to this route and for the base route this is a get request so this is working health check okay and model version is zero one zero so now we can for example use postmen to send a post request or what's really cool with fast api is that we get automatic documentation by using slash docs and here we can see all the endpoints so we have the home and we have slash predict and then we also can try this out and as you can see it it knows that we need this schema with a text field and then the string and this is because if we go back here we defined this as a base model with text so the text and it needs to be a string so that's why it knows that we need these fields and then we can click on try it out and let's say hello how are you and then execute and then we get the response directly here and can see language english and you also get the curl command if you want to try it from curl so let's try it with a german sentence so let's say hello we get as diem question mark and then execute this and then language is german so this is working so now as last step let's deploy this to heroku and now we can deploy the container everywhere we want in our case we do it on heroku so let's actually stop our local container and clear this and now um the first thing we want is a git repository so we can say git init and of course you have to have git installed on your machine this will initialize an empty git repository then i also want to add a dot get ignore so here we also ignore some files and folders we don't need so i only ignore the virtual environment and this file and now you could continue in this terminal i actually want to switch to my normal one so here we now say get at dot so everything so we can check get status um that all these files have been added and now we can say git commit and give it a message so let's say initial commit and now we can um start creating our heroku project so for this of course we need to have a heroku account and the heroku command line interface installed and now we can do everything from the command line so first we say heroku login and now this should open the browser and then here you should be able to put in your credentials so i think i already did this and this should be stored so yeah so now we can go back and see we are now logged in and now before we can continue we actually need one more file also in the root directory and this is called heroku.yaml and for this i can recommend a documentation site on the official heroku dev center building docker images with heroku yamo so basically we only need this part so let's copy and paste this in here then of course we have to add this to git again so we say git at heroku yamo and then we say git commit and add heroku yamal as message and now we can create a new heroku project so we say heroku create and then we have to give it a name so let's use language detection app and then it needs to have a unique name that is available so let's try one two and see if this works all right so this worked so now this will be the url that we can use to access our api so now we can say heroku and then git remote and then we set the remote for this project so this was called um language detection app one two and now this will create a remote and now we can say um heroku stack set container because this is using a docker container and now we only have to push this with git so now we can say git push heroku main but as we can notice we are still on the master branch so we have to say or have to change the name of the branch to main by saying git branch dash m main now we are on the main and now we can say git push heroku main and now it will push everything to heroku and start the app so this might take a while all right and this worked so deployment is done so now we can grab the url we've seen above so this one is where our api is now live so this time i want to test this with postman so we can check the home url with a get request and send this and then we see health check okay and model version and now we send a post request with this data to slash predict and now let's see language is german so now let's check a different um text so now let's say ciao and now let's send this and we get italian so this works so this is our app deployed at this end point and this is all that i wanted to show you i hope you really enjoyed this tutorial if you have any questions let me know in the comments below and then i hope to see you in the next video bye"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Containerization with Docker for ML Models",
        "Video Title":"Learn Docker in 7 Easy Steps - Full Beginner&#39;s Tutorial",
        "URL":"https:\/\/www.youtube.com\/watch?v=gAkwW2tuIqE",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/gAkwW2tuIqE\/hqdefault.jpg",
        "ID":"gAkwW2tuIqE",
        "Publish Time":"2020-08-24T15:54:20Z",
        "Channel":"Fireship",
        "Channel ID":"UCsBjURrPoezykLs9EqgamOA",
        "Transcript":"one of the leading causes of imposter syndrome among developers is not knowing docker it makes it hard to go to parties where everybody's talking about kubernetes swarms shuffle sharding while you hide in the corner googling what is a container we've all been there at one point or another in today's video you'll learn everything you need to know about docker to survive as a developer in 2020 we'll take a hands-on approach by containerizing a node.js application i'll assume you've never touched a docker container before so we'll go through installation and tooling as well as the most important instructions in a dockerfile in addition we'll look at very important advanced concepts like port forwarding volumes and how to manage multiple containers with docker compose we'll do everything step by step so feel free to skip ahead with the chapters in the video description what is docker from a practical standpoint it's just a way to package software so it can run on any hardware now in order to understand how that process works there are three things that you absolutely must know docker files images and containers a docker file is a blueprint for building a docker image a docker image is a template for running docker containers a container is just a running process in our case we have a node application we need to have a server that's running the same version of node and that has also installed these dependencies it works on my machine but if someone else with a different machine tries to run it with a different version of node it might break the whole point of docker is to solve problems like this by reproducing environments the developer who creates the software can define the environment with a docker file then any developer at that point can use the docker file to rebuild the environment which is saved as an immutable snapshot known as an image images can be uploaded to the cloud in both public and private registries then any developer or server that wants to run that software can pull the image down to create a container which is just a running process of that image in other words one image file can be used to spawn the same process multiple times in multiple places and it's at that point where tools like kubernetes and swarm come into play to scale containers to an infinite workload the best way to really learn docker is to use it and to use it we need to install it if you're on mac or windows i would highly recommend installing the docker desktop application it installs everything you need for the command line and also gives you a gui where you can inspect your running containers once installed you should have access to docker from the command line and here's the first command you should memorize docker which gives you a list of all the running containers on your system you'll notice how every container has a unique id and is also linked to an image and keep in mind you can find the same information from the gui as well now the other thing you'll want to install is the docker extension for vs code or for your ide this will give you language support when you write your docker files and can also link up to remote registries and a bunch of other stuff now that we have docker installed we can move on to what is probably the most important section of this video and that's the docker file which contains code to build your docker image and ultimately run your app as a container now to follow along at this point you can grab my source code from github or fireship io or better yet use your own application as a starting point in this case i just have a single index.js file that exposes an api endpoint that sends back a response docker is easy then we expose our app using the port environment variable and that'll come into play later the question we're faced with now is how do we dockerize this app we'll start by creating a docker file in the root of the project the first instruction in our docker file is from and if you hover over it it will give you some documentation about what it does you could start from scratch with nothing but the docker runtime however most docker files will start with a specific base image for example when i type ubuntu you'll notice it's underlined and when i control click it it will take me to all the base images for this flavor of linux and then you'll notice it supports a variety of different tags which are just different variations on this base image ubuntu doesn't have nodejs installed by default we could still use this image and install node.js manually however there is a better option and that's to use the officially supported node.js image we'll go ahead and use the node version 12 base image which will give us everything we need to start working with node in this environment the next thing we'll want to do is add our app source code to the image the working directory instruction is kind of like when you cd into a directory now any subsequent instructions in our docker file will start from this app directory now at this point there is something very important that you need to understand and that's that every instruction in this docker file is considered its own step or layer in order to keep things efficient docker will attempt to cache layers if nothing is actually changed now normally when you're working on a node project you get your source code and then you install your dependencies but in docker we actually want to install our dependencies first so they can be cached in other words we don't want to have to reinstall all of our node modules every time we change our app source code we use the copy instruction which takes two arguments the first argument is our local package json location and then the second argument is the place we want to copy it in the container which is the current working directory and now that we have a package json we can run the npm install command this is just like opening a terminal session and running a command and when it's finished the results will be committed to the docker image as a layer now that we have our modules in the image we can then copy over our source code which we'll do by copying over all of our local files to the current working directory but this actually creates a problem for us because you'll notice that we have a node modules folder here in our local file system that would also be copied over to the image and override the node modules that we install there what we need is some kind of way for a docker to ignore our local node modules we can do that by creating a docker ignore file and adding node modules to it it works just like a git ignore file which you've probably seen before okay so at this point we have our source code in the docker image but in order to run our code we're using an environment variable we can set that environment variable in the container using the env instruction now when we actually have a running container we also want it to be listening on port 8080 so we can access the nodejs express app publicly and we'll look at port some more detail in just a minute when we run the container and that brings us to our final instruction command there can only be one of these per docker file and it tells the container how to run the actual application which it does by starting a process to serve the express app you'll also notice that unlike run we've made this command an array of strings this is known as exec form and it's the preferred way to do things unlike a regular command it doesn't start up a shell session and that's basically all there is to it we now have a full set of instructions for building a docker image and that brings us to the next question how do we build a docker image you build a docker image by running the docker build command there's a lot of different options you can pass with the command but the one you want to know for right now is tag or t this will give your image a name tag that's easy to remember so you can access it later when defining the tag name i'd first recommend setting up a username on docker hub and then do that username followed by whatever you want to call this image so in my case it would be fireship slash demo app and you could also add a version number separated by a colon from there you simply add the path to your docker file which in our case is just a period for the current working directory when we run it you'll notice it starts with step one which is to pull the node 12 image remotely then it goes through each step in our docker file and finally it says successfully built the image id and now that we have this image we can use it as a base image to create other images or we can use it to run containers in real life to use this image you'll most likely push it to a container registry somewhere that might be docker hub or your favorite cloud provider and the command you would use to do that is docker push then a developer or server somewhere else in the world could use docker pull to pull that image back down but we just want to run it here locally in our system so let's do that with the docker run command we can supply it with the image id or the tag name and all that does is create a running process called a container and we can see in the terminal it should say app listening on localhost 8080. but if we open the browser and go to that address we don't see anything so why can't i access my container locally remember we exposed port 8080 in our docker file but by default it's not accessible to the outside world let's refactor our command to use the p flag to implement port forwarding from the docker container to our local machine on the left side we'll map a port on our local machine 5000 in this case to a port on the docker container 8080 on the right side and now if we open the browser and go to localhost 5000 we'll see the app running there now one thing to keep in mind at this point is that the docker container will still be running even after you close the terminal window let's go ahead and open up the dashboard and stop the container you should actually have two running containers here if you've been following along when you stop the container any state or data that you created inside of it will be lost but there can be situations where you want to share data across multiple containers and the preferred way to do that is with volumes a volume is just a dedicated folder on the host machine and inside this folder a container can create files that can be remounted into future containers or multiple containers at the same time to create a volume we use the docker volume create command now that we have this volume we can mount it somewhere in our container when we run it multiple containers can mount this volume simultaneously and access the same set of files and the files stick around after all the containers are shut down now that you know how to run a container let's talk a little bit about debugging when things don't go as planned you might be wondering how do i inspect the logs and how do i get into my container and start interacting with the command line well this is where docker desktop really comes in handy if you click on the running container you can see all the logs right there and you can even search through them you can also execute commands in your container by clicking on the cli button and keep in mind you can also do this from your own command line using the docker exec command in any case it puts us in the root of the file system of that container so we can then ls to see files or do whatever we want in our linux environment that's useful to know but one of the best things you can do to keep your containers healthy is to write simple maintainable micro services each container should only run one process and if your app needs multiple processes then you should use multiple containers and docker has a tool designed just for that called docker compose it's just a tool for running multiple docker containers at the same time we already have a docker file for our node app but let's imagine that our node app also needs to access a mysql database and we also likely want a volume to persist the database across multiple containers we can manage all that with docker compose by creating a docker-compose.yaml file in the root of our project inside that file we have a services object where each key in that object represents a different container that we want to run we'll use web to define our node.js app that we've already built and then we'll use build to point it to the current working directory which is where it can find the docker file and then we'll also define the port forwarding configuration here as well then we have a separate container called db which is our mysql database process after services we'll also define a volume to store the database data across multiple containers and then we can mount that volume in our db container and hopefully you're starting to see how much easier it is to define this stuff as yaml as opposed to writing it out as individual commands and now that we have this configuration set we can run docker compose up from the command line which will find this file and run all the containers together we can mess around with our app for a little while and then run docker compose down to shut down all the containers together i'm going to go ahead and wrap things up there if this video helped you please like and subscribe and consider becoming a pro member at fireship io where we use docker in a variety of different project-based courses thanks for watching and i will see you in the next one"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Containerization with Docker for ML Models",
        "Video Title":"Docker Simply Explained with a Machine Learning Project for Beginners",
        "URL":"https:\/\/www.youtube.com\/watch?v=-l7YocEQtA0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/-l7YocEQtA0\/hqdefault.jpg",
        "ID":"-l7YocEQtA0",
        "Publish Time":"2023-12-08T16:00:11Z",
        "Channel":"Python Simplified",
        "Channel ID":"UCKQdc0-Targ4nDIAUrlfKiA",
        "Transcript":"hi everyone today we will finally learn about Docker and we will do it by making a useful machine learning project specifically a translation program that we can easily share with the world now we will build it step by step solving actual real world problems using Docker only we are not cloning anything from GitHub and we will of course talk about images containers Docker files compose files and we will even see how to publish our own software on dockerhub so what exactly are we waiting for let's [Music] roll so first of all what exactly is Docker Docker is a platform that helps us build run and share software it uses something called containers to create isolated environments where programs live and even though they use the same Hardware as our system the same processor memory and all other components containers are separated entities that operate in a sandbox nothing goes in and nothing goes out which means that all the modules and libraries that our software needs are already pre-installed inside the container so we can think of containers as the perfect set of conditions for our software but why is it so important well let's say we are working on a team project Batman has a Mac machine I have a Linux one and Gandalf is using Windows our project may have the exact same code but it will probably manifest differently on every system or alternatively even if we all have the same operating system it doesn't mean that our environment is the same I have the newest version of naai but it fails on Batman's computer because it has conflicts with some other software it happens all the time but why do we need this headache if we can just create a controlled consistent environment that all of us can use and that's exactly where Docker comes handy we can all work on three different computers but if we use the same container we are actually using the same environment you will see what I mean shortly so let's quickly install Docker and I'll continue explaining it as we go first we will navigate to doer. we will select products and we will go for the personal version to download it we will need to sign up but since I already have an account I'm just going to sign in now once Docker desktop is installed we will accept the terms and conditions in my case I'm going to skip this lovely form and we will verify that the docker engine is running given this green bar at the bottom left corner if that's the case we can now close the guey and we can use our terminal instead in my case I'll be using the command prompt and I will run it as administrator and for the record this tutorial is using Docker version 24.0 point6 great so how exactly does it work well before we go any further we will need to understand the concept of Docker images Docker images are very similar to GitHub repositories but instead of just storing code they also store the ideal set of conditions for our code so we are not just getting a piece of software but we are getting the environment where our software is already installed now images have a readon format which means that we cannot modify them we can build new images but we cannot change existing ones so we can think of them as this static set of instructions but what are these instructions for well images are instructions for containers so for example if images are blueprints for a house then the container is the house itself and in technical terms a Docker container is a running instance of an image we are not just reading containers but we are also interacting with them and yes you can modify containers as much as you'd like so in summary we use images to create containers and inside them we are not just running programs but we are running entire environments so let's see it in action so let's search for an image of a nice machine learning library with Docker search tensorflow and even though there's quite a few options here we will go for the Jupiter tensorflow notebook so let's quickly copy this name with a right Mouse click and we will then download this image with Docker pool followed by the name of the image where Jupiter is the name of the community that maintains the image tensorflow notebook is the name of the image itself and a combination of the two is the name of the repository but what kind of repository are we talking about where exactly are we pulling this image from so let's quickly run this command and let's navigate to hub. do.com as in dockerhub and we will then click on explore which will open a giant collection of images so if we search for Jupiter tensorflow we will find the exact same repository we just pulled including the pull Command right over here so you can either search here or in the terminal it's entirely up to you now once we finish pulling our image we will turn it into a container with do run followed by the name of the repository and great we will copy one of those URLs that Jupiter provides us we will paste it in our browser and we get a nasty nasty error but why well if we go back to our terminal we see that we are not communicating with our own operating system my name is not Jovian and I am not using Linux these are the properties of our container which is essentially an isol ol ated process on our computer and because it is isolated we get an error when we try to access it from the outside so how are we supposed to solve it well first of all let's collapse our notebook with contrl C then we will press the up key to fetch the most recent terminal command and then right in front of our repository name we will add the flag of- p as in ports then we will choose a port from our host system in my case I'll go for Port A 8,000 followed by a colon and then the port from our container which is 8,888 and here we don't really get to choose we need to specify the exact same port that we got from Jupiter great now let's give this command a quick run let's navigate to our browser we'll type Local Host at Port 8000 and beautiful here's our notebook now the last thing left to do is to copy our token from Jupiter we will paste it as our password and boom we are in so we basically created this Corridor where Docker takes all the actions that we perform in this lovely browser window and it automatically applies them on our container in technical terms we call this process exposing a port great now let's quickly create a new notebook and let's make sure that tensorflow works to test it we will load a very nice data set with from tensorflow cars. dat sets we will import mnist which is an image data set of black and white digits to get those images we will call the mist. loore data method and we will assign it to data but the thing is our data is broken into train and test data and each of these is broken into samples and labels data so essentially our data is a nested Tuple with this type of structure and it's okay if you're not sure what it means it is not important for this tutorial now let's quickly run this cell and once our data is loaded despite all those warnings we will go ahead and plot one of our images with PLT do imshow as an image show to which we will pass our very first training image with xor train in the index of zero and yeah we might as well import the library first before we use it with import met plot li. pyplot s PLT now let's give it a quick run let's have a look at our sample and okay it looks a lot like five now let's quickly verify it by printing the matching label to our sample with Y train in the index of zero and beautiful it is five indeed and tensorflow officially works but what if we don't have the time to learn tensorflow and all the machine learning Concepts behind it can't we just skip the understanding part and go straight for the results of course we can we'll just use a library called Transformers that offers a very large collection of tools for beginners so it's quickly import it with from Transformers import Pipeline and look at that Transformers is not installed inside this container now usually we'll just install it with exclamation mark pip install Transformers but the whole idea of containers is that we never need to install anything and if something is missing from our image we cannot just add it we will need to build a brand new image instead now luckily we don't need to do it from scratch we can use the tensorflow notebook as a base and we can combine it with some new modules for this allow me to introduce you to Docker compose an alternative way of defining containers so for example if we'd like to reproduce the same container we ran earlier we will need a compos file that defines it now this file is using the yaml language which is all about indented pairs of keys and values separated by colums where the important points are we are using the tensorflow notebook image and we are exposing the internal Port of our container to the external Port of our host system now let's quickly save this file as compose yml we will then navigate to the folder where we saved it and with a right Mouse click we will open a terminal instance in the current directory and this time we are dealing with a Powershell terminal now let's quickly make the font larger and at the moment of filming I am using Docker compose version of 2. 23.0 now to run a container with Docker compose we will simply type Docker compose up then then we will navigate to Local Host at Port 8000 we will once again copy our token we will paste it back in the notebook and we are back in but isn't it a bit silly that we always need to copy and paste our tokens can't we just set a really nice password instead of course we can so for this we'll go back to our terminal we will first shut down Jupiter with crl C then we will stop and remove move our container with Docker compose down then back in our compose file we will add another key of environment and we will assign it to a value of Jupiter unor token in all caps now in my case I will set it to I am Batman now let's save it let's go back to our terminal and let's call Docker compose up again we will then refresh our browser and now instead of a token we will specify I I am Batman and beautiful we are in now the only problem is The Notebook that we created earlier is now gone so before we build a new image let's make sure we have a way of preserving our files now the way to do so is with something called Drive mounting where we expose a folder from our container to a folder on our host machine just like we've done with the ports so back in our compose file we will create a new key of volumes and we will assign it to a value of dot slash which represents the current directory of our terminal in my case that will be this lovely folder where our compose file lives then once again we will separate it with a colon from the directory of our container which in my case is slome SL Jovian now let's save it let's go back to our terminal let's shut down our notebook and let's call Docker compose up again we will then return to our browser where we see our beautiful compost file in the file tree yay now the original plan was installing new modules inside a container so instead of using an existing image we will build a new one instead now the way to build images is with something called Docker file which we will create right away so the build key is where we specify the location of this file in our case do slash or the same directory as our compos file now let's quickly save it let's create a new file and let's save it inside our project folder our lovely mounted drive and we will save it as Docker file with a capital D no format no extension just Docker file now to make it work we will need to begin with the from instruction where from specifies the parent image on which our container is based in our case Jupiter \/ tensorflow Das notebook then right below we specify our user which in the case of our tensorflow notebook is a variable named dollar sign and bore uid in all caps and what we really mean here is Jovian a user with the right permissions now if you are building from a different base image you will probably have a different username as well so if you're not sure what it is please try setting it to root which is the administrator so you will basically have unlimited permissions but since we know our username we will go ahead and use it instead next we will use the Run instruction to pip install D- upgrade pip and with the help of a double end symbol as well as a backs slash we will move to the next line where we will pip install Transformers next we will pip install a library called Pi s RT you will see shortly why and then just as a precaution step we'll go ahead and fix Dash permissions of a string of slome SL dollar sign and in a set of Carly brackets we will specify n bore user and as you may guess once again what we really mean here is Jovian and great our Docker file is ready we can now save it we can navigate to our terminal and we can call Docker compose up again then we will create a new notebook where from Transformers We Will import Pipeline and let's give it a quick run and despite all those warnings Transformers was successfully installed how do we know well let's create a new model we'll call it translator we will assign it to Pipeline and inside pipeline we will specify a task in our case a string of translation uncore e ncore 2core frr as in English to French now let's give it a quick run and once the model associated with our translation task was done downloading we can then call translator to which we will pass a very nice sentence in my case my name is Maria and I am a programmer let's quickly assign it to FR we will then print it right below and it all comes down to awesome but the only problem is we're not really getting a string in return we are getting a dictionary that is embedded in a list which is not exactly what we're looking for so let's quickly focus on the item at index zero to cancel the list and then we will focus on the key of translation text to cancel the dictionary and now when we reun run this cell everything looks much much better awesome now let's do something extra useful with our new set of skills so let's take the subtitles from one of my videos specifically in an SRT format which is a sequential file with a bunch of timestamps alongside their text and you can find it in the description we will then paste it inside our mounted directory which will load it into our container and then back in our notebook will first verify that our file was loaded there you go captions English and then we can read it with pi Sr t. openen to which we will pass the name of the file captions unor english.srt we will then assign this expression to Subs as in subtitles and we will of course import Pi SRT before we are using it then we will print the content of our file just just to make sure that we loaded it properly with for I in Subs we will print I let's give it a run and great there you go everything was properly loaded but because we don't really need to translate the timestamps let's focus on the text only by printing i. text awesome now let's translate it to French to do so we will create an inner loop variable called FR fror text and we will assign it to translate to which we will pass i. text now since we are looking for a string output we will once again focus on the item at index zero as well as the key of translation text then we will simply assign i. text to our French translation as in F frore text then once we are done with our for Loop we will go ahead and save our new translation with Subs Dove and we will specify the name of our file which in my case would be captions unor french. SRT now let's run it and this might take you a minute or two and once Jupiter is done we will then navigate to our file tree where we can find our brand new French captions file so let's click it and holy smokes you guys here's our beautiful beautiful translation we did it now the last task is to convert our entire software into an image not just the environment like we've done earlier but also our code that way we can upload it to dockerhub and share it with the world now before we do so let's quickly rename our notebook let's call it trun Slater and there's just one tiny detail we will add to our Docker file we will need a copy instruction that takes files from our project folder and stores them directly on our image so for example example we will copy captions english.srt and we will save it at the root directory of our container which is dot slash now we will do the same for our notebook we'll just specify it right after our captions with translator. iynb great now let's save it let's navigate back to our terminal where we will first need to stop and remove our current container now we can of course do it with Docker compose down just like we've done it earlier but on my end I'll go ahead and remove all the stopped containers not just this one with Docker container prune yes and once our container was removed we can finally go ahead and call Docker compose up again which will build our new image awesome we can finally navigate to Docker Hub we will click on Repository stories and we will create a new one now on my end I'll call it sr- translator and I'll describe it as English to French SRT video subtitles translator let's go ahead and click create and perfect now we have a remote repository name and because it is remote it means that our terminal is not aware of it yet so first let's find the local repository name and we will do this with Docker images which will show us all the images that we've pulled and we are currently storing on our system and then right below repository we will find the name of our machine learning project so it's quickly copy it and now we will need to change it so it perfectly matches the name of our remote repository to do so we will type Docker image tag followed by the name of the local Repository and since it has the tag of latest we will add colon latest to the very end of it now right after we specify the new name of the repository which I'll just copy from my browser and I will give it the tag of 1.0 because it is the very first version of our image now let's run it and if we check Docker images again we see a new instance of our image but this time with the remote repository name great but this repository still lives on our computer to upload it to dockerhub we will type Docker push followed by the new name and the tag of 1.0 once we hit enter we finally load our repository to dockerhub and once we are done we will navigate to our browser we will refresh the page and boom here's our beautiful beautiful beautiful repository which is now 100% publicly available yay and let's quickly verify it works as expected so let's prune our containers once again then we will remove the local instance of our image with Docker RMI as in remove image followed by the name of the image as well as the tag of 1.0 now we will verify that this image is gone with Docker images and beautiful it is gone indeed now in addition I would like to change the current directory of my terminal just to make sure that it has no files in it I want a completely new folder with nothing inside to do so I will type Mech dear test as in make a directory named test we will then navigate there with CD test and now we will go ahead and pull our remote image from dockerhub with Docker pull followed by the name as well as the tag of 1.0 now we will run this image and we're not going to use Docker compos this time we'll just type Docker run- P followed by the host system Port of let's say 5,000 we'll be creative this time followed by the container Port of 8888 followed by the name of the image as well as the tag of 1.0 now let's run it it let's copy the URL from Jupiter but we will need to slightly modify it we will change Port 8888 to 5000 and beautiful here are both of our files and when we click them we get the exact same content we had earlier including the warnings now the best part is if we navigate to our new test directory it is absolutely empty so those files they never came from our system they came directly from dockerhub now I don't know if you've noticed but we have officially learned how to work with Docker you can now continue exploring it on your own so congratulations and thank you so much for watching if you found this video helpful please share it with the world and don't forget to leave it a huge thumbs up if you'd like to see more videos of this kind you can always subscribe to my channel and turn on the notification Bell I'll see you very soon in another awesome simplifi tutorial in know meanwhile bye-bye"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Containerization with Docker for ML Models",
        "Video Title":"Deploying Machine Learning Models with Docker: Running ML Models Inside Docker Containers",
        "URL":"https:\/\/www.youtube.com\/watch?v=dprpvr7_X5U",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/dprpvr7_X5U\/hqdefault.jpg",
        "ID":"dprpvr7_X5U",
        "Publish Time":"2021-11-05T11:09:10Z",
        "Channel":"Sriw World of Coding",
        "Channel ID":"UCLl49tLX8ZVmHoFjXDTohrQ",
        "Transcript":""
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Containerization with Docker for ML Models",
        "Video Title":"Containerization Explained",
        "URL":"https:\/\/www.youtube.com\/watch?v=0qotVMX-J5s",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/0qotVMX-J5s\/hqdefault.jpg",
        "ID":"0qotVMX-J5s",
        "Publish Time":"2019-03-18T18:39:24Z",
        "Channel":"IBM Technology",
        "Channel ID":"UCKWaEZ-_VweaEx1j62do_vQ",
        "Transcript":"hi everyone my name is Sai Benin and I'm a developer advocate with IBM today I want to talk about containerization whenever I mentioned containers most people tend to default to something like docker or even kubernetes these days but container technology has actually been around for quite some time it's actually back in 2008 that the Linux kernel introduced C groups or control groups that basically paved the way for all the different container technologies we see today so that includes docker but also things like cloud foundry as well as rocket and other container runtimes out there let's get started with an example and we'll say that I wasn't developer and I've created a node.js application and I want to push it into production well take two different form factors to kind of explain the advantages of containerization so let's say that first we'll talk about VMs and then we'll talk about containers so first things first let's introduce some of the things that we've got here so we've got the hardware itself just big box we've got the guest or rather the host operating system as well as the hypervisor hypervisor is actually what allows us to spin up VMs already we've let's take a look at this shared pool of resources with the host OS and hypervisor we can assume that some of these resources have already been consumed next let's go ahead and take this JSON and push it in and to do that I need a Linux VM so let's go ahead and sketch out that Linux VM and in this VM there's a few things to note here so we've got another operating system in addition to the host OS is going to be the guest OS as well as some binaries and libraries so that's one of the things about Linux VMs that even though we're working with a really lightweight application to create that Linux VM we have to put that guest OS in there in a set of binaries and libraries and so that really bloats it out in fact you know I think the smallest node GS VM that I've seen out there is 400 plus megabytes whereas the the the node.js runtime and app itself would be you know under 15 so we've got that with and we go ahead and let's push that Jay s application into it and just by doing that alone we're going to consume a set of resources next let's think about scaling this out right so we'll create two additional copies of it and you'll notice that even though it's the exact same application we have to use and deploy that's that's separate guest OS and libraries every time and so we'll do that three times and by doing that essentially we can assume that for this particular hardware we've consumed all of the all of the resources and there's another thing that I haven't mentioned here but this jeaious application I developed it on my macbook so when I pushed it into production to get it going on the VM and noticed that there were some issues and incompatibilities this is the the kind of foundation of this big he-said she-said issue where things might be working on your local machine and work great but when you try to push it into production things start to break and this really gets in the way of doing agile devops and continuous integration and delivery that's solved when you use something like containers there's a three-step process when kind of doing anything container related and pushing or creating containers and it almost always starts with first some sort of a manifest so something that describes the container itself so in the darker world this would be something like a docker file and Cloud Foundry this would be a manifest yamo next what you'll do is create the actual image itself so for the image you know in again if you're working with something like docker that could be something know be a docker image for working with rocket it would be an ACI or application container image you know so regardless of the different containerization technologies this process stays the same and the last thing you end up with it's an actual container itself you know that contains all of the runtimes and libraries and binaries needed to run an application that application runs on a very similar set up to the VMS but what we've got on this side is you know again a host operating system the difference here instead of a hypervisor we're going to have things like a runtime engine so if you're using docker this would be the docker engine and you know different different containerization technologies would have a different engine regardless it's something that runs those containers again we've got this shared pool of resources so we can assume that that alone consumes some set of resources next let's think about actually container izing this technology so we talked about the three-step process we create some you know a docker file we build out the image we push it to a registry and we have our container and we can start pushing this out as containers the great thing is these are going to be much more lightweight so deploying out multiple containers since you don't have to worry about a guest OS this time really just have the libraries as well as the the application itself so we've scaled that out three times and because we don't have to duplicate all of those operating system dependencies and create bloated VMs we actually will use less resources so she's a different color here and scaling that out three times we still have a good amount of resources left next let's say that my coworker decides hey for this j\/s application let's take advantage of a third party you know let's say a cognitive API to do something like image recognition so you know let's say that we've got our third party service and we want to access that using maybe a Python application so he's created that service that accesses that third party API and with our node.js application we want to access that Python app to then access that that service if we wanted to do this in VMs I'm really tempted to basically create a VM out of both the JSO plication and the Python application because essentially that would allow me to continue to use the VMs that I have but that's not truly cloud native right because if I wanted to scale out the the j\/s but not the Python app I wouldn't be able to if they were running in the same VM so to do it Trulli cloud native way essentially I would have to free up some of these resources basically get rid of one of these VMs and then deployed the Python application in it instead and you know that's not ideal but with the container based approach what we can do is simply say since we're modulars we can say okay just deploy one copy of the Python application so we'll go ahead and do that there's a different color here and that consumes a little bit more resources and then you know with those those remaining resources the great thing about container technology that actually becomes shared between all the processes running in fact another advantage if something if these container processes aren't actually utilizing the CPU or memory all of those shared resources become accessible for the other containers running within that within that hardware so with container based technology we can truly take advantage of cloud native based architectures so we talked about things like portability of the containers we talked about how it's easier to scale them out and then overall with this kind of three-step process and the way we push containers allows for more agile devops and continuous integration and delivery thanks for tuning in for this broad overview of container based technology as always we're looking for feedback so definitely drop a comment below and be sure to subscribe to stay tuned for more videos in the future thank you"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Kubernetes for Scaling ML Services",
        "Video Title":"Kei Nemoto- Gentle introduction to scaling up ML service with Kubernetes + Mlflow | PyData NYC 2022",
        "URL":"https:\/\/www.youtube.com\/watch?v=S-UwQGvR9BA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/S-UwQGvR9BA\/hqdefault.jpg",
        "ID":"S-UwQGvR9BA",
        "Publish Time":"2023-01-24T18:09:48Z",
        "Channel":"PyData",
        "Channel ID":"UCOjD18EJYcsBog4IozkF_7w",
        "Transcript":"let's get started um and I was gonna say can you see my screen but I realized that we are offline yeah um but anyway yeah thank you very much for coming to my talk this is a gender introduction to scaling up machine learning service with kubernetes and ml flow um I'm happy to see all of you in the audience I guess using lots of buzzwords in the title worked well right before we begin um it's gonna be mainly about the uh introduction to kubernetes service deployment so I expect you to know the darker basics of docker so yep so let me quickly introduce myself uh my name is kaini Moto I'm a data scientist at the Montefiore Einstein Center for Health Data Innovations in my work I mostly do email Ops type stuff I build CI CD pipelines ETL pipelines in my previous organization I was developing the back end and I was developing NLP models a lot uh this is my LinkedIn account I'm happy to get connected with you and this is my GitHub account um and uh I'm gonna show some code uh for this talk and um all code is available on my GitHub so please check out okay so let's get started so let's imagine that you're working as a machine learning engineer and one of uh the data scientists um in your team um developed a random first model all right let's say Iris prediction and you are asked to deploy the machine learning model as a microservice so this is our Tech stack would look like so the random Forest is implemented in psychi run and we're gonna use flask to to create a rest endpoint and we're gonna use a g unicorn as a whiskey HTTP server and we're gonna package everything into darker containers so this is just the example code for the flask service I hope nothing new here so the model is random first classifier and we Implement a post method in flask and it receives requests as a Json we pass the Json input into model.predict model generates prediction and we generate a response including a prediction and Returns the response as additions Json string Okay so so if we visualize um the deployment strategy um for this model um it would look like this and uh let's call this single container deployment so first we have a host VM and on the host node we have a Docker container running and within the darker container there's G unicorn server running and the service running and so on and there should be a bridge between the docker specific port and the host VMS Port so the connection is established and client can send a HTTP request to the host VM specific port and since the request hits the the node it's going to be forwarded to the G unicorn unicorn sends the request to the flask service it's gonna generate a response using the circular model and since the response back to the client so this is uh similar to what I did in the past uh in my work I was happy I finished my task um I went home cooked dinner I went to my bed and this happened so let's uh let's review like what went wrong with a single container deployment what are the problems so the first problem is straightforward if the host VM goes down your service goes down and uh VMS fail for many reasons and I learned my lesson in a very bad way and what about update downtime while you're upgrading your service from V1 to V2 you have to switch um your container from V1 to V2 so there is gonna be a update downtime and this means that your service is not highly available which is not a great thing for the production service and what about scalability um the resource resources of the container are bound to the resources of the host node so your darker container cannot be bigger than the host node so it's going to be a scalability issue and to solve these uh problems I mean so yeah um so this means that single container deployment has an issue with um scalability and high availability and we want the production system to be highly available and scalable to achieve that we can use kubernetes so what is kubernetes kubernetes is a container management system so let's think about the life cycle of the continuous application so the life cycle consists of these four stages the first stage is the building you build the image and the second stage is the deployment you deploy your image into some image repository and the next stage is scheduling and scheduling means you decide to when and where to run your container uh the once your container is running the last stage is going to be the management of the container management involves the region of the container upgrading of the container and so on and it turns out that um Docker is a great tool to cover the the first two steps uh in this life cycle but it doesn't natively support the the last two steps stages in this life cycle and you can use kubernetes to natively support um uh the last two stages so I want to emphasize that kubernetes is not a replacement of Docker rather by using them together you can fully support this full container life cycle and when we think about the relationship between darker and kubernetes uh there is awesome documentary on kubernetes created by Honeypot on YouTube I highly recommend you to watch this documentary it's it's really good and um an interviewer uh asked an interviewee so what is Docker in kubernetes anyway and the interviewee's response is that um so let's imagine we invent the post office Docker creates an envelope envelope is a container and kubernetes is the postal system that delivers envelopes to the destination or the VMS so now we understand the relationship between Docker and kubernetes uh let's move on to the kubernetes 101. so um this gray area is a is the kubernetes cluster a kubernetes cluster consists of the control plane and the workers control plane is the brain of the kubernetes it has a scheduler for the scheduling controller manager for the maintenance of the container and the hcd is like the database of the kubernetes so it's kind of the memory of this brain and we have an API server talking to the workers and control plane can be run on a single VM or not ovms and we have workers on each worker we have a component called the cubelet cubelet is kind of the agent of each Walker and it's gonna talk to the API server receives the request from the API server process processes the request um and uh Walker is just a single VM so each VM can be a one VM can be a one Walker um and you see Parts uh in this diagram so part uh is a kubernetes component that executes the containers uh that's all what it does and uh it's very unique in kubernetes that um each part gets its own unique IP address so as long as you're in the kubernetes cluster space you can talk to uh Parts by using their IP address so let's go back to the problems we reviewed how can we build a scalable and highly available service on using containers so the kubernetes answer is called deployment a deployment consists of a set of replicas of the same container from the same image um so let's take a look at an example so we have part A B C and D running on the on this cluster um so these parts are identical replicas of the same container let's say our machine learning service image um so let's think about uh how can this um how this architecture can solve the issues we had so scalabit is very straightforward in this architecture um since um parts are just identical replicas they're going to generate the same response for the same request so we can just distribute you know a requests um across parts and you can add uh more workers to these clusters so you can horizontally scale out your service what about high availability let's say uh node one went down that means uh the parts running on the Node one uh go down as well and uh but it's not a problem because we have identical replicas in node two so when the client sends a request we can forward the uh for the request to the node to uh Parts running on the Note 2. while we recover node one maybe we can add a new node and create a random part foreign update um so this is how it works so while you upgrade um some parts to a version two uh you still keep um some parts alive so when the client sends a request the request goes to a V1 pods and once V2 paths are ready you can send a request to the V2 pods and you can update while you do that you can update the rest of the V1 pass so it's not going to be any downtime for the update so this is a configuration file um for deployment in kubernetes you can write a yaml file to create kubernetes components so let's take a look at line two um applying two you specify the kind kinda means what kind of components you want the equivalents to create which is deployment in this case from from line three to line seven we can specify the metadata meta information about this deployment at line nine you specify the the number of replicas um for this deployment and at line 20 you specify the image associated with this deployment and align 22 you specify the port for this service uh by the way this configuration file is available on my GitHub repo so please check it out later and once you create a yaml file you can pass the yaml file to the QBC they'll apply QBC to apply it's going to talk to the control plane to process your request so um and you can check the parts information using qbctl get parts um so this is what your deployment looks like uh as you can see there are three parts running for this deployment and each part gets its own unique IP address and they are coming from the same image okay and as I said um as long as the client is in the kubernetes space they can send a HTTP request to a specific part using Parts IP address so this is great but um the question becomes how can we decide which part to send a request to since there is just identical replicas of the same container and the kubernetes answer is called Caster IP service a Class Type service creates an endpoint for deployment not for the uh Parts but for the deployment in this and the cluster IP works as a load balancer that distributes traffic across parts so in this manner the client can send the HTTP request to the cost IP and cost IP it's gonna decide which part to forward this request uh in this example because typing decided to use part C so part C is gonna receive the request and processes the request uh generates the response and since it responds back to the cost IP and cost ip4 is uh the response back to the client so this is how you can create a cost IP service I put the configuration for the deployment to emphasize that you need a deployment to create a service um so but this configuration is the same as the configuration I showed you before um so let's take a look at the uh diagram on the right hand side so this is a configuration for the class type service at line 38 you specify the kind which is service and from line 44 to 46 you specify the port for this service and the class ID and at 48 you specify the deployment associated with this class type service and at the end of this configuration you specify the type of this service which is class IP and this is what the cost IP looks like you can get uh service information by using cubic CTL get services and um as you can see cluster IP gets its own unique IP address and it's listening at 45 000. okay so this is great but this gives us a question let's say we have a legacy system which cannot be run on kubernetes but the Legacy system wants to use our Machinery Service so the question becomes how can we expose our machine learning service to outside of kubernetes and the kubernetes answer is called node port no the port exposes a specific port on uh all the workers and it's going to expose the same port on every Walker in this cluster and in this manner uh the client can send a HTTP request to any of the workers at specific node board and as soon as the request hits the node Port it's going to be forwarded to the cluster IB and cluster B does the rest of the routing so this is configuration for the node Port the only difference between uh configuration for the class IP and the configuration for the node report is the last line uh the kind is still serviced but instead of saying type it's called type B you say type is node port and you can as you can see there is no the port running and as I said no the port creates cost IP and exposes a specific code in this case it decided to expose port 31304 um so this is great um Services outside of kubernetes can talk to our Machinery Service but this gives us a question how can we decide which node to send a request to and the kubernetes answer is called a load balancer load balancer service creates a load balancer instance um outside of kubernetes and all the balancer distributes traffic across nodes so there is a distinction between Class diabetes or the balancing and the load balances or the balancing because cost IP is a load balancer for the parts but load balancer service is allow the balancer for the nodes in this manner the client can just send the HTTP HTTP request to reload the balancer and it's going to figure out the rest of the routing and as since request hits any of the workers specific node report it's going to be forwarded to the cost IP and Class Type does the rest of your route uh so this is a configuration for the load balancer service again the only difference between uh other services configuration is the last line you specified that the type is load balancer um usually you run a load balancer service on cloud provider on any cloud provider such as a ALB by the AWS but there are some uh on-prem options uh for the on-prem loader balancer okay great uh so our service is fully exposed um can be accessed um inside and outside of kubernetes cluster but this gives us a question since we are only getting one request should we run all the four parts aren't we wasting resources and the kubernetes community's answer is called K native K native is a third-party kubernetes extension that allows you to create a serverless service uh and the K native exposes itself uh outside of kubernetes so it's going to create an endpoint accessible from outside of kubernetes and the definition of serverless can vary um and have multiple meanings um in this case um serverless means that uh there is no path running uh when there is no request coming and we run the parts based on the number of requests so let's take a look at an example so let's say the client sends a request to the k-native instance can native it's going to check whether any parts associated with this K native service running if there is no path running it's going to ask the API server to create a new pod API server is going to say no problem it's going to talk to the cubelet and cubelet it's gonna create a pot and as soon as the part is available can afford the request to the part and part is going to process your request and K native comes with uh many useful features such as automatic scaling so it's gonna figure out the number of uh Parts you need to process incoming requests so let's take a look at this example so as you can see we are getting many requests from the many clients and Canada says no problem it's going to talk to the API server to create more parts and the aps server is going to talk to the qubits on on the Walkers and cubelet cubelets are gonna create parts and once parts are ready okay native uh it's gonna forward the request to the parts and this is a configuration for the K native [Music] so let's focus on the line one uh the first line uh is API version and we are using K native API uh rather than the kubernetes native API and the kind is K native service and we specify the image associated with this K native service and we specify the port for this service okay so this is a great um our Machinery Services serverless we utilize kubernetes resources and we have to talk about uh important topic specific to machine learning service deployment which is models so Machinery Service deployment is unique because it involves model binaries um so let's uh think about a simple example uh let's say one of the data scientists in your team developed a prediction model inside key learn um and you as a Macedonian engineer are asked to deploy this model as a microservice so the data scientist is going to send you this model binary in psychi learn to you I hope she uses SCP not email sorry and um but you as a machine engineer receives this uh secular model and creates a image including this model binary and you push the image to image repository kubernetes it's going to pull the image from the image repository and schedule the Run parts so this is like okay as long as you're in a small team but as you can see we deal with uh two identical copies of the same model binary which is not great what if uh your team uh grows and you get more data scientists uh asking you to deploy their models and each data scientist may use you know different machine learning framework and each machine learning framework has its own dependency requirements so and you as a machine learning engineer you receive lots of model binaries and you have to keep them in your environment so it's going to be hard to keep track of the lineage of machine learning models once they're deployed and the communication issue it's going to be inevitably happen and this happens we don't know which model we're running so how can we solve this issue um one way to solve this uh problem is to use model registry I'm gonna talk about ml4 model resistry as an example in this talk but um ml4 model resistry uh creates um centralized model storage and for for a story in the model binaries in this manner data scientists can just push their model binaries into ml4 model registry and ml4 stores lots of metadata metadata meta information about the models and you as a machine learning engineer can view the information through ml flow UI and you can figure out the the model name and model version for your service and you can create and you can pass the ml flow um connection information to your image then you can create an image and push the image to image repository and once the kubernetes pulls the image you can program your script to fetch model binaries from the ml4 model registry in this manner manner you only deal with single model binaries and that's a great deal this is what the ml4 UI looks like as you can see it contains lots of meta information such as creation time of this model last modified time description tags version is very important because we potentially develop lots of models as a we're going to have a multiple versions for the same model and uh there's also um a tag for the staging information so maybe you as a machine learning engineer can say okay maybe I should use version one because it's tagged production so it massively simplifies the um uh Machinery Service development process and this is the example uh script to deal with ML flow model registry so left hand side shows how to register your model in ml4 model registry uh first component is the uh the training of your model and once you train random first model you establish connection at the ml film model registry and you log the model there so maybe uh this is a script for the data scientist and once the model is pushed and you're asked to deploy this model as a microservice you can make the flask app fetch the model when you start when it gets started you can pass the important information model name version ml for your URI in environment variables and first it's going to establish the connection to the ml4 model registry and close the model from there and pass passes the model to the first Gap and your app is ready to serve so maybe the second script is for the machine learning engineers okay so this is the final configuration file for our machine learning service I promise you there is no more so from line one to eleven uh it's identical um but sorry line one two fourteen yeah but from line 14 to uh to the end um I specify the environment variables so this is how you can specify environment environmental variables uh in kubernetes and also you can secret uh you can use a secret to store a sensitive information within the kubernetes space and you can use the secret as an environmental variable so this is how you can do it okay uh so before I go I wanna quickly talk about how I set up my uh kubernetes cluster for this demo um so for this demo I just used the vanilla uh Native VMS uh offered by this is the ocean and um I just I didn't use the uh the kubernetes product I just used their vanilla VMS um and for uh my kubernetes cluster I used a three Notes One node is for the control plane uh two notes for the walkers for the workers you only need one gigabyte of memory and one CPU this is okay but for the control plane you need two gigabytes for memory and one CPU otherwise you're not gonna have a pleasant experience um and as long as you don't run crazy number of services this is these configuration is enough for demo um and to set up my kubernetes cluster I used a kubernetes distribution called k3s okay 3s is a certified kubernetes distribution that allows you to create the kubernetes cluster that requires much less resources um if you wanna run vanilla kubernetes cluster this config configuration is not going to be enough you're going to need more computation powers like k3s is a lightweight kubernetes so uh this is gonna be uh enough for quesades and um and the k3s comes with a single binary so it massively simplifies the installation process it's really really easy to install uh vanilla kubernetes uh it's not very easy kubernetes is great because it's flexible kubernetes is bad because it's flexible and on my k3s cluster I run K native and for the image repository I use a darker hub for the ml4 model resistry I use one gigabytes of memory and one CPU VM on this the ocean I and I run it outside of kubernetes and I run ml4 server as a Docker container and I use the scfdp server as the backend storage for this ml flow storage so thank you very much um yeah we can get started okay yeah thank you very much for your talk it was very nice I have a question in particular about the challenges you face because in the way you show it kubernetes are awesome and it was very shiny and very clear but I would like to know if you have faced any particular challenge with this workflow or some similar workflows that you have had um in a more realistic scenario so I think your question is about like is there any uh challenges I faced uh when I set up my kubernetes course yes um that's a great question um first of all it's hard to set up kubernetes uh and uh so kubernetes is a distributed systems so it's a vulnerable to like network issues so it's sometimes it breaks uh and you want your notes to be highly available so if they're on the same rack let's say in a data center if the racket goes down everything goes down so maybe you want to use a cloud providers some VM for maybe some components like uh control plane but that's a great question thank you could you just show the slide again where you had the the link to your GitHub and all that thanks thank you for that I have a question for over here yes pertaining to when you were beginning to speak about k-native and how it was able to like as opposed to extending a lot of resources monetary compute otherwise it will listen for your API requests and then spin up pods accordingly um from a practicality perspective like when the first request comes in then wouldn't that take time to build from an image from the start so wouldn't you just need to like at least have one persisting like one pod existing in terms of like live production deployment yeah that's a great question so the question is about uh I think it's called warm start or code start so if there's no path running we have to create a pot and uh so what about you know the downtime uh the answer is yes uh and it depends on the size of your machine learning model if the model is Big you know it takes lots of uh i o connection you know time um and I think it depends on your use case um you can configure K native to run you know at least one service at one part yeah so that's possible and um I think I forgot to mention but the advantage of using K native uh for serverless services that um um so it's still kubernetes so it's gonna be portable so you can put your commands cluster between Cloud providers yeah thank you for the question thank you um have you ever set this up in like a production configuration um we we are using different kubernetes distribution for the production and it's more suitable for production purposes uh and I work in uh Healthcare so I we have to set up on-prem kubernetes cluster because it's hard to migrate to the cloud for some regulation reasons but um yeah um I had a lot of fun setting up the cluster I would say okay yeah yep I mean I worked for Healthcare Company too and we used to um please talk to my you know manager hey so thanks for first of all thanks for your informative essential you did talk about load balancer right so when you choose load balancer is there a specific list of ips you can cover or they can talk about different subnets how does you can figure that up when you set up kubernetes are you talking about cost IP or the restaurant yes yeah you can set up a range uh even you can set up a IP range for the parts using the cider so it's expensive cider cidr it's the idea okay forget that you know but what it stands for okay and can they talk with different subnets or is they can only work with one set of software ah so that's gonna be a difficult question um it depends on how you set up you know subnet because kubernetes does not set up subnet right it uses existing supplement so usually it creates an overlay network uh using UDP you know connection I UDP tunnels um but there is a way to formulate kubernetes cluster without UDP tunnel it's called um Calico so you it's gonna do IP in IP uh packet packaging and you can talk the other subnets but it's a kind of advanced topic and it's fully like devops you know yeah I love to talk about it but yeah thank you your ml flow set up do you do YouTube users any kind of tenancy requirements below one model right yeah so that's a great question the question is um like you're talking about multi-tenancy right for the ml4 um we don't have it we love to have it um but I don't think uh ml4 natively supports that so I was wondering if you had built something that would be a cool poster I mean see my Twitter list yeah yeah I think we have time for one last question yes what is the difference between kubernetes as the distributions you've been using and versus Google cloud and how it and how much easier is it or harder um that's an interesting question um so Google cloud is more broad it covers more services um Google provides a kubernetes service called gke so it's not an apple to Apple comparison in my opinion but when it comes to other distributed systems such as mesos yarn which was the darkest one kubernetes is known to be more stable but it's more complicated um apologies to the others who have questions yeah um and thanks a lot uh thank you Kai for the amazing session"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Kubernetes for Scaling ML Services",
        "Video Title":"AWS Container Day - ML with Kubernetes",
        "URL":"https:\/\/www.youtube.com\/watch?v=ZXTRzE8rt5A",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ZXTRzE8rt5A\/hqdefault.jpg",
        "ID":"ZXTRzE8rt5A",
        "Publish Time":"2019-06-13T16:00:07Z",
        "Channel":"Amazon Web Services",
        "Channel ID":"UCd6MoB9NC6uYN2grvUNT-Zg",
        "Transcript":"so hi everyone my name is Yin even from the Container Services team and in the last few months have been actually leading our AI and ML solutions on solutions like kubernetes for example and I'm going to show you why in a couple of minutes and I also have Mike root with me who's a senior engineer and the open source team here in AWS as well and he's actually going to show you a pretty nice demo and some of the things that we've been doing and the innovations which putting into some of the solving some of the specific challenges that people have today on running their machine learning workloads so let me start off by just saying that there's been a lot of hype around machine learning and AI over the last few years and Gartner keeps running an annual report and survey on the hype cycle of these kinds of emerging technologies and each year machine learning and AI are kind of in the peak of their hype cycle so it's coming to a tipping point in which the hype around this technology is actually starting to transform to real impact on businesses like enterprises large companies and also startups or all looking to leverage and gain the benefits from what it is and there has been a recent IDC report that shows that or predicts that in the next year or so about 40% of the digital transformations are going to be involved with AI and ml so this is just to kind of set the stage on why we're even seeing this and and how meaningful that can be for you our mission in AWS is to be able to put machine learning in the hands of each and every one of you to do that we have built the broadest and deepest compute platform to help support AI and machine learning workloads on multiple stages so for the experts machine learning practitioners who one have the control and the ability to deploy and develop test deploy and tune their own machine learning models we have built frameworks that can run on our compute services as you probably know we have a pretty broad range of compute offerings from P to s and P 3s the G instances the latest announced P 3 d n and some others that are right now in the works such as G 4 and the new chip inferential which we announced that is going to be launched later on so and under those compute platforms you can actually run a selected number of frameworks and interfaces of machine learning including tensorflow Amex net and many many other flavors as well on the middle layer we have what we call the ml services because unfortunately as it is today there are not enough machine learning experts and specialists in the world so in order to put machine learning in the hands of each and every one of the data scientists data engineers and machine learning practitioners we invented a bunch of machine learning services for fronted by Sage maker who is an end-to-end compute platform and machine learning platform destined to be able for you to get started with machine learning very easily starting from the notebook experience so you can provision Jupiter notebooks following on with data pre-processing training and then inference as well so it's a package that allows everyone to get started with machine learning very easily and also comes pre-built with a lot of the models and the public more popular models that are serving a lot of our customers to get started with machine learning and then on top of that we also produced a bunch of AI specific platforms there are for those of you who do not want to be able to model and build and train their models but just want it to be able to leverage AI as part of your applications with a bunch of AI offering such as vision speech language and many many others we introduced a bunch of managed services like recognition comprehend and many many others which you can just leverage with your existing or new applications running in the cloud and get started very easily and also drive the time to market and the cost significantly down so to summarize all of that a very deep in terms of breadth and depth compute an AI platform that allows each of you whether it's an expert machinery practitioner somebody who's getting started or just want to leverage our capabilities to do that with our services now obviously you cannot do machine learning with just AI and m\/l services machine learning usually requires a lot of data a lot of data processing and so you need analytics tools such as EMR redshift and other services to be able to help you digest pre-process the data and make it available for your machine learning development lifecycle and training you also need a lot of storage as I will show you a little bit later on some of the customers that we work with especially in autonomous vehicles industry are processing huge amounts of data and to do that they are leveraging a lot of our s3 services including deep archive which is very popular in that space in order to gain the benefits of saving a lot of data and making it accessible to your application and training and inference models in the fastest way and in the cheapest way possible but the reason we're all here is kubernetes so kubernetes has been becoming very very popular in a framework layer in order to run these machine learning processes or pipelines and to end on the platform and this is what i'm going to focus on today to show you what customers are using it for and how you can leverage that with AWS so first off why would we use kubernetes to run machine learning I mean kubernetes is a general-purpose computer form container platform orchestration why would you use it so people are using it from exactly the same reasons why they are using kubernetes for their micro services for their apps or any other application for that matter so the ability to compose a lot of different pieces and building blocks and build your own customized machine learning tool or framework using your favorite models using your favorite IDE tools and choosing each step of the way which bounine to which technology you want to leverage that's something that kubernetes and the frameworks that run on top of it will provide you portability so the ability to actually take your machine learning workflow from your laptop or your desktop GPUs to anywhere else including the cloud and be able to remain consistent along the process and have the exact same pipeline running on all environments and obviously scalability which is something that kubernetes offers you and especially if you're running on AWS you will get all the benefits of the underlying auto scaling services that we have that really allow you to burst into the hundreds or even more instances GPUs or CPU cores as required so I want to break it down to talk a little bit about the machine learning workflow itself let's get down to the details what do you have to do in order to run your machine learning workloads today so first of all people start off with running some prototyping and usually the way that that works is through either jupiter notebooks or other types of notebooks notebooks for those of you who doesn't know our popular technologies in the machine onyx face and also analytics in general because they allow you to bring in a lot of code data all in the same place and be able to share that with your peers or with other teams in your company and also interact from within that notebook with other services for example in AWS you can actually invoke jobs that are run in EMR or run in redshift straight from within a Jupiter or any other notebook that you will because basically it's all API based some other machine learning practitioners preferred the IDE approach so they like to program and code so they use Python or frameworks like pi torch and then there are others who use frameworks like our brain for example which is a machine learning framework built on top of Jupiter so it's kind of more tuned into the machine learning workflow one thing to note about that process though is that the process is iterative so you start off your prototype you bring in a bunch of data in order to test and be than train your model but then after you validate that with real data you may find that the model is still now not where it needs to be which means it's not really behaving the way chewed on datasets that have not been used before which means you have to go back and do some data wrangling re compute and tune your model and then do the same process all over again so that can be an iterative process that can take however time it is depending on the complexity of the model so let's see how that works so first of all in terms of Jupiter Jupiter itself is a very popular technology a lot of the frameworks including our own sage maker cube flow which we'll talk about in a second and others support Jupiter today as I said it's very well tuned to do analytics in general but machine learning specifically so that's very nice yeah let's bring it on so as I mentioned the data processing tool starts or the machine learning workflow starts with data pre-processing so data pre-processing is basically bringing in some clean data or raw data that comes in from multiple sources of your application or if you from your business that data first and foremost needs to be cleansed sometimes you need to drop the irrelevant data sometimes you're getting unstructured data that needs to be structured into a certain format in order to run the machine learning process on it and build models so there could be multiple variations of how you can clean and pre-process the data and then you get into an iterative process in which you're going to build your model you're going to train it and then when you do validation you may have to go back and do some data wrangling which will be an iterative process like I mentioned before at the end of the day you will get a model and that model needs to be deployed on an endpoint so it can actually start serving our inference requests which are what we actually came here to do we want our application to make a prediction and get a response from or make an API call and get a prediction from the inference endpoint so you can do all of the things that I just mentioned today on kubernetes alone without Q flow however when you do so you will have to take care a lot of the things yourself so things like how do I port the UX and the IDE that I want to work with in my desktop and how do I switch that over to the cloud how do I use different models how do I use different tooling because the tooling may change or evolve over time so cube flow really what it does is provide the set of tools that can work across all those environments for you and allow you to just set the stage on how your process should look like and have it run in a consistent matter across all of the places that you're developing your machine learning models with so what is Q flow Q flow is a collection of tools that help you as an ml practitioner to get the job done so we mentioned some of those tools like the Jupiter notebook but there are other tools for example there are framework operators so for example we have tensor flow operator and M X net operator so those are different frameworks for machine learning training that you can leverage each of them using the kubernetes operator that makes them available to cube flow and there are other tools in that like Argo which is a workflow management system and I will talk about coop flow pipeline which is another component which is relatively new on cube flow in a second so the good news that if you want to get started with cube flow on AWS you can do so today we have a product page on the cube flow project itself which has a deployment section that can get you started very easily so you'll gone you're going to be installing a bunch of tools like cube cut' okay SONET obviously the command line for cube flow itself and then the provisioning process is going to set up the entire eks cluster including the cube flow for you so it's going to abstract a lot of those complexities that data practitioners or machine learning practitioners may run into if they're not aware of the AWS constructs like vpceb networking etc q flow pipelines is one of the recently added features to cube flow what it does is it allows you to actually define a pipeline or a machine-learning pipeline end-to-end so think about the process that I just mentioned earlier starting from the data pre-processing training and inference all of those steps can now be represented as a step in the cube flow pipeline now the Q flow pipeline is persistent so it can be replicated and maintained across environments and it has a bunch of integrations to other tools that you can leverage to do different steps so for example for data pre-processing you can today invoke an EMR job using spark and spark is one of the most popular frameworks to date on processing large sets of data so the benefit of using or leveraging EMR is that you will be able to offload the job get a cluster or a spark cluster set up for you leverage spot instances which is a way for you to reduce the costs with 70 to 90 percent from an on-demand pricing on AWS which can be very significant because fart jobs typically run on large data sets so we topical talking about clusters of dozens or even hundreds of compute intensive instances that can be costly to process and this all gives you the ability to invoke all of that from within the cube flow pipeline itself so some of the benefits involved in leveraging cube flow on AWS some of which we already discussed so cluster provisioning with eks Caudill that's basically what it means is it's going to abstract the complexity of setting up AWS construct and the eks cluster from you so it's just a simple command line you can have an eks cluster with the instance types that you choose and the cluster this and the size that you choose set up with cube flow ready for you to get started with your ml process we also support a LB ingress controller which means if you want it to set up and tensorflow serving endpoints you can set it up through alb ingress controller so the way it's going to work is we're going to connect an optional plugin which is the sto gateway and all of the traffic coming in from the alb ingress controller will be flowing through the FCO gateway and then to the Ambassador inside the cluster itself we also have a dedicated support today for storage and that's not just a cube flow feature it's a to Brady's feature so we have drivers CSI drivers native to kubernetes for EBS EFS and fsx lustre that can be really significant especially when it comes to large-scale processing and I'll talk about an example of one of those workloads in a second then you also get the unified cloud watch logging so all the kubernetes cluster control control plane logs will be routed to cloud watch so you can monitor and see what's happening within your cluster if you're using the alb ingress you also get TLS and auth with certificate manager and Cognito and then we recently introduced private access to your communities cluster as kubernetes api server endpoints so you can leverage those as well if you didn't want them to be public and one last benefit is today when you're setting up your cube flow on AWS with the eks integration the cube flow will detect if you're running a GPU based cluster and consequently will provision or install the Nvidia device plugin on its own so you don't have to worry about the device plug in manually being installed by you so this covers all the way to the ml framework itself ie what is the set of tooling that you will use when running that machine learning process let's talk about training so first of all let's talk about the framework tensorflow is a very popular framework that a lot of the developers and especially on kubernetes I'd love to use so it's an open source library to develop and train animal models that was created by the Google brain team originally and can pretty much run anywhere I think what a lot of people don't know oh is that today 85% of all the tensorflow workloads that are running in the cloud are actually running on AWS now not only that we actually took the tensorflow implementation and customized it because what we have found and we'll get there in a second but I'm giving you a heads up a lot of those large-scale training jobs especially in autonomous vehicles require what we call distributed multi dpu multi node training jobs now multi node training jobs means that you need to leverage a lot of GPUs in order to increase your efficiency but as it happens the stock tensorflow the one that comes pre-built does not scale as efficient as the number of GPUs so if you scale your nodes to 256 GPUs for example you're only going to get 65 percent efficiency increase rather than 100 with our tune tensorflow algorithms however we managed to get it to an AWS optimized fraction of a hundred percent efficiency scaling which means that effectively the more GPUs you will add that's how much your efficiency will increase and that's very significant especially when it comes to GPU instances which are not cheap and that allows you to basically bring out bring down the overall cost of your training significantly now let's talk about one example for that and that is the autonomous vehicle space so ever since we launched dks in June 2018 we have seen a lot of customers running a lot of machine learning workloads high scale workloads running and some of them were from the autonomous vehicle space so as we engage we started to understand what are the requirements that are driving this industry so it turns out that the autonomous vehicle industry today is at a phase where they're trying to get and I'm sure all of you know that to the highest level of automation possible and as standards go those levels range between zero which means not automated at all and five which is pretty much the Nirvana that we are looking to get which is we are fully automated and in terms of control monitoring and fallback this is all being managed by the autonomous driving system rather than the driver of the human so typically those customers would range between level three four if they're really advanced today and they're all experimenting and deploying in production at the same time so if a certain company were able to reach sae level three for example that means they will already have level four in their development lifecycle now as it happens with every new stage of autonomy the amount of compute required is exponentially increasing which means that those customers incrementally require more and more compute from both GPU and CPU perspective now let's talk about some of the challenges that these customers are facing today so as I mentioned when it comes to training this is distributed multi-gpu and often cases multi node training what are the barriers or the challenges that they have told us that they have when trying to run those workloads so first off we spoke about this machine learning practitioners they don't want to know V PC they don't want to know EBS all they want is to set up their training jobs on their favorite tool run tensor flow models or such and be done with it secondly large datasets on s3 they need to be copied somewhere and that somewhere needs to be some staging area in the cluster that the job can then read in a very low latency high efficiency manner in order to run that multi node training next we have the fact that the data needs to be shared across multiple nodes because again this is a distributed job so all the nodes need to have access to the data there's also the thing about how do you distribute the tensor flow algorithm that's not a trivial thing to do now tensor flow does have a built in inherent API to parallelize and distribute the load but it's not as easy as you think so people start coming off and figuring out hey is there a framework that can help us do some distributed workloads on test though in a more easier fashion and we'll talk about that in a second and lastly capacity provisioning is not an easy thing when it comes to those kind of things think about this you need I don't know 20 P 316 excels and not only you need that that much you need them running in a single placement group because you need low latency and high performance network because typically what happens in a distributed job there's a lot of parameter exchanges between those nodes and so the efficiency of the job can easily be bottlenecks if the network performance is not meeting the needs of the internal computing so how do you provision such capacity well you have AWS but then how do you maintain that capacity and can traditional auto scalars really hold on to that capacity in a way that's going to work well for GPU workloads and ml workloads and we'll talk about that very very soon so one by one in terms of ml practitioners and the abstraction we already discussed this we have cue flow on AWS the setup script will just completely abstract the whole thing you will basically follow a simple list of commands run it and you will get a in standard endpoint of queue flow ready to go on the large data sets that does require copying but here's the good news first off we now have a driver for for example fsx luster how many of you know what fsx lustre is in a show of hands so fsx luster is a distributed file system which is meant to be high-performance but still shared across multiple nodes so it's a read many a read write many notion that a lot of nodes and a lot of processes a lot of pods can actually access the same file system and typically provide millions of AI ops to the typical average workload so with that file filesystem a lot of these workloads have started to leverage significantly fsx lustre as their go-to platform to have that stage data so the process only copies the data once to an fsx lustre and not only that it can actually create that and provision that for you and it can do so dynamically and dynamically meaning that by using the API it will actually generate a persistent volume in a persistent volume claim that you can then use within your Cuba native code to access that file system or you can create it statically by just creating the file system first and then handing over the file system ID to the underlying code to be able to leverage but in either way NFS X luster has a very interesting property that it can actually shadow an s3 bucket so you don't actually need to copy any of the data yourself what you need to do is you need to have the customer or yourself as a practitioner put all the staging data in s3 which it already is and then use that as a path for the new fsx lustre filesystem to kind of cache all the data from that location so all the staging of the data is actually done behind the scenes by fsx lustre itself and by giving you the integrated CSI driver that will also get provisioned seamlessly for you through kubernetes as far as the distributed training there is a framework called hor Avadh that framework was originated by uber and was open sourced we're finding that framework to be extremely popular across our customers that are leveraging machine learning at that scale and that's actually accessible to you today through our cue flow itself so it does nothing more than actually simplify the process of distributing the job in the tensor flow across multiple GPUs or across multiple nodes or both and finally from a capacity provisioning we introduced updates to a project called escalator which is a project that's providing you with auto scaling capabilities that are more tuned to Xin learning and batch workloads then the traditional kubernetes autoscaler or not' autoscale that you all known well so at that point I want to introduce you to Mike Mike root who I introduced in the beginning so he's gonna walk you over escalator and some of the improvements we made there and show you a demo of how that actually works today with the cluster [Applause] okay all right hi I would like to walk you through a demo of escalator so escalator is a cluster autoscaler that was built by Atlassian it's an alternative to the kubernetes cluster autoscaler and we're really excited about it because it has the potential to really help with machine learning workloads it was designed to be a batch autoscaler and and to try not to move pods around it's designed to grow your cluster as quickly as possible and try to keep it stable while your workload runs to completion the toothed features i'd like to demo for you today one is already released in upstream and one is coming soon the first is integration with AWS fleet AWS fleet is an API that can be used to acquire capacity very quickly and and is particularly useful when you're using very popular instance classes particularly like p3 classes which are the big GPU instances that a lot of people use for training before we get started let me just let me just show you a little bit about what we're going to be working with here it's gonna be a lot of black text on this or a lot of white text on the screen the ants of AWS console I've got a basic eks cluster and I've got three node groups in this cluster one node group is completely uninteresting that's our t3 large that's just going to be running escalator for us I have a c5 x-large node group which is going to be used to run a CPU intensive job and I have a p38 X large node group that is going to be run to you going to be used to run a GPU intensive job I started each node group out with a single instance this is not because escalator can't start from zero it's more that when escalator starts a node group from zero it first much must launch one instance wait for it to warm up and then scale the cluster out so it's just gonna be really boring if I if I make you wait through all of that so let's take a look at our demo so we have we have just a basic a degree a basic qks cluster a tourist notice our CNI but the the other interesting part here is the Nvidia device plug-in demon set what this demon set does is it runs on it can run on any instance type but if you have GPU accelerated instances it will tag your instances with GPU capacity so that you can schedule your GPUs as resources just like CPU so I'm going to run that everywhere on my cluster let's take a quick look at what a escalator config looks like so I've got two auto scaling groups and these model the auto scaling groups I have in the console one has a upper limit of 15 nodes that's our c5 group one has an upper limit of two nodes that's our p3 group I'm not going to go super crazy on acquiring a ton at p3 capacity just because these are really popular instances and I don't want to limit that for anybody so let's let's deploy this then there is we're gonna take a look at this this is just the full version so I'm not going to step through that so let's deploy escalator and we'll see that escalator has launched on our t3 instance and let's just take a look at the logs we've got here yeah there we go so escalator has because we've configured it for a node groups you can see that it knows all about them it knows our upper and lower bounds and because the cluster is in steady state there's nothing to be scheduled so no no demands on CPU or memory and this is going to pull your auto-scaling groups about I've got it configured for once every 30 seconds but by default you can change by default at 60 seconds and you can change that so let's take a let's let's deploy a workload on here I've got a job that's mostly designed to just kind of sit and pin down some cores just to just to give our cluster a time to to give our cluster time to scale the important thing to note about this is it requires one CPU and a gig of ram that's probably a little excessive but it just gives us something to to show on our cluster here so I'm going apply this and that job will create a bunch of pods and those pods are going to sit in pending state and if we look back at escalator you can see that it has it has detected that that there is a lot of work coming into the cluster and that it should be scaling up the cluster so if we take a look at our AWS console you'll see that it is acquired and it already has running 15 new nodes it's done this using the fleet API the nice thing about the fleet API is when you request a set of nodes from it the way escalator uses its API they will either all be immediately fulfilled to you or you will be not denied those nodes so there's no more dial up the auto scaling group let capacity trickle in you're definitely going to get it if that API call succeeds and if it does an escalator will continue to retry behind the scenes at its normal polling interval so those are all coming online in my cluster we should be able to see see those nodes and yeah so those are gonna those are kind of going to kind of trickle or those are gonna kind of come online and join the cluster they've just booting up and doing the normal bootstrapping work letting the CNI come online so well I think that finish those are gonna retire our work pretty quickly let's take a look then wow that's going at a GPU job so this is the same job we've got all it does is pin down a a core but it also now has limits around GPUs each one of these jobs is going to try to hold down a single GPU so I've started my cluster out with a p38 x-large which has four GPU cores and I'm going to try to run two of these at first and so those pods are going to get running and you can see though you'll you'll see them get scheduled on the existing GPU and then if we take a look at our other Pods though we should see that our our PI job got done and escalator will begin to scale down the auto scaling group past that the first thing it's going to do is it's going to taint the nodes to allow the workloads to finish and drain off the nodes and then it will batch terminate them I'm not gonna make you sit up here and wait for it but you'll basically see if we were to wait for it you'd see all of those 15 running nodes now that they have no pending pods all the pods that were scheduled on to them are completed we'll just be terminated and your auto scaling group will go back to the minimum capacity bounds there we go and that's those are the two changes so we've got GPU based cluster auto scaling which will be committed to the upstream project hopefully pretty soon within the next week and we've got the fleet instant capacity acquisition which is already in the project and should be released very soon and then turn it back to you you need so let's talk about inference so we talked about the fact that we require large-scale jobs so on the training side that boils down to distributed training jobs multi-seat multi-gpu multi node what does that boil down into when it comes to workloads or inference workloads so in autonomous vehicles specifically there is no real inference taking place in the cloud the inference will eventually take place in the car but in order to build or simulate an environment which is as close as possible to the landscape that the sensors in the car are seeing there needs to be a lot of simulations going on continuously testing out the models in like simulated conditions under under what we call like an artificially generated vision environment which is as close as what we will get to what the car will actually seen on the road so those are what we call the large-scale simulation workloads and I want to give you a little bit of sense of like what that scale looks like and and those all cpu-based work those those are not requiring GPUs but they are running at a very large scale so I want to talk to you about koalas we always used that example because koala was a species that started to decline back in the 70s so it's declined about 17% and the new University in Australia was very worried about how we can actually impact that survival rate and what they done is they actually conducted a research that examines first few factors of how the habitat barriers and the climate conditions can actually affect what we call the DNA mutation or the evolution and the genetic diversity of that species so in order to run that experiment and that was just a few years ago they had to sequence I think about three billion base pairs of their gene genome and that requires a lot of compute so they used AWS and ran the whole thing on spot instances and so that thing was running on about three million cores overall averaging in about five hundred to a thousand cores in parallel in each step of the genome sequencing so that was a pretty amazing and and you know ambitious project at the time and now I want to show you what the typical AV simulation workloads would require you know in terms of compute compared to koala genomeics so for those of you with a 20\/20 vision there isn't short yellow line somewhere on the lower bar next to the word koala genomics which you can barely see because that is the scale that our customers are now starting to run in autonomous vehicles and that is the amount of compute and the concurrent CPUs that they would require and similarly on the total requirement for the job that's also running at a pretty different scale than what we seen with that previous example that I showed now I'm talking about autonomous vehicles but this is starting to emerge in many many other industries so if you look at pharma pharmaceutical companies robotics financial institutions so a lot of those industries and of course manufacturing in robotics a lot of those industries are now starting to leverage machine learning at scales in order to solve a lot of those industry specific problems that they have so tana most vehicles is actually one of the early ones that have reached those kind of scales but this is definitely not the last ones we are working with a lot of those industries to get them ramped up onto pretty much a lot of similar and common challenges and actually involving a lot of the common models that these deep learning I it's important to say it's not just traditional machine learning those are deep learning algorithms which usually involve what we call convolutional Network cnn's or dns and that that's the reason why they are more complicated require more computing power and require those scales that we have just seen so just to give you a sense if I'm looking at here these are the top 10 supercomputers on the planet as of today and we're trying to measure that with what we call the amount of peda flops the floating-point operations that they can process so on the bottom of that we're at about 20 or 27 pedo flops so just to give you a sense a single p 3d and instance today at AWS can produce one petaflop so if you take a network or a fleet of 20 p 3d n instances you have actually done or achieved what we call supercomputer scale in terms of processing and this is this is basically where we want to be we want to be able to provide our v customers and not just AV you can replace a V with any other industry a supercomputer scale like one of the depth or top 10 supercomputers that are available today for those kind of projects so going back to the specifics there are a lot of challenges involved today with setting up containers for ml so how does containers fit into all that story so think about this if you're a practitioner and you need to run in tensorflow on of our instances how would you go about do that are you going to install your own Python are you going to customize your tensorflow version so typically up until recently we would provide what we call the deep learning ami which is our machine images you would customize that ami install whatever frameworks you want on top of it that could be a little bit tricky and therefore a little bit complex to some of the data scientists that have kind of told us that they don't want to be in the business of customizing Amazon ami is it's not there for DES so what we have done is we actually launched what we call deep learning containers in March so deep learning containers are nothing more than a predefined set of containers that are coming pre bundled with the frameworks that you require in order to run your machine learning workloads so basically think a bit of it as a permutation of running your favorite framework like MX net and tensorflow along with some characteristics that are more tuned to training or for inference depending on what you want to run more tuned to GPU vs. CPU and also with your favorite Python version whether that's two point seven or three point six so those permutations are distributed as specific containers that you can just take and inherit your base container from and then add your custom code in order to run that so that's available to you today through the marketplace and an ECR our container registry so pretty much that's it for me to call outs one is if you want to get started with machine learning on AWS this is kind of your I would say the simple getting started experience we also have blogs that are more complicated complicated for the more advanced users but I would go if I would get it started with this experience I would get to that link first check out the documentation try out all the startup scripts and and make myself comfortable with the environment secondly I am very well interested if any of you is running machine learning workloads today whether it's similar to the way I just described on kubernetes or using some other framework whether it's Amazon or another vendor any way you want I'm very very interested to talk to you and learn from you on how you run today and what are the challenges that we can help solve for you in the future so I'll be around here until the end of the day if you guys want you can come find me thank you so much [Applause]"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Kubernetes for Scaling ML Services",
        "Video Title":"Kubernetes Explained in 6 Minutes | k8s Architecture",
        "URL":"https:\/\/www.youtube.com\/watch?v=TlHvYWVUZyc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/TlHvYWVUZyc\/hqdefault.jpg",
        "ID":"TlHvYWVUZyc",
        "Publish Time":"2023-01-11T16:41:34Z",
        "Channel":"ByteByteGo",
        "Channel ID":"UCZgt6AzoyjslHTC9dz0UoTw",
        "Transcript":""
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Kubernetes for Scaling ML Services",
        "Video Title":"AWS re:Invent 2020: Scaling MLOps on Kubernetes with Amazon SageMaker Operators",
        "URL":"https:\/\/www.youtube.com\/watch?v=VgalY-kbWag",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/VgalY-kbWag\/hqdefault.jpg",
        "ID":"VgalY-kbWag",
        "Publish Time":"2021-02-05T18:46:40Z",
        "Channel":"AWS Events",
        "Channel ID":"UCdoadna9HFHsxXWhafhNvKw",
        "Transcript":"Good morning, good afternoon and good evening to wherever part of the world that you may be. Thank you for tuning in today to today's session of Scaling MLOps on Kubernetes with Amazon SageMaker Operators. Regardless of where you are, 2020 definitely has been a trying time. If there's any way that we at AWS can help, please feel free to reach out to your account manager or to anybody on our team. We really love seeing how machine learning can better impact the world. Today's session will be led by myself; I am a Senior Product Manager on the SageMaker, Oregon specifically. I lead our open source integration and products. I am joined by my friend Srivathsan and I will let him introduce himself. Hello, everyone. My name is Srivathsan, and I'm part of the engineering team behind Intuit's ML Platform. In today's session, we'll be talking about how Intuit is leveraging machine learning across all of their different product areas. We'll talk then about MLOps idea of taking models that you may have been building in an experimentation state or local and taking it into production. Next, we'll be talking about open source tools, integrate with the SageMaker machine learning platform, this managed service ecosystem on AWS. And then next, we'll talk about how you can integrate your own custom internal tools that are built off open source technologies with SageMaker. And then finally, we'll be doing a demo and a wrap up of this entire session. Thank you for the introduction, Alex. To give a brief overview of Intuit, we are the makers of finance and accounting software such as TurboTax, QuickBooks, and Mint. Our products serve the financial needs of consumers, small businesses and those who are self-employed. Intuit's mission is to power prosperity around the world with an AI driven expert platform. And that mission has never been more important than now. A lot of experiences of Intuit products are driven by ML. For example, ML powers the search bar on the TurboTax website which is used by millions of people to file their taxes. Intuit ML also enables small and medium sized businesses to better predict their cash flow. And we have helped many entrepreneurs plan effectively during COVID-19. And this is something we're extremely proud of. Intuit has also been a leader in ML using SageMaker, we have been leveraging SageMaker for machine learning predictions since an early private beta. We have also tested nearly every feature of SageMaker from the beta time onwards. We have showcased our ML platform at various conferences and forums such as Kubecon, OpML, and other ML meetups. We are also the creators of the open source, Argo that is used for data pipelines and ML orchestration. And Intuit uses an entirely Kubernetes native ecosystem across its various services and products. The question that many customers are asking me now is what is MLOps? MLOps is the process of taking a model developed in a notebook or local environment and efficiently managing the deployment into production. It's one thing if you have a lot of models at your disposal that are trained in local notebooks. But unless it can be served on an endpoint efficiently, gracefully in terms of being able to A\/B test and rollout with efficacy without impacting users, all these models cannot be used for serving the core business. So, idea of MLOps really is the process of deploying models into production. And for this, Amazon has built a product called the SageMaker Operators. The SageMaker Operators allows customers in Kubernetes environments to access SageMaker for all of the infrastructure and DevOps without having to provision any of these resources on your own. And specifically, you can tie this in to your Kubernetes environment following a GitOps pattern and approach by creating custom resource definitions, also known as CRDs, that allow you to deploy models, train models, do batch processing jobs, stream and view logs, and then also any of the other features available in SageMaker, directly from your native Kubernetes interface. And what this ultimately allows you to do is, you can submit the same job over a YAML file, and take advantage of the full cluster of Amazon SageMaker. And it's attached into your Kubernetes environment natively without any major changes to your code. And the benefit of leveraging a tool like SageMaker is you can get your CPUs, your GPUs, or custom chips like Inferentia on AWS, while also having the benefits of most of the major tools in open source as a managed service. All the different tools that you might see and opensource today, SageMaker has a version of that, that allows you to take advantage of our managed infrastructure under the hood. So, for a side by side comparison, you can see different steps within the model lifecycle. Specifically, here, you can see, at the very top, for something like feature processing and feature loading, you have the Spark Operator and opensource. And within SageMaker, you have a tool called SageMaker Processing that has the latest Spark container available where you can do real-time transformation of your underlying tabular data and prepare it for machine learning training. And when you leverage SageMaker for your machine learning processes, the experience for a developer is actually pretty similar. On the left-hand side, you can see a coop flow serving operator. It allows you to define the name of the model, the storage location, and everything below that in terms of the instance types, the sizes, the replica sets are handled on a Kubernetes level, or also at the overall cluster level. By leveraging SageMaker Operators, you can do the exact same deployment of a model, except you can configure this all natively from within your operator itself, and the custom resource definition that you submit. So specifically, here, you can see that we're deploying on the US East two region with a single variant in terms of the model traffic type. And we're pulling in a model that is from S3, that we named XGBoost model. And we also attach an ARN, an ARN for the permissions that the model is allowed to serve with on the instance endpoint. We even include different tools like Auto Scaling Endpoints, a common feature if you use EC2 regularly. And you can auto scale your endpoints similar to a replica set, except with even more fine-tuned granularity. So once again, on the left there's attaching a auto scaling for the KFServing operator, and on the right, you can see that we're attaching an auto scaling policy to the same endpoint that we showed in the previous slide, you can scale out the capacity in terms of number of instances, you can define the custom metrics. And also, you can change around the cool down in terms of how quickly the instance scales up and scales down. What this allows you to do end to end from the SageMaker Operators is move a majority of your DevOps work within your machine learning platform, while also be able to take advantage of the many different features that you get in SageMaker. For instance, if you leverage SageMaker Operators into your ML platform that's built on top of Kubernetes, you can use things like Model Monitoring, which allows you to baseline training data using SageMaker. And then from there, you can sample new inference jobs that are applied against your endpoint. And whenever the values deviate statistically away from the expected norms, you can kick off an alarm through CloudWatch, which then can invoke a Lambda job, and then you can re-train the entire model. And all of that lineage can be tracked through SageMaker Experiment. And by leveraging SageMaker and opensource, you get the benefits of both worlds. From SageMaker you get the fully managed infrastructure tools like Managed Spot Training, scalable inference endpoints, automatic model tuning in hyper parameter optimization, model monitoring and things like first party algorithms that allow you to use common models like XGBoost natively within SageMaker, while also the same time you get the benefits of Kubernetes, which allow you to do things like hybrid infrastructure, both on the cloud and on premise. You can also reuse existing infrastructure. For instance, if you have a Kubernetes cluster serving your applications, you can tie that in with SageMaker and have ML capabilities without any additional work. You can also get full portability and the composability from the standard YAML configuration that is pervasive in GitOps. And I'm going to hand this now over to Srivathsan to share more about how Intuit does this within their ecosystem. Thank you for this great introduction, Alex. I cannot wait to use the capabilities provided by the SageMaker Operator and scale Intuit's Machine Learning Platform. What is scaling MLOps for Intuit mean? Currently, there are over 400 models that are actively used in production, and with many of them getting trained every day. Intuit's features hosts over 8000 features. Collecting features from the diverse sources of data at Intuit is the responsibility of the streaming and batch feature processors, which totally collect a staggering 15 billion plus features everyday from Intuit products. We now want to take this platform to the next level to power more prosperity for our customers. As we think about MLOps, a question that keeps coming to our mind is how can ML practitioners focus on their craft, which is data science, and not worry about the MLOps part of it which is under the line. Alex showed an elegant way by which the operators can enable ML practitioners to access the managed service natively with Kubernetes YAML based declarative specifications. As an ML platform, we want to simplify that experience further. How can we reduce the YAML configuration so that our data scientists have to provide just enough information, no more and no less? In an enterprise, there are many challenges that data scientists have to overcome. And without an robust MLOps, it just becomes difficult. Some of these are around cloud infrastructure management, Enterprise Security and Compliance, observability and building clean interfaces, be it Web, API, SDK or CLIs. How can we simplify all of these to provide an elegant MLOps experience? This is where Intuit has built a full-fledged machine learning platform. This platform automates and abstracts the hard stuff from our data scientist. The platform offers capabilities for the entire ML lifecycle, starting with feature processing, storing these features in the feature store, training on these features, evaluating the train models, inferring in real-time and batch versions, and providing model efficacy feedback and feeding that back into the lifecycle so that the next version of the model can be much better than the previous version of the model. The ML platform has provided significant benefits for enabling MLOps at Intuit, and is a driving force for accelerating the AI vision for the company. On the flip side, the technology's landscape that is stitched together in order to achieve this surfaces multiple technical problems for the platform. How do we do consistent authen [INDISCERNIBLE 00:23:35]? How do we do clean rules and handoff between rules across these multiple systems? Compliance and security has to be uniformly applied across all of these systems and how do we enable that? New features and capabilities that SageMaker enables need custom development in order to enable them in front of our customers. And how do we make that simpler? Is there a better way to do this? Can we tie these all together? We looked around and we realized that Intuit has a heavy investment in Kubernetes with our Kubernetes service and Argo workflows and Argo CD ecosystem. We also use Kubernetes operators for various things. For example, we use the Argo operator extensively in our training and batch pipelines. We use the SageMaker Operator and the Spark Operator for various data processing. Can this ecosystem be better when they are all tied together? That's why we are starting off with Kubernetes native MLOps. Using the Kubernetes operator-based approach, the platform becomes much simpler, and it enables us to accelerate AI at Intuit to even greater heights, we are able to produce a single reusable control plane based on Kubernetes. And abstract out platform concerns such as security compliance, cost efficiencies, so that they can be built once and reuse across the platform. With this approach, the time to market for new features and capabilities offered by downstream systems be it SageMaker or Argo or Spark is extremely quick and efficient as operators go through their upgrade cycle. And the benefits are realized by our customers. To go down a couple of levels, this is how the platform declaratively expresses workflows that are easily composable. What you're seeing here is a snippet of an Argo workflow that is used for training one of our models. The Kubernetes programming model allows for elegant implementation of the platform which constructs such as mutating and validating web hooks, custom operators etc. We are on a journey of reworking the platform to work with the Kubernetes programming model. And we will see a demo of how this will work later in this talk. Customers interface with native Kubernetes manifests corresponding to the operators be it SageMaker, Spark or Argo. In this picture, we are seeing a native SageMaker CRD of a hosting deployment. This allows for a lot of flexibility for our customers while the platform does its work in the background. Let's now see a demo of how Intuit's ML platform ties all of these pieces together with a simple example. In this demo, I'm going to show how Intuit ML platform leverages Kubernetes and AWS SageMaker Operators for managing model life cycles in a Kubernetes first manner. What do these terms mean? A look at first principles on which the ML platform is built. The platform is designed to allow model developers to focus on their craft. Rather than having to worry about the MLOps part of it. There's a clear separation of concerns between platform and developer. The platform leverages Kubernetes operator objects natively, and for all runtime configurations of the model and it also provides a GitOps foundation, so we can provide transparency and operational control to the data scientists and [INDISCERNIBLE 00:27:35]. In a Kubernetes first world, the role of an ML platform is extremely important. Because this platform now provides capabilities and addresses platform concerns such as cloud resource management, VPCs, and network controls, security, logging, monitoring, and providing a robust CI\/CD platform. I'm going to walk you through a model code repository. This repository contains the code of an ML model that predicts the odds of credit card approval. This model is based on tensor flow, and is representative of a real-world model that Intuit uses in its product. This model we trained using the latest available data sets. The code for this model is under the folder called credit card approval. And here you can see that it has three files on it. There's batch prediction, there's online prediction, and there's a training code. All of this is the code that the data scientists work on and they are focused on. Let's look at the training code. This is the standard tensor flow model that the data scientists have written a code and also an algorithm to go and predict the success rate for a credit card approval. Other than that, this repo has a few other files that tie it into the ML platform. Of interest is the Jenkins file. This Jenkins file provides a robust CI\/CD mechanism for running, taking this model from code and putting it in a fully operational environment. So, this CI\/CD mechanism, make sure that the Docker files are secure and all the quality gates have passed successfully. This is a sample run of the CI\/CD mechanism. Going back to the code, we want the data scientists to focus on these three files and make sure that they are writing the best algorithms that can solve our customer's problem in the best possible way. Everything else is taken care of by the platform. What do we mean by this? We also said that we want to separate the platform concerns from the developer concerns. This is what the developer, the data scientist looks at and is operating on a daily basis. The platform concerns are now separated into a separate repo called the config repo. Here is an example. The config repo contains files that declare how the training should be run, how the hosting should be run, and how all of these should get tied together with the pipeline. Let's look at how the training YAML is defined and let's go into some details here. The YAML file here is a native Kubernetes CRD as defined by AWS SageMaker, the kind is a training job. This file allows our data scientist to specify attributes for deploying the model to production, for instance, the resource config, how large should an instance be? How many of those instances should be there, and how large should the size of the volume that is attached to the instance be? It also specifies other things such as the data sources, from where to read from and where to write to the training output. There are other things such as VPCs, network configurations, and security groups, which are missing from this picture. If you went to normal SageMaker, you would have to provide all of those details, but they are not here. The reason they're not here is because the platform takes ownership of those. And we will look at how that is applied in a little bit. Let's now walk through, what if a data scientist needs to make a change? For instance, I now want to change the size of my instances, so that I can run my training faster. I'm going to make this change directly on GitHub, and commit the changes. This then triggers a web hook. This web hook is how GitHub communicates to the platform to say that, \"hey, something has changed in the model, code and config, and we need to take care of that.\" So, as soon as this code is committed, this gets picked up by our web hook, which then triggers an Argo workflow. And here is a ping that the platform has sent on to our Slack saying, \"hey, I have to start a training because the training configuration has changed.\" We use Argo extensively in the platform for training and batch inference jobs. Here you can see that the training for this model has already started. And this pipeline will now invoke SageMaker and start running the training job in SageMaker. As you can see, the training job has already started. And this job would have provided SageMaker with all the configurations that the developer provided. Here is that training job in progress on SageMaker. Let's now look at a previously completed job, so we can see what are all the capabilities that the platform has to provide. This is an example of a job that previously completed and as you can see here, there are a bunch of configurations that were not added by the data scientist but rather added by the platform. For example, what role should this training run as? What is the volume encryption key that needs to be used? These are details that the data scientist or the ML practitioner does not know and does not have to worry about because the platform enforces it in an opinionated manner. There are other aspects such as the network VPC, there are other aspects such as the network isolation and the subnets and the security groups that the training has to run on, which are also not specified by the ML practitioner in their configuration, but is rather added on by the platform. In this case, do we want network isolation for this training, so that it does not have the ability to communicate externally to other data sources? And we've said yes, we want network isolation for this training run. This is how the ML platform leverages SageMaker Operators, and enables MLOps for data scientists in a very easy manner. The other important aspect of MLOps is logging and monitoring. How do we make that simple? The training run that started has now sent these logs in a streaming fashion to Splunk. And we introduced a Splunk as its internal logging mechanism. And this is something that data scientists are used to. And they can now go and look at what is my loss value? What is my recall? What is my precision? And so on, in a streaming manner, on a live stream coming from the SageMaker training job that's running there. This is another way how the platform integrates tightly with the existing monitoring and logging ecosystem, and ties it with the SageMaker Operators. To recap, the config repo of the model has the hosting, the training, and the pipeline YAML files which are declared by the data scientists, and they can now stitch together an end to end pipeline, coming from featurization to training, to hosting the model. All in a seamless declarative fashion that is GitOps driven, so that you have full transparency and visibility into what's going on under the hood while providing the right separation of concerns between platform and the data scientists only has to focus on this, which is their code repo and write fantastic models. Thank you. In summary, the Intuit ML platform is built towards enabling GitOps practices in a cloud native pattern. And we want to enhance these by using Kubernetes native operator patterns so that we can then build a model dashboard, which is one place for everything to do control actions, status logs, and metrics; all of these in one place so that a customer does not have to jump through multiple tools in order to understand what's the operational state of a model. This also allows for easy and maintainability. Last but not the least, we do have an aspiration to open source parts of the Intuit ML platform so that the wider community can benefit from the work that we have done here at Intuit, integrating it with SageMaker. Thank you Srivathsan for that amazing demo. It's really exciting to see what Intuit has been building on top of our SageMaker Operators. We want to close out this presentation by really acknowledging many of the partners at Intuit who were not on this presentation, and also the engineers, the marketing teams, and the many product managers who helped make this product and also session possible. And with that, thank you so much for tuning in to our session on leveraging SageMaker Operators to build machine learning platforms. We want to really thank you for your time and for tuning into your really valuable sessions that are here now during re:Invent. And please do fill out our survey. This helps us improve our content and ensure that we are really showcasing tools and ideas that can help improve our customers ability to do machine learning at scale."
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Cloud Deployment for ML Models",
        "Video Title":"Tutorial 6 :Deployment of Machine Learning Models in Google Cloud Platform",
        "URL":"https:\/\/www.youtube.com\/watch?v=xcODUk0o6tU",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/xcODUk0o6tU\/hqdefault.jpg",
        "ID":"xcODUk0o6tU",
        "Publish Time":"2020-01-12T11:24:31Z",
        "Channel":"Krish Naik",
        "Channel ID":"UCNU_lfiiWBdtULKOw6X0Dig",
        "Transcript":""
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Cloud Deployment for ML Models",
        "Video Title":"How to Deploy a Machine Learning Model to Google Cloud for 20% Software Engineers (CS329s tutorial)",
        "URL":"https:\/\/www.youtube.com\/watch?v=fw6NMQrYc6w",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/fw6NMQrYc6w\/hqdefault.jpg",
        "ID":"fw6NMQrYc6w",
        "Publish Time":"2021-02-14T06:21:38Z",
        "Channel":"Daniel Bourke",
        "Channel ID":"UCr8O8l5cCX85Oem1d18EezQ",
        "Transcript":"have you ever deployed a machine learning model maybe you have maybe you haven't but the exciting thing is if you watch through this entire tutorial by the end of it you'll have done so you'll have deployed a machine learning model to google cloud you will need access to google cloud to do this you don't necessarily have to use google cloud but that's just what this tutorial is focused on this is a tutorial that i gave on machine learning deployment i recorded the whole thing for stanford's cs329s or machine learning system design but that's all i'm going to say for now because i'm just going to let you get straight into it all the materials you need will be in the links below if you have any questions leave a comment or an issue on the github repo that i link and i'll answer them there but happy machine learning this is exciting you get ready i've got a little presentation ready to go after we um sort of just had that mix up but we won't let anyone know that happened so i can you're right i can start uh first of all hello everyone i'll uh i'll start sharing my screen if you want and then we go over here share is that working i'm still a zoom novice by the way and we're doing this across time zones so uh this is like six time zones so i'm in the future to everyone watching this i believe so uh let me know if you can uh see what's going on i need to figure out how to get that chat back we'll figure that out later maybe okay there we go chat all right so this is the beginning can everyone see this is this all good to go yeah all right beautiful so let's get started so this tutorial is going to be on deploying machine learning models mostly to kind of take away this element of uh unknowingness of what it's like to deploy a machine learning model because oftentimes you'll see step one is build the model and just this beautiful little outline here and then step two of course you just gotta deploy it right but as you can see drawing the owl there's a few steps to connect these so this is what we're gonna do we're gonna take away the veil and we're gonna make deploying machine learning models just like building the actual machine learning model so i love this tweet from chip um machine learning engineering is 10 10 machine learning and 90 engineering you know it's right because elon agreed with it elon thank you for taking doge to the moon and all of us to mars later on so i broke this down into a little bar here you can see ml engineering 90 engineering ml but there's something you should know about me is that i'm probably 20 engineering and 80 ml so i used to be about 10 engineering or maybe 0 engineering and completely ml but i'm slowly learning these skills so as much as i'm teaching this deployment lecture now i'm still learning it as well so this is a little bit more about me the video game character i look most like is ps1 malfoy so shout out to 2004 harry potter i used to drive uber to pay for my studies i had a 4.9 for uber driver rating so i'm very proud of that and small apps small machine learning powered apps i can help you with um but large apps not quite yet so probably ask me in about six months once i've had some actual practice doing all the stuff that we're going to go through in this tutorial [Music] in 2015 i graduated with a food science and nutrition degree so that's kind of however you want to say my background but then i decided you know what i want to learn to code so it was actually about the fourth time i decided to learn to code and when i started to learn the code it was about 2017 peak ml hype so that's all i could find on the internet i decided i want to get involved with this and i created my own artificial intelligence master's degree because i thought i'd spent enough time at university in the past decade so i'm like i'm just going to learn online after doing that for about nine months i had a machine learning consultancy role at a brisbane ai startup called max kelson we worked on a range of different industries um from the health sector to retail to uh insurance you name it whole bunch of machine learning projects that was kind of like a trial by fire of my machine learning skills however a lot of the time we would just work on proof of concepts so we'd do the machine learning stuff with their data but when it came to the deployment phase that was still a little bit uh touchy feely so we haven't quite um that's what i'm learning at the moment and my two favorite activities in life are fighting and writing writing of course words and code words are beautiful because it's such an open space you can write whatever you want who knows what's going to come out but code typically you write code and the same thing happens every time um so that's why i like writing codes and words and if you want to see what i'm up to now you can go to my website mrdebert.com now there's a whole bunch of stuff on there so that's enough about me if you want to get all the stuff that we're about to go through in this tutorial you can go to this link it will take you to this github spoiler alert the readme is still a work in progress we're going to go through it anyway so don't worry about the readme the code should work so fingers crossed um so that's where all the stuff for this tutorial will be um the slides aren't up there yet but i'll just i'll add it to it um yeah we've got the stuff that we need is there so if you want to follow along go to that link and we'll uh have a fun time there i don't actually know what slide is next oh yeah there's my disclaimer the readme is still a work in progress i'll fix it after this tutorial and so this is what we are working towards food vision uh we're building an application let's say you were a machine learning practitioner but you've got a background in food science and you want to educate the world on food you can kind of guess where this inspiration came from but we're going to build an app called food vision and this is our deployment recipe so this is this is more this is like a machine learning deployment tutorial but it's also like a cooking show so the ingredients we need data 10 food classes from the food 101 data set so if you want to just look it up food 101 this is the data set that we're using public data set it's available on kaggle i've just downloaded it and formatted it to the way we need it but that's food 101 we're going to build oh i spell tensorflow wrong every single time i type it out so for some reason my fingers like to type the n before the s who knows why um you need a tensorflow pie torch model and by the way actually i'm going to get to this in a point we're going to cover a lot of ground in a short amount of time so if you have any questions i actually need to get this chat open so please feel free to ask any questions how do i see this chat i want to get this open oh chat more there we go okay beautiful okay i got it thank you very much so there we go okay learning on the fly oh spoiler alert and then we have a bunch of python scripts it's really only actually two um and then a make file slash a dockerfile um we'll go through these as we go then the utensils we're going to use is streamlit so to build our app we're going to use streamlit so streamlet is an amazing little tool to build not a little tool actually they've got a whole bunch of they've got millions of dollars of funding so they're not little anymore but yeah this is a great way to build data apps look at this you can just see the demo so the app that we're going to build is in pure streamlit we're not going to go through this step by step because streamlet have great tutorials essentially you'll be able to replicate this app that i've built with streamlet if you just go through their their one hour introduction to streamlight tutorial so that's what streamlit is we will also be using google cloud to handle all of our deployment stamps so tools that you might need or you will need is the gcloud sdk so that's what you write in terminal gcloud command something something you will need an existing google cloud project and you will need access to google storage so you'll have to turn on all of these apis in google cloud you'll need access to google ai platform docker will be very helpful container registry app engine and i know i'm listing a lot of things you might have never heard of these before but we're going to to go through them as we go um and here's the method we're going to to go through on this cooking show we're going to get the app working locally so hopefully i can live up to my promise and saying everything in the github should work we're going to clone that we're going to deploy the model to ai platform i'll show you how i trained a model in a second and we're going to deploy the app to app engine step 4 who knows hopefully it works on app engine i mean this is a live like across six time zones deploying a machine learning model application i've never done this before it worked on the twitch stream hopefully it works on this stream number five you're going to see i've left all of or a lot of the errors in on purpose so that we can uh we can go through and solve them together because when you deploy machine learning models you're going to your model's eventually going to fail what do you do when that happens so that's um that's a very important point there and step six of course is profit so 10 minutes we're going to spend on the ingredients kind of doing that now utensils and method i think it's going to take about 40 to 50 minutes if not this is a this is a cooking show as i said so of course i've prepared things earlier we're not actually going to have to wait for things to to to bake and put in the oven etc outcome so this is what we should finish with you can't see the url there but that is this app here so this is my google cloud project hosted on appspot.com welcome to food vision identify what's in your food photos and so this is what we're going to end with this is our if i uploaded an image here i'm not going to reveal it just yet because i want to save this when we actually do it this should read the image ping google cloud with our hosted machine learning model and then our it's going to return back in here what it predicts and as you might have guessed it works on a certain number of food classes of course this is just an mvp app so it's only 10 classes out of the food 101 data set all right oh and of course the the end very goal is this little symbol you're going to come across this a lot but this is the main objective of building machine learning deployed mvp apps or in fact almost any machine learning powered feature or application is to build a data flywheel and so this is an idea i've stolen from josh tobin who had a talk and he discussed about the data flywheel and i've kind of put it in this slideshow as well so full credit to josh for that concept but um here's your big warning if you do any of the steps in this deployment tutorial if you don't have credits on google cloud this will cost you money so um i've spent i think it's 100 or something over the last week or so on google cloud but i've been very reckless with my spending i haven't shut anything down i've just just been going willy-nilly okay so just be aware if you use google cloud it costs money it's not like colab collab's free google cloud costs money um questions so ask questions please i've said this before whenever you feel like it i've definitely missed a whole bunch of stuff in here um so if something sticks out as confusing or whatever please i'll try to monit uh what should i call it monitor the chat as much as i can and as i said we're going to cover a lot of ground fairly quickly so please be like daniel you're going too fast um your australian accent you combine seven words into one let me know if anything needs uh clearing up because that'll also help me explain what's going on all right all right so thank you for that chip but here is our objective so this is building a data flywheel the holy grail of ml applications so we're going to get into the recipe soon but i just want you to know what we're working towards um so of course you're going to collect data verify and process data model it train a model deploy the model make predictions with that are they good or bad and the important point here is that your predictions are out in the wild now if they're bad find more of the bad samples or the hard samples that your model is having trouble with and add them back to your data collection and then repeat the whole process and ideally this is going to the whole idea of flywheel is that it keeps itself spinning now this concept of course you could draw these i mean there may be more steps here i've just tried to keep it simple but this concept is if you're like me and you learn to build machine learning models in notebooks right so this is the notebook that we we've trained a few models in this is on the github so don't worry about what's going on there i've kind of assumed that a lot of you have experience building machine learning models so we're going to kind of breeze over what's in there anyway but if you learn to code in a notebook you kind of miss this whole whole step or this whole process you might have only pre-processed data and then trained to model like this is me right so i've only learned these steps but i'm starting to learn the rest of the piece of the puzzle um in our case as well the data's already been collected for us it's food 101 so we've actually got these three steps but what we're going to do is go deploy model make predictions good or bad predictions out in the wild if they're bad find more and add to the collection so very hard to understand this as well when you've got no context so let me introduce you to tesla's data flywheel now this is from a talk by tesla called autonomy day so if you want to go and watch that actually probably one of my favorite talks on the entire internet on just ai and ml in general tesla probably the i don't know how would you say aside from apple google facebook or probably the like number one company deploying machine learning at scale and actual products so can i build this no i can't can i build smaller versions of this yes that's what we're going to work towards in this tutorial but if you imagine tesla's data source is their cars of course they start collecting data then they might find a problem hey our cars are doing really bad in tunnels and what's the signal for that that they're doing really bad in tunnels well tesla have a feature called autopilot which means that the car drives itself basically and their signal might be when does autopilot turn itself off in other words it doesn't know what the scenario is and so that's their signal maybe in a tunnel autopilot shuts off and so that that signal goes back to the tesla engineers and they're like wow we found a few scenarios where our autopilot system is shut off let's collect a whole bunch of other scenarios that are very similar to that instance where our model was having trouble with so in this case autopilot is the model and then they're going to re-label those scenarios so fix up whatever's going wrong in the tunnel we've got another six six uh problems here in a tunnel like wow okay let's um let's fix this up so maybe there's something wrong with the lighting or the lanes who knows but they fix that they add it back to their data set pass it back through a model redeploy that model and hopefully the whole thing just starts to make the data flywheel so that's a very brief overview of course if you want to see anything more i'd highly recommend watching uh tesla autonomy day i believe uh it's andre capathy talking about this section so yeah anyway now let's bring it into the concept of food visions data flywheel so this is what we're working towards so our data source is food 101 so 10 out of 101 classes and then we might find hmm well we're using 10 food classes but our model is predicting sushi for photos of donuts now apologies in advance working with food classes typically makes you hungry i speak out of experience and so our model is predicting sushi on the donut class and we're like well that's that's that's an issue so what do we do well we add more images of donuts back to our database and then we bring it back in here so now we've got 11 out of however many classes we want to work with we added in the donut class and then we retrain our model in our case when i trained a model i used efficient net b0 so just transfer learning um and then we redeploy and then so on and so on and so on until um we create food visions data flywheel so this is what we're working towards this is the the holy grail of food vision the holy grail ml apps so that's the beginning if there are any questions um let me know but we're up to the middle now i'll see what uh oh here we go here's what we're going to do deploy a model on cloud i've stolen this from chip slides so i think this you went through this in your last lecture but we're going to just go through this train the model i've used tensorflow of course you can use pi torch or whatever you want model.save model name export the model upload the model we need to store that this is google storage a lot of these symbols um you just this is from google so if you get used to using gcp you're going to all they they like using some cool symbols um upload a model to google storage and then we're going to deploy that model to ai platform so that means we'll be able to our model will be hosted somewhere on the internet somewhere on google servers and we'll be able to send we'll be like hey model here's this photo of food what do you think of it and then it'll be like send us back a tenser and then because we've created an app our app will make that look pretty even though it's just a model finding patterns and numbers and then we're going to deploy that app to app engine which will allow us to access it anywhere in the world so that's a very quick overview and here's the main concept i want to drive forward through today is model building is like model deployment sort of if you imagine you build a neural network architecture this is a computer vision model tiny vgg lego blocks of layers but model deployment is remember when you started learning deep learning machine learning all these layer names what the hell is a convolutional layer you just it just made no sense well it's similar with model deployment it's just there's a bunch of things out there that do certain things you just don't know the names of them um but once you start to piece the puzzles together it's like lego blocks of tools so if we we store our tensorflow model on google storage the model gets hosted on ai platform we built our app with streamlid uh app wrapped up in docker container the docker container is pushed to gcr gcr equals google's container registry gcr hosted container deployed to app engine app monitored with google monitoring again a lot of names here that we've just sort of thrown out but it was like the same when you learned machine learning a whole bunch of layer names what the hell's going on here now we've got self-attention layers etc etc same with all of the services on gcp and just as like you can build a model with almost any combination of layers you want you can do the same with model deployment this recipe that i've created here is one of of many so same as building a model there um wonderful so that was a massive ramble of um introduction to what we're actually going to go through how about we do it so this is where the demo starts let's uh let's see if we can work it so what i'm going to do first is get the app working locally now of course oops this zoom is popping up i don't want that i want this tab so my app is working locally so streamlit i've got this as i said professional cooking show here um we've prepared something earlier but so it's running here on localhost this is terminal nothing outlandish here but what we're going to do is get this we're going to replicate this um but as if you were coming to it from scratch so if i upload an image here of ice cream fingers crossed this should work this is going to ping my model which is stored on ai platform we will have a look at this in a moment i'm going to click predict and that's some delicious ice cream i think that might have been from new york there we go ice cream confidence 1.0 the confidence is the prediction probability i'll show you where this streamlit app is built of course all the codes in github i'm not going to step through this step by step but it is about 100 lines of streamline code [Music] and it's pretty rough right so you could it's not software engineering level code but it does the job and then utils.pi is another file that just includes some helper functions so like this one predict json we're going to see that later just a modified version of google cloud's demo code load and prep image this converts images into tensors and then logger we'll see what that is later but that's it three functions and about 100 lines of streamline code that could probably be reduced to 50 if i was better at coding now let's see how we get this running on your machine locally first and then we will deploy it to appspot so i'm going to come in here we're going to clone this github repo first so i've got a i'm going to deactivate conda actually because um i don't want conda so again we're going to this is where we're going to start to if we haven't already gone fast we're going to like really just we're going to hammer in a few different steps here i'm going to keep referring back to the keynote when i can to sort of discuss what we're doing got some pretty pictures coming up as well so yeah just uh bear with me while we run through this we're going to get clone and ask questions if you have anything so actually we need a working directory so i've kind of jumped the gun there let's um i'm going to cd into my code directory it can be whatever directory you want now i've got one called test which i like to just run tests in there clear so this directory has nothing in it and now what we're going to do is clone the repo so hopefully everything in the repo is formatted correctly it should be this worked on twitch yesterday so now we have um cs329s ml deployment tutorial if you'd like to do this you can do the exact same steps that i'm running now so we've cloned the repo that's all we've done now we're going to of course cd into there clear ls so we've got a bunch of things here not going to go through them all the readme is still a work in progress food images you can probably guess what's in there images it's just images to go along with the presentation the crux of what we need is inside the food vision app so that's the directory with all the stuff we want model training this is where i trained all the models so if you're interested in that it's just a an efficient net backbone trained on 10 classes of food 101 so i've been told you guys are familiar with uh model training so i'm kind of glazing over that a little bit here now what are we doing we're getting this running locally so we're going to cd into food vision clear ls there's a little file in here called requirements.txt which contains all of the packages that we need for these things to run and so what we have to do is we're going to create a virtual m because we are [Music] tidy software engineers and pip install virtual m there are a number of different virtual ends you can use to make i'm just using virtual m pick and choose oh great we have an error straight away that's right clear now we're going to create an m you can call this what you want so you could call it food vision virtual m food vision but i'm going to call mine just m oh well it's actually n now clear ls so my environment is called n not m and now we're going to activate n should be m then activate right so now we have n activated you can call this what you want i wanted to call mine n but i got to trigger happy with the enter key and so now we have an environment so now we can also install pip install r requirements.txt and so you probably you might have done this before i'm pretty sure you would have but this is just going to install the dependencies that we need what do we need we need requests we need tensorflow 2.4 again you can do the similar steps with um pytorch i'm just i'm just most familiar with tensorflow um we've got a question here so this is just installing some stuff i'll jump to the chat we've looked at some methods for manual identifying hard types of examples in class plus the homework in practice what's a good way to go from one hard example to a set of hard examples is this mostly manual qualitative right now so yes there are you're this is a great question because this is actually one of the biggest challenges in ml right now it's how do you identify what your model is not sure of in food vision we've kind of got a i'll show you what we're doing we're kind of using a hacky way but there are a lot of methods coming out now such as there was actually what's a recent paper by google it's one to estimate the the difficulty of i think it's only during training though so inference is quite hard and [Music] what's it called here we go track i n so these are the kind of papers i'm actually getting really excited about is a whole bunch of different methods to look at this february 5 2021. so this is this is real recent stuff a simple method to influence estimate training data influence so i'm not going to go quite through that you can read that on the google ai blog but so this is training data influence so again it says for training data but it's the synopsis of it is how do you know which sample is hardest for your model to learn now how do you apply that to what's in the test data set i'm not entirely sure that's one thing that i'm trying to learn myself there was something out of berkeley the other day that that was around the test data set stuff but yeah this is real recent research so that question is still being answered um where were we we're in terminal okay beautiful so we should have all of the dependencies installed um now what we're going to do is we have an app here which is based in streamlit which is just the code here again i'm not going to step through this too much um streamlit is is actually amazingly uh straightforward in terms of the things you'd like to build so [Music] yeah we're going to see exactly what's come out here we should be able to run this streamlit run app.pi all this is saying is hey streamlit run our app and again if you went through streamlets say introductory tutorial it takes all of an hour you'll be well across all of this so this should boot it up on our localhost it might go to 8502 because i've already got it running on 8501 may take a little while to kick off um because of um it's the first time we're running it it's a great question trenton thank you for that beautiful look at that live demo working so this is going to open automatically this is where we're going to run into our first error i told you i've kept all the errors in on purpose you might be saying i don't you just said that so uh when you run into an error you can just say that we've left it in on purpose maybe so this is the one that we've just cloned exactly what's on the github and we're going to try and upload an image here i'm going to use ice cream from before again um when i showed this to chip originally she brought up a good point does this go off the file name uh like so i could just read the file name and then it the model just produces um whatever class is in the file name no it goes off the pixels and you can see that in the in the code so predict running making prediction this should error there we go okay so this is our first error all right this is actually a massive massive thing when working with any cloud platform for that matter it's a security issue so default credentials error file daniel's playground so this is my google cloud project we'll have a look at that in a second but what this is is this is a key so this is an authentication key now again i told you it's kind of a play on words here that this is a major key of working with any platform and this also is a key file so what's why why is this an issue well if we come to the github uh food vision it doesn't include any json file and now i've done that on purpose why is that well in the git ignore file in here you should be able to see right down the bottom is star.json very important here the next steps we're going to go through why is this important well if i uploaded this key file to um where is it i'm going to get out of this 8501 just so we only have one version of it we're going to fix this error right so if i uploaded this to github and you had access to it you could access it's got secret details in there it's like sharing my password online so if you ever download a key do not commit it to anywhere that's public okay so add it to your git ignore etc etc so very important point here now let's figure out how we're going to fix this where does this occur i'm going to go into code test so this is the this is the original all right but test is the one i've just cloned so if i look at my original there's the key not going to touch that because that's private information but we're coming to test this is what you've just cloned from github okay no key in here but oh sorry in food vision no key now if we open this app.hi um i still haven't set my default to be visual studio code what a rookie error um set up the environment credentials you'll need to change these so this is to do with your google cloud project okay so mine is looking for now this environment variable as i said this code could probably be improved to put global variables somewhere better but i just wanted to keep it front and foremost this is what google cloud looks for when you deploy your apps to google cloud right it wants to know hey do you have permission to use the services and so this this is basically telling gcp yes i do have the permission to use your services but you're going to need your own version of this which will be something like your project name whatever this is dot json so let's see how we can figure that out your project is also probably going to be called different to mine we'll see how we can do that but my project name is daniel's dl playground daniel's deep learning playground so this is what we've done we've taken down this step get the app working locally now to get our key if we need to deploy the model to ai platform now what's the model let's go into here i'm just going to run through this collab code i'm not going to wait for a model to train this is one of the things that i've told you i prepared earlier we need a model so the reason why this error is coming up is because when i click the predict button i've got a function in app utils.pi which predict json this is basically just pinging our hosted model on google cloud all right you'll be able to we'll be able to look what look look where this sample code came from in a second but when it pings that model because the api key doesn't exist google's returning an error okay so that's utils.pi i'll probably get out of that because we want to get into the test version of utils.pi but we'll stay here for a second now we've got a gpu yes some simple helper functions run through that now here's our model or i said we're using efficientnet this is with tensorflow you may have had experience with tensorflow or pi torch um you can i'm pretty sure pi torch has a pre-trained efficient net somewhere just efficientnet pre-trained on imagenet and then we've just added our own output layer with a certain number of classes it's going to do that load in an image again if any of these points you want you want me to expand on just please ask a question because i'm going over this quite fast but the problem we're trying to solve is authenticating our app to gcp um download some data so this is just um the 10 classes of food 101 i've stored it on gcp if you want some info over the data we're using we have 750 images in each of the training classes 250 in each of the test classes um yada yada tensorboard callback we actually don't need that this this this model code's probably a little overkill for what we're doing but whatever so we trained a model so this takes about i think four minutes to train on a g on a collab gpu then we can test it out but the real thing that we did was save and uploading a model to gcp so again i've prepared this something we've prepared earlier but these are some steps that you can save a model directly from google colab to google storage so i'm going to just see it would actually be cool if we did all this in one hit maybe i'll just show you my existing model so while this model is training actually let's create a bucket on gcp within our project so um we're going to go to the google cloud console remember what error are we trying to solve right now we're trying to solve the fact that our app is not authenticated to gcp so we'll come in here google cloud platform um now i've already created a project so this is called daniel's dl playground for for you to go through these steps you're going to need to create your own project and wherever you see daniel's dl playground substitute that for your project name so i'm not going to go through creating a new project you can just click new project and go from there but as i said this is a big warning you're going to get charged for this stuff so um i've spent 130 dollars in the last week just but i've been very flamboyant with my google cloud usage so keep that in mind that's your warning now while our model is training i've got all of these pinned over here so again this is you're probably taking it if you never use google cloud platform probably taking in a lot of information at one hit here i was the exact same it wasn't until i just actually started using gcp that i realized wow there's more services here than i'll probably ever need so what i've done is i've just pinned the ones i find i use most often and they're going to appear at the top so i'm going to come into here into storage if you don't have it pinned you can scroll down you'll find it there you will need a project existing project to be able to enter storage so i'm going to click on the storage by the way that could be one classic data leakage bug yes you're totally right leaking your gcp keys they're actually crawlers that go through um that go through github looking for keys so if you upload it to github um reverse it really quickly now to create a bucket what's a bucket so it's just basically a hard drive on google cloud so google cloud has done an amazing thing it's just basically storage and compute that's what it offers and a whole bunch of different services in all of those so we need something to store our model and then we need something to um compute our model so just run our model so the storage of our model is a google cloud bucket now how far is our model off training haha this is exciting so models training we're going to create a bucket i've got one that exists already as i said ml deployment let bucket zero that's our existing bucket i'm going to create a new one and call your bucket whatever you want so cs329 live bucket creation you won't be able to take that name anymore because i've stolen it um so continue now where do you want to store your data this is very important depending on where you're if you're working in a company right i remember when i was working in my company a lot of the data we worked with couldn't leave australia some regulations i don't know i didn't make the rules so this is where whatever project you're working on if you have data privacy concerns make sure you know where you want to store it for me i don't really care where my data's stored so i'm going to choose multi-region you can go through the different region types here i think region is the cheapest multi-region i'm going to store it in the us that's kind of central i believe many of you are from the u.s it's just going to make it fast oh tidbit colab notebooks are hosted in the us so if you store your data on a google cloud storage bucket in the us the transfer rates so when we downloaded this this data up here are basically zero so if you want to use google storage to store large amounts of data and use colab to compute storage is very cheap compute is what costs more store your models in a us region then when you download it into colab it's very cheap sorry store your data and whatever in us now continue us choose a default storage class standard don't worry about these other ones for now if you want to use your data at all probably just use standard continue how to access these objects access control fine grained is just google cloud loves permissions right so everything you do is going to have some sort of permissions tied to it so if you're in an organization um people are going to have different permissions so just i like to just use the default and then advanced settings optional create look at this cs329s live bucket creation this is the name of our bucket it has nothing in it right now because we've just made it okay so i'm going to copy that save that to the clipboard come back look at this perfect timing our model is finished training so that's just an efficient net working on 10 classes of food 101 it's doing pretty darn well right 90 accuracy across 10 classes we can even practice it this is within a collab notebook i just downloaded an image prepare that image make predictions pizza so what's that image look like there's my dad two thumbs up eating pizza at one of our favorite cafes and food vision to just show you that it's not full of crap is predicting pizza okay now we want to save and upload to gcp so in tensorflow i just do dot save right that's going to save a model pi torch is probably very similar and then we're going to get a model over here oh a mill thank you for the tip git commits can be permanent on github if you have the ss hia so yeah very important point do whatever you can to not make your google cloud keys um public at all so i'm saving a model right look at that efficient efficientnet model one ten classes nice and simple now i'm going to run this off screen because i believe it um i'm going to authenticate colab so this is another place where you're going to have to get a key i'm just going to make this really small over here so i'm just going to a link i'm just basically telling colab i can access my google cloud stuff here this is pro pro level live demo hiding keys look at that right very important point so what i did i just ran this cell and basically what's this saying this is saying hey google co-lab authenticate my user account my user account is the same one i use for google cloud so that's a little bit of a mixed max you might have to line up but what are we going to do here i'm going to run this cell this is just so we can upload directly from google colab to google cloud storage right because as we all know colab is temporal this gets deleted when collab shuts down and just a reminder what error are we trying to fix we're trying to fix this authentication but to do that we need a model on google cloud so come back here i'm going to run this cell this is all on the github by the way a whole bunch of output basically we're just installing the gcloud sdk into google colab um and then we're going to initialize it here there's a tutorial there if you want to install the gcloud sdk very helpful you're basically going to have to do that if you use google cloud full stop um and then we're going to get asked a bunch of questions i'll show you them rather than just talk about them so please enter your numeric choice what do i want to do pick the configuration i want to use no i just want to use default settings i don't want to create my own custom choose the account you would like to perform operations in this configuration daniel at mrdbook.com that's my google account and what we're doing right now may seem like a lot of steps but this is because we're doing it through collab if you were to set this up in like your software developer environment you can automate a whole bunch of this number four is daniel's dl playground these are the projects i want to use we come back to console remember this is my project name anywhere you see my project name replace it with yours so number four and then do you want to configure a depo default compute region and zone no i think uh google just defaults to um whatever your project is set on set that to wherever your us is usually generally good okay now look at this so we've authenticated google cloud we've got our bucket nothing in here remember nothing in here now we can use to upload anything to a bucket you will need an existing bucket that's what we've got look at that how good is that so here's the one i used previously now what this line is saying is it's going gs util now gsutil is google storage utility it's a part of g cloud so that's why we installed gcloud google cloud sdk copy recursively the folder containing my model to this bucket now if you ran this line of code it wouldn't work for you because you don't have this bucket on yours and you don't have my user account authenticating your collab so that's an important point there so what do we have to do we have to replace this so this is the one i did on twitch live stream worked there so this is what i'm saying hopefully it works here we have to replace it with our bucket name right so this is this is this is the lego blocks coming into action so we have gsutil copy recursively all of the files in efficientnet model 1 10 classes 2 our google storage the bucket we just created remember google cloud offers storage and compute we need to store our model on there this is what we're doing ready we're going to do this live running it together ha ha look at that it's going to be very quick very very quick because it's all online nothing in our bucket click refresh boom look at that right so this is our efficient net model so how cool is that we now have a machine learning model living in a google cloud storage bucket now we're on to the next step so that's our cs329s live bucket creation and this is my bucket that i created earlier this is what my app is set up to run with we've got some more models here but i'm going to let's go continue to work with our our cs329s live bucket creation okay so we've got a model hosted let's uh come back to the keynote get the app working locally deploy the model to ai platform now we could google this how to deploy a model to ai platform um google's documentation is very verbose um but this is basically the steps we're going through look at this in order to deploy your trained model and ai platform prediction you must upload your save mode to a cloud storage bucket what have we done we've just done that boom now create an ai platform prediction model resource okay and let's see how we can do that and then we've got to create an io platform prediction model version resource specifying the cloud storage path to your saved model let's do those two steps now how we're going for time by the way because i'm kind of just uh burning through this and not really paying attention to time i've got okay sweet well this is this is probably the longest step of the whole thing um so let's go back what did it just say um upload your save model to a cloud storage bucket create an we need to do these two this is what we're doing creating an ai platform prediction and we're working with lego blocks here i've got too many tabs up where did all my pins go is this google cloud playing with me right now did it just did you just see that it just removed all my pins what the hell anyway okay so uh classic live demo ai platform i did have that pinned but just disappeared wow been using google platform for like two years and that's never happened so this is ai platform right again a lot of this stuff we're covering is um we're doing the clicky around way okay so we're clicking around google cloud platform console this is like the lego blocks i was talking about but again with the gcloud command line tool you can do all of this programmatically i just want to show you the visual visual way of doing things um so we want uh where did it this is really strange there's usually a permanent sidebar on ai platform i might just refresh this you know there we go see look at this google cloud's going whack on me okay so you should have this sidebar but we're just in ai platform if you don't have this screen available you may have to enable the api which is usually just yes i want to use ai platform it's just a button models so here are the existing models that i have all right there's the three models we haven't spoken anything about those just yet but what we need to do is create new model now we're going to go cs329s live model creation delete later very important point if you don't want to get charged delete all of this stuff that we're doing later again that's a simple way of just going pressing delete u.s region u.s central i'm going to keep it there you can change it depending on what what you're working on again if you have data privacy rules think about that but i don't i'm going u.s central i can only contain underscores jesus um live model deployment we'll just keep it at that u.s central description efficientnet you can call it what you want but that's just what mine is do you want to turn on logging i'm not going to go through that but you can just imagine what this does it's going to enable logging but you can turn this on later so this is it right this is the step one in deploying a model uh step two sorry upload your model to a saved cloud storage bucket tick create an ai platform prediction model resource what we're doing right now create model create we'll let that run through i'm going to come back to the keynote i actually have a visual way of doing this ah here we go this is probably a bit more fun yeah how about we look at that that's way better than looking at some text here's where we're up to have we done this yes build model mvp style app the code for that is on github we've kind of glazed over that upload model to google storage we've done that we are now up to hosting the model on ai platform so here we go cs329s live model deployment that's our model but the guide says create an ai platform prediction version resource so the model is like just the overarching thing the version is versions of your model as you see here i've got models here and they're versions this one should really be zero zero one actually doesn't matter what you call your version we'll come in here why is this important well let's say you had one model trained on 10 classes using efficient net b0 your version 1 could be efficient net b0 your version 2 might be efficient at b4 and you want to a b test those later on push b0 to some people push b4 to other people see how the results go let's create a version this is where we connect the lego blocks of google storage so i'm just going to call this v01 description efficientnet b0 10 classes pre-built container this is also important you're usually probably going to use pre-built container um they include tensorflow scikit and pytorch learns uh versions now what is a container that is docker we haven't actually talked about docker but just imagine um docker is like a service that helps you wrap all your code into like a nice little box and then you can run that box anywhere that docker runs and that's a lot of places so that's how i imagine containers so unless you have experience building your own custom containers i would suggest use one of google's pre-built ones they probably satisfy most of your requirements pre-built container settings i want python 3.7 i want to use tensorflow if you need pi torch refer to the documentation version i'm going to use 2.3.1 because i tried 2.4 yesterday and it was busted you may have a different experience uh ml runtime version 2.3 usually with all of these it's like the latest unless you're using some really specific version of tensorflow or pytorch or python or whatever usually it's it's the latest so now we go the model this is gs where have we seen this before we uploaded our model to a bucket before so we need to link that bucket so let's go here again this is just lego bricks with google cloud tools there we go cs329s live bucket creation select boom done model linked to ai platform now how do we want our prediction to work scaling do we want to put it to auto scaling what does that mean well the beautiful thing about google cloud is it's infinite infinitely scalable and i say infinitely that's like a star what that means is say you started with a small computer and you only needed to serve one request per hour while you're testing your app and you're like okay that's enough compute but then all of a sudden you do a live tutorial for 40 or so people and everyone's accessing your app and a small computer is not enough it's going to automatically upgrade you to however many computers serves all the people and so that's a very helpful thing but then even better once those people stop using your app it'll go back down so that's something to keep in mind i like to choose the smallest by default whenever you want the smallest with google cloud it's usually just whatever has the lowest numbers so n1 standard it's going to have a machine with two cpus seven and a half gigs of memory scale up if it needed if we want a gpu to run gpu predictions like if you wanted actually a full-blown application you can power this ai platform with the gpu faster predictions cost a lot more so just keep that in mind and you can also you could have multiple gpus so if we wanted to really just turn this right up we could have four p 100s power and food vision but how do i turn this off now oh well i can't look at this this is uh this is a design flaw from google if you select a gpu you're stuffed anyway i guess ours is running with the gpu now um and then service count we'll go into that in a second anyway we need to save that what is happening here of course there's an error contain exactly one oh sorry we need to pass the path to our model so that's we need to pass the direct path to the actual model file that's all we had to do so this is launching with a my gpu it's going to cost me a whole bunch that's a very big design floor on google cloud once you select gpus no change in it so this is actually going to take a few minutes so what we'll probably do now is where are we up to come back to the keynote um so this is what our model is doing it's hosting on ai platform um now we need to create a service account to access that model so we've done these three steps um did someone oh was someone telling me that i could have actually changed the gpu anyway we'll just leave that because we've got to fit we've got a couple more things i want to cover this is probably the longest part as i said that's going to take a few minutes so how do we create a service account well google's done the right thing and just removed all my pins but that's okay service account is in iam and admin service accounts so you're going to spend a lot of time in here usually this is like the role of a devops engineer or whatever but i like to just get familiar with things uh myself so this is just i am an admin i don't actually know what im even stands for all i know is that you need to go in here to give permissions to certain things so let's see what that looks like while our model deploys to ai platform so i've got a bunch of things here you can see the key ids here but you can't actually do anything with that because that's just an id the actual key is a json of like thousands of characters so this is the one i created yesterday for the twitch live stream so this is what we're going to do this is what this is tying it way back into here fixing this error we don't have the right key so let's um again all of this can be done from the command line interface but i'm doing the clicky around version create service account service account name cs329s live service account creation what will this do able to make predictions using uh ai platform create now this is important point here the whole reason we need to make this we need to grant this service account access to a project so by default when you create any service account they're given the bare minimum permissions so basically nothing which is good because you should only give service accounts permissions of what they need to do so in our case what do we want to do look how many roles there are this is how many services google cloud has right all of these our account doesn't need to do most of them all it needs to do is be able to make predictions on ml engine now they haven't ml engine developer ml engine is the same as ai platform little tidbit there um google did a recent rebranding because ai is more hyper than ml to name it ai platform but the the role is still called ml engine developer so if we click that continue now the developer role just means that it can access the the model we just hosted um this is more for if you wanted to give other users in your google cloud organization access to this role i don't i'm just using it for my personal thing but you might if you have multiple people done now this is quite quick so where is it cs there we go this is our account we haven't got a key yet okay well that's all right we can just click here create key now it's going to download in a json i actually need to uh hide this again because um otherwise i have a risk of showing you all my key don't show your key so i'm going to click create and it's going to download okay so that's all it did close all right so all i did was click create key and then click download and then it's going to be in my downloads folder so what do we have to do now you can probably imagine where do we have to put this key there's a key i'm going to move this into code into here into test i'm going to put it into my food vision directory done all right now this is very very exciting actually um we may have just we might just be able to fix our first error i'm going to copy the path to that key and let's see while we're at it i said that our model takes a while to deploy let's go back to ai platform let's see if it's live because then we can really just make sure all of this is is live deployment um no the default version is not ready yet but that's okay i've got a few models that i've prepared earlier this one is just the exact same thing as the live deployment let's come back into our app.hi all i'm going to do here is change the path to the key to this key so i'm just going to copy this this is the old i'm going to just leave that there and this is the new key the brand new key that i just created very important point using any g cloud services is having keys so there we go new key i'm going to save that now the beautiful thing about streamlit is watch this it refreshes itself it actually knows when you refreshed it so localhost source file change what have we changed all we did was we updated the key file to be the one that we just created and we put it into the working directory so let's refresh this you're ready for this live prediction ice cream and we're going to predict ping's a model fingers crossed oh yes look at that right so that just pinged our model did it ping the one we just hosted no it didn't because uh it's not deployed yet it's not deployed yet but it is if we come into here open um food vision nope we want tess yes here test this is one that opened food vision utils open so what opened my this one here model name you're going to have to change that to whatever you want if we wanted to open our um this one we would change it to the model name there okay a few steps there we covered a lot so now we fixed our first error but guess what we can deploy our app now because it's got the right key so this is the model that my application just used so i'm going to come to the keynote we've just taken care of that step major step there now this is actually quite fast because i've automated it and i will show you what i did to do that but i'm going to go to the chat um why is a key on google cloud cause the app run on local to work um so bob that's a great question we um so what we're doing here is so this is the environment variable right we're setting it up with os environment google google application credentials it's going to read this key but utils.hi i haven't quite actually explained this this function here predict underscore json what this essentially does is it creates an ml engine like a link an api i'll show you what that is in a second and then it creates an ml engine resource endpoint and then it asks ml resource to predict and then to get a response we use the python request library which is just like something to go hey internet give me some information back after i've sent you this we send request dot execute now this because this request is going to google cloud when it hits google cloud google's going to be like hey where's your key um can you make this prediction and then because our environment variable is set up to be our key it's going to go well here's my key please give me that information back and google's going to be like yep that's a good key here's your information if it's not a good key it's uh it's going to block you out so that's how i understand it but if you want to know where this function came from let's go into models this might be ready now nope not ready so that takes a couple of minutes probably taking a while because i put a gpu on the back end but if we come in here this is our version one default this is something i made earlier this is if you want to test and use go here um this is how so this is i'm not going to dive too deep into this but this is how google's ml engine wants to see our data as a json with instances with there's a tensor in this list so that's basically how google cloud wants to see um our data as a json with the key instances inside instances is the the data we're sending it and this is a sample prediction request just this function here using google's api client library so that's where that's where the key part comes from all of this but i'll let you go through that we've got a we've got another step to um a fair few steps to cover here but we're gonna this is this is quite quick let's do all of these in basically one hit are you ready so terminal so this is running locally um let's uh create a new terminal window i should really have this tab shouldn't i but uh condor deactivate wonderful let's cd into code tess cs so ls cd food vision ls clear so there's a fair there's so now in here i know we need to um activate the end ls um wait oh no it was in food vision wasn't it so this is this is the the repo that i cloned earlier but now see how it has the key in here but that's not going to save to github because we've got we've got json and git ignore what we're going to do here is there's a few files in here make file and dockerfile make file is like just a well let's have a look make file so we've got a fair few commands in here or actually only three so if we did make run it would run this command if we said make run container it would build our app into a docker container if we do gcloud deploy it's going to deploy our app to app engine now that's because we've set up make so if we just type in make g cloud deploy i'm not going to hit enter but it's just going to run this command here so that's that's the helpfulness of a make file if you just google what is a make file you'll there's the first resource is probably the best for that um but when it when we do this we're actually going to run this and i'll talk you through the steps so let me just activate the environment and you might be like daniel we've covered a whole bunch of stuff do i really have to do all this every time i want to deploy a machine learning model we've gone the very long route the clicking around version you can put all of this into a script and just execute that script and it's going to do itself so before we run make gcloud deploy all that's going to do is deploy our local app right where is it this one it's going to deploy this it's going to wrap it up into a docker container remember just basically saying hey take all of the stuff we've got working locally put it in the box send that box to google cloud run that on one of your computers exactly how it runs locally and we should end up with something like this and pumping out errors just great isn't it all right so just for in case the deployment doesn't work this is the end product okay the deployment's going to take a couple minutes so i'm going to kick that off you can see it now as it goes back we've got five minutes remaining far out okay well i just need to quickly um i'm going to run this make gcloud deploy run do i want to continue whole bunch of stuff just saying yes do you want to do that we need an app.yaml file haven't discussed what's in that but that's basically two lines just telling app engine what type of um computer we want here we go beginning deployment of service you will need app engine api activated for this but this is going to take a few minutes so i'm just going to let it kick off and then i'll uh i'll wrap up um but i'll show you the um i'll show you some of the errors on the local version of the app so where are we at go to the keynote we'll pretend we did all of this but this is wrapping a docking container these two steps happen automatically when you call g cloud app deploy and then if you want to monitor the app you can monitor it later everything works nicely until you deploy your model in the wild a smart machine learning engineer said that by the way okay we'll keep checking back in on that and i'll just finish off these slides so there's some problems you're going to run into what do you do when your model fails when if not what shape does your data come in so for example jpeg versus png for text you're going to have a whole bunch of other different issues there too ml engine has a hard limit of 1.5 megabytes we'll probably see we'll we might have time to take care of one of these um what do you do when your user has a class you've never seen when do you retrain your model what do you do when a user has a class not even in your scope so while this is deploying i'm going to show you probably two out of the five problems that we've got here um just to finish off but this is building okay it takes a little while you can again you can automate all of this so you don't have to wait for it it'll just do it in the background and but let's see this um oh there's a spoiler so this is the funnest one what do you do when your user has a class so this is problem number one everything looks like daddy so machine learning model is what probabilistic so it means it's going to make a decision no matter what you pass it to um so we'll go to our local version you see you might have seen over here we've got model one model two model three these are all deployed to google cloud now model one can work on these classes it's trained on ice cream so it works on ice cream what happens if i upload a photo of donuts it's going to run in that prediction what's going to happen hamburger right because our model is probabilistic so this is the data flywheel coming into effect i've got to hear an option where this is the equivalent of test tesla's autopilot disengaging this could be a user selecting no this is not correct what should the label be donut beautiful thank you for that we'll use your help to make our model better of course this is just a text field you could put this into a pre-loaded text field that only works with certain number of food classes that's another thing that's an extension we come back to our terminal it's still building and pushing the image so again i told you that's going to take a while but look what happened here this is our locally running streamlet version we've got here oh my gosh so 8502 and we've got a log here this is the image tensor that we just passed it so this is a donut in tensorform in color channels so the model we used was our efficient net model 1 10 classes the pred class was hamburger the confidence the prediction probability was 0.54 was this correct false because the user labeled it as false and the user label was donut so maybe that's something we re-feed back into our model all right and then we found people who are taking so many photos of donuts is not funny well then we've got model 2. look at this we refresh we'll use model 2. this is another trained model what classes is our model 2 trained on we've got donut there now well let's upload donut so these are the things you you don't really come across when you just train models in notebooks you need people to interact with your model or you need to test them personally so what do you think is going to happen now that our model's been trained on donuts look at that prediction donut is this correct yes thank you for that let's go in your feedback we'll come back to terminal another log so we've got the image there of course this is logging in my local terminal you would probably want to log this in your say google cloud storage or something like that or bigquery so the prediction class is donut is the label true correct um so you know you're seeing if your model was to make 100 predictions you could index on all of these entries find the ones that are false and then update them finally what happens if yeah you're right that's so fine okay thank you thank you i was a little nervous but uh i had such a fun time um and of course it's uh it's going over a little bit however um all of the code and whatnot and all the stuff to go along with this will be on the github but i will just show you this is probably the last one maybe then that'll be we can wrap it up maybe have a question or something but another problem that you're going to run into we built food vision and of course it's an image recognition model but what happens if someone uploads a photo of something that's not food oh my gosh we totally forgot about images that aren't food what's it going to do what's our model trained on it's only trained on 11 different classes here it's predicting that this truck is a hamburger that's not a very good user experience is it so then our user hopefully goes no and then what should it be truck or not food or something like that that's what we could prompt the user to to put in whatever they want and then again that gets logged in terminal we forgot to hit enter anyway that will get logged so we find out that this is another problem with our model it's food vision but people can upload photos of whatever they want so model 3 comes into play right this is another hosted efficient net model and so i created this one it's got a not food class so you can probably guess what's going to happen with this image of a truck prediction we didn't actually do that but the way streamlit apps are built is that whenever you change something it's just going to rerun the whole app so prediction not food is this correct yes and so what are we creating there that's our data flywheel that's the whole objective that we're trying to get to because you never really know these problems until your model is out in the wild until you're building something with it that's the only real time you're going to to get these sort of issues and of course i've done this by training a model on 11 classes of food plus one class of not food so just random images from imagenet put into that just random classes and so i've just labeled all of them as not food but you could have something like a model cascade this is too much to cover in the last two minutes of this but you could go it goes through one classifier is it food not food if it's food it does this if it's not food we tell the user hey please upload photo of food so i'll just wrap up these slides there's not much left so problem number one everything looks like daddy problem number two that was everything still looks like daddy because we had to solve for not food so beautiful timing we're at the end did our model deploy we'll come back to terminal um oh it's still building the docker container so all of this will be on google on colab by the way so if you want to run through these steps i'll fix the readme today but finally oh that's not the keynote let's go into here we'll wrap up we've got like two more slides left google cloud services cost no credits equals um money flying away so make sure you shut everything down um learn gcp where you can go to google.qwiklabs.com great resource you can learn a whole bunch of the stuff that we just discussed in there there's some specific machine learning apis and whatnot that you'll you'll probably want to go through extensions if you really wanted to and this is where i'm progressing towards so ci cd you'll hear this a lot continuous integration continuous deployment so basically what this means is every time you make a change to your app you would upload it to github you'd have a script that notices that change and then your app that's deployed in the browser um say this one updates automatically um so that's you can do that with something like github actions have a look into that codify everything we did a lot of the clicking around version of google cloud but with the gcloud utility the gcloud sdk you can do basically everything we just did with command line arguments command line commands and then actually log the data from your app into say bigquery or google storage or something like google monitoring that way you can come back to it later and finally i think that's it so yeah thank you very much yeah we didn't we didn't quite reach the deployment but this is going to deploy and it's going to come to that url that we uh that we um we sent but uh yeah go to github repo you'll be able to reproduce all of this stuff yourself thank you so much i appreciate it one and well welcome to the back half of this video if you watched the previous half you saw that that was during a tutorial i gave for stanford's cs329s however as usual i went overtime my video got a bit verbose so because you're watching this on youtube how about we finish it off we kind of did the last back half of the the deployment and whatever in a hurry we did briefly look at what do you do when your model has a class you've never seen so you update your model you retrain it you add that class we use our data flywheel as feedback so we gave the example of our model not working for donuts uh then we added the donut class train another model again if you want all the the model code it'll be in the model training notebook um in the github repo so you can see there so model 2 is 11 food classes i'll put here same as model one but we've added the donut class all right so if we come into here this is our app running on a local host model one i'm just reiterating what we did change it to model 2 show the classes for model 2 we have donut now let's upload donut and then make a prediction this is going to ping our model that's hosted on google cloud if i come into my dashboard ai platform i have i have three models here one was the one we did during the tutorial and the other three is one trained on 10 food classes 11 food classes and this one's trained on 11 food classes plus one not food class these are things you have to think about when you start to deploy your machine learning apps what happens when your model faces something that it hasn't been trained on and you'll only realize those when you start to put your stuff out into the wild it's very hard to get that information from just working in a notebook okay so let's go back to our app it predicts donut wonderful and we can get our data flywheel going by telling our application that that is correct so if we come back to terminal the last place that we left off was running the command make g cloud deploy but as you see for some reason this is this would have happened in the tutorial if we had time my model deployment failed so let me just show you what's in the make file because we kind of brushed over that in the tutorial what happens in the make file so make is just a collection of little scripts that you can run and here we go g cloud deploy this is what we want to run when we run make gcloud deploy it runs gcloud app deploy app.yaml and then if we do the same thing with cat app.yaml this is what's in our yaml file just two lines so this is telling google sorry google's app engine um the app.yaml if you go into the app engine documentation tells app engine what type of runtime it should use runtime is another word for container so we wanted to use our own custom runtime which is what i said before when we run docker it's going to containerize our app or when we run this command it's actually going to build a docker container around our app so this is what docker does builds a container around your app and then it passes that container so it's a custom container running everything that's running locally we want to pass that to app engine so that's why we have to tell it runtime custom and then the environment we want it to be a flexible environment so again that's another just a parameter to say that it's it's flexible let us customize it ourselves you can dig into the documentation for google's app engine if you want to find out more about what to include in your app.yaml again you can run all this from the command line or you can do it from within the google cloud console so let's rerun this we'll clear this let's run make gcloud deploy and i just want to show you where all these files come from as well we'll let this go through here's some information the target project this is going to be my google cloud project it'll swap out for for your project from google cloud whatever you've called it and then if everything works it's going to end up at this url which is currently running here okay so if you're watching this video in the future this url may or may not work depending if i've turned it off because remember google cloud costs money so we're going to leave that run it's going to take a few minutes while we talk about some other things with our app and so let's go or let's have a look at app engine um gcloud app deploy here we go what does this need does it say yaml deploy with the specific app.yaml that will replace the one defined in deployable um so what is app.yaml in app engine python here we go app.yaml reference this is python2 i believe we want python3 app.yaml configuration file there we go so you configure your app engine settings in the app.yaml file the app.yaml file also contains information about your app's code such as the runtime and the latest version identifier so there you go you can read through the documentation there this is a pretty it's a longer one here for a python 3 application okay so there we go we could have included like maybe a bucket in there so that when we upload images to our app using this it will upload them to a google storage bucket so that's that's something for an extension we're not going to cover that in this tutorial let's keep going with the deployment anything happening here not really so what this is doing is building and pushing an image for service an image is a immutable version of a container so once docker creates a container of our app it turns it into an image so then that image is you can't change that image it's just it's like that so that's calling docker in the background if we go back to our keynote while we're waiting for our app to be pushed um and wrapped up in docker and push to app engine uh what do you do when your model fails when not if so we kind of saw that with these two examples of when our user uploads a class that we've never seen before and when a user has a class not even in our scope so first was a food that wasn't in uh our models and then was something not even related to food so the the uh another scenario could be like a self-driving car um and it's driving along uh a beach but it's only trained on roads so what does it do there does it work there who knows maybe maybe not a human can go from driving on the road to driving on the beach but can a self-driving car so what do you do there that's a very important point it's going to change no matter what sort of project you're working on but it's worth thinking about and what shape does your data come in so this is another one to really think about so what what basically is a deep learning a deep neural network it takes a tensor of some sort of input it finds patterns in that tensor and it produces some sort of output so let me show you an example with our uh local app if we go into app.pi uh actually better off utils.pi i'm going to change one line of code here so i'm going to change this to this that's all i've changed so what happened there well i went from image equals tensorflow.io doctor codeimage name and it has no channels equals three this one has channels equals three so let's see what happens we'll save that we'll refresh we'll actually refresh here so we'll use model 2 that just worked before in our donut image browse files we'll upload donuts what's going to happen here let's see it's going to run same model same image we used before all we did was change one line of code in our app oh no what's happened well incompatible shapes one two two four four verse one one one three huh that doesn't make any sense so i'll tell you what happened was this doughnut's image used to originally be a png which has four color channels versus a jpeg having three so that one line of code because our model was trained on images with three color channels when we tried to make a prediction using an image with four color channels it errors out so i'll just go into here we'll be able to see the shape so how about we go um app.hi and we'll print out the image shape let's do that where does it come here make prediction we'll go [Music] print actually it gets changed in here so we may not be able to print it out but i'll try to anyway but this is just to demonstrate print image dot shape okay or rerun upload the image of the donut predict is it going to come out i don't know i don't think so maybe because it's only in our pre-processing function oh i'll tell you where we could probably do it if we go back to the code in here make prediction image there we go print image dot shape save rerun do the exact same image so this is i wanted to include the errors in the original tutorial that i was doing because that's you're going to run into errors all the time and i just want to highlight some of the things you have to make sure the inputs input shapes you're giving to your model the same with the output shapes that you need okay so it's not it's not printing all right well it's the wrong shape so if we come back here to utils.pi and we uncomment this and we re comment that so now we're fixing to make sure that instead of four channels it has three channels so see here this image currently has four channels so let's refresh this oh actually i'll have to save you till stop high refresh browse donuts upload or actually i'll change to model 2 and then predict now this is another thing where your app will break actually if you're doing this on your side is i have three models deployed and you can see this in utils.hi um model one two three if you only want to run one model uh i'd get rid of model two and three from this dictionary here and in the streamlit code you can also get rid of this logic here that decides you to choose a model so just delete this and change it to model only have option for model one rather than model two and model three so that's just a heads up if you're running it from your end you only want to train it on one model i just made three models to exemplify the problems that we're talking about so there we go it works once we change the shape there's another thing that i want to show you it's that if you're using ml engine which is ai platform it has a hard limit of 1.5 megabytes what does that mean well i found this out when i was building my app if we come in here um so i've got a little line here this print didn't work um i've got a line here that says image equals tf cast tf expand dim image tf so i'm turning my tensor image tensor from a default float 32 that's tensorflow's default data type to int 32 why am i doing that well if we uncomment this i just want to show you the error and we'll see what happens come back to our local app we'll refresh that how about we choose model 3 this time browse files you don't have to choose model 3 you can just run one model if you want but just remember to change that code i showed you before truck.jpg make a prediction while that's running we'll see what's happening here beautiful we'll go back through this code in a second a whole bunch of jumble stuff but what's going to happen here what do you think i told you ml engine has a hard limit of 1.5 megabytes we've just asked our model model 3 hosted on ai platform you might only have one model that's okay to predict on a truck oh no look this is what we get so this is the errors that you're going to run into when you start to deploy your machine learning models it's saying that this image once we convert it to a tensor and go hey google predict on this tensor it's over one one whatever that number is one million five hundred seventy two thousand eight hundred sixty four bytes so that's one point five megabytes so how how do you fix that well if you have a look at this float 32 verse float 16. um no we need to have precision floating point so in computing half precision sometimes called fp16 is a binary floating point computer number format that copies occupies 16 bits two bytes in modern computers in computer memory so we need to go float 32 versus int 16 storage we need to see how much memory each of these take up here we go this might tell us that's what i want the higher the precision point um here we go i'll zoom in on this the higher precision point types require more disk space for each stored value look at this float 16 points use 16 bits or two bytes per value float 32 and float 64 use four and eight bytes per value respectively in 16 and in 32 use two and four bytes so that's what we've changed it to int 16 from pl float 32 so it's using half the memory as usual so instead of tensorflow's default float32 using four bytes per value we change it to n16 using two bytes per value so if we come in here this is called mixed precision or half precision or something along those lines that's what you'll see so by changing it from this is default float32 we changed it using this line to int 16 so we're taking up half the space half the memory we're compressing uh the information in our tensor still going to have the majority of the same information it's just being compressed so we'll save that come back to our app rerun we'll use the same model model 3 and we'll use the same image of the truck upload and then we'll run predict running make prediction boom not food confidence not 0.929 so that's the prediction probability output of our efficient net neural network with our own custom dense layer on top how exciting is that so they're just a handful of the issues that you're going to run into what do you do when you when your model fails that's when not if in your machine learning applications what shape does your data come in for example jpeg versus png it's going to be different depending on what kind of data you're working with sometimes you may have a size limitation on how much you can compute we're using an api to compute if you were on an embedded device so say like an iphone or a mobile a raspberry pi or something you may have even more size limitations on how you can compute so you have to start thinking about how big are your tensors and how big are your models what do you do when your user has a class you've never seen we saw that we trained a different model we added a different class and what do you do when a user has a class not even in your scope we saw that we also had to adjust for food vision is what if someone uploads an image that's not food again these are all problems that you're only starting to going to think about when you deploy your applications but let's have a look what's terminal telling us okay we're going to scroll back up here there's a lot lot going on but let's just step through it right so it starts off it needs to build uh a docker container and how does it build a docker container well let's see let's go back to the github um cs329s if we go here and if we go into this all the files that you need are going to be in this github as of recording there are some things that i need to still do in this but by the time you watch this video on youtube i'm going to the readme will have all of the steps that we we talk about in this video um so if we go into the food vision this is where we're running all of our uh files from this is what you get when you clone it so then if you go to the make file right we've just run the make file this is the make file on github we've we've discussed what's going on in there i'm just going to zoom in there okay if you want to look up what a makefile is just search what is a file this is the first link i'm pretty sure we'll have a great description docker file so here's what's running now let's um watch this in unison to what's going on over here okay so build so can you see all that yes you can um we've got step one of eight so we have one two three four five six seven eight we have eight commands in this docker file a dockerfile is just a text file of a bunch of different commands to run through sequentially that's what you have to imagine for docker where can you learn docker docker have a great tutorial it's very verbose so i'd probably look up maybe you can probably get it all in half an hour or just the not half an hour but just the the crux of it in a half an hour youtube video by searching how to use docker for python now step one from python 3.7 i'm going to i'm going to close this a little bit so we can um how do i i don't want to make the the terminal unreadable i want to keep it readable so we can look at both of these in unison okay so step one of eight from python 3.7 that means it's going to create an environment python 3.7 docker have an existing python 3.7 environment it's going to create all of these all of these co like little weird number numbery things is what's building that container around your application starting with python 3.7 as the foundation then we come down here so i don't even know what's going on with all of these they're all jumbled up numbers if you if you know you can leave a comment below step number two is expose port 8080 where have we seen a port so our streamlit application is a web application and you see here it's running on localhost 8502 we can also see that in our streamlet run so this is using it on localhost 8502 if we go back to terminal um in app engine we're exposing the default port of 8080. so that means in our app engine computer that's what app engine basically is it's just like a computer stored on google cloud in app engine we're exposing port 8080 okay that's what that line is in our dockerfile expose the port you want your app on now next it's running pip install upgrade pip so it's just upgrading pip in our um in our container so that it's got it's got the latest version of pip requirement already satisfied wonderful and then what's it doing next it's going copy requirements txt into a folder called app slash requirements tst so it's standard convention in your docker container to create if you're running something on app engine to put it in a directory called app and so that's what we're doing here we're copying requirements.txt from the main food vision branch all right so copy this file here into a directory called app and then it's running if we go back to the dockerfile and then it's running pip install app slash requirements txt it's getting all of the requirements into our docker container just like we did when we get cloned this repo way back in the beginning and we installed requirements.txt in our local environment it's doing the same thing in the docker container so then you'll see a whole bunch of things here collecting requests collecting tensorflow collecting streamlit if we scroll down it's going to do a look this is all just a bunch of installing okay pickle share pigments decorator etc etc all installing in that container because remember what is docker doing it's wrapping up our local app what's running on our local machine and saying put this in a box then i want to be able to run that box anywhere where docker is and app engine can run docker that's the beautiful thing about docker building wheel all this sort of stuff a whole bunch of outputs i think this is streamlined output streamlets requirements and whatnot now we get up to step seven of eight um oh sorry it's copying so this is copy copy dot so copy everything in food vision into the working directory app and then we change into the working directory app and now entry point another thing that you might see for entry point is cmd if we go back over to here this is this this is just saying this is the command we want to run once our container is ready so just like before way back at the start when we ran strip streamlit run app.pi um we didn't do this but this is just telling um this is just extra tags to tell uh our container what port to run our streamlit app on because remember we exposed port 8080 so rather than streamless default of 8501 it's going to run on this server port and the server address is just 0.0.0.0 but this is a docker convention of how your command should be it's in a list uh strings separated by commas i'm not entirely sure why it has to be like that but if you read through the documentation that's just what it says okay so that's the command we want to run once our container is built so look at this successfully built this is our container id now what's happening is our container our docker container this is all happening in one command make gcloud deploy thanks to our make file our docker container is now getting uploaded to google cloud so that is beautiful specifically gcr so google container registry let's go into here container registry this is just like google's a cloud hosting uh for docker container images okay so our container being wrapped up our local app uh saying hey google run this exactly how it is on my local machine we're going to push that to where you store your containers and so when you start up your app engine you just have to load that container and run it everything should be built in there as we wanted so then it's going up here the push refers to the repository so this is just giving us a list gcr stands for google container registry dot io um slash daniel's dl playground that's my google cloud project yada and it is going we get another error actually not sure what's going on here let's have a look check logs let's see what the logs say hey maybe it's because i've already got something running project equals cloud builds we're seeing this together but this is what you should see you should see this app running here see how we got the same app i told you i want to keep the errors in what's what's going on why is this an error starting build etc operation sending okay so the docker this is all the stuff that we saw before on our terminal yep yep yep yep this is very strange because this worked this has worked i've ran no different codes to what i've built um okay it timed out maybe it took too long retry see this is the one i ran the other day and this worked so maybe my docker container now takes too long to build so that's something you'll have to watch out for too if we go retry build log hmm this is strange because my app is running see this is on appspot so let's try it out so this is the deployed version of the app hopefully you get this um or you should if you follow these steps now this is i'm not sure how to fix this one so it is sad but we're probably going to leave this on another error that we have to cross another bridge is that we're getting this request error status code but i found if you just sort of play with it for a little while eventually it starts to work oh my goodness it worked on the second try two thumbs up look at that there's some beautiful pizza so what should this do the exact same model that we've got running on our local we click predict so this is from the twitch stream actually so this is running the app version i got running in the twitch stream hmm which is interesting ah okay ah because i deleted the twitch stream model that's that's okay now all the errors are starting to come out so if i change to model 2 model 2 should exist correct is pizza boom look at that model hosted on appspot.com prediction pizza now i believe the error that i'm currently getting is that my docker container took too long to build uh i think that maybe because i actually i don't know so that would be something that i'm going to have to to look into so let me know how you go if you end up going through this entire tutorial with your app to be built if i find out exactly what's going wrong i'm going to include it in the in the github repo but i ran this exact same code the other day and it ran fine on a twitch live stream and it built that's why you can see on the deployed version of the app is that um if i choose model one it tells me twitch stream model delete later i can even change this to model three and let's see what happens pizza wonderful okay so the extension of this tutorial would be to figure out why this uh docker container build breaks for this build here maybe it's just a matter of going retry i'm not gonna this video has already gone far too long so i'm not going to wait for that to go but let's just go back to our keynote and remind ourselves of what steps we've gone through um yeah this will be a good one so here's what we've done another extension for you we built an app with streamlet uh and a tensorflow model you could also do the same with pytorch we uploaded our model to google storage we hosted our model on ai platform we then created a service account to access the model remember that's a key very important never to upload that key to uh anywhere public um we wrapped our app in a docker container using our make file and dockerfile then we pushed while we didn't actually do this the the gcloud app deploy command pushed our docker container to gcr google container registry but if that takes too long to build it won't deploy to app engine so just be aware of that so that's what the g cloud app deploy command does we had to specify a yaml file for that as well now the app.yaml file describes what the app engine settings are and our app.yaml file was as simple as saying hey we want to run time custom our docker container and the environment can be flexible so that's it our app.yaml file was two lines of code and that's not even really code that's just uh word colon word times two and okay so this one looks like it's going a little bit quicker so this might actually push but again i'm not going to wait because this video has already gone on for too long if you go to this url and it's not working this is my thing i've turned it off that's a good reminder so oh that's the final extension you can also monitor your app with monitoring i'll just show you what that looks like if we come into here for some reason google has rearranged all of my stuff where is monitoring deployment manager no it looks like a screen monitoring there we go so this will be all the information about your google cloud services that you've got running and logging so i've got one instance of app engine i've got six cloud storage buckets i've got three disks so if we come into app engine what does this look like there we go it's running response latency etc what else do we have overview i'll let you i'll let you explore this but yeah this is how you can tell how your app's doing um back to the keynote so there's a fair few steps that we did but again the point i'm trying to stress here is that model building is similar to model deployment it's about putting pieces of the puzzle together everything works nicely until you deploy your model into the wild remember the problems that you're going to run into what shape is your data what data limitations do you have problem number one that we have is every we had is everything looks like daddy remember we changed our model from model one to work which didn't work with donuts to model two which did work with donuts to model 3 which worked with donuts as well as the 10 food classes as well as not food classes again this will be problem specific i don't want to go to that keynote i want to go back to here problem number two is everything still looks like daddy remember because the machine learning model is what probabilistic okay so you won't the reason we emphasize the data flywheel is so you can figure out where your model is going to fail um that's in our case it was the simple user feedback prompt to tell us whether our model was right or wrong we could log that in a database go back through our database figure out where our model's most wrong and improve those scenarios making a better user user experience more people use our app our model gets better and it creates a beautiful flywheel this is the end the official end cloud services cost money remember no credits equals your money is going to fly away so make sure you go back through um this might actually build this time that's crazy in the live demo of course it doesn't build make sure you go back through all of your google cloud services so if you go home go to ai platform delete all the models if you don't want to be charged as you see here i've got some billing that i have to pay for delete your storage buckets delete your app engine instances and often times that's just clicking on whatever service you have running option delete that way you won't be charged so we'll come back where can you learn google cloud platform as we've discussed google.qwiklabs.com i'm also going to include a few little extensions um in the the github repo just down the bottom here somewhere extensions uh or where can you learn all of this stuff so there's a couple of great blog posts here that have inspired a lot of the work here so big shout out to marth mark douthwaite and lj miranda great blog post there and also google.qwiklabs.com you can go through a lot of different google cloud tutorials um made by google themselves so again this readme will be a bit more tidied up by the time that you um you come and have a look at it and then where were we keynote extensions again i mentioned this but you could try to incorporate continuous integration continuous deployment which means if you had your code on github which we do you could every time you make a change in food vision on the main branch and you could have a service that watches for that change and then once it's incorporated into the main branch it gets automatically redeployed as a new app new version of your app so say something like this codify everything everything we did was throughout the console and so you can actually make scripts to do that and then actually log the data from your app so save the logs to bigquery google storage or set up monitoring to really understand what's going on so this has been a machine learning deployment tutorial for google cloud for oh it's done what the hell see this is what i'm saying what is this did it just rebuild our app on appspot as well okay i think this is still running the original app but anyway the build the build is now working i'm pretty sure if i ran uh g cloud deploy run uh our app is going to work so again a lot of this stuff is about just troubleshooting um trying something out and when you get stuck searching for the answer and then going from there but as always happy machine learning if you have any questions leave a comment below or add an issue to the github of course my code is not perfect but that's that's something that you can improve on all the best building food vision and thank you so much if you've watched it all the way into here i really appreciate it um thank you so much for stanford to stanford and chip and all of the students from cs329s for having me as a guest lecturer that was my first time teaching anything live really and uh any time at a university so i had a lot of fun and would love to do it in the future i'm gonna make more videos like this again if you have any requests please leave a comment below i will see you next time peace why don't we try one more image for good luck hey and how about this beautiful photo of coffee how does that work coffee predict chicken curry damn it we're going to need model 4. [Music] you"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Cloud Deployment for ML Models",
        "Video Title":"How to Deploy Machine Learning Models (ft. Runway)",
        "URL":"https:\/\/www.youtube.com\/watch?v=tSiS15ubQFQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/tSiS15ubQFQ\/hqdefault.jpg",
        "ID":"tSiS15ubQFQ",
        "Publish Time":"2023-05-10T08:30:33Z",
        "Channel":"Thu Vu",
        "Channel ID":"UCJQJAI7IjbLcpsjWdSzYz0Q",
        "Transcript":"model deployment is one of the important steps in a data science pipeline it's getting more attention of businesses and data science teams nowadays as data science and machine learning Fields become more mature however it's been found that up to 87 percent of data science projects never make it into production that means that most Jupiter notebooks create by data scientists are put on shelves to collect dust and never get used it's getting easier than ever to create a predictive machine learning model but in fact many data science teams struggle to bring a model to real life to create actual impact so in today's video I'll be showing you how to deploy a machine learning model using Runway an Enterprise machine learning Ops platform developed by Makino rocks who has kindly sponsored this video this video is also an extension of an earlier video I made about how to learn four stack data science so feel free to check it out here for more detail if you're not yet familiar with it model deployment is the process of making a machine learning model available for use in production environments so what does it actually mean it means to take a machine learning model that has been trained on a data set packeted and stored in a format that can be used by the deployment platform you are using then we integrate this model into an application say a web app or system that can be used by end users users or other stakeholders can now interact with the model for example give the model some data input and get back the model output this is of course a metaphor to help you easily understand the concepts in reality you don't need to physically interact with the model but you can do it remotely through the model API we see how this looks like in a bit there are essentially three steps in model deployments firstly we need to prepare the model secondly we deploy the model to selected deployment platform and thirdly monitor and update the model we need to monitor the model performance to ensure that it is performing as expected over time the model performance will start to deteriorate as data is changed or there is new economic system situation or there are new customer behaviors or preferences when that happens we need to debug the model investigate what goes wrong and then the model should be retrained and redeployed it's good to mention that in big companies they usually have a whole team of machine learning Engineers to take care of productionalizing machine learning models using the model codes created by data scientists and the data pipeline created by data engineers in reality though in many smaller or less Tech oriented companies this is often not the case and that's part the reason why the model would stay forever as prototypes in Jupiter notebooks and never get used there are many machine learning Ops Platforms in the markets for deploying models they are complex products and It's tricky to compare them in detail now let me walk you through how to use Runway by Makino rocks it's a very simple and easy to use machine learning Ops platform I find it a bit more native and friendly to data scientists because it can work directly with jupyter notebooks but with a clever twist to avoid the limitations of notebooks which I'll show you in a bit in this stylized example we'll be using the auto MPG data set to build a regression model to predict the miles per gallon of a car we then learn how to solve the model via the HTTP API and learn how to interact with the model to get the predictions for a given data input alright let's go to the runway main page and I'll sign in using my credentials currently Runway is an Enterprise solution so if you want to try out a platform within your company you can contact them for free trial they are very very nice people to work with they are also now working on making the platform available for individual users so let's keep an eye on that after we signed in the platform we see here this is the runway service console and now we can go ahead and create a new project and here we can enter the name of the project and I call this project Auto MPG we can optionally put some description of the project here so let's go ahead and click on create now that we have have the project over here let's click on it to enter the detail page of our project at the bottom left of the screen you can see that there's a link area over here and if you click on the three dot button you can select either the CPU or the GPU for our project and I'll use the CPU for now because this is a very small project as a computer finished setting up you can see that the link area now has turned to the blue color let's click on the link area and now we'll be directed to a new web page that's opened up and this is the model development environment which is essentially The Familiar Jupiter lab environment but this is hosted remotely on the runway platform you can choose to either create a brand new Jupiter notebook over here or you can also upload a jupyter notebook that you already have in my case I have already created a jupyter notebook for the model let's now take a look at the model I insert some necessary packages here it's good to mention that this notebook makes use of the link extension which is developed also by makina rocks the idea of this extension is that it allows users to create a pipeline on top of the existing code cells to explain what that means if we click on the link tab over here you can see a diagram showing the whole pipeline of this model so if I zoom it in a little bit more each of these components on this pipeline corresponds to one code cell in this jupyter notebook it's all up to us how we Design This pipeline how the components are ordered and how they are connected to each other for example this cell or component load data set has the parent component import packages which means that the component is only run after we import the packages so the purpose of this link extension is to help everyone easily understand the dependency structure of the code cells and the execution order of the cells this can make projects built in Jupiter notebook more production ready I've made a whole video on the link extension a while ago on my channel so feel free to check it out if you want to learn more if you want to add a new component into this pipeline it's also very simple let's say we have a cell to print something like hello data nodes you can simply click on add to the pipeline button and enter the component name here and choose the parent components so it is very very simple but let us remove this cell for now so let me quickly walk you through this notebook to show you how this model is created first we import some libraries so pandas numpy and cycle learn and some additional modules from cycle learn then we import the auto MPG data set using this public data source but in practice it's most likely that we have to pull the data set from a database in your company and take a look at the data set we have the mpg column which is the column we want to predict and all the other attributes of a car that could be you use as features and then we can do some data preparation removing some missing values we also Define the target column which is the mpg column this step is actually much more complex in reality we may need to do a lot of exploratory data analysis and cleaning the data and transforming the data to get the data ready for modeling and we may also need to iterate several times to get it right and after that we'll split the data set into the training and the test set now we'll declare the runway regressor basically this is a model object where we Define the model to be used and the functions for fitting the model as well as predicting output and scoring the model then we actually train the model using the training data set and the training labels after training the model we test and evaluate the model next we'll declare an input sample by sampling a row in the training set in production setting it's often useful to provide model users with an input sample so that they know how the data should look like so let's say everything is good and this is the final model that we have we can now save the model to the runway platform as you can see all the cells in the notebook work correctly we can also verify if this whole pipeline actually works by running all components over here in the link tab if we go back to the runway console we can see that a new model called Auto MPG Rec motorcycle loan has been created if we click on it we can see all the methods of the model so in this case there's only one method that is predict and the schema for the input data including all the variable names and we also have the output data schema which is simply a numeric number as the output of the model prediction going back to development environment we can click on Save Pipeline and click on new pipeline because we don't have any existing pipeline yet and and name the pipeline as Auto MPG Pipeline and click on Save now here comes exciting Parts we'll create a model API so that other people can also access our model through the API to get the predictions on their data input so let's go to the API menu and create an API endpoint over here and let's give our API a name let's say it's API endpoint for tutorial we can now select the model that we want to attach to this API endpoint so we'll select the model that we have just created and the model method is predict and the model instance type I'll just use a micro one gigabyte memory let's click on deploy now that we have the model API running as you can see the status is set to working if the model service is running on the right panel you can see all the information of this API endpoint we have the content type we have the API endpoint URL that we can share with model users and we also have the data sample that we can download let's try to download this sample here and have a look at it this is what the data sample looks like and this input part specifies all the variables together with the data types and also the values based on the data sample that we created just now in the model pipeline one way to interact with the model API is through the terminal if you want to get predictions from the model for our own data input we can use the client URL comment using this format we are basically creating a post request to the model API endpoint which I'll copy from the platform over here and we can specify the input data that we want input to the model which is the sample that we have here and some additional arguments over here let's now run this comment in the terminal and Tada we have the model predictions for our data inputs as mentioned at the beginning of the video after some time the model performance will almost certainly degrade and this is when we need to retrain and redeploy the model suppose we have already done the retraining part by using new data or adjusting some things in the model code we'll then navigate to the pipeline section on the runway console click on the pipeline and then we can run the pipeline if our model has some parameters we can also input new parameters over here for our model but in our case having a simple linear regression model we don't have any parameters so let's just run the model by clicking on this run button and after the whole pipeline has been rerun meaning that everything is now updated we can go to the model page to redeploy the model so if we click on the model and click on the three dots over here we can redeploy the model we choose the target API endpoint for the model and select the methods which is predict and the deployment instance and we can deploy the model again this is basically how you retrain and redeploy the model on the runway platform at some point in the future we may no longer use this model and we want to delete the model and the whole project so free up the resources to do that we'll just first delete the API endpoint and then the model and the pipeline and finally the whole project this is quite straightforward on the platform we can easily deactivate or delete the API endpoints the models or projects throughout the workflow with a single click so I hope this demo gives you a better idea of how the model deployment process looks like if you want to try out the runway platform to deploy your own model check out the link in the description below and feel free to contact Makino rocks for a free trial and if you want to learn more about topics around model deployment and the Practical advice on how to design a machine Learning System I highly recommend the book designing machine learning systems by chiplin I have another video over here reviewing this book so feel free to check it out for now hope you enjoyed the video and thank you for watching see you next time bye [Music]"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Cloud Deployment for ML Models",
        "Video Title":"ML Model Deployment using Flask-webapp on Cloud: A Tutorial",
        "URL":"https:\/\/www.youtube.com\/watch?v=IiZReWJ0b98",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/IiZReWJ0b98\/hqdefault.jpg",
        "ID":"IiZReWJ0b98",
        "Publish Time":"2021-02-18T08:55:59Z",
        "Channel":"AI with Sohini",
        "Channel ID":"UC76OHAu-hC2GpcvR_hveJmg",
        "Transcript":"hello everyone and welcome to my channel this is sohini from south bay california and i welcome you today so in today's video i'm super excited to release my first video on ml model deployment so for anybody who is interested in learning how machine learning models can be made you know useful to to to people it can be released to customers in order for them to try it out so deployment using cloud platforms that is what we will be covering in the video today if this is of interest to you please give this video a thumbs up and subscribe to this channel and hold on for the blog that i will be posting in the description box below in towards data science which will lead you through a very systematic tutorial that i have released in order for you to try out your own first ml deployment pipeline so watch out for the video so let's start with understanding what is machine learning models machine learning models typically are in order to understand patterns which exists in data and in order to apply the same patterns that it's learned from some amount of training data to future unseen data sets and the best way in which you can make your your trained model usable is if you deploy it deployment means that it is made available to a certain you know link web link so that other people then can you know log on to that particular website and you know put their own use case or put their own data set in order to see what the predictions are and in this case a lot of times it could be like an a b test scenario where people provide feedback if this model is any good or not now in realistic production kind of scenarios what happens is machine learning models are generally deployed as a continuous integration or a ci and a continuous deployment or a cd pipeline where multiple people are committing code to a particular github or maybe say gitlab or just a git repo and from that git repo a build is then created for this particular web app which is then deployed to the cloud platform so this is called the ci cd pipeline now in today's video what we will be looking at is let's say that you have a group of machine learning engineers and they have committed code to a github link and i will be providing you the github link that you can fork and use in order to understand how this whole process works so what we will be doing is we will be we will be going over this continuous deployment pipeline piece and we will be doing it on on gcp so if you want you can actually sign up for your own gcp uh you know account and there you can go through the systematic steps in order to install and launch your own first ml deployment pipeline and once you are there now you know what you need to do in a continuous framework in order to keep deploying code to a github account and then to use that code and deploy it as a web app we are going to learn about continuous ml deployment and i'm actually going to be using uh this particular um ripple that i have forked so ideally if you know this is the the main repo what you would like to do is you need to start here and you just hit fork if you hit the hit fork then it's going to create a fork for you so i have already um you know forked this particular ripple it's on my cloud i've actually made one small change so it is a little different and i'll show you uh what that difference is in a minute first things first you need to sign up for google cloud and for for google cloud again you get 300 if you just sign up you will have to start creating a new project first so you go up and you say hit new project and you know you give your details and you know you have to add a billing account to it like i mentioned you have 300 free to to begin with but as soon as you get you know your new project so this is the project my ml project this is where i i will be focusing so this has a billing that is already uh attached to it and here today i will be showing you working with two things first is google cloud platforms the cloud build how to build a continuous deployment in the cloud and how to see the service running in the cloud so these are the two consoles again i will be putting them in the description box below that is what we will be running okay so the first things first what we do is we will download this uh this particular uh you know uh folder we need to see if the app runs locally or not so we are going to cd to the to the app and again cd to the app files and here we are going to say python app dot py and as soon as this happens you will see that it says something is active at the local host so this is what you copy and if you paste it this says your first gcp web app runs congratulations right and so this is how you are you know getting a hold of the app and it is running now in order for you to figure out if your continuous if the health of this app is continuously fine or not we actually need to run this loop kit so what we will be doing we will be opening one more terminal inside this request underscore test and and now if i hit python loop underscore get dot py and this is asking if it's a local test yes it is a local test and you see this is actually running right and you say congratulations let's try loop post dot py is this local test yes and you see uh your your data was was output so this is the actually the outcome of your um of the of the ml model that you have so this is the prediction and you see this is telling that the app is actually running so at the local host level this is continuously running so you know that this app works and you can you have actually now uh checked it on the local system so now the job becomes that things are working in the in the local system we now need to run it on the global machine right now the two things that is important that uh that we understand that the two significant files that are that are going to help us is the cloud ml the yaml file and the docker file so let's look at the docker file first and i'll show you how you can actually generate a trigger point using either docker or the yml ideally yml is is more robust and then uh the you know docker file is is thereafter now what dockerfile does is it creates a containerized version of this app so that it can be uh you know uploaded as as one single entity because there are a lot of files right in this whole application so this is containerizing everything into one place and making it as an app so it's very uh very thin requirements uh it has and so that's the bare minimum format that it has and then comes the the yaml file so the yml file first it is it is giving you the steps so pull the container if it is already built in this case yes we have already dockerized it and then you are giving it a particular name and then what you are doing is is you are you are pushing this this docker uh you know with all the container tags and then you are calling it for a cloud run so let's uh go to this triggers uh the google cloud build and let's create a trigger and this is uh what let me call it first first ml pipeline okay and what we want to do is we want to look at push rates because pull requests there is nothing much you can do from it so that means whenever you're pulling from uh from a particular gate so here you really want to be looking at the push request because you're continuously deploying it to a particular uh you know application now the repository is you can either connect to a new repository if you say connect to a new repository it will let you authenticate to your github now i have already authenticated my my github so that's why it is already showing up so i already have the repository here and this is the ml model github and here of course the the branches will also show up so i write now i only have the master branch so that is all i want now if i give the automated configuration as auto detect so it is either going to focus on a yaml file or it is going to look for a docker file right ideally your yaml is always preferred over docker so i just hit create so now that this is this is run we need to do one more thing is in the settings we need to go and we need to enable the cloud run admin this all these by default you see they are all disabled so the only thing you need to enable is this cloud run admin if the cloud run admin is not enabled you will not be able to see the final outcome all right so make sure your cloud run admin is uh activated and once it is done all you need to do is hit run and this is going to run the trigger it will say that this branch has actually started and you can start seeing the the the building blocks as they happen so now it's it's done successful and you see this final url shows up this is the url corresponding to where your app has been deployed so it takes a few instance uh for you to see that the app is running or not you will see your first gcp application runs congratulations right and the same thing now you can actually figure out uh using your localhost as well python uh loop underscore get dot py is this a local test no now you're running it at uh you know at the local host so this is what you copy and you paste here and you see your first gcp application runs congratulations so successful the other thing you want to figure out is if the post runs as well or not so loop post dot py this is local now i need to put the address and see it it works so now you have officially uploaded uh or or built your uh your your web app and you have deployed it and you have done it as a pipeline sequence and now you can also check the history of the pipeline sequence you can check the history of triggers and you can you know run diagnostics on it from time to time to ensure your pipeline is running smoothly you"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Edge Deployment for ML Models",
        "Video Title":"Training and deploying ML models on edge devices (TF Fall 2020 Updates)",
        "URL":"https:\/\/www.youtube.com\/watch?v=0d-2551pQcM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/0d-2551pQcM\/hqdefault.jpg",
        "ID":"0d-2551pQcM",
        "Publish Time":"2020-11-25T18:00:01Z",
        "Channel":"TensorFlow",
        "Channel ID":"UC0rqucBdTuFTjJiefW5t-IQ",
        "Transcript":"KHANH LEVIET: Hi, everyone. My name is Khanh, and I'm a developer advocate working on TensorFlow Lite. And in this video, we'll learn how to train and deploy machine learning models on edge devices. We will use TensorFlow Lite, a cross-platform framework for on-device machine learning. I'll show you how to train TensorFlow Lite models and deploy them to mobile apps. We'll start with building a simple machine learning app with TensorFlow Lite, and then I'll show you how to make use of the pre-trained TensorFlow Lite models so that you don't have to train models by yourself. And finally, I'll show you how to easily train TensorFlow Lite models at scale with Cloud AutoML. Now, let's start by building our first machine learning app. We'll do a \"Hello World\" example of on-device machine learning building an image classification model. And in this example, we'll use a flower data set, which contains five different types of flowers-- daisy, rose, sunflower, and so on. And we'll train an image classifier that can recognize these flowers. And then we'll deploy it to an Android app so that our app will be able to recognize the flowers that you are pointing the phone camera at. There are two steps required to build such an app. First we'll use TensorFlow Lite Model Maker, which is a library that allows you to train TensorFlow Lite's model on your own data set. You can train a model by just four lines of code. You start by specifying your data set. Then you choose which model specification that you would like to use. Then it will just work. The default one is already a state-of-the-art model architecture for on-device machine learning, so that you can just leave it with the default option. Then after training the model, you can evaluate the model. And finally, export it in the TensorFlow Lite format. In on-device machine learning, you have to constantly make a trade-off between accuracy, inference speed, or model size. So that's why we want you to not just customize models for your data, but also be able to easily switch between different model architectures. As you can see in this code sample, it can easily switching by choosing to use either ResNet or EfficientNet by just changing one parameter. Now, let's switch to the demo, and I'll show you how to train our flower classifier with modeling. We will use Google Codelab to train our flower classification model. We'll start by installing Model Maker. And then we import the necessary classes. And next we'll download our flower data set from TensorFlow. If you look at the data set, you will see that it contains five different types of flowers. And in each folder, there are many images of each flower. Next, we'll split our flower data set into training data and testing data. We'll use 90 percent of our flower data set as training data and the rest as testing data, to see if the model can generalize well to the new data that it has never seen before. And next, we'll train the model with the default model architecture. It will take a while to train the model. OK. So now the model training has completed. Let's evaluate the model with the testing data. We can see that the flower model is doing very well. It can recognize new flower images that it has never seen before correctly 93% of the time. And then we'll export the model in the TensorFlow Lite format. And then we download it to our local computer. In this example, we've trained an image classification model, but Model Maker also supports other use cases-- for example, text classification and question and answers. The R&E use cases such as object detection are coming soon. Once you already have a model, you can import it to Android app with just a few clicks using the Android Studio plugin. Now let's do that. We start with a skeleton Android app which can capture a video stream from the camera, but it can't recognize anything yet. The app only shows random labels in the bottom of the screen. And we'll build the app to use our TensorFlow Lite model to recognize the flowers. Here's the Android app's source code. First we'll delete the placeholder code that showed the fake labels. Then we'll import the model to Android Studio. We'll rename the model to something more descriptive, like Flower Classifier. Very important. And next we'll add code to use the model in our app. Here what it does, we'll start with loading the model. We convert the input image into the bitmap format, then again convert it into Tensor image, which is required by the TensorFlow Lite model. Next we feed the Tensor image into the TensorFlow Lite model, and then take the output, sort it by probability, and then take only the top result. Finally, we convert the output of the model into a format that can be displayed on the screen. Let's see how our app works. You can see that our app now can recognize different types of flowers that the model was trained on. You can also go through the tutorial that I have showed you in details, with all of the sample code, by following this Codelab. Please check it out. Now you know how to train a TensorFlow Lite model and integrate that into an Android app. But you don't have to train a model by yourself all the time. Actually, in many cases, you can just leverage the pre-trained models. And let me show you how to do that. You can go to TensorFlow Hub to download pre-trained TensorFlow Lite models. TensorFlow Hub is an open repository for TensorFlow model, just like GitHub for open-source software. There you can find pre-trained models not only from Google, but also from developers around the world. For example, there are image classification models to recognize about 1,000 types of birds, or over 2,000 types of plants, or another model to recognize more than 1,000 types of insects. There are actually many more models on other domains as well, such as text or [? sound ?] classification or speech recognition. So please check them out on TensorFlow. Integrating those models to your Android app can be as easy as importing the models into Android Studio, just like I showed you in the demo earlier. And beside TensorFlow Lite, Google also has a product called ML Kit for using machine learning on mobile. Because ML Kit builds on top of TensorFlow Lite, you can use it for both Android and iOS. It provides a list of pre-trained models through easy-to-use APIs for most popular on-device machine learning use cases. The APIs are very simple, so that you don't need any machine learning expertise at all in order to use them. For example, it can recognize text, detect objects, translate between languages, and many more, all on-device. Let me show you one example of an ML Kit API-- object detection and tracking [INAUDIBLE].. You can point the camera at an object, and the API will help you to detect where the edge of the object is. And here's how you use the object detection API. You start with creating the object detector, and then you convert the input model to the format that is required by the model. Then you run inference with the model, get back the bounding boxes of the objects in the image and the category of the object. So that's how you leverage pre-trained models on TensorFlow Lite and via ML Kit. Next let's talk about how you can train TensorFlow Lite model at scale with Cloud AutoML. AutoML Vision Edge is a service in Google Cloud that helps you train TensorFlow Lite models without writing a single line of code. Here are the three steps that you will need to go through. First, you will need to prepare your training data set, and then you train a model with Cloud AutoML, and finally, deploy the model into a mobile app. In the first step, you just need to upload your training data to Google Cloud using this GUI. And then you can choose some configuration to train the model. For example, you can choose to train a model with higher accuracy, but has a trade-off of a larger model size. Or you can choose a small model, which sacrifices some accuracy. Then after training, just download the TensorFlow Lite model and import it to your Android app using Android Studio. It was just like I showed you in the demo earlier-- the model can be integrated in your app in just a few lines of code. AutoML currently supports two on-device machine learning use cases-- image classification and object detection. So that was it for today, and I'm looking forward to seeing what you will build with TensorFlow Lite. [MUSIC PLAYING]"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Edge Deployment for ML Models",
        "Video Title":"Machine Learning Model Deployment on Edge Devices - Part 1",
        "URL":"https:\/\/www.youtube.com\/watch?v=HOG-o8fbOLk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/HOG-o8fbOLk\/hqdefault.jpg",
        "ID":"HOG-o8fbOLk",
        "Publish Time":"2020-03-01T15:07:58Z",
        "Channel":"AIEngineering",
        "Channel ID":"UCwBs8TLOogwyGd0GxHCp-Dw",
        "Transcript":"hi in this video we are going to talk about how we can deploy a machine learning model in a low-power and low compute environment typically what we called as the edge of the device right it can be your industrial machinery it can be your mobile phone it can be a drone that is flying it can be anything like how you can deploy a model into an edge of a device now what is so special about these scenarios when we talk about edge of the device right the they do not have the luxury of memory that our computer or our servers as it does not have the luxury of CPU that our service our service can run up to 96 core CPUs but typically the edge of the device may have maximum quad-core octa-core right so there are a lot of constraints that you need to take and deploy the model and when we talk about a typical neural network model it has lot of parameters and finally the saved model will be multiple MVC and even in some text can be even GPS as well so how can we make these kind of models more efficient that it can be deployed on an edge of a device like there are multiple names to it a general etics that is like post model quantization that is there are like kind of weights tuning now these are all true to it but it's an electic so you talk about a genetics this is not the only way we can do as your analytics regenerate this is combination of even advanced analytics you don't need a predictive modeling as well but you can also deploy a predictive solution over that so let's focus on how we can do it right so before before going there let's talk about some of the use cases of ng analytics first of all I have a detailed video on edge analytics and if you want to know what's a analytics then you can click the link on the top like some may feel this are like more bleeding edge technology not really this this is something like happening today right and we can see more and more instances and use cases coming out in the future so you can click the link on the top I or even and you can watch what is a general now let's let's quickly talk about some use cases the first is if you will be me know self-driving car at autonomous vehicle now if you see that typical autonomous vehicle it has lot of sensors attached to it now if you add enormous vehicle need to recognize the environment the weather that car is driving on well that's a pedestrian before where there's a car in the front backs right hand side left hand side it's taking a turn it needs to know about the environment or when changing a learn it needs to understand signal it means to understand object so basically I said autonomous vehicle contains nothing but an a bunch of all sensors it can be a combination of deep learning model or machine learning model running over there now these are one kind of constraint devices right and when you are deploying a model you want to keep the model as clean as possible at the same time you also want to make sure it is more accurate right when you when you are kind of cutting down the weights or pruning the weights you don't want to lose that BAC aspect of it so we will see some scenarios how to do it so this is this one of the example the second example is in the defense so so some of the country cell large border to maintain and you want to make sure people don't need filter it and everything so can be can be used lightweight video monitoring devices over that that can that can alert in case of any activity in the border so basically you can take a model and deploy it in the video via video system and then it can monitor it and if it finds any anomaly it can more like alert the the main station back that is predictive maintenance today in industry and how did initially can cause like lot of production basically production backlog now they may have alternate machinery as well to compensate for it but during your peak season they want to continuously monitor the machinery and if something happens to the missions they want to get notified as soon as possible so basically these kind of models can be deployed in the inside the machines so that like the the model like monitors continuously the activity and data and alert in case of an anomaly mobile device or you can also make mobile device as an example where you want to do some AI for good right there are a lot of people who cannot see small letters properly maybe you can deploy a model inside inside the mobile phone and when they scan that particular text it basically reads or it just converts the text into speech now so lot of people who are kind of disabled or not able to see properly can get benefitted can get benefitted by from e-i so we need some solution that can make it happen and that's what we are going to talk our talk today right so let's get into the notebook quickly now I have mounted my collab drive which has all the models as usual if you have seen my previous video would have you know kind of know it I'm going to list on my folder so I have a lot of models here and in my previous one and use the sentiment analysis but in this I am going to use the fashion TPU model this fashion TPU model was created using tensor flow and into either star in one of my videos where I am talking about GPUs and TPU training in case if you want to go and see and run the model and save the model you can just click the link on the top by taking through like how to start with tensorflow 2.0 how to done tensorflow pro 2.0 and GPU 70p use what distribution strategy to use so you can use the link on the top and you can go over there and check it out but but I'm just going to take the output of the model and run it alright I am starved I am importing all the packages that are required the only thing I need is tensorflow but I am just importing numpy and matplotlib to visualize the output so now now tensorflow is selected so let me yeah so first thing what I am doing is I am I am basically loading the model facet is right this is the this is the model that we have tensorflow TPU I'm using the chaos load model and then I am printing the model summary it will give me like what is the model layers and everything so you can see basically this model as around 1 million trained weights that's a compilation world network that's a another compilation world but there are multiple compilation neural networks and everything but let me skip it Sam that are around 1 million close to 1 million parameters in the model the very first thing I am going to do is I am going to use the TF light over here so basically tensorflow light is provided by tensorflow itself it is designed to execute models more efficiently on mobile and other embedded device which have limited computer memory resources now some of this efficiency comes from a special format that they used to store the models typically they use flag buffers to store the model and tensorflow light is compatible with Android iOS or C or even Swift or other languages so basically you can save the model in tensorflow light and then you can use use the other libraries to deploy the model right and the converting this model you can see like how it reduces the file size I will show that around but converting the model reduces the file size but it also made sure the accuracy is not impacted so that's that is the primary purpose of tensorflow light so let me let me quickly show you the show that to you so if you see your pencil flow I'm what I am doing is I am from TF I'm importing tensorflow light converter and from Kira's model the model that we had on the top right from the Charis model I am loading the model object and then using the converting into TF light and then I'm using the converter object dot convert so if I run this it's going to come take this model and convert it into more convert it into more efficient storage provided by TF light now if you go on the top the the model size that a fashion TPU model size we had was around er I don't know how much it is 14 MB right it's on 14 MB now after the conversion let's see like how much this particular model sizes so if you see the model size is this in Cadiz so this become MB it's 4 MB so from I don't 10 10 MB of size it has become 4 MB so almost like it's it's kind of of the less than of the size right now this is one part of converting it now I can further you know when we do a tree of line it just that's a efficient storage it does not it the accuracy will be mostly same that will be some point naught naught 1 percent drop but that is fine that will not impact the output the one more thing I can do is I can do post model quantization so what is post model quantization is it takes the output weights typically the output fields are stored in a float 32 format if you do a model dot weights it will show all the weights you can see it's a float 32 format in this case what I am going to do is similar to a quantization I am telling basically go and approve the weights go go and trim down the weights so basically I can tell whether I want to cut down the float 32 to float 16 or float 32 to 1816 or indeed so as I go more lower the the model becomes more more lesson size but the same time you may see a drop in accuracy as well it's up some drop in accuracy so that's what the quantization does so in this line if you see what I am Telling is use the optimized method and optimized for layer latency now there are multiple methods to it if you click on this you will find optimized for latency optimized for size and default so you can give any of the options I have given optimize for latency now and then I am calling again converted or convert and context and again I'm saving it as a tensor flow rate counter is model it will further reduce the size and we are going to see how much it reduces the size by default it's going to do a float 16 now if in case if we want to do even further down I have commented out some lines you can use it you also need to pass a data set because when it's trying to quantize it its equals to make sure that the model that the data it has seen also it's run across see how it performs so that's why Dean sedate I said I'm not going to do that but it's commented out if you want to do and this for integer eight if you want to do it in integer it you can do it yeah I'm just doing optimized for latency and then saving it now let me turn this slide and then let's quickly see the size of the context model so now you see it's only like one MB so from the 10 MB model the T of light converter converted into around 4 MB and now after quantization basically the it's on 1 MB so it has reduced even further right now how does this model perform compared to our previous model and and if you want to know more about quantization model quantization again I have a separate video on that you can go and look into it it's a pretty detailed video it's a short 3 minute video but said you take video you can click the link on the top and go and check out post model quantization there are other methods like prima quantization during training of the model but but sometimes like you train the model and do a post model quantization that is also an option and the video I'm talking about is post model quantization and what we have done over here is post model quantization we have taken thicken the bottle the touchscreen and then applying quantization so that's what we are doing here right now let's see how it performs compared to our regular model so we have one model that is chaos model and then we have one model that is ta flight model and I'm going to use the effect model to check the output but if you want you can just change the quantized model and also check the output but let me quickly go and import the data set I am using the fashion mm steel data set by crosswind on and I am going to use the test data set to evaluate the performance the test data set has around 10,000 records that's a good number to check the accuracy of the model the first thing I am going to do is I am if you see here basically it has the number of instances it has the height and width of the image it does not have a batch dimension we need the badge dimension because our model was trained using mass dimension so I am just expanding the dimensions to include a batch dimension right now this is how I load in so when you are deploying the model you you need to start from here right now this is how I loaded tensor flow light model this tensor flow light model it's nothing but the object that I have saved during the T of light converter right I can also use the TF light container context model over there if I want to use the quantization model but let me show you the T of light model which gave us from 10 MB to 4 MB size right so I am calling the TF light interpreter and then once the interpreter comes I am just allocating all the tenses from the output of the model and what I am doing is I am getting the input tensor which is the if you in the water will have an input tension outputs are basically input tensor is the data that we feed him so I am getting the input tensor from the interpreter and I am also getting the output tensile where my final output will get stored after the modulus after the models cause the instance so this is what I am doing in this four lines and this this four lines is what you need to deploy a model so if you are going to deploy on a Android device just install a TF light package and then you again get started you don't have to install the entire tensor flow library you are the inside entire tensor for library will be pretty big you just have to install the T of light right there's a Java version as well you can use Java to also run to your flight right now I will quickly get the tensor details so basically it shows all the tenses that are there within the model and its key I am not using the contact context version so it's selling the context is only 0.10 right now next to what I am doing is I am printing all the layers just to see the weights if you want to check the weights you can check it this is the weights from the TF light model and if you want to check the weights from the Charis actual model also you can check because internally in the model you want to make sure how the weight looks like and when you do do the same thing for a quantized model you will see basing the weights are represented sometimes as hint because it was able to optimize it for them that's why the saving from 4 MB to 1 MB sighs okay now what I am going to do is I am going to take all the images right the X test data set I am going to take all the images and then I am going to basically call the interpret the interpreter is basically the object the T of light interpreter that we are we are going to invoke the interpreter and then we are going to kind of see the we are going to store all the output in the prediction output list that we have created on the top so whatever the score is the final fashion immunised as ten classes so whatever class output is that it's going to store that in the prediction output once we have the prediction output in the next one what we are going to do is we are going to validate the accuracy so here if you see now I got this this ran so I have all the ten thousand images that I have I am having the prediction score as a list Here I am going to run to the prediction output I am going to compare if the prediction output is equal to white s that means the model has predicted accurately otherwise it does not and then I'm scram I am finally printing my accuracy so the accuracy is around 91.6% and that was the same accuracy if you are seeing my tens of retaining video that's not where your accuracy we got in the model itself right so basically there is no loss in accuracy and how can we make sure let's quickly go and visualize some ten images from the test dataset and see like how well it has done compared to the regular model so what I am doing here is I am basically running for ten images and I am going a plot or two images I am getting the actual label which is the widest data set the tensorflow regular model that we have the tensor flow model which we loaded in chaos they are active it is not happy of light or quantization I am printing the prediction of it I am also printing the petition of quantized model which is the prediction output I used on the top and let's quickly see how the data looks like so if you see this particular ten images you can you can see the actual label is seven and tensor for regular model label is seven and context model is also seven and similarly if you see on the top the actual label is Phi tensor for regular modulus Phi context model is also five all right so basically there is no difference in accuracy over here but in case if you are using the context model you may find some accuracy difference may be in in thousands of images you may have liked point not one percentage that it may not be same because when you are training the weights it may not highly impact your image model or text model but it can impact your a model if you are running a new network on tabular data you can see some impact but otherwise it's not going to be of a big impact to you right now let's benchmark the time it takes I am going to take the 10,000 images and run it with the context model and also with the final final what will the Charis model right so first I'm running the contest model and the context model what I am doing is I'm taking all the 10,000 images the extras 10,000 images and I'm just called Carney interpreter in book and I'm just printing the total time it takes I am taking the start time here between the start time and end time how much time it takes for the 10,000 images and you can see it actually takes 19 seconds right now let me do the same thing for the chaos model in this case I am using the model dot predict function of Kira's so this a regular model so I am NOT going to wait till it completes but you can see the time I just save that time Yates takes around three not five seconds so basically this is almost like if you if you take this time is almost twenty times or eighteen times more than what the TF light output is so the parent is like when you are deploying on edge of a device you want faster inference and because you are adding a low computing adornment and at the same time you need a lesser model size because you are deploying in a low memory device and TF light converter along with the quantization of model can give you a very lightweight model of your regular model you are not doing anything we're just taking the model and doing a post treatment to it and also the runtime can be substantially increased because when you are deploying on edge of a device you want to immediately detect and send the response back right so the this this more about quantization the quantization can also work on GPUs so if you have a GPU device on your H device like you have a HTTP you or any of your GPUs or something like that you can also use GPUs to do your petit which will increase your runtime even faster now in a future video what I'm going to do is I'm going to take a similar model a quantized model and maybe I will deploy it in a Raspberry Pi and show it to you but that's going to be later in time but this should give you pretty much to practice and if you want to try with the Android have deploying this model there lot of documentation or tensorflow website you can go and check it out thank you"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Edge Deployment for ML Models",
        "Video Title":"Deploying ML Models in Production: An Overview",
        "URL":"https:\/\/www.youtube.com\/watch?v=Mrv3CZNWYEg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Mrv3CZNWYEg\/hqdefault.jpg",
        "ID":"Mrv3CZNWYEg",
        "Publish Time":"2022-06-27T08:15:55Z",
        "Channel":"Valerio Velardo - The Sound of AI",
        "Channel ID":"UCZPFjMe1uRSirmSpznqvJfQ",
        "Transcript":"hi everybody and welcome to a new mini series in this series you're going to learn how you can effectively deploy your machine learning models into production the mini series is going to be made up of two videos this video and the next in this video you'll learn different strategies that you can use to deploy machine learning models into production from a theoretical perspective i'm also going to give you an overview of the different ml deployment tools available on the market in the next video we're going to take one of this ml deployment tools namely bent ml and we're going to use it to deploy a sample machine learning model into production let's get started there are different strategies that you can use to deploy machine learning models into production one is to take a model and wrap it within a service and serve it through a rest api using endpoints another one could be to take your model and directly deploy it on an edge device like arduino raspberry pi but if you don't need to have external access and rather you need some sort of offline computation you can just do batch serving of course you're gonna decide on different deployment strategies depending on the use case that you're tackling in my practice as a consultant in machine learning machine learning operation most of the time i see option one strategy one used in other words going for serving a model through a rest api now let's take a look at a basic approach to ml deployment and in particular to deploy a machine learning model within a rest api of course we would start from the model itself the model can be a keras model could be a pie torch model scikit model whatever you want really now you take the model and you build an api around the model using a web framework like flask or potentially fast api at this point you have all the ingredients to serve your model to the external world so you can take your service and deploy it directly on a computation unit like aws ec2 and instance right but there is a more sophisticated and better practice approach which is rather than taking your service and directly deploy it onto a computation instance you could just containerize it so you create a docker image out of it and now you can take the docker image and then deploy it directly on an ec2 instance for example or on gcp wherever you want really or if you have a system that's more complex because perhaps it's made up of many different models with a lot of different use cases uh the best case scenario here would be to take your docker image and then deploy it on in a kubernetes cluster in this case you would have sort of microservice architecture now this is an approach that i've seen used multiple times well i mean all of the different approaches that we've seen so far but this basic ml deployment strategy or strategies because it's more than one really have some drawbacks so they have some disadvantages first of all the whole process is quite convoluted you have to go through a lot of different steps for example you have to package both the ml code and the model artifacts together and you have to do this in a custom way and it can be a little bit of a hassle doing that also you have to create some infrastructure around your solution the infrastructure is needed for yeah of course creating the kubernetes cluster or creating a monitoring infrastructure that will monitor all of your models deployed into production or it can be as simple as creating some documentation around your service api none of that comes for free because you're doing everything from scratch on top of that there's the issue that the web server that you're using most likely is not going to be optimized for ml inference don't get me wrong here i think that flask and fast api as two web servers that you can use are fantastic but they're not particularly designed for ml inference they would expect high throughputs like for example for many web applications but not high computation so in that respect the web servers that you're using are not ideal for the use case of ml inference for all of these reasons we see now the advent of ml deployment tools on the market some of these tools are open source others are property here i just want to list a few of them and guide you through some of the pros and cons that they believe they have and then we'll focus on the one that will analyze more in detail okay let's get started from tensorflow serving now tensorflow's serving is a deployment tool or framework if you will which leaves within tensorflow extended tensorflow extended is a sort of framework that manages the entire lifecycle for a machine learning project so tf serving is just a part of that but it's quite powerful it allows you to deploy directly your tensorflow models now this works like a charm whenever you're using keras and tensorflow models but if you're using other models like pi torch or psychic learn of course tensorflow serving is not going to work for you also the other issue that i have with tensorflow serving is that just like for most of tensorflow documentation isn't really that great another valued option is ammo flow once again ml flow is not just a tool for deployment there's way more um to that and indeed ammo flow takes care of the entire life cycle of a machine learning project but it has out of four units that makes makes it up it has one that's called ammo flow model now ammo flow model is a standard that allows you to package an ml project in a way that can be easily deployed into production but now you don't necessarily have a simple way or direct way of doing this deployment you just have a very nice way of packaging uh your project and to easily dockerize it containerize it and then once you have your container you can deploy it wherever you want so ammo flow model i think it's really really cool as a solution but it has some issues so first of all it can only be easily deployed into azure and siege maker if i remember correctly then the other big issue that i have with this approach to deployment is that you're gonna get a lot of libraries you're going to get a lot of noise that you don't want and so it's not really a lightweight solution let's take a look at another option which is selden now selden is yet another framework that uh manages the entire uh sort of like ml workflow uh the cool thing about it is that it is completely integrated and built on top of kubernetes so if you are thinking of using like kubernetes and deployed there your models then seldom is a really good solution the only issue that i have with uh is that it is uh sort of like created by a company and it is distributed by a company so i personally prefer to go with completely free completely open source solutions and regarding selden so i said that this is a full framework for ml projects but it has a part of it that's called selden deploy which is responsible only for deployment on uh the department of your projects there is also another great alternative if you want to work on kubernetes and this one is completely free completely open source and it's called k serve k serve is a part of kubeflow kubflow being this ml workflow framework that has been developed by engineers at google and it's been used internally to work on machine learning projects this is quite cool because it allows you to serve directly your models into a kubernetes cluster and it is part of the the kubeflow environment so if you want to do more than just deployment you can easily do that and of course if you want to do all of that on kubernetes now the problem with k-serve is that it is quite complex to set up and uh also it's just like seldon you are completely locked into kubernetes so if you don't use kubernetes then neither k-serve nor celgen are good solutions for you in most of my projects i've used banter ml this is a killer solution for deploying machine learning models into production benchmarks tagline is simplified model deployment i think this sentence here really summarizes well what benchml is all about so benchmark is an open platform that simplifies ml model deployment and enables you to serve your models at production scale in minutes let's take a look at the different features that banter mail has to offer first of all it is service oriented deployment in other words with band to ml you can create rest apis if you use ben2ml you can throw out of the window flask and fast api whatever web server you're actually using because benchml is going to replace that and the great thing is that the option that benchml offers is way more performant than flask or fast api because it is indeed optimized for machine learning inference now the other great thing about benchml just like ammo flow model it allows you to package all the necessary artifacts for a successful machine learning deployment into a single unit so you're gonna take your model you're gonna take your code and you're going to package it into a bento a benter is the unit of deployment used in banter ml the great thing about mantis is that they're going to be stored locally in a registry and this registry is going to also version the different banter another cool point about benchmale is that it supports all major machine learning libraries so that you can use python scikit keras tensorflow no matter what you use here you have a solution that works with all of them a banter also integrates perfectly with docker you can take a banter with a simple command line instruction you can take a docker out of that that then you can deploy wherever you want bento makes deployment on kubernetes also quite straightforward thanks to a tool called a yatai let's take a look at it so yatai is built on top of bento and it allows you to take your banters and deploy them at scale on kubernetes in a very simple and straightforward manner another great feature that bantamow has is documentation automatic documentation this is a little bit of a hassle when you create apis right so you have to create also a an open api kind of documentation and all of that is generated automatically for you so this is really really cool of course bentaml has some cons and i think i've identified a couple of this so first of all benchmade works only with python so for example if you are using go for your machine learning models well then benchml is not going to cut it for you because it's a python only library and more than that the other aspect is that benjaml is a tool focused only on deployment which basically means that all the other aspects of your ml pipeline like building and training your models or tracking your models it should be done with other tools so if you're using benchml you have an extra tool to deal with and this is different uh if you compare it for example with a kubeflow or ammo float there you basically have the entire um sort of workflow ml workflow done for you in this case you have to deal with an extra tool that does only one thing but i think the price is worth paying because deploying with benchmark is actually quite easy by now you should have a decent understanding of different strategies to deploy machine learning models into production and the different tools available in the next video we're going to be focusing on bento ml we're going to create a sample model that classifies the famous amnest digit data set and then we're going to deploy it into production using benchml i'll see you next time"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Deployment and Serving",
        "Topic":"Edge Deployment for ML Models",
        "Video Title":"Secure ML Model Deployment for Edge AI Systems at Embedded World 2025",
        "URL":"https:\/\/www.youtube.com\/watch?v=gkPt_Xor5U0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/gkPt_Xor5U0\/hqdefault.jpg",
        "ID":"gkPt_Xor5U0",
        "Publish Time":"2025-03-12T12:24:34Z",
        "Channel":"aicas",
        "Channel ID":"UCCbn0WKjgmfpL0TqeYxXRug",
        "Transcript":"hi I'm bman from we here at ined World 2025 I'm joined by like us we're here to present something very Innovative which is a secure ml deployment mechanism solution that was developed between the two companies to address uh security concerns that the new markets needs to address very very soon which is to really make sure that your IP is protected so I just want to get to a dialogue with cl so we see a lot of customer requests and I I'm and machine learning Is On The Rise um it's a it's a trend where we also support and where we try and for our product and we are a fles man manufacturer bringing Out Security Solutions that memory stuff and um we joined um basically to help um find solution securing um updates and hard and software comb andw comb excellent so Michael we approach the challenge that we see in privacy for AI model upd models by combining the capabilities of the two companies where IAS is respons for spending a secet on Hardware to make sure that the device runs robust and never has down combining this security enabling security byid comp Sy prodg into the system we can that content that we deliver to embedded systems is not tempered with and no private data gets perfect and I I know there is also a video online somewhere that shows that is that right right you find more details [Music] about okay great and I all theity you [Music] system so basically we are FL manufacturer and we don't care about so we can translate this into all of our phors which we can provide okay so if you want to know more there's two ways you can do that number one you come to a better girl in all one what's the group number here 510 510 don't miss it and you can see more to the weinar on April 8 which is actually going be uh show the Sol and you can learn more register the i.com website or shown on the video so best regards from World we're having fun I hope you have fun too by"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Pipelines and Orchestration",
        "Topic":"Building ML Pipelines with Airflow",
        "Video Title":"Managing ML pipelines with Airflow -English version-",
        "URL":"https:\/\/www.youtube.com\/watch?v=XNkiYUtgczg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/XNkiYUtgczg\/hqdefault.jpg",
        "ID":"XNkiYUtgczg",
        "Publish Time":"2019-12-18T04:07:24Z",
        "Channel":"LINE Developers",
        "Channel ID":"UCuzvuO3E9XIoreE9u7B1ETw",
        "Transcript":"hello my name is Khalid I am from AI services lab line I'm data scientist and software engineer sometime so today's talk is about managing machine learning pipelines using air flow so anyone have used air flow here okay so I'll try to adjust depending on people's opinion so here is a agenda basically for topics so first of all about introduction about air flow so airflow is workflow management system where you can schedule your jobs and may it it can work for ETL for machine learning or any job patches you have so here is an example on the left side of code basically you can write workflow using Python so first you make deck tag means directed acyclic graph so you make the deck and then you can add tasks to your deck so in our case we have three tasks so pre-processing and training and then inference and finally you can set the dependencies between your tasks so in the result after interpreting this code you will get something like this on the right side okay about architecture of air flow I'm not gonna go details of architecture it's basically a master\/slave architecture so you have master node and slaves you can scale your app by adding more slave nodes and where is a queue to distribute the jobs and where is meta store for consistency and then we deploy using ansible basically it's important to have all your nodes in sync the code on all the nodes should be in sync and for H a we have the standby node so if the active node is down we switch to the standby node now I'll turn move on more to the ml part how we use airflow so this is an example of our deck basically the whole machine learning recommend so we're working on recommendation system so whole recommendation system you can see in one view so you can see all the logs from pre-processing to training inference validation up HBase so this is a system view so we use park for pre-processing then we do training it's local machine training then we do inference in kubernetes and after everything is going through Kafka - HBase so this is general view one thing I want to talk about about is the first stage the first operator so in airflow every task is called operator it can be SSH operator pison operator or bash operator so the first one here is something called branch operator so in branch operator you can separate your work flow for example first the top one is for production and the lower one for beta stage so depending on environment you can select multiple passes of your code so for in production we run the green green means successful so in production we run this one and on the beta we have running that first pass so basically you can use branch operator for different passes okay next thing is usage of dates so usage of dates is very important in air flow because we want to keep our tasks stateless first of all and secondly consistent so you want to have the same date in all the tasks so air flow provides something called ginger templates so you can maybe you have heard about flask and ginger templates so you can use exactly the same ginger template in air for here is an example of that the double curly braces means the start of syntax for the ginger template so you can basically use TS min State stamp so basically before executing the airflow this task so this is bash operator task it means it's very strained SH train SH is just you're starting your training right and you pass argument argument is date you can have multiple arguments but one of the arguments is normally date and it will be rendered something on the right side so depending what format you want you can select different formats D s or D s no - or you can actually access the price on paid time object the third example is accessing the date\/time object so before rendering you will execute Python code actually ok so a little bit more details this is a little bit tricky part so for people who are not aware of airflow it's a little bit complicated to explain the key part here is two key parts first the start date doesn't necessary mean when it will be run so it may it will be run actually after start they start date + scheduled time so the first task if we start date put today it will actually write around tomorrow and the execution date will be today so this is a little bit tricky part for the advanced users of airflow so after you start using I would suggest to go in details of this slide okay another thing very useful in air flow is notifications how you can set up notifications especially in machine learning pipeline because we want to know how is the training going we want to keep track of the loss of training loss validation loss or what is the our accuracy of algorithm etc so keeping track of the whole pipeline is important for that we have something called callbacks in airflow you can have success callback or failure callback for the deck or you can have same callback for the task level so this is more fine-grained this is tag level and then you can receive this kind of alerts for example training successfully finished and training took took four hours for example 22 minutes and this number is the learning rate was this one for example so this is very important especially in the machine learning pipeline next how we implemented the monitoring of training and inference part so for that let's take a look about training so in training one of the key metrics is loss the validation or training loss so sometimes depending on the data the training the value of the loss can be overflow or underflow on it can be nun or if or some other values so in order in that case we cannot use the training the training is invalid the model itself is invalid so we need to restart training by adjusting hyper parameters one of the hyper parameters we adjust is normal learning rate we decrease learning rate and restart the training again so in this case if the loss is invalid we basically restart training so keep training until the loss is valid so this is one of the key important parts which you can check all in the airflow so air flow will keep the logs and also will send the notifications so this is our implementation you can send notification to slack channel to lion messaging group etc anywhere another thing is monitoring the inference so once inference is done we need to check if the classification scores are valid if it's between zero or one for example what is minimum maximum average or we need to keep track of duplicates or coverage so what is the coverage is the coverage change or no so this is done on the stage after inference we do validation year and check was a format so this is for example score minimum score maximum score an average score okay and one last thing I want to talk about is using something called communication between communication between tasks XCOM feature so if you want to pass some argument in our case signature signature of the model you can push it to the task instance and you can pull it from task instance it's not you shouldn't use it always but sometimes it's useful okay so I think this is it yeah thank you very much for attention yeah thanks"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Pipelines and Orchestration",
        "Topic":"Building ML Pipelines with Airflow",
        "Video Title":"What is Data Pipeline? | Why Is It So Popular?",
        "URL":"https:\/\/www.youtube.com\/watch?v=kGT4PcTEPP8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/kGT4PcTEPP8\/hqdefault.jpg",
        "ID":"kGT4PcTEPP8",
        "Publish Time":"2024-06-11T15:30:11Z",
        "Channel":"ByteByteGo",
        "Channel ID":"UCZgt6AzoyjslHTC9dz0UoTw",
        "Transcript":""
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Pipelines and Orchestration",
        "Topic":"Building ML Pipelines with Airflow",
        "Video Title":"How to build and automate your Python ETL pipeline with Airflow | Data pipeline | Python",
        "URL":"https:\/\/www.youtube.com\/watch?v=eZfD6x9FJ4E",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/eZfD6x9FJ4E\/hqdefault.jpg",
        "ID":"eZfD6x9FJ4E",
        "Publish Time":"2022-03-07T22:36:17Z",
        "Channel":"BI Insights Inc",
        "Channel ID":"UC8aox1k3cd00tTKuBNt4tMw",
        "Transcript":"[Music] hello and uh welcome one and all today we will automate our python etl pipeline we developed an etl pipeline in the previous session we will build on it today to give you a complete overview of extract transform and load link to the previous video is in the description below we will use apache airflow to automate our etl pipeline apache airflow is a widely used open source workflow management system it provides data engineer with an intuitive platform to create schedule monitor and maintain their complex data pipelines airflow enables you to manage your data pipelines by altering workflow as directed acyclic graphs also known as dags of task you manage tasks schedule as code and you can visualize your data pipeline's dependency progress logs code and status via the user interface airflow enables you to set up data pipelines over data stores and data warehouses run workloads at a schedule create and manage script data pipelines as python code in this session we will use the task flow api introduced in airflow 2.0 task flow api feature allows data sharing functionality between tasks however the data exchange is limited only to small json serializable objects such as dictionaries we can't share large data frames from one task to another without serializing it first task flow api makes it easier to author clean dags without extra boilerplates by using the task decorator airflow organizes your workflow in dags composed of tasks so what is dag a dag directed a cyclic graph is the core concept of airflow it represents a group of tasks it is organized with dependencies and relationship to say how they should run tag is defined as python script that represents the dag's structure tasks and their dependencies as code to run a dag with any service we can use airflow's ui to set up connections such as configuring our database credentials so we can connect to sql server and postgres airflows ui enables you to observe the pipeline running in your environment monitor the progress and troubleshoot issues when needed airflow's visual tags also provide data lineage which facilitates debugging of data flows and aids in auditing and data governance let's open pycharm and start with our first tag i will refactor our python etl script to make it compatible with airflow as usual we will import the required libraries at the top along with usual libraries we will import few airflow libraries especially dag task and task group then we import few hooks these will help us establish connection to our databases we define the connection in the airflow ui i have two connections defined here to our source and destination databases under admin console we can add a new connection by clicking the plus icon here we can select a database and define connection details we reference the connection in our code by the connection id okay i'll move back to the code and finally we import pandas and sql alchemy we can use the task decorator to define our tasks first we define a function to get table names from sql server just like the previous session since our connection is defined in airflow we can get a hold of it with connection id and with the help of ms sql hook then we supply it a sql script that gets the table name from the system schema in sql server the sql hook has a built-in function to get panda's data frame it takes sql query as an argument we save the data frame in a df variable and print it since we only can share serializable data between tasks i'll convert it to a dictionary and this function will return a dictionary with table names we will define our second task with a function called load src data this function takes a dictionary as an argument which is returned from the previous function we get the connection detail for postgres with base hook remember connection details are saved in the admin panel and we save it in a con variable with the help of sql alchemy we create a connection and provide the connection details via con variable let's define an empty array i'll call it all tbl name to store the table names and we will return this at the end of this function we iterate over the dictionary items to get the table names let's append the table names to our all tbl name array to display the number of rows we are importing i'll declare a variable and we will set it to the length of the data frame we create a dynamic sql query here with the table name coming from the dictionary value and with the help of f string and once again we will make use of sql hook to query sql server with our dynamic sql query and we will save the output to a data frame we print a message how many rows we are importing and from which table now is time to persist this data in call the 2sql function and prefix the table name with src to indicate that this is a source table we print a success message and return the table names array at the end we are done with data extract it's time to transform this data and once we're done with the transformation we will persist the data in staging tables in the first task we will transform dim product table let's query the source table to preview the data this will give us an idea where to apply transformation we see that there are columns in french and spanish that we can get rid of there are a lot of columns with nulls or missing values we can set defaults for these also there's a large photo column that we don't need for analysis so we can delete this as well last we'll rename few columns where the name contains english for example the description column is english description so we can drop english since we drop other language columns let's see how we carry out these transformation with pandas we will name this task transform dim product let's query the source table from postgres into a data frame we will use pandas to implement transformation if you need a refresher on pandas i have covered this in this video feel free to check it out first let's drop the unwanted column we will provide the list of columns we want to keep from the data frame let's save it into a new data frame called revised then we replace nulls with zeros for numeric columns and n a for string columns let's go ahead and rename columns where name contains english since all of our columns are in english now we are done with transformations and let's save this into a staging table with two sql function we perform similar operations in product subcategory and product category drop the unwanted column and rename few columns and save the updated data to tables with sdg prefix to build the final product model we will query all three tables there is a data type mismatch here so we'll convert the column to integer to match the data type and join or merge the three tables into a single data frame then we save this data frame into a table with prd prefix so this will be the final presentation table let's declare a dag and define some properties first it is the schedule interval you will set this to 9 am with a cron expression if you need more details on cron then i'll leave a link in the description below then we define a start date when this schedule should start and we will set the catch up to false if you set a start date in the past and catch up to true then the schedule will run n numbers of time for each day it missed till the current date it is designed to backfill the job last we'll set a tag for this tag tags help us group together multiple dags and you can use tags to filter tags from the ui under dag we define task group to call our tasks we group the extract and load task under one group we call the first task and save its return value in a variable then follows the next task and supply at the return value from the first task we set the dependency between them and define the order in which they should run we will see the visual representation of this in the ui we group the transformation tasks together let's call this group transform product we will call the three tasks here we group the task as a list and these can run in parallel in the last group we call the product model task and this will execute once transformation tasks are complete we define the order of the task group and the airflow will execute these groups in this order let's go ahead and save our work just an fyi once you save a new dag it takes a while for airflow to pick it up so it may not show up in the ui right away i will open the ui our dag is disabled by default i'll locate it and enable it with the slider we see the dependencies between our tasks we can get a better picture under the graph ui this is the directed acyclic graph this gives us the visual representation of the order in which our task will execute these are the overarching groups of tasks we can click on them to see the underlying task and we see the dependencies with the arrows on the top right we have the schedule information for this tag we can wait for it to run on schedule or we can manually trigger it with run button we can see it in action under the tree tab so we'll head there and see it execute there are different statuses here and they are color coded we click on individual tasks and see the logs for it the logs provide us with all the details and they also log errors if there are any i'll let this tag complete okay our etl load is complete i will refresh our schema in postgres to see our source staging and final model is persisted i'll query the final prd product table to see the transformation we don't see any nulls which is good and the column names are updated at the end we see category and subcategory columns so we have denormalized the product dimension our transformations are successfully executed and persisted in the final product model this is how we automate our etl pipeline with airflow i hope you enjoyed this session this is all for now share like and subscribe take care and i'll see you in the next video"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Pipelines and Orchestration",
        "Topic":"Building ML Pipelines with Airflow",
        "Video Title":"Apache Airflow: Building Smarter Machine Learning Pipelines",
        "URL":"https:\/\/www.youtube.com\/watch?v=KVcsEttQl1w",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/KVcsEttQl1w\/hqdefault.jpg",
        "ID":"KVcsEttQl1w",
        "Publish Time":"2024-10-15T10:51:02Z",
        "Channel":"Elinext",
        "Channel ID":"UC0ZcMC_Y1CdDTADlr-oOndQ",
        "Transcript":"are you looking to streamline and automate your machine learning workflows whether you're dealing with complex data pipelines or automating repetitive tasks Apache airflow could be the perfect tool for you Apache airflow is an open-source platform designed for orchestrating workflows programmatically it's highly beneficial in data engineering data science and machine learning helping you manage intricate workflows automate processes and ensure that your machine learning models remain up to with new data so how does it work at the core of airf flow are three key components dags directed A cylic graphs that Define the order in which tasks should be executed tasks each task represents a single operation in your workflow such as fetching data or training a model operators these Define the type of task from running Python scripts to waiting for specific events Apache airflow is particularly useful for orchestrating machine learning pipelines let's take a look at some real world examples in the financial sector airflow helps institutions detect fraudulent transactions in real time by managing the entire pipeline from data ingestion to model deployment telecommunications companies use airf flow to predict customer churn automating the entire process from data collection to model retraining and environmental scientists leverage it for air pollution forecasting ensuring accurate up-to-date predictions across regions building a machine learning Pipeline with air flow typically involves multiple stages it starts with data ingestion fetching data from sources like databases or apis next data pre-processing takes place cleaning and transforming the data to prepare it for model training then comes the model training phase followed by model evaluation to assess its performance finally airflow automates the deployment of your model into production creating a directed assist graph dag and airflow is simple and Powerful you can Define tasks for each step of your machine learning pipeline using python code with python being airflows native language it's easy to integrate custom workflows and leverage python libraries within your pipelines from data extraction to deployment airflow enables you to fully automate and monitor your machine learning workflows Saving Time reducing errors and ensuring your models stay current with fresh data interest in learning more we've just scratched the surface visit our blog for a detailed guide on using Apache air flow with machine learning from Real World case studies to step-by-step instructions for creating your own ml pipelines everything you need is just a click away see you there"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Pipelines and Orchestration",
        "Topic":"Kubeflow for Kubernetes-based ML Workflows",
        "Video Title":"Machine Learning for Kubernetes Workflows with Kubeflow | #shorts",
        "URL":"https:\/\/www.youtube.com\/watch?v=mE5lpwEoNNw",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/mE5lpwEoNNw\/hqdefault.jpg",
        "ID":"mE5lpwEoNNw",
        "Publish Time":"2023-09-15T12:00:47Z",
        "Channel":"Mirantis",
        "Channel ID":"UCuUpUWYE6oL3lEmisbfZLYw",
        "Transcript":"<b>Today's Tech Talk is about Kubeflow.<\/b> <b>Let's go into Kubeflow & have a look into what Kubeflow is.<\/b> <b>So it's an open source platform for machine learning.<\/b> <b>It is designed for Kubernetes & designed to make deployments<\/b> <b>of machine learning much simpler, portable & scalable.<\/b> <b>When we look into Kubeflow with, for example, k0s...<\/b> <b>...it looks pretty simple, but of course<\/b> <b>it will be, and can be, very complex.<\/b> <b>We start with the infrastructure. We can, for example,<\/b> <b>integrate with CUDA, so we have a partnership directly with<\/b> <b>Nvidia, so it's possible to use GPUs out-of-the-box with k0s<\/b> <b>We have k0s, which is the Kubernetes distribution that also<\/b> <b>runs a lot of other companies already.<\/b> <b> We have the essential components. Those essential<\/b> <b>components are for the Kubernetes cluster itself.<\/b> <b>[It] can be Prometheus, Grafana, but also Argo, for example.<\/b> <b>So you can have a GitOps approach, even for machine learning<\/b> <b>We have Istio; Istio, of course, can be complex,<\/b> <b>so we have Tetrate as a partner. And we have dex.<\/b> <b>dex is another simple solution, but it can be very complex<\/b> <b>in this regard because of multi-tenancy & multi-users.<\/b> <b>We have the data components, for data collection for example<\/b> <b>We can provide support with portworx as a partner,<\/b> <b>and the actual machine learning parts too.<\/b> <b>Of course, Kubeflow, Knative...<\/b> <b>...and everything can be installed with Kustomize.<\/b> <b>Demo time - means we will have a look, or can have a look,<\/b> <b>into first of all Lens, which makes it simple now for me to<\/b> <b>show you what is actually happening.<\/b> <b>Now we can see its type is Jupyter notebook.<\/b> <b>And I can as soon as it's started I can launch this notebook<\/b> <b>Get again a very simple redirect into that container,<\/b> <b>and use it for machine learning or for my machine learning<\/b> <b>Python script, for example.<\/b> <b>Today's Tech Talk<\/b> <b> is about Kubeflow.<\/b> <b>Let's go into<\/b> <b>Kubeflow,<\/b> <b>and have a look into<\/b> <b>what Kubeflow is.<\/b> <b>So it's an open<\/b> <b>source platform<\/b> <b>for machine<\/b> <b>learning.<\/b> <b>It is designed for<\/b> <b>K8s & designed<\/b> <b>to make<\/b> <b>deployments<\/b> <b>of machine learning<\/b> <b>much simpler,<\/b> <b>portable & scalable.<\/b> <b>We start with<\/b> <b>the infrastructure.<\/b> <b>We can,<\/b> <b>for example,<\/b> <b>integrate with<\/b> <b>CUDA, and [we]<\/b> <b>have a partnership<\/b> <b>directly with Nvidia<\/b> <b>so it's possible to<\/b> <b>use GPUs<\/b> <b>out-of-the-box<\/b> <b>with k0s.<\/b> <b>We have k0s,<\/b> <b>we have the<\/b> <b>essential<\/b> <b>components.<\/b> <b>Those essential<\/b> <b>components are for<\/b> <b>the Kubernetes<\/b> <b>cluster itself.<\/b> <b>We have Istio;<\/b> <b>Istio, of course,<\/b> <b>can be complex,<\/b> <b>so we have Tetrate<\/b> <b>as a partner.<\/b> <b>And we have dex.<\/b> <b>We have the data<\/b> <b>components, <\/b> <b>for data collection,<\/b> <b>for example.<\/b> <b>We can provide<\/b> <b>support with<\/b> <b>portworx.<\/b> <b>Of course,<\/b> <b>Kubeflow, Knative...<\/b> <b>...and everything<\/b> <b>can be installed<\/b> <b>with Kustomize.<\/b>"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Pipelines and Orchestration",
        "Topic":"Kubeflow for Kubernetes-based ML Workflows",
        "Video Title":"Webinar: Planet-scale ML Workflows with Advanced Data Management on Kubeflow",
        "URL":"https:\/\/www.youtube.com\/watch?v=nOPtZguTER8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/nOPtZguTER8\/hqdefault.jpg",
        "ID":"nOPtZguTER8",
        "Publish Time":"2019-01-15T23:09:09Z",
        "Channel":"CNCF [Cloud Native Computing Foundation]",
        "Channel ID":"UCvqbFHwN-nwalWPjPUKpvTA",
        "Transcript":"alright looks like the numbers are stabilizing so let's go ahead and get started welcome to today's CMC f webinar planet scale machine learning workflows with advanced data management on cube flow I'm Kaitlin Barnard marketing manager at CNCs and I'd like to thank you all for joining us today I'd also like to welcome today's presenter bangle is cookus CTO and co-founder at a Rick doe so just a few housekeeping items before we get started during the webinar you are not able to speak as an attendee there is a Q&A box at the bottom of your screen so if you have any questions that come up throughout the webinar please feel free to drop them in there and then we'll hold Q&A until the end and get to as many of those questions as we can this session is also being recorded and will be sent out along with the slides and after the presentation so with that I'll hand it over to Benglis to kick off today's presentation Thank You Kate it's a great pleasure to be here today nice to meet you I hope you enjoy today sooner so as Caitlin mentioned my name is Curtis founder and CTO of erecto what we're doing here it too is planet-scale data management and we focus on kubernetes and ml environments so let me switch let me make sure I can switch our slides share my screen you should be seeing the slide yep you can feel great so today we'll be talking about climate scale and mail workflows with advanced data management on Cooper what does that mean it means we'll go over what planning scale means for us we'll talk about machine learning on kubernetes coop flow which is BML to kid for kubernetes focus on how managing data is very important for efficient machine learning talk about data management in coop flow which is the area in which we have made a few contributions to cook flow and then move the platform and to end coop flow along with our integrations so by now it's a definite fact that we live in the kubernetes era kubernetes has become the default container istration engine no one can question that anymore across platforms across different kinds of infrastructure kubernetes is the single simple way to declare your containerized infrastructure and then the underlying platform does whatever you guys could do but this is a sincere planner so there's no reason for me to argue for kubernetes parents who are already so company's the question is what happens with machine learning now that kubernetes is gaining ground so fast so although everyone is doing their own thing right now we believe that eventually ml will run on kubernetes with kubernetes primitives gaining all the benefits at kubernetes brings with kubernetes primitives means that machine learning will be running in a declarative way users will be declaring to kubernetes the kind of a mail infrastructure they need and kubernetes will make it happen users will have a single way of asking what they want to have across infrastructure across locations and kubernetes will make it - it will make it happen and because users cannot really don't really have to speak to kubernetes directly because their ml users the data scientists what do we need in between we need coop flow that's the overall idea so why should a man run on kubernetes because this will bring containerization but better more efficient handling of hardware sources auto-scaling portability across locations across infrastructure extensibility and no vendor locking so we are just getting started on things but the very good first step is coop flow is here and coop flow is the ML toolkit for kubernetes it is the first native machine learning to teach for kubernetes announced by Google just over a year ago end of 2017 it is already picking up speed to become the de-facto ml platform it's a way of declaring your ml platform using kubernetes primitive music Burnett as components so I will spend the next few minutes with some slides I got from a presentation by J Smith and David our own chick David our own ship is the co-founder of the coop flow project this presentation was given in koukin North America 2018 in Seattle last December machine dominguez code to essentially introduce coop flow to those of you who may not have heard about it before or maybe that week recap to get upstream so when people think about machine learning today a big mistake many people make is that well we need a data scientist and this data scientist is gonna be build a model for us and this is a now well building a model is part of an email workflow but it's not what the mail is all about building a model is just one stick in a very big email workflow it's a multitude of components but we need to take care of and we need to stitch together - how about working ml a pipeline end-to-end from an initial data set that we have to ingest we have to analyze we have to transform and validate and split and then we have to train an initial version of our model and validate that it works and maybe it doesn't so we have to go back and train it again and modify it so it's an iterative process and then we have to train at scale and when we have a model that in train that scale and works you have to roll it out and serve it in production and then you have to monitor it and then have a blog which progress and then we have to go back and fix things so from the initial data to having a model running in production there is many steps and these steps sometimes have to happen in different places and sometimes have to be executed by different people different personas so this becomes a very complex talk and it requires expertise in a multitude of layers because it's not just the top layers that the data scientists or the data engineer would care about it's not just the framework and the tooling and being you X and the model building it's also the lower level things what about my runtime what my drivers my operating system my GPUs the hardware we ran on how do i scale my infrastructure so this is complicated enough but then given that different parts of the work flow will run in different places and you need to replicate all this so what we talked about so far is the upper layers if you have to replicate this for experimentation locally maybe on my laptop and then to train at scale in some bigger infrastructure and then to have the model running in production on the cloud I have to replicate all these layers how do I do that so the first step is let's stop thinking about these lower level hard layers let's get kubernetes to take care of them so kubernetes takes care of the low level handling of the runtime so kubernetes is the platform on which I run my machine learning so now I can move from location to location and from one kind of infrastructure to another from one cloud provider to the other with Burnett as being the language in which I declare my infrastructure and then let's get cook slow as the single way of the developing deploying and managing machine learning on kubernetes so let's get could flow to be the single language in which I describe kubernetes my machine learning infrastructure so again kubernetes takes care of the lower-level stuff and then coop flow becomes the language in which I declare the way I handle my steps in I'm island mail workflow so coop flow becomes this part and then have coop flow everywhere so my job is much simpler I guess kubernetes in notations and then I declare my machine learning infrastructure using coop flow that's the basic premise behind control okay so coop flow and kubernetes in our view are pivotal in enabling next-generation machine learning and what do we mean by next-generation machine learning we means the ability for distinct roles the data engineer the data scientist the DevOps a person all interacting across rotations to make this workflow function so we need collaboration we need different people doing different parts of this workflow and still being able to collaborate in a way that's reproducible whatever producer would mean that when someone has a step completed some other person can they can hand it over to some other person some other person can take over their work and they can reproduce near exact state so they can continue the the workflow we need a simple streamlined unified workflow from the first pre-processing steps to developing the model to training to get into production from my laptop to the cloud to the autonomous car may be well the model relation will eventually run where inference will happen and ideally in next generation and milk would like everything to execute fast so the question comes where is the data for the steps to happen the data should be ideal in local if I keep a data somewhere remote on a slow data Lake if I depend on a single slow object store then no matter how many GPUs I have how fast my infrastructure is I will be bound by data access latencies so this is where we as a Richter coming to the picture and this is how this was the trigger for our involvement with cook flock the question is how do you manage data for this ml workflow when this data has to be in multiplication how do you manage data in a pipeline that runs across locations because I'm a milk pipeline is as good as its data so I will spend the next few minutes describing we are we come from a systems engineering background we work mostly on data management for kubernetes our involvement with coop flow is contributing code to it so coop flow manages data as kubernetes native resources and then if coop flow Margie's data as kubernetes native resources our software can sit on the side and provide the efficient data management for these resources what we do explicitly is we allow you to snapshot version package distribute and eventually share with others your full environment your resources on coop flow along with their data so this makes the pipeline the ML workflow work across teams across locations and across different kinds of infrastructure on Prem or on the cloud this is the overall idea in some sort of experimentation locally you do it on your laptop so if you run control and coop flow is extended so it becomes data where it stores your data on kubernetes objects named persistent Bolden claims and then these persistent volume claims are stored on whatever storage you happened to have local your local disk at some point but this is a full single could flow way of describing your pipeline so you use the exact same primitives at the exact same API to do your job and then when you're done you are another person uses cook flow again to spin up the exact same pipeline on another location for training on the Google cloud for example our software seeds are long kubernetes to enable this kind of interruption so our contribution to tube flow is a two-fold we extend it so it works over PVCs and then we use our software on the side of PVCs to enable cross location collaboration this is the overall picture of what I will be coming to you today what you get with this is a way to version packets and distribute your data and then you can have a unified hybrid pipeline that works both on Prem and on the cloud and because we work with snapshots because when we move data we essentially snapshot the data you can always go back in time and see what the state of the pipeline was so you can always reduce it and try again and come back to the exact state where you were I've talked a bit about these things but it's best if we can watch it live so this is the demo that we'll be doing we have a machine learning pipeline same as I showed before on crude flow we have two users Sarah which will be doing the experimentation part the data ingestion and pre-processing part in location a and James who will be doing the training part location B so you build a model and train it so given this and Mel pipeline Sarah will live will essentially be doing these steps a bit shortened and James will be doing these steps and for this we'll use an example that a cube flow community has created called the github issue summarization example this example is a machine learning pipeline where we get data from github issue data is she text and the model tries to create a summary but describes the content of the issues as good as possible so let's start with this I'll use two locations this virtual desktop is location a he trans on Amazon in zone uswest 1a it runs coop flow on kubernetes and this desktop will be location B it runs on Amazon again zone he West West is West one be running with Cooper so this is where changes on B and this is where Sarah is on a so what does Sarah do she uses coop flow she starts a Jupiter notebook a Jupiter notebook is the a very well-known development environment for ml so she logs into Jupiter hub using coop flow and she will be creating her notebook okay so she needs a development environment she'll use a standard image she requests 2 CPUs 8 gigabytes of memory what about your data she'll have a simple workspace will she'll store your code and your libraries mounted here and she'll also use a data volume let's call it become Delta 10 gigabytes in size which will be mounted here so Sara will be managing her data as another locally mounted filesystem this is backed by a persistent volume on kubernetes so Sara creates your notebook so what happens here is Sara uses code flow to allocate a new development environment for her do a map and this development environment will be a jupiter notebook we give it some more time underneath what happens is our cube flow asks kubernetes to create a new pod to serve Sara's notebook and this user interface managing data volumes was one of the first contributions will be to the Cupra project so this is development environment she has data volume worship extortion data she has the ability to start terminals or create notebooks so let's start a terminal and what we'll do is a increase the font size a bit get it full screen also okay so we will close the issue summarization example from cook flow so Sarah brings her code here this is your code okay and this is where she bookstore her data this data volume is backed by a local ephemeral nvme disk SSD on an amazon ec2 instance currently so Sarah stores as fast as she can with minimum latency to a local disk so she can work as fast as you can this could be her laptop even she is working on her laptop and storing to her local hard disk okay so let's say Sarah needs to install a few missing libraries before she starts so she can install mission missing libraries so she can do machine learning inside your development environment and these libraries become part of your development environment managed by cook flow and us so she can install him libraries as well okay and now she starts with a notebook we already have a notebook to start from we won't be creating from we won't be creating it from scratch this is the notebook we will be using so Sarah can now import shared libraries okay and now what's the first part in animal pipeline Sarah wants to experiment so the very first part is let's get the data that we're going to be using into our development environment let's ingest the data into the pipeline so Sarah has this empty directory where she will store your data mounted onto her pod she remembers it in a variable and she uses W gate to fetch the issued data as a big zip file somewhere from an external data lake so this is the first data ingestion part and then once she's brought the data into her environment she will also uncompress it she will unzip it so this is an initial data transformation step so Sarah runs W gate she goes to the external data leak register data into her environment this is quite fast because all storage happens locally and then she unzips a zip file to create our comma separated values file a CSV file this is not a small data set this is a in the order of three gigabytes data sets let's take a look so it's a one gigabyte compressed file and it will expand to about 2.8 in a box total so this is done Sarah pretty quickly stored four gigabytes of data in her environment so going back to all joys I keep clicking okay going back the road book so sarah has unzipped the data she has done an initial transformation step okay let's assume this is the first type of the pipeline so let's assume sarah has completed that to the ingestion and analysis and transformation part okay now at this point she may want to hand over her work to somebody else or she may want to take a snapshot of her work so she can go back in time and review what she did okay so for this because of data is stored on a compare method object a persistent volume we can use our software to take a snapshot of the whole infrastructure so let's take with two of our sofa this is rock this is what runs alongside coupe flow and this integrated would cook flow so you can manage your data across locations so this is Sarah's deployment on your local infrastructure there is two buckets Sarah's bike it and James bucket buckets is where we keep all user data snapshots of food environments let's have a look in a previous budget this one for example it contains a few snapshots of environments we took previously some time ago each environment is a snapshot of the workspace volume where Sarah keeps her libraries and the data volume for Sarah would keep your data so the Alice an environment snapshot is a group of two snapshots a workspace snapshot and the data snapshot and then for each environment we can have multiple versions so you can go back in time and see what your environment looked like ten minutes ago 20 minutes ago or a week ago so it's essentially a time machine for their full environment based on snapshots and this is how we enable the pipeline to span multiple locations we move the pipeline State from one location to another to allow to move from location to location so these are these are pre-existing snapshots older ones let's see how Sarah can go into here empty bucket no files here yet and ask for a full snapshot of your environment so Sarah will snapchat your full lab she'll snapshot both your workspace and the data this is essentially get like Sarah commits her food environment into our software into Rock including the data suggest landlord so ingress data unzip into CSV ingress the entry date run initial decompression step these are notes she keeps for herself in the complete message okay I'm at this point what happens is our software creates an a synchronous task that gets to the kubernetes native objects the PVCs that Google has created and takes a snapshot of them so we take a snapshot of whatever libraries she has created and whatever data she has brought into her environment and it's interesting to note that we do it thinly so we know what parts of the Degas it has changed who have changed and we only read the changed part so if you look at the data volume she has brought in more or less 4 Giga 4 gigabytes of data that's why we're reading this many data pieces next time Sarah wants to take another snapshot of your environment this is going to happen much faster because she won't have touched us much they go so we give it some more time and click snap ship is done so Sarah now has a full snapshot of your environment at this point in time containing her data volume and your library body this is it anak can continue now she can pre-process the data okay so she will split the data into a training set and a test set we will use a limited sample size of 200 samples so it executes is quite quickly so this is the pre-processing part now running I gain these runs over in local SSD so it's as fast as possible we read a full CSV file into memory and splitted so we can ask Shoom that serve essentially runs the data splitting part here okay this is a sample of our issues the data we are working with and then we do some more conversion into data structures so we can continue with our training and then we do pre-processing again following our notebook we import libraries continue working instantiate a text process or we have our text process or okay we have processed our data a small sample of it anyway so it finishes quickly and now we'll serialize everything to disk so at this point Sara will dump the pre-processed state into the data volume she'll dump files into her mounted filesystem okay so this is the result of the data pre-processing step so if server goes to the data body you see that we have the original dataset at the zip file and zipped and then we have the result of the pre-processing step so at this point Sara's done with the pre-processing step she has a trainer already it's part of the notebook so we're done we now need to build the model so we need to move the pipeline now so James can work James has to start from where sara has left the pipeline or sara may need to come back to this step if things fail and try over or modify a few parameters or try out the new things try out some new things and she needs to do it in a way that retribution she needs to know exactly what she did to reach this point so what Sara will do again after having completed processing she will go and get another snapshot so she gets our software snapchat her full environment again pre-processed date disk so she has completed the pre-processing step and she has done toast a to this podium so she takes another snapshot this time the snapshot will be way faster because she didn't actually touch all of your data we will detect that she has only inserted new state in here volume the happy processed data and this is done so now Sarah can go back either to this state or to her previous state before starting to processing okay and now that we're here we can move the pipeline to James so James will reproduce Sara's State in another location and continue with building the model and training it so Sara's done huge James James is on a completely different zone completely different location AB doesn't actually share anything with Sara they don't use a single s3 as a data length exchange data say James won't go to Sara to find the data he needs the data has already been synchronized locally for him by our software so this is James's view of what's going on this is the sarah bucket it already contains data on his side because our software took care of synchronizing it here is more information on this this packet Sarah actually has two connected peers two occasions one is James's location one B the other is Sarah's location if three or four or five people or five locations were involved you'd see five items here underneath we do peer to peer synchronization of these buckets so you can have a pipeline that extends to multiple locations and down locations synchronize data state with peer to peer encrypted connections so James already has the data he needs this is the day that he needs this is the latest snapshot if he looks into the file she will see that it has two versions the original one right after bringing the data into the environment and the second one right after pre processing the data so James can start from the latest snapshot get to his instance of cook flow log into his distance of cook flow and pre initialize the environment with whatever Sarah gave him so what I did was copy the mink to Sara's snapshot from our software and give it to cook flow so it can proceed so here's what change will start from James will start with two CPUs and eight gigabytes of memory because we maintain all metadata for the environment so it's reproducible James will have the exact same environment pads on kubernetes manage button flow James will have access to Sara's workspace so she'll see the exact same libraries unchanged we'll also see the exact same data right before Sara took the snapshot Sara can continue to work in her local infrastructure can continue to produce snapshots and James can continue with his branch of the environment so this is essentially the combination of coop flow plus rock our software is get full data and your Holdeman environment so James spawns the environment we wait for a few seconds underneath coop Flo asks kubernetes to create the pods the pod to support his environment the data will be stored on our kubernetes persistent volumes this took no more than 120 seconds let's give it some more time okay so this is James's clove of the environment and we see the environment is already initialized so he has reproduced several state her code is here data is here both the original data set and the pre-processed data so he can pick up and continue the pipeline from where Sara left so what did James do now chill train so let me increase the font size engage screen okay so James knows where the notebooks lead and he can see the data this is our four gigabytes of data and now James will use a trainer a training module in Python to train his model this is the training module and it needs some information so James would just use the local file that he conceived this is the input data and he won't be doing any pre-processing because we've already done it doesn't need to use in temporary directories he'll just train the module into this file for sample size of 200 issues because the sample size that Sarah gives during pre-processing so now James grant runs training on a different location which could be the cloud using GPUs maybe we have extended the ML pipeline from service location into James's rotations to run training put different hardware or maybe at scale so James uses the pre-processed data to train the model we're only using a very small sample size or training she can click quickly okay and this is the trend model so James has completed training and then how can this continue two options James could spawn a distributed job right from within the notebook using the same primitive using a snapshot of his environment to power the distributed our training job so he can submit a cook flow tensorflow job object to start a distributed jug or and then this is what we'll do now she can take a snapshot of his food environment including the model so he can give it to sir or some other collaborator maybe a production person who would try it out and deploy it to production so again now that the model is done James can go here and request this is his bucket request a snapshot of his environment training the model so he keeps a note train the model something size equals 200 model is in this file okay and again this will start a new snapshot job in our software super fast so at this point James is snapshotting around 5 gigabytes of data but this happens very quickly because the software keeps track of what has changed so again location B we move back location a and Sarah can go to James's bucket and she can see but here is the data but James just purchased so the snapshot that James took has been synchronized back to Sarah's location so she can continue she can iterate on the model within seconds so we are kind of yeah exactly when the presentation should finish I'll stop the demo here so we also have some time for questions I'll hand it over to Kate name for this awesome thanks so much for the presentation so we do have some time for Q&A right now so just a reminder there is a Q&A box at the bottom of your screen so please drop your questions in there and then we'll get to as many as we have time for so a few questions have come in during the presentation um how is ROK authenticated to get access to Sara's volumes this is a good question okay so Rach manages Sara's volumes Rock is integrated with kubernetes as what it's called a CSI plugin so whenever our coop flow requests new volumes it asks Rock to create them to manage them so the question is the reverse how does the Kukla user interface the Spooner interface this interface right here let me destroy certain environment and show you the question is destroy the in the interface anyway I have to destroy it either we have to stir them in order to show you the question is how does the crew flow user interface authenticate itself to rock so it has access to its snapshot we maintain secrets in kubernetes Sara maintains our own secrets in kubernetes giving access to whatever tokens Rock needs for coop flow to access it so the very first time Sara will access the control environment she'll see a warning message that says this could flow deployment is not authenticated access Rock please go to rock get your tokens and insert them as a secret in kubernetes accessible by the coop flow user interface so cook flow can access rock so it's not Rock accessing service volumes it's Sara crude flow accessing rock bottom's does this answer the question back and forth missions yes yes if you want any information on any other questions that we're answering please drop the follow-up question in the Q&A and we'll get back to it another one what does the trained Python script run okay that's a good question we didn't go into the ml specifics the this repository the cube flow examples repository is open on the web so you can either download the original version by the cook flow community or our version accessible here and see exactly what the script does the the training code is all available right here ok this is the vagina so it cleans the model and trains it but I'm not familiar with the immense specifics alright so the demo makes it look like the same blocks are stored twice once for Sara and once for James is that a common thing to do mm-hmm okay so the question is Sarah James work in different locations okay seven James work completely dependent Sara works on her laptop and James may be working on the cloud would they be able to use the exact same block storage would it make sense for James to to have the impact of latency to go all the way to service laptop would he be able to go do it the service laptop to actually fetch the same blocks so if seven James are working on the same location that can actually be actually the exact same blocks but if you're working across locations then the question becomes does your code Traverse regions to find its data in another data leg if you're on an account if you're in an autonomous in an autonomous car do you actually multiply to find your data this example and our approach is let's bring the data set close to where the compute part runs let's have the data set be the globally accessible resource refer to it and synchronize it locally whenever they snapshot they don't create new blocks locally write all the snapshots locally maintain a shared set of blocks but yes when talking about distinct locations because we actually want locations to be independent in terms of security and speed we maintain two distinct sets of blocks one for location a 1\/4 location B all right so is there a concept of teams for example so that by default anyone in the team has access to the volumes of other members of the team for example to enable access cleanup when a member leaves I didn't show it we have we have what we call the rock registry the single hub where all locations essentially discover one another because one question I didn't really go into is how did Sarah's bucket get to be synchronized with James's Park how did the bucket Sara on Sara's infrastructure come to be synchronized with budget Sara on James's infrastructure so what they did is they went through a publish and subscribe step on a single datum cog that we called the rock registry so on the rock registry you can define organizations similarly to how you would define organizations on github you can share your your buckets your datasets with whole organizations and then if a person leaves the organization whatever infrastructure is authorized by them to access this data is automatically thrown out of the synchronization storms so you can have members of the team leaving the team you can have new members of the team joining into the team and then the peer to peer synchronization is authentic is updated to take into account your changes to team members and then this is also a very good comment that I see on the Q&A panel snapshotting is not just for sharing yes it's for versioning even if you're local somewhere if you maintain multiple snapshots of your own environment even if you share with no one else even if you're using just a single local data Lake it makes sense to be able to smash up your environment because this is essentially an immutable deep like timeline to your work so when something goes bad because not everything happens perfectly with the very first trial you can go back in time and reproduce your work so sharing this immutable timeline across data centers is an extra point is an extra value added thing that we do but even if we you keep this inside a single data center it makes sense to work with snapshots to maintain an immutable timeline of your steps alright there's a couple more that we you yeah there's a couple to get to from the chat that we answered in there but I want to get to them live as well is the snapshot stored locally is the snapshot stored locally okay rock so for each individual location rock needs some sort of persistent store for long-term archival of its the duplicated blocks so all the live volumes are stored on kubernetes persistent problems so they're stored on local SSDs even super fast that can't get any faster than that but when you snapshot rock the duplicates your data and needs up an archival place where it stores your data in the long run and this is different for each location so each location is completely dependent from other locations and that's how you can scale to potentially hundreds of locations that's how this can work across a few thousands of autonomous cars running on the roads right so you need some sort of local storage storage local to the region to store your archive blocks if you're on Amazon we use whatever local s3 instance you have so if you on us west 1a we use the s3 region us West one if you're in another region we use us three for this region if you're on Google Cloud we use Google object storage if you're your laptop we use part of your local disk a local directory if you're on on-prem kubernetes we just find with using Mineo for example as an s3 endpoint to store the archive data so we just need some sort of local or tribal endpoint will restore that the duplicated chunks out of which we form the virtual snapshots all right and then another question could you talk a little bit about how rock is different from something like rock or port works okay I like the questions where they're very focused exactly the questions we would like to talk about so Luke is a great project for orchestrating storage locally Luke can create a safe cluster pretty easily locally so you can ask it I need safe to store my local golems okay so Luke will give you a local safe plaster then you can have local problems so part of our work with coop flow is getting coop flow to use local persistent volumes so in this case coop flow would use safe volumes managed by Luke to store the data but then how would you snapshot these volumes so you can go back in time you could use safe snapshots maybe or you could use our software alongside safe volumes to maintain these snapshots because once we maintain these snapshots in our own format then we can take these snapshots and synchronize them in a peer-to-peer network across tens or hundreds of locations because if you use route to manage a local safe cluster how do you then synchronize the content of this safe cluster with other surf casters you may do and most storage technologies do this kind of PA of point-to-point master-slave replication right you can have a primary site on the secondary side and you can be synchronizing your volumes from one side to the other but this essentially means that the two sides trust one another and this also means that it's not the user who's in control of the synchronization process it's the administrator who has set it up explicitly our approach is let's make the data a kubernetes managed resource something that the users themselves can actually refer to and then let's allow the users to specify what gets accessed where ruk manages primary storage we do not target primary storage we do not store your data so if you take a look at this part of the presentation we assume that there is some way to store data locally PVCs this is our contribution can flow making could flow work or PVCs so this could be route unsafe provided PVCs also and this could be route any storage and then our software sits on the side and keeps track of what has been changed and maintains immutable snapshots and then synchronizes these snapshots and this is the the this is the same answer as with port works hardwood this compare to port works port hooks is a very fast very nice way of managing primary storage on a kubernetes cluster but then how do you move from one location to another how do you synchronize locations that ran different kinds of storage maybe that's our approach given our software on the side of what table primary storage you run on each location we maintain snapshots we expose them as user accessible resources and we synchronize them independently of whatever primary storage stars all right before we run out of time are there api's to programmatically snapshot the work space and\/or volumes that will allow a CT CD pipeline with minimal manual intervention okay so I didn't have a chance to show it I can show it now yes there are whatever you see here is actually API driven so the ROC user interface is actually am API client so whatever you can do from the user interface you can do programmatically exactly because and this is our next step and this is what we will be demoing next is running an automated pipeline system like OOP flow pipelines or our go over our snapshots so you can have two visibility on whatever the output of one step is and whatever the output of another step is so you can go to each individual step of a pipeline and say what is this step do why did have a simple and what it produces out can actually mount its input and its output so I can very easily have first-hand experience with what happened this is great for debugging by the way it's full visibility into all steps of a pipeline by being able to retrace whatever every step did by mounting it's immutable input and it's snapshot at the middle block so let me show it this is James inside his environment and he can actually access rock programmatically and say let me get my notes for this though and he can say let me create a new snapshot of my environment inside the James budget and at this point this is an API call that we do to rock so this is the same thing that the user interface would show but in a text r2 in CLI tool so demoing API driven snapshots okay just taking us using the API okay so we submit this to kubernetes and again I see a light - so totally automated takes a snapshot of James's environment and if we go into so this is happening right now right I switch to the visual interface and there's a new task that's actually progressing so I used the API here to create a new task programmatically and this task is what is running here and this task is almost done this task is almost almost done this task is done and yes this task is done here the emitter boolean's I can use to access my snapshot and this is it I cannot take this identifier and this is an immutable permanent identifier that I can use to access my snapshot so I can use it in pipelines programmatically I hope this answers the question awesome thank you and thank you all for those great questions that brings us to the end of our time today so once again thank you so much Vangelis for the great presentation in to all of our attendees just as a reminder the webinar recording and slides will be online later today I mean we look forward to seeing you all in the future ciancia webinar thank you thank you for giving me the chance to present thanks everyone have a great day"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Pipelines and Orchestration",
        "Topic":"Kubeflow for Kubernetes-based ML Workflows",
        "Video Title":"ML Engineering Kubeflow - Pavel Dournov, Google Cloud",
        "URL":"https:\/\/www.youtube.com\/watch?v=TXoUn9oLvlU",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/TXoUn9oLvlU\/hqdefault.jpg",
        "ID":"TXoUn9oLvlU",
        "Publish Time":"2020-10-04T05:36:06Z",
        "Channel":"Anyscale",
        "Channel ID":"UC7L1tZw52rtgmIB4fr_f40w",
        "Transcript":"hello um my name is pavel i'm an engineering manager in google cloud in cloud ai platform and today i'll talk about ml engineering with kubeflow so in this talk um i'll cover an overview of of the keyflop project and its architecture so we'll drill down a little more into the details of the queue flow pipelines as one of the q flow component is a foundational for enabling the best practices in ml engineering with kubernetes uh we'll discuss uh and the the details of those uh of some of the best practices uh that we infer from um from uh from prior work and then we'll see how uh those ml engineering practice best practices uh uh are implemented with tensorflow extended tfx and how they work in in action on kipl and an elder demo of the product all right so introduction to queue flow so uh qflo is a platform for building and deploying reliable and scalable ml systems on top of kubernetes uh keep flow is uh independent on any particular place where kubernetes would run as long as kubernetes is deployed keep law is using the standard kubernetes primitives to operate um kiplo is a collection of multiple projects that are contributed by multiple authors it's an open source product uh it's an open source project that is that is developed in in in open source um it has the components in the kubeflow has multiple upstream components that they they infer the work from uh for for example uh a key flow includes uh such components as tf job for distributed for orchestrating it is distributed training of tensorflow jobs so that has a dependency on tensorflow which is also an open source project that is developed separately yet it is um uh the dependency on that is taken there with with kipl and then for the tf job operator is implemented in the kubernetes native way uh uh some projects are originated and and maintained mainly within the with within tuples such as uh qfloor pipelines for example so you can learn more about kubeflow on thecube.org and which has which has pointers to the source code and git on github and then documentation for getting started for deploying that to different cloud providers uh and then uh writing examples and it has some guidances of of how to create your your content and your workloads um kubeflow is cloud independent uh so it runs on uh google cloud as well as uh as other clouds is azure and aws and ibm uh some history of kubeflow qflo was originally released in december of 2017 [Music] mainly with the google contribution in it and then it came with um just a few basic components uh integrated jupiter hub gf job operator for an industry distributed job and a a way for deploying tfs serving models into into into kubernetes for inferencing uh it has grown since then by adding additional individual applications to it uh such as argo orchestrator selden for model serving pi torch operators and so on and at that stage probably throughout the 2018 uh it remains to be a collection of those individual applications and individual components so later on and in late 2018 and early 2019 uh we have introduced ml pipelines and keyflow pipelines uh uh project and uh and the metadata project which served as a connection fabric to connect all these applications into a consistent experiences and in the later 2019 and 2020 leading to queue flow 1.0 release the main improvements in the project went into hardening and then getting it ready for really reliable production deployments so that's where the project is at right now uh i i believe uh right now it's actually have uh introduced an incremental version on top of 1.0 is 1.1 and and then going towards 1.2 uh by by adding even more functionalities so uh what what what is particularly interesting and powerful about kubeflow is the uh is the community of developers that are contributing to it um uh which are coming from a lot of various companies and individually so google remains as one of the biggest container con contributor to the q flow but it overall in in aggregate it is not it it it uh much majority of of the code is actually contributed by uh outside of google from outside of google so um now let's talk a bit more about the qfloor pipelines so um one of the projects there so and i'll start with this that it is typically the perception that uh for for systems with a male the most complex and uh and then costly component is the ml code itself um that takes the most time to develop and build but oftentimes in in the reality uh all the surrounding infrastructure that is required to reliably deploy and run ml uh is actually constitute the majority of the engineering cost and and the complexity in the uh in the in the infrastructure such things as data import feature extraction validation of data resource management functionalities um model serving infrastructure monitoring all these are complex and uh large parts of the infrastructure that is needed that are needed to to to to build reliable and scalable scalable ml systems um q-flow is mostly focused around those parts and and qfloor pipelines is one of those those key projects in there so um also uh typically the collection of processes and practices about um building and and integrating ml into a into developer workflow is called as a mail ops um and then here uh it shows one of the actually for an advanced version of the ml ops uh where uh what's been developed um in the experimentation and development phase is the entire pipeline that builds a model autonomously in production and then how the queue flow actually feels fills in the gaps and fills in the components on that map um and then we'll talk about this in more details of of how this this this uh keyflow components work with each other so so there are a few key pillars in building kubeflow pipelines project so um they're the kevlar pythons project is built around the authoring experience for creating reproducible and scalable workflows and then operation experiences for deploying and debugging and tracking those workflows so the authoring part stands on or starts off with um components and the tools for creating and sharing those components and and then building the components based on previously built ones and reusing them then composing the components into um pipe into pipelines that also can be shared and they can be based on a pre-built templates uh that are targeting to solve a particular problem in in a particular ml problem in a given domain um then an engine that serves as the foundation for running those pipe pipelines at scale which comes with the orchestration engine scheduler the tools for monitoring and debugging those workflows and ml metadata for recording and tracking the execution and uh and and and exploring the historical aspects of this so um here's an a user interface snapshot of queue flow pipelines that shows um a deployed uh workflow and and the result of one of the highlighted component and then we'll do a demo uh later in this talk where we will be able to look at that in more details here's a 10 000 feet view on the keyflop pipelines architecture so there are several key parts to this um the first is that all of these components are kubernetes native components so there are built and uh packaged in a way that makes it easy to deploy and run them on on any kubernetes kubernetes deployment that makes it across environments across cloud and available in different form factors or cabernet runs so it is a collection of custom resources and custom application and the local user tools so one part of this is the is the sdk and tools which are shown in the right upper corner here so those are the tools that uh help the users to uh build components uh build them into into a shareable form uh discover them discover the metadata about them bring them into the pipeline definition scripts um composes pi pipelines run them locally validate them and then compile them into an intermediate representation so the intermediate representation is a compiled version of the pipeline which resolves the dependencies between steps and then that intermediate position is handed to the pipeline api which uh stores it and provides apis for scheduling and running and and and creating the runs so the runs are uh executed by the orchestrator um which interprets the pipeline definition and and schedules uh kubernetes posts for every step of the pipeline um and the metadata store uh with with a metadata publisher flash recorder which watches the execution of the components graph captures the input output information about the steps and stores them alongside with the lineage information about the uh those those uh the artifacts that are that are produced and i will talk about that in the next few few slides what that metadata does um uh the components each component in the pipeline is uh is fully isolated from the rest of the components and the pipeline and other pipelines and this is in the system itself so this way the user can compose the workflow uh from uh from the steps that use different versions of uh frameworks different libraries different dependencies and then run each each each step runs in in in fairly eyes well isolated sandbox the components can do processing locally inside the ports then just use the computational resource of the node on which the port is running or uh can invoke external services uh for uh doing things like um executing data flow job or apache beam job or or a spark job those jobs can be executed in the same cluster if there's a corresponding uh resource deployed or it can be uh it can be executed by uh another sh in another managed is service uh that is provided by the cloud manager um so as i mentioned uh a a pipeline in in in kublow is a collection of containerized uh task implementations um which can use a pre-built components or or have a fully custom built containers um each component as described with metadata that captures the input information output information alongside with the types uh for every input outputs uh it's a specification of a of a directed testicular uh cyclic graph um that prescribes the dependencies of the of the step from one another uh the the the the this graph can be constructed directly in in in yaml and into intermediate representation or using an easy to use sdk in python um which will uh which we'll see some examples of so um ml engineering uh ml engineering is an emerging discipline uh that are extending the software in the software engineering and bringing the practices that matter a lot in building uh reliable ml systems and then this isn't a comprehensive list by by any means but something but something that is inferred from practice um uh i'll mention the few key ones um the first of the call out is the data and in treating the data artifacts in ml systems the same way as one would treat code means that having the ways to test the data having the ways to version the data and and and validate the data against uh some predefined um assertions and constraints which is much harder than typically to do the validation of of of code mainly because the validation of data requires um a more statistical and weaker approach than the exact comparison of the outputs of the test test test outputs and another is models model is essentially a program um that is automatically built by the training algorithm uh an invalidation of the models is just as important as validation of code but the ways of validating the model is actually very different you know this is also um a bit more based on the statistical techniques um uh another important and interesting part the same time is it isn't uh is it systems and tools that enables the users to build code that builds models as opposed to just building the models [Music] and then deploying the model so the artifact that has been built by the developer or ml engineer in those cases is is the pipeline or code that that is deployed which builds the model in production directly that enables reproducibility that enables continuous training that enables adaptation of the model to new arriving data which is different from the cases where the model itself is deployed to the production in which case it it it remains static and not adaptive to to the changing environment and changing data composibility is super important in this case this ability to compose various versions of framework version of libraries and then still connect them to together yet been able to version them independently ability to compose models take them uh take a previously trained models and uh train it for changed or a new data set uh and then therefore speed up the training and make it more comprehensive the ability to track lineage in the middle world they number the volume of various artifacts and the dependencies across them is very complex and in order to mo effectively debug they say that the the debug such as such systems and reasons about them one needs to understand all the things that went into building a model or building a particular car or creating a particular outcome and that requires traversing the lineage of those dependencies and then an infrastructure in for ml engineering is to provide this capability uh and i can do a continuous learning and ability to run the training code in in production training pipeline in production that that that is reacting on the data arriving or changing as opposed to relying on any particular schedule to interval or or or or or a human trigger so um here's a an example of a typical m2m machine learning system um which starts with a model deploys that in production and starts serving requests it is trained from uh from the data so some code needs to be written to import the data some code needs to be written to train to train the model then some some code needs to exist to infer the feature from from this and then validate the data and then capture these results in the way that can be used in the feature validation and then the feature themselves are affecting the affecting the results of the of the data analysis so um as the model is deployed and running in production some code may need to exist to uh uh to capture the the data that is given to the model for for prediction process it potentially label that the data and use it in subsequent training cycles so all of those require both data and code to be treated at the same level and then here's an example of a pipeline that comes with a tensorflow extended tfx which is deployable and runnable on on on kubeflow which which has um a lot of those components pretty pretty built in the way that conforms to those best practices and then here's a set of examples for uh for scenarios that are taking advantage of the rich metadata in lineage tracking such as ability to to to to traverse the lineage graph ability to start the training from pre-previously trained models compare models across different rounds and et cetera so let's look at the uh kevlar pipe pipelines and tfx in action so here is um i'm going to switch my screen to a deployed cluster of the pipelines here's the user interface of the kevlar pipe pipeline that shows a pipe a template um which is a non-configured uh template of a pipeline which which represents essentially that that i are in the uh intermeter position in a in a graphical in a graphical form uh uh the kefla pipelines are well integrated with the notebooks uh here i'm running uh just a standard jupiter lab which has a pipeline different different definition script a pipeline definition script uh lists out all the components that i want to use in my pipeline and then uh connects the input and outputs between them this is so one note here is that um the script does not define the topology uh explicitly it defines in instead a data dependency for example for my schema generation component that is expected to generate a schema from my data set i take an input for the statistic generator so um i i'm i'm not telling the system that the schema jan has to run after i'm just telling that it depends on the data that is produced by statistic generator so that enables the execution engine to actually execute it smartly and then execute some steps in parallel some steps sequentially some steps uh uh can run continuously so as as i define my pi pipeline uh with a with a command line utility or an sdk i can compile it uh into the intermediate edition and deploy it to my cluster so as i deployed and and run it in my cluster i have um uh i have a few runs that i run previously uh it takes a few minutes uh to run and then here is already an executed graph to tell you that i have so here i can explore every step that i've executed uh i can look at the operational logs and see exactly what happens during the execution so in this case the logs are not showing much because this steps have been already previously executed and the result been taken from cash um if i take the trainer so the trainer actually did run this time and then it actually shows it shows with the full log so a few words about the caching so um the execution engine and it is smart enough to understand that uh if the steps that that in in the new pipeline run have already executed in the past so they use exactly the same inputs that were are pretty previously used and the code did not change uh there is a way to turn on the cache and then in this case the out the the step that doesn't run it just uses the output from the previous uh from the pre previously recorded run so uh but every input and output of the uh steps are are captured so like in the case of um a schema generator for example i know that my schema generator have uh consumed uh the statistics and then produce the schema um and here is the uh uris to actually fetch the fetched underlying files i can also look at the graphical representation of the of of the scheme so components can produce a more rich visualization of their outputs and then i can look at the metadata that has been generated and captured by by the by by the metadata recorder so uh here i can find my model artifact for example and i can look at the lineage of that model artifact which uh which shows me uh where did model was used as it was built and so i can see that it was used in in this particular evaluator and the pusher the evaluator produced the blessing flag on that model whether the model is blessed for deployment and the details of the evaluation and the pusher use the model to to to push to push the model to production so i can see where my model came from i know that the model was built by the trainer which used this three inputs so this particular input the schema was i can shift the focus of the viewer into the schema now from from from the model and then the schema was generated by the schema gen the schema gen actually run twice on different runs but it the but the second round was taken from from cache and that's that's how i ended up with exactly the same as schema instance so and then so on so uh so so this this uh uh part of the uh cube for pipelines allows me to navigate through the dependency graph uh in in metadata so uh this concludes my talk and then i'll see you in the q a thank you very much"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Pipelines and Orchestration",
        "Topic":"Kubeflow for Kubernetes-based ML Workflows",
        "Video Title":"Document processing with ML on Kubeflow",
        "URL":"https:\/\/www.youtube.com\/watch?v=oU8AFOSPZkg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/oU8AFOSPZkg\/hqdefault.jpg",
        "ID":"oU8AFOSPZkg",
        "Publish Time":"2020-11-18T17:16:49Z",
        "Channel":"Canonical Ubuntu",
        "Channel ID":"UCJ65UG_WgFa_O_odbiBWZoA",
        "Transcript":"hello everyone welcome to kubecon 2020. today we're going to discuss document understanding and processing with machine learning on kubeflow my name is huevas gonzalez i'm a product manager for ai and machine learning at canonical and with me today i have charles eddie loya co-founder and ai consultant at maven code our partner to deliver and to end ai solutions charles is going to start by introducing the document understanding and processing use case then i'm going to touch on cube flow and operations around machine learning on kubernetes and finally we will show a demo of all these pieces in action charles the floor is yours thanks roy um so thanks and thanks for everyone for coming in for our talk today uh today we're gonna talk about large scale enterprise data understanding and processing with machine learning on kubeflow but before we get started a little bit a little bit about us mevencode is a artificial intelligence solutions company we partner with canonical to deploy and develop end-to-end ai solutions some of the things we do include data pipeline implementations for processing data that you use for your model training and information operationalization of ml model basically getting your ml model from your development environment into production and large-scale automated document processing which from most of the part of what we're going to be talking about today every enterprise deals with document and they come in different forms and variety and one of the things we aim to do with document ai is to be able to help you as a business classify or search your documents adapt to different document variations coming in and be able to basically extract structure information from the document and also save the overall employee time in other words ends your productivity as an organization a little bit more context around document understanding and processing with ai document understanding with ai is a machine learning solution that platforms information extraction from paper-based and electronic document so um for you to be able to do that you need to do the following uh basically you want to be able to like understand the document type is a pdf document or jpeg or png and once you understand the document type then you can now perform the no the appropriate document extraction on it so um it could be ocr if it's pdf or scanned image or you could use apis around pdf vectorize pdf documents or word document or excel document then once you're on once you've extracted the content the next thing is like you want to be able to categorize the content according to category or document type so what would mean in this case could be like if you're a big organization it could be like you sorting out your invoices separating it from resume proposals or receipts and things like that so for you for you to be able to do that um we need to do i mean build a model that will be able to understand all the different categories of document and once you do that you can now build a specialized model for the named entity extraction and once you've extracted the content in the structure format you can now keep it in your data store index it and build a knowledge graph around it if you need to do so so document classes vary by using industry content the overall goal of this process is to be able to extract the contents of your document and put it in a structural format that you can use to do some analysis or analytics downstream so uh to just put a little bit more picture to what we're saying so uh this is a sample invoice uh that we picked up and using ai we're able to like convert the info invoice into json text that basically we can put it we can install in the database and use for for the analytics downstream so basically the model is able to identify this as a type of invoice we pick up the invoice number the customer information and the dates the invoice date the due date and the line items in the invoice and we extract that information we convert it to json payload then we push downstream and you can install it in your database you can put it in a graph database and things like that so those are the kind of things uh you can do once you've extracted the structured information out of the invoice just to put a little bit more big picture into it uh so this is a high level overview of the kind of system we do deal with now we build out the machine learning workflow so imagine you have all your documents flowing in and you've been able to successfully dump it at the blob storage in this case we're running on on on set running on prem on canonical ubuntu infrastructure and um you have different kinds of document pdf word doc excel and you've been able to do text extraction through api or uh the next thing you want to do is to classify these documents uh put them in different buckets categories so that you can easily process and build models to extract things inside that category so in this case we have um let's say proposal document bucket uh receipt pocket an invoice blocker so we'll build our model to basically classify and put these documents in different brackets and once we've done that we can now do the density extraction which is a model that is not enough to be able to extract all the information that we need from each of the different items in the different buckets that we've created so um let's deep dive a little bit more into the document classification workflow so this imagine this is a document and the first thing we do is a beta accusation i mean connecting to the data source pulling up the data and pre-processing it so what we do is we connect to uh in this case the blob storage uh that we're using uh we pull in the data um if we need to do ocr extraction we'll do it if we need to do some pre-processing like um cleaning up dropping pages that don't really contain any information um and things like that are a data set that we cannot really process because it's not well extracted uh we'll drop all those things into like um in most cases in production you drop it into like an exception bracket and once we've processed and extracted uh the content we need uh like uh we try to extract the features and that comes in that document and that allows us to build a model um that basically can understand the context and what kind of document it is and we build that model and for that you can use different algorithms to do that linear regression that garden surprisingly works so well for classification so we run our model training and we generate a model uh once the model is good enough we can push it for model serving and we'll deploy the model and we can start inferencing with it so any document you send to this model it's going to be able to tell you what category the model belongs to what kind of document it is and things like that and the output cannot be placed on the category bracket where you can do any other extraction downstream so let's see then any our identity recognition that we do on the document to extract the text that are relevant so for us to do that um same thing we go inside the subcategory blocker where we've classified the document we do data annotations and pre-processing so you know before i would take some document that we tag and we should train in um so once we type the document and annotate it properly uh we do some cleanups if needed and uh we do the model training we run it through multiple iterations uh to do to derive a model and once we have a model from this process we can push it downstream from model seven and um from model seven we can now deploy the model and start influencing against it so we can make an api call i'll send in a document to the api endpoint and it's going to be able to tell us what kind of document i mean it's going to extract the json payload from the document and tell all the type of document it belongs so by this time you cannot chain together your classification model and your name density extraction model to create a kind of branded model where once the document flows in the first thing you do is to try to know what category or class the document belongs to is it like an invoice is there a receipt or is it some other document or claims document and things like that and once you understand the category you can now send it to the next stage which basically allows you to apply the right ner model to be able to extract the content out of the document so um just to deep drive a little bit more on the model 7 company we're using kf7 qfloo7 and basically this allows us to um basically create models uh and promote models and manage all the different versions of model that we have because over time as we get more document we need to retrain and to make sure that model is not drifting so for that uh we use careful a lot and it's been really helpful for us and we're going to talk a little bit more about it during the demo so imagine you have a document over time you have different models um so you can do like um you can select your proper model and that model basically gets allows you to make an inference so let's say you have a new version you can basically select the new version of the model use it for airfresh or let's say you still want to use the old version because it's an old document so you can select the appropriate version to usually influence it then we have the model management environment where you can load models put more models and get them up already uh for four four seven and once you send the document to the model it basically helps you with the information and you have your additional payload that comes out of it so uh you may decide to want to build a knowledge graph or just passive results of the inferencing in a graph database so uh you can extend this and basically take whatever json payload you have and then that's it in elasticsearch database our new uh neo4j grab database where you can now search or do something internally intelligent knowledge drill down on the documents where you want to see the relationship and correlation between document i love all this use case we see it a lot in legal documents where you're trying to correlate and connect our references to a particular actor within the old script and be able to like find the occurrence in the relationship between them so we can do a lot of all those analytics once we've extracted the information so uh with that said i'm gonna hand over to rui and he's gonna talk a little bit more about kubeflow setup in production environment aren't we you want to take it off thank you charles for the great presentation on the document processing use case i will now briefly cover how you can do analytics at scale using kubernetes to orchestrate your machine learning workloads the key concept here is ml ops or machine learning operations which derives from a need of well-defined data science processes on top of scalable infrastructure this makes the development and deployment of ai models to production efficient as opposed to having sets of siloed notebooks and scripts that are hard to share and collaborate on the cloud native toolkit for machine learning operations is called kubeflow and is the one we will dive into right now and use also in the demo flow includes a multi-user central dashboard that data scientists and data engineers have access to and which efficiently aggregates machine learning frameworks and libraries such as tensorflow by torch or scikit-learn with operators to run these jobs on kubernetes keyflow pipelines which we will dive into right after experiment tracking hyperparameter tuning with a tool called katib and jupiter notebooks the most used ide in data science keyflow pipelines are a key differentiator of kubeflow it's a built-in feature and a pipeline is in essence a direct acyclic graph of steps of your machine learning workflow each step being packaged as a container with inputs and outputs this promises to make your ai more composable or made of interchangeable components which makes it easy to share and reuse when you run the pipeline the blocks or the components of the pipeline will run asynchronously taking these dependencies into configuration containers are run into pods that are orchestrated through the kubernetes api and because it's run on top of kubernetes it inherits the scalability capabilities of kubernetes as well as the ability to seamlessly run on different environments keyflow is indeed a great open source tool with only one cabinet the more than 20 micro services inside the kubeflow bundle all have to be integrated configured managed and upgraded so day zero and day two operations can be quite challenging and this is where canonical's charm operators can help if you did not attend the open operator day on the 17th visit charmhub.io to learn more about what term operators are and check the list of all the operators we have specifically for keep flow for easy consumption and day zero and the two operations keep flow canonical has created charm operators for each application or microservice inside the kubeflow bundle and integrated them into what we call charmed keep flow you can also read more about charm keep flow on charmed kubeflow dot io and with this we can jump right on a demo of the document processing use case with charm keyflow um so basically we talked about the ner the classification pipeline and the kf7 which is more of for the deployment in the presentation earlier so i'm just going to show you what a classification pipeline looks like uh basically we have a dark loader that connects to the data source and um pulls in the data that we need for the training and we pre-process it we basically uh split it into different categories um based on the processing type we want to do so this basically identifies the content type and we create different categories for the preprocessing then we basically do like um the text extraction uh without the contents based on the encoding type if it's a jpeg or pdf and things like that and we'll write it out into text and we do some photo pre-processing in some cases we need to remove the stop words and things like that so we're doing during this stage and we do the training uh to basically train the model based on the information we've extracted from all the different categories for classification then we do a quick prediction and validation to make sure our model is okay if it's okay and it meets our threshold then we do the export and we can export the model so that's our classification pipeline uh the next one i want to touch is the ner which is basically after classifying the model and getting our model in different brackets we can have our connectors that connect to all the different brackets then we do the ner preprocessing which is the clean ups that we need to do fill in the annotated data sets from all the different locations and basically run the training operation that trains uh the model to identify and build the different models for the different data sets we have coming from the different categories and the annotations we have about them and we run a quick prediction just to make sure the model is predicting the right output it will validate it and make sure everything is okay then we export our model um to like kind of that blob storage and things like that and once we've done the uh the model training we have a model for nbr classification but we have a kf7 deployer that allows us to just deploy the model and stage the model for influential so um these are like these are like our pipelines so if everything runs to complete the classification pipeline looks like these for each of the stages that we run and um this is for just for the server it's a simple model then the any air which is a lot more interesting after doing the name density extraction our pipeline looks like this so i'm just going to show you quickly what this looks like once you have it for inferencing so for that um i just picked up the model that we generated have it in a container that we're about to deploy for inferences so i'm going to show you that real quick so this is a sample document uh it's just uh for insurance claim and we're trying to like um basically extract some basic information like the patient name the provider information and things like that uh we're trying to process this document uh using uh ai and ml information to extract the key information that we need and we want to push it uh downstream find out that system to consume it so this is a document um i'm going to show you the api endpoint so we can wrap our model into into a container a custom information container that gets soft without kf7 and um i'm gonna just try it out on the pdf document and we can check out the results so i'm gonna go in uh pick up that sample demo file that we'll just look at and i'm gonna try to inference on that document so let's see there you go so uh so this is a document that we just pulled up uh we pulled up the organization name the address the provider information uh the phone number the mpi the patient information and the procedure code and they took that payment mark so all this information what we couldn't file from that document and we can now have it in a structured json format that we can push downstream uh for further uh processing or for further analytics uh downstream but the beauty of this is like we've been able to basically extract the valuable information that we need from the document and we have it in the structural format that we can close it down screen thank you"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Pipelines and Orchestration",
        "Topic":"Data Validation and Model Monitoring",
        "Video Title":"Data Validation and Monitoring",
        "URL":"https:\/\/www.youtube.com\/watch?v=l6BZxOG7HjE",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/l6BZxOG7HjE\/hqdefault.jpg",
        "ID":"l6BZxOG7HjE",
        "Publish Time":"2018-03-06T21:56:43Z",
        "Channel":"EKCEP Workforce Services Training Channel",
        "Channel ID":"UCGn84y4hkwDI5dECXfNPOhg",
        "Transcript":"monitoring can serve many purposes monitoring insurance compliance with federal and grant requirements it confirms that proper systems are in place and being followed within the grant management through the use of established monitoring procedures issues of non-compliance may be detected early and corrected without resulting in sanctions or question costs monitoring should be viewed as a multi-faceted management activity directed at achieving program goals and financial requirement standards monitoring activities are linked to program planning design implementation and evaluation monitoring determines the need for technical assistance with grant activities within the program the data validation and monitoring tool and corresponding tips was created by accept staff using WIA laws regulations tins and t goals to ensure client file requirements this tool is used by accept monitors to review compliance with eligibility client interactions paid services follow-up and performance documentation requirements contractors should utilize the data validation and monitoring tool to ensure client files meet WIA requirements this will allow staff to identify and correct file issues prior to funding stream monitoring career advisors should self monitor their client files often WI la directors should monitor client files regularly as part of internal review [Music] files may be requested by accept program staff Kentucky Career Center state monitors Department of Labor monitors or private and independent monitors once files are requested they are typically due to the requesting agency within one to ten business days the file monitoring may be focusing on a specific funding stream a specific WI o a service or consist of a randomized pool of false career advisors must prepare files to be monitored once a file has been requested no attempts should be made to modify the file the career advisors should print the case notes from the eCos file the entire client files should be scanned into one PDF document and label the file with the clients name and eco sadi career advisors should review the PDF document prior to submitting to ensure that all documents are clear and easy to read once the client files have been pulled the file monitoring and findings report typically takes between 30 to 45 business days once the report has been completed career advisors have 10 to 30 business days to correct any file findings when the findings report is received career advisors should review the report to determine areas where that Corrections are required and should make corrections to those findings as appropriate if there is a question regarding a founding career advisors should ask the file monitor what the founding means or how to correct the founding if the file monitor is unavailable ask your WI o a director or accept work force coordinator for systems once Corrections are complete career advisors should print the case notes from the AKS file and scan all corrected documents into one PDF document make sure to review the PDF to ensure that all documents are clear and easy to read if a finding is unable to be corrected a written statement should be provided documenting the reason why the finding is unable to be corrected [Music] there are many benefits to data validation and monitoring this process ensures compliance with federal and grant requirements and that proper documentation is found within a cloud file data validation and monitoring provides checks and balances for all WIA programs"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Pipelines and Orchestration",
        "Topic":"Data Validation and Model Monitoring",
        "Video Title":"Data Validation and Monitoring",
        "URL":"https:\/\/www.youtube.com\/watch?v=NArU2hARqPY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/NArU2hARqPY\/hqdefault.jpg",
        "ID":"NArU2hARqPY",
        "Publish Time":"2019-01-10T03:07:49Z",
        "Channel":"InfoTrust",
        "Channel ID":"UCy8ZuL935I8mZujv8j-8ZHw",
        "Transcript":"[Music] hey guys it's amici Aki here head of growth and strategy and for us again for another snackable content video today's topic or right now is topic is data validation and monitoring what it is why it's important and some of the tools that we built and use to make sure that this is followed in a best practice within your organization so to do this I just put a few slides together and I'm gonna walk through just a few very quickly and then do a quick demo of a technology that we built around us so data validation monitoring why it's important well first I'll start with some challenges that occur in digital marketing and analytics to summarize the slides the text on this slide you put a lot of effort into setting up tracker deploying pixels using tag management system getting your analytics platform in place your sites are constantly being improved and your development team is releasing new changes and you know everything looks great for some time but then suddenly down the road you'll lose tracking something breaks something happens and there's no way of really understanding what may be happened or how to prevent it and really that's a big challenge particularly for anyone that manages more than just one website one platform yes there's QA testing and lots of things in place to prevent things from breaking but when you know so many tags so many sites so many things to manage and maintain understanding when a tiny event or a goal or some kind of pixel stops working is the challenge so really that's the thing that we wanted to try and address is that validation ongoing so that it's not countless hours and manual time spent testing and testing making sure every single thing is perfect when you have really robust and a lot of tracking in place how can we automate that make sure there's no regression and get a visual across your business particularly again if using our managing multiple sites multiple platforms how do you get that overall business with you that everything is working everything is healthy and how do you automate that and then also get alerted as soon as something happens as soon as something breaks so they can immediately jump to fix it these are the challenges that we face with our clients that our clients have faced for you know a number of years then we have sought to solve and we think we have with our validation and monitoring solution so our solution is broken up into two tools or two main aspects if you know them for us you probably have heard of our technology tag inspector and tagging specter real-time is essentially a way to validate that tracking that happens on your site the pixels that are acquiring your data layer or attributes and making sure that that remains in place so our technology tag inspector real-time sits on your website sees how users are interacting and in real-time can tell if a particular event action or data layer attribute starts dropping or missing it will alert you sent a validation alert failure and we have dashboards built for cross-site multiplatform officials to see overall how's the business health in terms of tracking are we maintaining our data layer before and after releases is everything firing all the pixels continually tracking for those key interactions and validation points so what I'm showing on the screen here is just a dashboard we've built in data studio powered by tech inspector real-time data and you can see there's many different example sites could be different countries different platforms domains whatever and all the little touch points that were checking for making sure specific events aren't firing things and the data layer are still captured they're GTM and GA IDs are correct they don't get swapped or switched in the time aegis is somewhere in the source code and what's our rate of success another visual that we can see powered by the same technology and same tool is outages over time or trends so what we're showing here is rate of failure over time in this particular dummy example it's around 10% or less but you can see there's a little bumps every so often maybe this correlates with releases from your time management system or releases from your developers or some sort of campaigns that trigger a lot of data not to be tracked or something to break so tracking this over time seeing your trend lines of when there were failures failures across your platforms across all your rules and things that you're checking for I'll help me be more pre-emptive and prescriptive if you want fixing your data collection making sure that I'm going to monitoring and data validation remains any of integrity with the data that you're collecting now the other side of this that is a custom technology and platform that we built is around validating Google Analytics data it's stuff so tool number one powered by tag inspectors all technical checking the data later on site checking the pixels firing make sure events are track as users are interacting but sometimes it is something that happens in Google Analytics configuration that might cause the data to drop or change or on the flip side maybe a campaign was launched for a few markets or sites and there was a massive increase and without checking analytics daily with lots of hours spent you might miss these fluctuations in traffic so we configured a tool that essentially looks at Google Analytics data yesterday compared to a 90 day average which is not possible to do in Google Analytics interface alone and comes up with a dashboard that shows you all your views and all of your key metrics and demand and activities all in one visual with that percentage change yesterday versus the 90 day average sure google analytics has alerts but those are limiting it's only yesterday worse the same day before same dated previous week doesn t show you that average and that average is really key to see if the spike or a drop is noteworthy or is it just long our average here so getting this global visual of all of your views all of the metrics and the percentage changes I can be really really helpful on top of that with the Google Analytics dashboard tool we can set up daily email alerts so that across all of your different platform sites brands what have you you can see those top keen conversions or metrics or data points that you are collecting what was yesterday's volume versus the average the percentage change and the reason I'm showing this mobile visual here is because the way our clients use this every day they get an email morning they can scroll through all one quick email all their platforms and say is my business healthy is there any major drops or is there any major spikes from campaigns or things that happened in the last 24 hours compared to that 90 day average which is key so without further ado let's jump and do a very very quick demo of this monitoring platform so this is the monitoring platform you can see we have example views that are maybe different markets different websites different domain whatever the different things are you're managing this is particularly useful for multi brand organizations and all the different metrics that you want to see in one visual with changes over time this is all dummy data but you can see for example the Jaypee production example view here had a drop of 60% for this particular type of product that was purchased or utilized so scrolling down you can see there's some major drops of 100% or no comparison possible because there's no data at all and this is all configurable as far as the colors and volumes and thresholds that you want to and which views you want to include there's even a section where you can add annotations so you can have some context provided to those spikes those increases in dips again if you're managing this in Google Analytics across different properties you have to recreate the annotations potentially or use a rollup property which again only gives you that trend line doesn't give you yesterday versus that 90 day average to really truly see something meaningful happened or changed you see there's a other tabs where you can have other key dimensions or attributes that you want to check time and the trend line alerting you can also change some account settings so this is all plug-and-play product once you setup with us you plug it into your Google Analytics data directly and on a daily basis using the API this is refreshed but you can decide which views you want to include in the dashboard as well as which metrics or attributes you want to measure against in this platform and in there under the views is where you can select do you want to show it in that daily email report of which tab and view you want to analyze this in so the way this is utilized by our clients and by us is on a daily basis from regularly check this platform to see what happened across all of our systems all of our sites we need was there a meaningful change an increase or drop and what happened this is more on the GA side of this platform where tag inspector in this visualizations or source code changes and really gives us that confident that if we see something failing we can immediately jump to see what happened is a data layer via the tag management system or tracking itself or is it in the platform Google and in this case directly so data validation ongoing monitoring these are our solutions and tools to really give that confidence everything after you deploy will maintain that integrity and continue to track exactly how you need it so that you can be successful in your online campaigns so hope this was helpful background for you again this is immune shock be back here but let us know if you have any questions or you're interested in doing some ongoing monitoring data validation or learning more about that process thanks for watching you"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Pipelines and Orchestration",
        "Topic":"Data Validation and Model Monitoring",
        "Video Title":"Community Spotlight - Build Data Validation and Model Monitoring Pipelines with DVC and Evidently AI",
        "URL":"https:\/\/www.youtube.com\/watch?v=qFnwZ653Aks",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/qFnwZ653Aks\/hqdefault.jpg",
        "ID":"qFnwZ653Aks",
        "Publish Time":"2023-04-18T11:00:37Z",
        "Channel":"DVCorg",
        "Channel ID":"UC37rp97Go-xIX3aNFVHhXfQ",
        "Transcript":"welcome and thanks for being here everyone Mikhail rashkov is an ml engineer and mlap solution architect with over seven years of experience in machine learning and data science he is a founder of the ml reppa community and author of courses on ML automation DVC and ml Ops topics if you've taken our online course you will recognize him as the lead instructor he has helped over 200 Engineers learn advanced concepts and tools for ML engineering and ml apps and has collaborated with 50 plus companies and ml teams in the U.S Europe and Asia to design mlap's processes and integrate open source tools he joins us today to teach us how to build data validation and model monitoring Pipeline with DVC and evidently and with that I'll turn it over to Miguel take your imagine thank you very much iterative team to invite me to this Meetup this is the first time I think I present here uh even when I was a part of the iterative team I didn't know it somehow I don't know my birth so um let's uh correct this mistake today okay uh hello everyone uh let me share my screen then and uh I'll start with some introduction and uh that about some slide about me and uh we'll jump into the topic and uh later we'll have time for discussion I hope we'll have time they need our do we have around one hour right uh for the whole meeting okay yeah today we'll uh talk about the data valid vision and model monitoring pipelines and uh I'll show you an example file that to uh tools like division evidently I could work together for these purposes uh as Jenny said I I am a machine learning engineer and Tech consultant in mlops and uh I had that opportunity to work with iterative Team it was a very good time and now I mostly uh work with independent consultant and we also organized some meetups for ammo Reaper Community uh this community about uh tools for machine learning automations or envelopes and actually my this community was started around DVC five years ago or four years ago and we started with DVC and then and added more different tools and scenarios around that pipelines of the niche okay today uh we'll talk about data validation and modern monitoring and specifically we'll focus on how it works with evidently and how we can build DVC pipelines with this functionality that evidently brings to machine learning projects okay let's go on I'm sure that you know this picture about uh stack of different technical technical problems and tasks for machine learning projects and allow some part of this of course are about dateable Edition and monitoring and for this we start with we have some tasks for data validation at model training stages and then of course we have we need to monitor our data predictions and system when we run our models in production and generate some predictions so let's uh jump into this visualization of envelope's workflow from the Google envelope Sparky uh partitional guys guide uh sounds like that sorry I didn't uh add the reference link to this but uh sorry for this and I also hope that you saw this picture and here I only added this timeline and the idea of this timeline that uh if at the current step current time we have for our model in production and the model generates some predictions we need to monitor this right but usually this monitoring stuff like the model performance monitoring Accused at the next time period because we need at least four uh bad scoring machine learning Pro projects like applications for banks telecoms uh we need some we have some time lag before we get the ground rules labels for our Target data and only after that we uh then run the monetary tasks so that's why here is like typical plus one so this is some uh period of time in the future and uh the time in -1 here is that uh usually we train our model somewhere in the past uh so with this we can jump into that uh into discussion where we need to uh have data quality and model evaluations and validations so uh definitely we uh can do some data quality checks and data validation at uh training steps right so before uh for example we we run uh training our model for production use on the the whole data set we have uh we need to check that this data is correct and there are no not uh like much problems with the data drift Etc and also we have some model evaluation usually this is a part of for the training pipeline but uh uh like from when we automate the whole pipeline this actually looks the same as a model performance monitoring at product at production stage because we use uh usually the same networks at least like uh the uh most common metrics they are usually the same uh next or uh prediction Theory step we need to monitor our system that runs uh our prediction service the data quality that we use to generate this prediction and also we here we can estimate the prediction read and uh at least before we get information about ground truths labels at least we can check that distribution of our predictions are the same looks the same as at the previous steps that means like nothing nothing changed too much and when we go to the monitoring step we get the grown tools data and we can do model performance monitoring and check that real Target read so this is uh just mapping of different data validation and monetary tasks into these envelopes workflow okay so this is the whole picture Let's uh jump to the out tools like evidently ndvc and understand how both tools can help us with uh to handle these tests so uh for monitoring we have like a let's say Let's uh we'll focus on two problems uh one problem is what and why we should monitor for our projects and this uh like these requirements uh mostly are from uh from business perspective from uh user experience and some specific specific machine learning applications so in some cases we also uh metrics for regression and classifications are different uh timeline and uh offset frequency that required for uh to monitor monitor out data and our system Also may be different etc etc so this is like uh I call it uh ML and business and ux requirements so what and why should we Monitor and the second problem is how to make this monitor work and this is some operational requirements so how we can schedule uh uh and Trigger the monitoring jobs how we are we should run it so what infrastructure and resources how we can get access how we calculate metrics etc etc and uh at the end how we visualize and use this uh monitor results of this monitoring jobs so this operational requirements and in this example we'll show that DVC in an evidently AI uh covers different parts of these requirements so DVC uh can help mostly with these operational requirements and can automate this submandatory pipelines and deal with the accessing and managing data versioning Stuff Etc and evidently uh helps to calculate the metrics that you need for your project and generate different visualization that we need so in the simple scenario is just HTML reports uh in the most complex scenarios is uh results can be stored in databases and visualized with uh grafana dashboards or some other bi items for example okay and uh let's jump to evidently Ai and for whom like I I just briefly discuss uh introduce you to evidently AI so availability AI is a library that helps to uh help you to calculate different metrics for model quality data drifts and data quality and also can help you to generate some visualization around this and it has new Core Concepts first concept is a metrics so Matrix is some components that evaluate specific aspects of the data or model quality so in this example we have widgets that just visualize these metrics so Matrix is uh each each value here for different ml metrics are like metrics concept from um from evidently AI the same is this distribution so actually is like a common column value range Network that can help us to Define this ranges Etc so there are some metrics that you can calculate with evidence AI and those metrics can be uh aggregated into different reports so the reports are just combination of different metrics so you just select what matters you want to see in the report or your purposes and uh just save these results as a report evidently I have some metrics presets so this is just pre-built reports like of the commonly used metrics or different tasks from data drifts and progression uh tasks to NLP projects for example and also it has tests and this is just a very similar to metrics but the tests has some specific value that is used to compare the calculated value against this test value and decide whether your system or metric pass this test or not and also you may have such kind of visualization okay and so how how does it work uh there to calculate these metrics and run this uh reports generation you need two data sets so evidently use a reference data set is a for that like that that uses a basis Baseline and uh it can it needs a second data set the current data set that and both data set just used to generate these reports calculate metrics and organize them in these reports so this is how uh like what we need into this monitor pipeline just Define what is a reference data set and what is our current data set are and then uh run this calculation uh of metric screen uh if you are interested in the like uh like simple example how these reports can be uh built and visualized in simple UI using stream lead you can find the links to our uh blog post and example of uh that we presented just a couple of four weeks ago with evidently team so and I think it's very like simple uh example of what you can build with evidence Ai and how to start with but we go go to the uh second uh topic is uh design monitoring pipeline so uh I hope that most of the people know about DVC and how it works here on this Meetup that's how I why I don't have that separate uh a section about DDC Basics uh right and uh I just so we'll continue with some uh high level uh requirements for the these pipelines first and then just jump into the DDC pipelines and how they can be configured so for this example I choose the bike demand forecasting data set and thanks to evidently I only have a lot of examples with this data set so it was much easier to prepare this example or integration example and this data set actually predicts um like used to to train the model that predict the demand for uh for the renting bikes at different times for each hour there are some set of features like a temperature humidity holidays Etc and so we just use this feature and generate prediction for each hour how many bikes uh will be in demand uh in some City and for our example I choose to three separate pipelines the train pipeline that actually download data through in the model and then run model evaluation to calculate metrics then the predict pipeline the prediction pipeline of course generally prediction for new data and monitoring pipeline that's uh like Waits until we get the ground rules labels and then calculate performance model performance reports uh in this uh for this example of course we have only one data set and we only use different uh I'd say that time periods to Define what data we use to train model what data we use to uh generate predictions Etc uh just to simplify this uh configuration but uh it will it looks uh similar to the real life projects where all these three steps are separated in time and of course the first you train the model and at the end at the next separate time period and and usually in using the different resources I mean computation resources not clouds servers you run the prediction and then also as a separate pipeline you run these monitor jobs and for each of this pipeline we'll uh generate different reports model evaluation report for the train data quality uh when we're on prediction and the target driven model performance for monetary instance okay and results of this um monetary monitoring jobs will save in different subdirectories under the reports directory uh separate uh three subdirectory or the training Pipeline and uh the subdirectory that they use uses this uh start and end period uh time period for the our prediction period I used as a name of uh subdirectory where we store this code here data quality and Target drift reports okay let's uh go to so this is like a conceptual uh idea what what we want to achieve and now let's talk about DVC and how it works with this uh first to achieve this solution we prepared three separate DVC pipelines right in the most use cases and examples uh and get started Etc we have only one DVC pipeline only one DVC dot yam of configuration file with a different stage inside and this is very good scenario uh with when we have the um when we need to run the whole pipeline uh using one command like DVC array Pro or dvcx prong right and DVC uh managed how to run all the stages inside and track uh outputs uh dependencies etc etc right but uh here uh we want to achieve us another result we want to run these stages independently and uh yeah that's why uh we decided to split it into separate DVC pipeline so here we have uh three different Pipelines train predict and not pretty but monitor here of course okay and to run each of this pipeline uh you may use the command like this dv0 Pro and then the path to the uh dvc.yaml file under the this subdirectories so for each of these pipeline okay later I'll show the the code for this uh pipelines if you like so this is how it looks like in the uh repository we have this directory in the repository route the pipelines and inside for each separate pipeline we have DVC not yaml and param.yaml files and like we pretend that this strain pipeline is used for development purposes right we need to train our model and this is usually a the pipeline that looks like uh just like uh most of the use cases on DVC uh documentation uh the pipeline with a lot of stages for for to train model and then evaluate model and the other pipelines to predict so generic predictions and monitor our models are like a broad Pro production scenarios and they will be used and run in this separate time and probably in like separate environments specifically for production use right and this is uh the second solution we need here is uh uh manage our monitoring configuration using params.tml file this uh the params.tml file is a the main configuration way to configure ml pipelines using DVC right so why not support all our configuration we need to run this Monitor and stuff into this parallel to tml file uh as uh so when you start to work like okay to to run uh monitoring jobs with evident AI you need to define the column methods so evidently you want to understand which column for example we used uh as a to store our predictions what are the Target column and what are numerical and categorical features in our data sets so all this information we will use to build this uh the object uh or columns mapping and this object then used to the in the reports generators and also we can use some information about uh the the model and the reference data set we used to generate these reports and where and how we store our reports final reports so in this case this is a monitoring job we have we save two reports the model performance reports and that Target drift reports okay uh or what else so okay next step uh reports version because we use DBC and we specify these reports as outputs of the DDC pipeline so DVC automatically uh control versions of these data sets these artifacts I mean these are monitor reports and you can just automatically keep versions of your monitoring reports aligned with the configurations right and version of code that used to generate this report so this is like out of the box the the fee by default that features that DVC provides you with the DVC pipelines and this is very cool and and the last solution we use here is the access or feature of DVC feature is the ability to provide access to artifacts in the production stage so for example in this exam in this example we run train pipeline to generate our models and also it stores their reference data set that will later use to generate our monitor reports uh and after you push this reference data to DBC storage later you can just pull this data set uh to the machine that you use to run the monitoring pipeline or prediction pipeline in these cases for monitor monitoring pathway so uh or use K4 Solutions or features we use to build this example are separate pipelines for different purposes for different environments and different time then uh monitoring configuration we put into the params.kml file version of our reports and uh just automatically DVC provide us this access to artifacts we saved and version with DC that's it and this is what we have in this example also we can add more improvements like using Hydra configuration to simplify configs because right now we have a separate parameter tml file for each Pipeline and they are overlapping and duplicate some configuration and it can be difficult to manage this in production and it just needs some tweaks to use hydro configuration just to reuse the uh common params that are just used for each pipeline like model names reference data set rooms etc etc and also we can use some cloud storage like S3 to store entry uh this monitoring reports so uh I guess that's it from the uh slides perspective uh now we can jump into the code I'll use I'll show you a little bit of this code and uh so um how this uh looks like this evidently reports okay let's start with this uh this is a model performance report that is generated at the monitoring stage in our Pipeline and for we use regression uh preset are here and it automatically calculate this mean average uh I mean absolute every uh so uh yes uh m-a-m-a-e and uh m-a-p-e this Matrix for uh for regression model performance evaluation and also can plot some uh distribution of uh predicted versus actuals and uh some errors etc etc so it's a quite a lot of things that it uh that uh evidently automatically adds to these monitor reports and they're quite extensive you can control what kind of data you want to put into these reports of course but for Simplicity we just use that preset so how to uh we run this report for model performance let me just navigate to uh project so in this Repository just close it uh here are pipe is a pipelines directory right and inside we have this three DVC pipelines so let's jump into this uh uh pipeline or monitoring task so this pipeline just uh runs uh our monitor model python script and saves our monitor monitor reports for Target drift and for model performance so let's go to this uh monitor model script and inside it's just uh we just load some configuration right we then get load the predictions and actual data set the reference data sets and yes here we use the numerical features and categories categorical features we extract them from a config and then we use them to build this circle mapping object that is used by evidently and then so this part actually is the the part that actually calculates this report so for model performance reports we use the uh just pass the regression preset object to the reports class and then just run it with a reference and current data sets and we also passed this column mapping to this run method and that's it as a result we get this model performance report object that we can save as HTML or for example we can save it as a dictionary or as Json or get some python dictionary from this report and use it in some way we need as a result we save this model performance dot HTML file into this subdirectory and this is how it looks like ultimately alternatively we can let me show you this the another pipeline it's a training pipeline so here of course we train the model and we have this evaluate stage inside and this stage downloads the model we just trained and uh generates model performance reports as well but specifically uh the performance on the training data sets right and also it saved the Matrix for this stage so let's open this evaluate stage and I'll show you that we use the same regression preset that we use for monitoring purposes but we only uh pass the different data sets here so uh for reference data okay its data is the same but test data is the specific data set that we use to test our model that's it and as a result we get some uh model performance reports we extract from their metrics we need and save these metrics into the metric Json file that looks like this so just a simple Json and actually DVC can understand this Json Matrix files right and this why we just plus this Matrix file as a matrix into the DVC pipeline and DVC I understand that yes this is Matrix um and it can show us this metrics from so you can work with this metrics uh like in in the way that like that DVC can work with them so uh we use the evidently I not only for monitor purposes but also uh you can use it to generate the evaluation report for your model at the training step and uh save this metrics information for yourself um I guess that's it from my side uh the only one thing probably here [Music] no I think that's it and just uh short advertisement that if you are interested in such kind of topics please uh go to our community and we'll have some few another interesting talks in these months about other other uh topics and tasks about ml Etc and yeah that's it cool thanks so much Miguel that was awesome very detailed does anyone have any questions for Michael I think I have one questions uh could you please tell more about how you run this monitoring pipeline every day and they should be for instance mark this pipeline as always change because DBC might think I but nothing has changed and there is no need to run pipeline or monitoring so it might be a problem yes thank you this is a good question and this is uh like what I don't have as an example in this repository in this tutorial right uh so here we just focus on the to design and build these pipelines with the DC and evidently Ai and we just leave aside this uh how you should schedule this runs but the idea here is like it's a training pipeline you can just use uh like uh uh in a common way just rerun it as soon as you need to experiment with the model you train uh those pipelines for um a full monitoring and prediction to schedule them you need some separate like external tool that will run them and Trigger them at the specific timeline uh like for example is the prefects airflow or you use cicd pipelines uh to schedule running this is a pipelines right so and it you can also pass some updates to these pipelines like this uh that you want to use to run this pipeline and you to make sure that DBC actually runs this pipeline again uh you can just specify that Force argument to dvcx Prime if you like but uh yeah yeah to be sure this is one uh this is exactly uh the reason why we separate this prediction pipeline from the model training pipeline because the you need to rerun this again and again maybe it's or either on a regular Embraces and you uh don't need to be like you don't want the DVC just keep running this pipeline because nothing changed so specifically for this pipeline you'll use like minus F to uh Force run this pipeline all right thank you very much we have another question from Natalie in the chat she asks can you recommend a tutorial or maybe give some tips on how this can be integrated with fast AI training is done in AWS sagemaker uh okay good question thank you Natalie uh uh actually I don't know such kind of tutorials or specifically for fast AI like uh this approach looks like uh framework agnostic inside this train stage uh like you may choose any framework and any model you want to trade so uh from for DVC it doesn't matter actually uh what kind of model use what kind of framework used to train the model right um I don't know actually I never tried to run the DDC pipelines on stage maker and uh I'm not sure that how it works so now I think no I'm sure that it's possible I hope it's possible but I never did it and I I don't know such tutorials I was recently talking to another Community member that is working on with label studio and DBC and how to combine those things together and he was asking for exactly the same question so um I think it's content that we need to work on internally and make some kind of blog post work so it's duly noted does anyone have any other questions yeah only the only thing here actually that evidently Ai and this like right now don't have functionality to calculate to to monitor uh image data sets and uh and calculate some metrics for for computer vision okay so evidently it just covers tabular data is that right yes yes it works as table of data and Text data so for NLP tasks uh yeah I think that it's possible to calculate some classification reports uh based on if you like have prepared some uh data for the evidently format but it's not like straight away and I don't think this is the use key so for computer vision as I know as I know maybe I'm wrong but uh they don't have functionality [Music] right now okay yeah I think like what he was asking for is just the actual part of automating a pipeline using stage maker and and DBC together so uh yes uh I understand like like okay so from uh evidently parts for monitor Parts uh probably you can uh switch to another tool that can provide this monitor for computer region and it's it's okay or uh sagemaker I don't know exactly and how to run this uh DVC and how like yeah what limitations Pipelines [Music] thanks for watching if you enjoyed this product update video please like And subscribe thanks Stevie and feel free to post comments and questions below on our YouTube channel we share videos on product updates tutorials and how members of our community use our tools as they solve problems in their domain spanning a wide variety of fields in the machine learning and AI space see the description below to find links in the docs for our tools also visit our blog where you can find tutorials on our tools as well as product and Company news join our Discord server to get support help others grappling with the same issues as you connect with other like-minded folks and discuss our tools or other topics in the mlop space we also have a job Channel where you can find relevant job opportunities in the space finally if you're really serious about taking your ml lab skills to the next level we offer a free online course that is designed to help you understand the iterative philosophy and achieve your mlab schools thanks for making it to the end DB and I will see you in the next video foreign"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Pipelines and Orchestration",
        "Topic":"Data Validation and Model Monitoring",
        "Video Title":"SREcon21 - Model Monitoring: Detecting and Analyzing Data Issues",
        "URL":"https:\/\/www.youtube.com\/watch?v=pD8LEhnnajw",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/pD8LEhnnajw\/hqdefault.jpg",
        "ID":"pD8LEhnnajw",
        "Publish Time":"2021-10-13T19:11:26Z",
        "Channel":"USENIX",
        "Channel ID":"UC4-GrpQBx6WCGwmwozP744Q",
        "Transcript":"hi i'm dmitry meliken i'm the founder and ceo at graph signal where we specialize on operational model monitoring i'm going to give you an overview of machine learning model monitoring in the context of production applications i'll be focusing on data issues as a new type of failure mode that sres and operation teams need to consider as models are being deployed to production increasingly more often so let's start from model serving it's just a short intro for those who are new to machine learning a machine learning model can be trained manually or automatically producing a model artifact in various formats when doing inference when the model runs for example it takes input data as image and outputs a classification score so model serving is usually implemented as a web server which exposes a rest api for mobile inference typically it loads and runs the model file using the same machine learning library the model was trained with for for example tensorflow pytorch models can be served by web servers for instance python flask or fast api or specialized model servers such as those in cloud platforms from google or aws there are also on-prem versions like kf serving other than providing apis models can just run against batches of data and streaming pipelines or schedule jobs so the model receives input data from other parts applications for for example from a microservice third-party api or a user input also other parts of the application can rely on model output due to this bi-directional service and data dependency model serving needs to be a part of an overall application observability strategy now here's the problem unlike traditional web applications the model is fully data driven it's trained uh it's not coded so the data output is fully dependent on input data this is also the case with non-ml applications but to a much lesser extent this means that even if there are issues with input data the model may still run predictions without throwing an exception even if data is properly validated for unexpected values things like data drift or concept drift may cause the model to output garbage that would treat it normal by the rest of the application this is well-known garbage in garbage out problem illustrate this uh imagine one of the features can be zero from time to time to a normal operation but due to an error in upstream api all data instances have zero for that feature so this case is not detectable by a simple data validation script another example a third-party service api has been updated changing the data format or returned data field the value may still be passing validation scripts but the model may start predicting a different class such problems may be detectable looking at the long-term anomalies and analyzing input feature distributions so such silent failures are hard to detect uh in real life when we usually don't have ground through to compare model outputs with so we don't know if model predictions make any sense or not unless we monitor data for anomalies and various types of drift now let's look closer at data dependencies that may affect data quality here uh there are two main risks to consider first are the changes in upstream data dependencies this could be a release of a microservice and an entire application a change in data format may not be coordinated with ml team and the model will start receiving unexpected data second are the bugs in upstream dependencies resulting in missing features for instance nulls or zeros model deployment is another dimension to consider usually model deployment is subject to model training process rather than application release cycle this may mean that a new version of a model may be deployed without going through integration cycle integration testing this is a common problem because models are trained and validated and thoroughly prepared and cleaned up training data not on unseen production data even the feature engineering code can differ from training uh to deployed model in other words uh a release of a model may cause model to fail on production data and needs to be quoted closely monitored okay now let's start talking about monitoring so in machine learning the term monitoring is not only is used not only for monitoring in a sense of ensuring system performance and availability but also for detecting model stainless bias and fairness as far as the letter case is concerned data scientists and machine learning engineers need to log all predictions collect ground through analyze model performance by measuring feature importance model accuracy and so on those analytical jobs are rather use case specific and require custom pipelines or notebooks if it's done locally automation is possible of course but later in when things get stable on the other hand when looking at the operational issues rather than model specific validation and improvements the monitoring function is optimized for low mttr for this purpose continuous monitoring and detection is preferred just like it's done in traditional application infrastructure monitoring so here's a simple diagram to illustrate high-level reference kind of architecture for an operational monitoring system basically the logger runs inside of the application computes prediction data statistics and sends them to the server for for visualizing and issue detection in in some situation it might not be possible to integrate the logger into a model server in those cases the the logger could monitor api requests and response to the model in the downstream service proper data windowing is important when logging predictions as statistics are computed based on time windows and not single prediction instances in terms of technical requirements the logger should be able to handle high number of prediction with low cpu and memory overhead so to make sure the computed statistics have low error rate and small size data sketching algorithms can be used otherwise sending all predictions for processing would be necessary and and that would require setting up a separate data processing pipeline just for the purpose of model monitoring one more thing to keep in mind uh is that pre-computation of statistics when logging needs to be data aware for example support text or images well it's more or less clear what to monitor in tabular data extracting statistics for text images sounds videos may be application dependent in one case the number of words in the input text may be an important matrix in other case not it might also be helpful to log model metadata or other model and system attributes to simply simplify further further analysis and troubleshooting it could become useful to pinpoint the differences between a b tested models or subsequent model versions and to complete the monitoring system of course the statistics should be visualized for troubleshooting or proactive data analysis and undergo automatic issue detection when analyzing data statistics for a running system visualizing time series is essential to notice any changes just seeing summaries for selected period of time is not very useful one important requirement for such dashboard is the ability to analyze feature distributions and see how they change over time statistical distribution distance metrics such as certain movers distance will help to detect as well as visually track distribution changes but i'll not be covering data analysis and drift detection methods which is a big topic by itself automatic issue detection is critical because there are usually too many input features and properties to be monitored it's hard to manually set up alerts for each and there is a risk of missing things detection is done on prediction time windows this can range from short-term anomalies to long-term distribution changes such as seasonal data drift it is out of scope of traditional alerting systems to set up alerting based on complex statistical computations and custom baseline so there are three main takeaways from this summary um data issues in model serving is a new failure mode for machine learning engineers and analogues teams to prepare for in order to reduce mttr and meet slos traditional monitoring systems do not address the data quality aspects and therefore the visibility into model serving operation will be limited and finally model inputs and outputs both should be monitored depending on the application the implementation can range from simple checks to uh machine learning specific monitoring tools yeah so this was a short overview of model monitoring feel free to contact me if you have any questions or comments thank you you"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Pipelines and Orchestration",
        "Topic":"Feature Stores for ML",
        "Video Title":"What is a Feature Store for Machine Learning?",
        "URL":"https:\/\/www.youtube.com\/watch?v=DESBDukN9gw",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/DESBDukN9gw\/hqdefault.jpg",
        "ID":"DESBDukN9gw",
        "Publish Time":"2023-08-10T09:02:55Z",
        "Channel":"Hopsworks",
        "Channel ID":"UCtuK0GKJl8TVLqj2702y_Fg",
        "Transcript":"foreign [Music] the concept of the feature store what it is how it works feature store first emerged in around 2017 in hyperscaler startups to help them manage features in short they needed a way to bring more models to production faster and they had to build a platform and abstractions to help them do so today and while they might have different names in different organizations they are used by most of the Fortune 500 companies and a wide range of hyperscalers they have become pivotal as piece of infrastructure for the AI and data and they help maintain train and improve models so first what are those features we are talking about anyways well features are essentially the data for AI they are the measurable input The Source used by the model for both training and prediction as such they are often considered all referred to as the fuel for AI the easiest way to represent features is usually ask columns in a table those columns could be the monthly purchase churn customers profile or any other significant signal that could help a model or help build a model the features I use during all the stages of the ml life cycles so managing the data those features is a significant challenge in machine learning particularly as they need to be organized stored and readily available for the models this is where the feature stock comes in it's a platform that manages store and provide access to both historical and real-time features all the data that is needed by the model for the model to either be trained or to make predictions now that we have a sense of what are features and what is a feature store we can probably consider who does it help well as a collaborative platform it is mostly catered to data scientists ml engineers and data engineers in essence the feature store is at the Crossroad as the at the intersection between all the decision makers in an organization in an organization working with AI and ML and it is often the responsibility of those professionals to operationalize models at scale in other words to make more models and to put them into production as quickly as possible again this is where the feature stall steps in the value of the feature store lies in its ability to bridge the gap between different stages of the model life cycle and the many stakeholders involved from creating features from raw data and ingest it to providing that new data for the training of the model and finally making predictions on new data each stage belongs to a different team or demands a different set of skills the feature store remove those barriers between teams data and operation and that's not it as connecting those stages also simplifies the operationalization of ml models which in turn brings efficiencies scalability and smoother collaboration the modern feature store facilitate most importantly the ReUse of features which can take a lot of efforts to be engineered and can represent sometimes around 80 percent of the time spent on building a model so ensuring the correctness whether in training or during inference the prediction in batch or online is Paramount and being able to reuse them is a significant advantage in a nutshell a feature store is a data platform that connects different ml pipelines streamlining machine learning operations to bring the models that bring value to the organization to life faster and if you want to get started easily with a feature store consider using hopswork serverless it's free python native demands no infrastructure and that's it for our video on the feature store thank you for watching I hope to see you next time"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Pipelines and Orchestration",
        "Topic":"Feature Stores for ML",
        "Video Title":"MLOps Feature Store Explanation",
        "URL":"https:\/\/www.youtube.com\/watch?v=l6xfFYZAyns",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/l6xfFYZAyns\/hqdefault.jpg",
        "ID":"l6xfFYZAyns",
        "Publish Time":"2021-01-05T13:15:02Z",
        "Channel":"MLOps.community",
        "Channel ID":"UCG6qpjVnBTTT8wLGBygANOQ",
        "Transcript":"okay so you have a bunch of raw data that you need to make useful but wrangling that data can be challenging requiring expertise computational resources and a lot of work to set up to take that burden off the data scientists businesses are now using a tool called a feature store to help them not only store their data but also transform it into useful features that machine learning models can use directly to make predictions that solve problems that raw data is typically coming from various data sources of various types structured unstructured streaming batch real time it all needs to get pulled and stored somewhere that somewhere can be a feature store and the feature store then takes that raw data and makes it available for consumption and in some implementations provides feature pipelines that can transform that data the pipelines produce process features that can be used for both online and offline environments those process features are then used by data scientists and ml engineers to produce high quality predictive models the outputs of those models then get used to solve a plethora of business problems for end users feature stores allow data scientists to work more efficiently by allowing the reuse and sharing of features so that other teams can use them too and thereby increase their own productivity since they don't have to do it from scratch this helps make the most out of the data and makes things better for data scientists by providing solutions to common data engineering problems feature stores can also provide data governance through feature lineage and versioning those with monitoring capabilities allow the business to track the distribution of their features over time and discover problems such as training serving sku or concept drift overall a feature store functions as a powerful interface between your data and your models and is a powerful tool within the mlap stack you"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Pipelines and Orchestration",
        "Topic":"Feature Stores for ML",
        "Video Title":"What is a feature store in machine learning?",
        "URL":"https:\/\/www.youtube.com\/watch?v=Cm8An5VYARk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Cm8An5VYARk\/hqdefault.jpg",
        "ID":"Cm8An5VYARk",
        "Publish Time":"2023-05-15T20:58:28Z",
        "Channel":"Alexander Sergeenko",
        "Channel ID":"UCHad_trDmJ7dzcodX431IeA",
        "Transcript":"what is a feature store machine learning does not operate solely based on models to make predictions you'll also need features according to Wikipedia a feature is an individual measurable property or characteristic of a phenomenon but this definition may seem confusing and needs to be clarified in simple terms a feature is a descriptive attribute that's relevant to predicting how Thompson will behave with the world features are usually numeric and can be conveniently described by a feature Vector in other words a feature is a set of attributes that forms an input Vector for a model for example the feature may be produced from a row of CSV file but what is a feature in terms of the person as an example your credit history is apparently relevant to predicting which loan will be approved for you"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Pipelines and Orchestration",
        "Topic":"Feature Stores for ML",
        "Video Title":"Feature Store for Machine Learning - MLOps",
        "URL":"https:\/\/www.youtube.com\/watch?v=ZeJdr0nZ9PA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ZeJdr0nZ9PA\/hqdefault.jpg",
        "ID":"ZeJdr0nZ9PA",
        "Publish Time":"2021-04-18T03:01:19Z",
        "Channel":"AIEngineering",
        "Channel ID":"UCwBs8TLOogwyGd0GxHCp-Dw",
        "Transcript":""
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Management and Versioning",
        "Topic":"ML Experiment Tracking with MLflow",
        "Video Title":"MLFlow Tutorial Part 1: Experiment Tracking",
        "URL":"https:\/\/www.youtube.com\/watch?v=RnYa3QsXRAc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/RnYa3QsXRAc\/hqdefault.jpg",
        "ID":"RnYa3QsXRAc",
        "Publish Time":"2022-12-28T19:45:16Z",
        "Channel":"Anton T. Ruberts",
        "Channel ID":"UCabrZbL35w6LPZxwFNELzuw",
        "Transcript":"hey everyone welcome to this tutorial about Ammo flow and experiment tracking for tensorflow and some other libraries as well feel free to follow along all the code is in the GitHub and the link is down below just keep in mind that some parts might take a bit too long to run so it might be best if you take this as an example follow the syntaxes and then apply this to your own project okay now let's begin so first things first let's go into the GitHub page of tutorials that I have here and I'm interested in getting these two notebooks so what you can do you can just clone it like this and then to git clone and then paste the base the link that we've just copied as you can see I already have it so here I have on my local machine these two notebooks and I even have them opened in the jupyter notebooks so let's just do a quick run through you don't really need to understand much that's happening here but the most important parts are that first of all I'm getting the data I'm doing some pre-processing and then I'm running a bunch of different models trying to predict the test data set and then recording the test metric which is root mean squared error in these separate variables and so I'm doing a random Forest I'm doing a cad boost and then variations of this failure like very fancy Transformer architecture for tabular data I have a whole like Block series about it if you're interested in but basically I'm doing the same thing here so I have some sort of training happening and then I'm doing the predictions for different variations of it and storing the results in kind of yeah in just different variables and the main output of this notebook is this table here well not table but like the printout as you can see here I'm attempting to compare different models and looking at their rmses so why is this bad is because I have no recollection of what happened before I got these values so data run experiments multiple times which parameters did I use I can kind of like just try to guess them from like the parameters that are specified here but nobody would tell me that like because I I don't know I changed it to 50 here and then just saved it because I wanted to run the experiment but I didn't so nothing really tells me that like which parameters are responsible for this specific metrics so this is what experiment tracking is supposed to fix for us it's supposed to tell us which parameters have led to this specific results and even it can save us some model artifacts maybe models themselves so it's really useful and really comment using kits and this notebook has refactored this code and started using the ammo flow experiment tracking so which I'm going to tell you more about let's go to the command line and here I'm using a virtual environment called blog so for me it is already activated so what I'm going to do I'm going to just pop install ammo flow if you don't have it yet I already have it so it's going to be you know quite simple and quite fast I would say like it's a quite a friendly package like it doesn't have any weird dependencies so it should be working with your environment as well now when I've run it the first thing that I want to start running is the actual ammo flow UI which is quite useful in when you're tracking the experiment and to call the ammo flow UI so it's going to be the command ml flow UI and then we also need to specify which database we're going to use to store the experiment results and different artifacts that we specify so I think the command is something like backend store URI and then you need to specify that it's a sqlite and then ml flow dot DB so this is going to spin up an ammo flow server which is going to build kind of showing you and tracking all of your experiments which is super neat so let's copy and paste this link to a browser and there we have it okay so this is like the ml flow UI which is yeah which is going to be like your entry point to to the experiments that you run as you can see it has two tabs experiments and models we don't touch models for now uh but what we can see here is that we have one experiment which is called default and it has no runs so this is created automatically for you now what we want to do we want to create our own experiment run and to specify first of all uh how it's what its name and which primers do we want to store in it and now let's go back to the code and I will show you how to do this in this refactored notebook so as you can see we import it just like this import demo flow and this part is important okay one second let me specify which kernel I use to run it so as I said I use the virtual environment blog so I'm going to use the same kernel to run it now let's import all of this good stuff might take some time there we go imported everything just setting some parameters so this is the first thing that is important to get right so the set tracking URI so this is going to be the database that we've kind of specified before and as you can see mlflow has created for it it for us already it is here same folder where you run the experiments so we specify it here and then this set experiment income so this is the name of the experiment that we want to run I call it income because this is the income data set um so but yeah like you can call it whatever you want and it has created so as you can see it's smart enough to know that there is no income experiment before so it has created it for us which is super nice so let's load all the data let's pre-process it and first thing you can see here is that we have this option to auto lock so mlflow has different Integrations with different libraries so it is a scale learn live GPM CAD post tensorflow Pi torch and there is a lot of like parameters which you might want to track and it lets you to drive them automatically but in this case we don't want to run it so yeah we're not going to auto lock it and instead we're going to do this manually so how to do this manually is first of all every experiment is run in the context so when you specify with ML flow start run you specify context of the ml flow and the first of all the and the first parameter to specify is the runway so the brand name for us is going to be here it's going to be RF Baseline so that's a random Forest that you can saw as you saw in the previous notebook which was not that well formatted and you know just like did the training and the predictions so get all of these steps without any of the logging so now to do the logging there is few things that we need to do so first of all all of the logging starts with ammo flow and then we call different methods of ammo flow so the first useful method to do is the set tag so this is kind of like a custom elements of the of the of the experiment run that you want to specify so here I'm going to set tag of the model name and it's going to be random Forest so because my main purpose here is to compare different architectures between each other so Random Forest CAD post people are unique models I want this tag to be there so that's going to be the first one and then log params lets you to log multiple parameters if they are stored like this if they're stored as dictionaries so I can so I can log automatically number of estimators and maximum depth again so this is done so that I remember which parameters led to a certain result if I don't do this then it will not get locked and this information will be lost and I will not be able to recreate the experiments or to communicate it clearly to the to my colleagues now okay so I locked the params I do the random Force I I specify the random Forest I specify the fit do the predictions do the calculation of the metric and then as you can guess it so log metric logs the metrics that you want to your experiment to have so this is going I'm going to call it test rmsc and then this log model so this as you can notice this is a specific to SQL learn because there is different ways to save models and a lot of them are different between different different libraries so what you want to do you want to usually see if there is specific methods for the library that you're going to use to save your model and in this case it's an sqlr model so I want to lock this model using sklearn and first argument to specify is random forest and second arguments is just the path of the artifacts um it's nothing fancy so I'm going to just run it I'm going to wait until it finishes running and then we can see in our UI how the experiment looks like okay so it has finished running now let's go to the ml flow let's update it so as you can see the experiment income was created and here we can see the Run link that we have also created which is called RF Baseline so there is not much here but what we can do we can add metric and we can add a model name because that's what I'm interested in so yes test rmsc is 0.5 model name is RF you can also click on it and you can see different parameters that we stored so the max depth was 20 number of estimators was 100 metrics tags and the artifacts as well so because we chose to save the model we can actually load it into our environment later if we choose to uh they even give you like a few useful snippets on how to on how to do it uh so yeah we can explore it a bit later but for now it looks really awesome right we didn't need to do much just specified a few parameters and it has locked a bunch of stuff for us so that's uh I would say it's a really good it's a really good workflow to adopt and it also makes you a bit more consistent with the way that you specify the parameters and the way that you log your experiments so let's go back down to the training and try another model so this is going to be catboost now for the cad boost we need to specify specific data set type again not that important for this and here we can see again so we're specifying a new experiment run now this one is going to be called called catboost um again I'm setting a tag to the model name cadbos because I want to compare them and want to train it I want to predict I want to test unlock the metric and save the model so the same workflow as we had before and as you can guess it's if we go here you can see that it will already knows that something new is here so we can refresh it and here is the cad boost so the duster MSC is lower and we yeah we know that this model is better we can even yeah click on it download it if we want and use it in our kind of next next steps awesome okay so now the other other things is so we want to know how to use it with tensorflow and luckily for us the procedure is mostly the same right so we still want to do tags we still want to do parameters we still want to save the model the only thing that changes here is you need to be a bit more um how to say you need to be a bit more consistent and a bit more structured with the inputs that you provide so let's start this entangling this bit of code here so first of all when I run this function I want to specify a name so this is the experiment run name as we did before right it was CAD boost RF Baseline so this is going to be something like MLP one mlp2 now the MLP parameters so this is the first set of parameters that I want to store and these are the parameters of the architecture itself so this is going to be a number of neurons in a dense layer this is going to be an activation function of a dense layer and it's going to be a dropout rate between different dense layers and as you can see here so we're going to end up having one two three four parameters and they're mostly so yeah three of them are about no sorry we're going to have five parameters and three of them are going to be about these number of neurons in the dense layers so this is the first parameter now the second parameter is going second set of parameters that we want to store is the training parameters and because uh you know to train neural networks they're quite sensitive to the training parameters such as like learning rates early stopping grounds um white Decay and all of the good stuff so we would also want to track it and to store it so that next time we can recreate this experiment and as you can see here so we pass them into the train and MLP function together with already kind of specified MLP parameter MLP struct architecture and as you can see it has a loading rate it has a weight decay it has number of early stopping rounds and it has a number of feedbacks so that's the parameters that we want to store maybe we would want to tune them as well in the next runs so yeah that's what we want to do here and now let's see what we do here so we store these parameters as well so we log them they're going to be locked together with this MLP params then we want to set a tag again because the main purpose is to compare different architectures then we're going to build the MLP we're going to train the MLB then we're going to make a predictional test and then evaluation of the rmsc on the test data and then the same thing as we saw previously so log metric and then the tensorflow has its own log model methods so the same way that cut boost had the same way that ESC alert has and we're going to save this model so yeah that's basically it uh to do this this required us to think a bit differently and not just like Define model and then do training and then forget about it but instead try to try to modularize it so that it takes the parameters that we want as inputs and does what we want as an output because it makes it so much easier to store these parameters ah if if the function is defined really well yes and let's define some of the parameters so first of all this thing just specifies what is the training date or what is the validation date and what is the test data it is a bit different from all the other ones because tensorflow has its own data set structure now the MLP parameters the ones that I talked before so this is the size of the first layer size of the second layer size of the third layer dropout rate and an activation this hasn't been done using some sort of hypervamp parameter tuning so feel free to adjust it yourself uh so this is the train parameters again which are going to be stored so the learning rate with Decay early stopping patients and number of epochs and the only thing that is left to do is just to run this function that we've defined so specify that it's going to be MLP base now and specify all of the parameters that we did here so yeah let's just run it it's going to take some time to train perfect the model is saved so as you can guess we go here we can see something new and this is the MLP base perfect um so yeah we can see that it performed worse than even random Forest Baseline so maybe we want to change something about it because I don't know we're not happy with the results and we believe that well MLPs should be performing much better on this task so what we would end up doing because we've set up so nicely is basically just copy paste this one specify it as MLP prompts too and MLP base 2. so this is going to be second run and now let's I don't know let's try something simpler it should be like quite a simple task to do so let's just do all of them like 64. and maybe reduce a Dropout Trace I don't know I'm just guessing but using this would it should be so as you notice we just changed a few a few things here and um it should keep trading it until it's done and if we go here so we can see that it has started the second one and and and and and and it's when it's done with it uh it should save the model and it should show us the metrics then we will be able to compare both runs and we can see what is different between them so that's quite a neat thing that mlflow does for us let's wait until it saves the model it's also not really mandatory to save all of the models because if we save all of the kind of parameters properly you can recreate it but I just found yeah that's the saving functionality is quite neat so maybe let's uh okay let's update this one should show that it's been yeah it took a bit longer to train because the model was simpler as you can see okay so here test RMC got smaller so that's a good sign and now what we can do we can just select two of them and compare them between each other there's like a quite a few from quite a few plotting functionalities to do but it makes more sense to do when you have multiple experiments like 20 or 30 then you are going to end up with pretty nice plots so in this case what we want to do we want to just look at the table so as you can see yeah we close the visualizations close the Run details so just the parameters and let's just show the differences only basically we can see here that so the layer 1 and layer 2 size is different and then the dropout rate differs and then the tester on the C is lower model name is the same so that's what we wanted and this way we can consistently evaluate experiments and make sure that we end up with the best model so quite neat right now okay I went back so we have all of them still here now let's go to the models that's kind of this tutorial before was focusing on so they're called ft Transformers feature tokenizer Transformers the really powerful architecture models and we want to see how they perform so again this let's transform the data into the appropriate format and now we are ending up doing the same thing like we did for MLPs so we have separate functions for building the model because we want to save the trade because we want to save the architecture parameters we have a separate function to train the model because we have our trained model parameters except in this case we're also going to have parameters to skip so these are the parameters that are not that important and I don't want to be storing them but as you can see they're still required to build an FC Transformer encoder here so we have these two functions let's define them and again the third function is going to be ft Transformer ml flow run so that's the same function that we did for uh for the MLPs before and we specified the name we specify the model yeah we specify the attack we specify the parameters and log the metric and log the model as you can see here so I was expecting I was trying the auto logging as well and it is quite a neat thing to do I just didn't find it that useful in this case but you can explore it yourself or maybe I will do a tutorial next time about it so let's run this and let's actually specify the parameters to train and parameters to skip and then I'm also going to specify parameters to save so yeah lots of different parameters all of them kind of get saved and uh to find out more about them again please go to the blog post that I did about these Models Super interesting they might be you know the future of tabular deep learning who knows and let's just train the model it's going to take some time so you'll probably see a Time skip here all right so while the final model is trading let's go here we can see that there is four new runs so the first one is linear uh the second one is periodic quantile and the target one so the target one is not done yet but what we can see we can see that the test test rmsc looks actually pretty decent especially for the ple quantile it's 0.456 which is just a bit lower than catbus um so what we can do now we can actually select three of them and let's again just compare them so you can see that having three models is already more fun you can see some interesting visualizations so first of all the test rmbc you can see this is our favorite the ple now what we do wants to like this is actually yeah again we don't have enough models to compare them visually but instead we can show the difference in parameters so you can see that the linear doesn't really take into uh doesn't really take numerical bins as a feature and we can see that ple does and periodic does so yeah just by looking at this we can see the ple indeed delivers better performance here we can also wait until the last one the last Model finishes training before we can get like another set of features and we can see which one is going to be uh better um but yeah like this is pretty much uh this is pretty much all we wanted to see so if we can go back we have this visualization at glance with different model names with different parameters so in this case I would probably want to add a a to add the embedding type we go just to differentiate between the Ft Transformers and having this uh having this view of the experiments is kind of similar to what we had in this notebook right in the old buildbook but instead now we also have tester MSC you have all of the models stored we can get all of the detailed parameters if we ever wanted to we can also see the duration of training once it's done training I will also show you how to reload these models from the from this ml flow repositories our final model finally finished training so we can go back here and we can see that the ple targets is now listed as 0.447 that's the RMC so it is actually the best performing deep learning model but you can see that it took significant time to train um so okay this is it basically for the training and logging and now what we might want to do once all of this is done is to load our best model because we've been saving all of them it's quite easy to do with ammo flow so what we need to do we need to go to the model and then in the artifact section you're going to see this example of how to load it actually tensorflow models have their own methods so what we're interested most is just copy pasting this ID and then going here and as you can see I already loaded it here you can just copy paste it here and then load the model so this particular model if you actually decided to go and train it with me it's going to take quite a long time to load just because it's quite a chunky model and my laptop is really bad so you might want to choose other model to load or maybe yours will be much faster at it but basically once we've loaded it what we can do we can use it as our regular tensorflow model it has a predict method it has everything else and yeah so once this is done predicting we will be able to get our outputs so it gets stored in the output and then maybe we can even do at this plot okay it is done predicting now we just plot it and here we have it okay perfect well this is it for this tutorial I hope you enjoyed it I hope you learned how to log your experiments so it's just to read the rate there is a few things we can log there is we can lock the parameters we can lock the metrics we can lock the tags and we can save the artifacts the models artifact can also be basically anything like for example as a standard scaler that we defined in the very beginning of the notebook it can also be a nice artifact store if you want this if you want this experiment to be truly repeatable so this is it for this tutorial I hope you learned something new about demo flow I definitely have seems like a really great tool and I will try to use it more in my next project maybe I will find out new things and I will share them here but if you liked it uh don't forget to give it a thumbs up and to subscribe I hope to start posting quite regularly so there is going to be always content for you to enjoy take care bye"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Management and Versioning",
        "Topic":"ML Experiment Tracking with MLflow",
        "Video Title":"MLFlow Tutorial Part-1 : Introduction to Experiment Tracking with MLflow | MLFlow | Karndeep Singh",
        "URL":"https:\/\/www.youtube.com\/watch?v=v1vLP2C6vn0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/v1vLP2C6vn0\/hqdefault.jpg",
        "ID":"v1vLP2C6vn0",
        "Publish Time":"2022-10-06T07:00:31Z",
        "Channel":"Karndeep Singh",
        "Channel ID":"UCswr5O1wBDdyULYqIfgtY4Q",
        "Transcript":"hello all and welcome to my YouTube channel so today in this particular video we are going to see an introduction to experiment tracking and an introduction to ml flow so in this particular tutorial we are going to focus on the induction part like what are the terminologies involved in experiment tracking and what basic introduction to the ml flow in my later videos or in my upcoming videos we are going to uh take up the ml flow and we're going to do a quick experiment and tracking using ml flow so that's going to be our next video or next episode of this particular particular tutorial so but in this particular tutorial we're going to focus on the introductory part of the terminologies that are involved in experiment tracking so let's get started uh so there are various Concepts involved in experiment tracking uh first is uh what is experiment what is ml experiment so MN experiment is nothing but as the process of building an ml a machine learning model uh that is uh that is from the stretch that is from collecting data from the source and preparing a feature engineering and then training the model and then and experimenting lot of hyper parameters and coming up with the best model so that's called an ml experiment and then there comes our next terminology that is called experiment run so when uh when this particular experiment ml experiment is being performed you generally try to run as as Trials of your model like you're gonna try a different set of models different bits different set of parameters so a single execution of this particular ml experiment is called ml run so that's what an ml experiment run it does it means in ml experiment and the third terminology is urban artifact so each and every stage of an ml experiment uh during the building of ml experiment uh we generally try we generally try to store some of the some of the files in a pickle format or stores some some of the files involved during the ml experiment right so those files are nothing are called as uh artifacts so these are associated at any stage of the ml experiment uh it can be in the form of Serial file it can be an in form of a pickle file Market file so whatever the files that are involved or in the stages output of the ml experiment are called as a run artifact right and then there is a metadata information that is being uh taken taken up in the ml experiments that is uh the what are the parameters that are involved in the ml experiment what is the start and end time of the ml model uh what what does the time it took to train and what are the methods it involved while doing the model what is the Matrix so a lot of metadators get collected while training an Amazon experiment so these are some basic concepts that get involved when we uh involve when we get involved in ml ml experiment tracking these are the general uh Concepts that's come uh when we develop a particular ml tracking system so now next uh let's move to uh the basic question what is uh experiment tracking so the experiment tracking is nothing but the process of keeping a track of all the relevant information from an ml experiment which includes a source code environment data model and Hyper parameters and metrics so when you execute our particular ml model or when you train a particular ml Model A lot of things are involved when you when you try to train a particular MN model right so you prepare a source code you prepare an environment you prepare a data you train a model with some hyper parameters and then you lock some metrics right so this all things uh where this particular training is happening and all the environment information so there's a lot of setup and there are a lot of information is being there inside this particular uh training of the ml model right so tracking all this relevant information is called experiment track and this relevant information could be in in any form as I discussed source code environment and data analysis Etc now let's come uh to the next Point why is experiment tracking is so important so there are basically three reasons why experiment tracking is important first is reproducibility so reproducibility means uh as a data scientist you want to run as many experiment you want and and you want that particular experiment to be reproducible that you can use it later on also so that's where uh the in the importance of experiment tracking comes into place so you want to choose the best model out of those experiments and then you want to reproduce that particular a model right so that's the most important thing uh why you need experiment tracking and then you also a required experiment tracking while organization while organizing the things out so let's suppose you have a lot of people working in your team and you and each of the people are are doing their own experiments and then you want to organize them right and you track each of their experiments uh simultaneously so that's where the experiment tracking will also be helpful in organizing the things so the next important point is optimization automation help us to track the information of the hyper parameters and the Matrix involved in a particular ml model training so this tracking of the hyper parameters and Matrix is also very important part of the experiment tracking and thus experiment tracking is helpful in this optimization as well so these are the three reasons why uh experiment tracking is important uh there could be more recent out of this but these are the most important ones that that why we need to use experiment tracking then there are various tools that have been developed in recent uh years that is ml flow that is most famous uh tensor board has also been doing something and clear mlq flow uh guild.ai and DBC so these are various tools these are various open source tools that can be used for tracking experiments so in this particular tutorial we are going to take up ML flow as an uh open source tool and we are going to see a the basic introduction of emerald flow like the components of ml flow but in the later videos or in the next videos uh we are going to experiment something with ML flow we are going to prepare some uh codes and we're going to experiment how how it's going to help us to track the particular training of a model so in this particular tutorial we are going to focus on the understanding of ml flow so let's just jump into the documentation of ml flow okay so now here you have the amplifier documentation page so in the in the ml flow documentation page you can see that it it clearly says that it's an open source platform for machine learning cycle so it can help you on logging The Information Tracking the experiments registering the model preparing the model serving the model versioning the models so a lot of things are involved in particular ml life cycle so it help you in in logging all those information and carrying out your ml lifecycle properly so now let's move to the component part of ml flow uh enamel flow you you're gonna see ml flow tracking ml4 projects ml flow model ml uh flow model registry so these are four components that are involved in ml flow so in ml flow tracking you can at your experiments code data and everything the results are metrics in ml fruit projects you generally uh try to pack all the source code the environment and everything uh into a particular project and so that it can be executed by any of your data scientists when you share them so that's a part of ml projects and in ml M flow modulus Is Nothing But You're Gonna Save the model in form of uh inform some format like pickle format or any of the uh expected format that can be deployed in particular server in a batch or in an inference way so that's where the ml flow models uh will help you and in modular history is nothing but where you can track your models or versioning so let's suppose if we have two to three models running one one is in the stitching and one production production so to version that or to track that which model is good and which can be served to the production and which can be in the staging uh section so that's where model register will help you to version your models and and serve in the production or in the staging area so these are the components that have in ml flow and if I open ml through tracking you can see there are a lot of components that are involved in it and there are few Concepts that can be that is being taken care in the ml flow tracking so if you see uh a lot of things can be tracked like code version start and end time of our own parameters metrics artifacts and source so a lot of things get tracks in this ml flow uh tracking if you go into the ml4 projects you can see we can store the all the projects in a form of yml file it has been mentioned here so all the environment files and everything and even your projects get a yml file so that's where ml ml projects will help you to to do the packing of the particular projects where you have to specify the input and outputs of each and every stages that you want to do in a form of yml file and hence any other data scientist can be easily able to uh use your uh projects whatever the model you have developed in a very easy way third thing is ml flow models where you have to uh where you want to save the model in a different format so it can be in pickle format it can be in any other format that particular format is expecting you to uh be in a inferencing stage or in a in a form of production stage so it can be in a pickle file format it can be in a Onyx format or it can be any format like the particular infrastructure is expecting you to uh to serve the model and then ml flow registry uh you're gonna get to uh track the particular model uh and version the particular model right so you're gonna get a UI kind of modular HD you can see here um in the ml flow where you're gonna specify the model name and where you can specify the staging and production area and versioning of a particular model so we're gonna check which model is good for you and which is bad for you for your for your production environment so I think this is all about the particular ml Pro induction and in the next particular video we are going to uh take up some examples or we can we're going to take up some use cases and we're going to use the ml flow as in tracking experiment and then we're going to see this ml flow UI working and we're going to track some of the experiment log some of the experiment informations and then uh we're gonna understand the following perspective of mlflow I hope you enjoyed this particular introduction to experiment tracking and introduction to ml flow in the next in the next tutorial we're gonna Deep dive further thank you"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Management and Versioning",
        "Topic":"ML Experiment Tracking with MLflow",
        "Video Title":"Never lose a model again with MLflow Experiment Tracking",
        "URL":"https:\/\/www.youtube.com\/watch?v=jZk4triALGo",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/jZk4triALGo\/hqdefault.jpg",
        "ID":"jZk4triALGo",
        "Publish Time":"2024-04-24T11:00:42Z",
        "Channel":"Databricks",
        "Channel ID":"UC3q8O3Bh2Le8Rj1-Q-_UUbA",
        "Transcript":"when I was an analyst I used to keep track of my models in so many terrible ways I'd use Excel sheets I'd have random Nines in code I'd even have Post-it notes on desks but there's a much better way and that's mlflow experiment tracking I've been iterating on this Airbnb data set and here are all the models I've created each model is called run and they're grouped together under an experiment here I can compare all of my models various metrics I've captured parameters I'm interested in and any tags I might have added now tables are good and all but what I like is a good visual here are some default charts but we we can add our own too if we want to narrow it down even further we can compare a few in detail all of our Run Names automatically generated are adjective animal number and we can see that stylish crane 103 is our best model for root mean squared error sorry funny Goose when I go to the Run itself I get even more useful information each run has details including a unique ID the massive list of parameters used and each metric the model is evaluated on we can turn these into charts if we want along with the cluster usage metrics but the artifacts have is my favorite it's not just the model we're interested in here we can see the required inputs and outputs for the model which is really useful if we're working with another team to do the inference which they can do with either a spark or Panda data frame with only a few lines environment set up for the new team is a breeze whether you need a yaml or a text requirements file or a pickle or a Jupiter notebook it's all made and saved for you if you're using data bricks with a machine learning runtime all of this logging is done automatically for you but on the off chance you're not install mlflow and set auto logging for the framework you're using if you like data bricks want to see what else is hot data and AI follow for more"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Management and Versioning",
        "Topic":"ML Experiment Tracking with MLflow",
        "Video Title":"MLFlow Tutorial | ML Ops Tutorial",
        "URL":"https:\/\/www.youtube.com\/watch?v=6ngxBkx05Fs",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/6ngxBkx05Fs\/hqdefault.jpg",
        "ID":"6ngxBkx05Fs",
        "Publish Time":"2024-08-08T13:30:07Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"MLF flow is an important tool in the world of mlops or machine learning Ops in this tutorial I'm going to explain you the purpose of ml flow tool then we will install ml flow locally on your computer uh we'll do some experiment tracking and then we'll deploy things to centralized server to Cloud using a platform called dags Hub let's get started with ML flow tool before I talk about the tool itself let's first try to understand what kind of problem does it solve let's say there is a data science team working on anomaly detection problem and Kathy is a talented data scientist she is creating this notebook where she has used logistic regression to train the model and C is using data set V1 V1 means version one of data set okay she might try few other approaches where she trains logistic regression model but with different parameters you see here C value is one here is 0.1 and so on so this is called experimentation as a data scientist we will run different experiments with different parameters and so on you might have seen this uh while going over grid search CV okay because we are trying to get the best performing model after some time let's say she has worked on this for one month and after 1 month business comes and they say hey our data set has changed and we have introduced this new feature which will help you in your model building and now my data set is V2 and it has this additional feature F10 it had F1 to F9 before now on K's computer there are multiple notebooks multiple notebooks have variety of data sets variety of models parameters maybe different uh feature engineering techniques and so on there is another data scientist called wut in this team he's also working on the same problem but the data set that he's using is V10 it is different because it's anomaly detection problem the data set is imbalanced and the team lead has given the task to wut to handle the imbalance in the class and then use that data set for training in my company at Technologies uh we have a client based in North America and we are helping them with anomaly detection process problem so what we did is we had two parallel initiatives in one we are using the time lag moving average those kind of features in the time series to detect the peak in the parallel initiative which is track B we are using convolution features because convolution features can also be used for anomaly detection and now these are kind of two parallel initiatives we are trying to solve the same problem similarly here Kathy and wenat are working on same problem statement but they are using maybe different feature engineering techniques wut is handling class imbalance and Kathy is not and he's using see random forest with different parameters and so on now wut also has uh four different notebooks on his computer and one day the team lead Mr Tony Sharma comes to Kathy and says hey how is it going you know can you show me your model performance and she will open that Notebook on her computer she will scroll through it you know these notebooks are often big she will get some parameters like Precision recall then she will have another set of parameters then she goes through uh 27 other notebooks that she has on her computer similarly Tony goes to wut and wut does the same thing and Tony is like tired it's like that has to be an efficient way of doing folks and anytime I come to you you are opening up your shop okay and you are just going through it and just telling me okay this is my Precision number this is my recall number and when I'm asking you which data set it is you don't have any idea so this team who is very talented comes up with the most Innovative idea and they start putting all this number in an Excel file Excel yes that's what people do so they create this Google sheet or you know like Excel file on one drive where they have started putting all their experiment results so these four experiments are conducted by Kathy these are the dates OKAY model which parameters did you use what is my data set version and what kind of uh performance did I get okay so these are model metrics and these are model parameters while this approach can work it has many problems first of all what if they forget to log the experiment or what if they don't record these numbers accurately maybe Tony looks at this uh sheet and he says you know what it's anomaly detection and I care about recall for a minority class and recall for class one is maximum in model number s let's deploy that model now I don't have any magic button here where I can directly download that model from this Excel file so then wut will go to his computer he will export the model to JB and give it to Tony again not an efficient approach what if we have a tool which shows all this information and also it has all the artifacts so I can click some button here I can download the artifact not only that I can maybe download a data set I can look at all the parameters also I can compare models you know I can say Okay I want to compare these two models five and seven and it has a nice UI and it displays uh the comparison in a beautiful way well that tool exist and it is called ml flow you can see you can see all your experiments here models then you can go to specific model you can see all the parameters see F1 score Precision recall then you can also select all these runs and click this uh compare button to compare different models you can compare different metrics Precision recall and so on and that way you will get good Clarity on your experiment see in the tabular format it will show you your uh model metrics so mlflow tool helps you make your experiment tracking efficient and better it has other benefits too such as producibility and deployment and also Model Management you can export the models to something called Model registry and you can say okay this is V1 model V2 model and then you can deploy that same model to production it makes your ml application uh development easier and ml flow is an important part of this thing called mlops now that you have got some idea of ml flow let's go ahead and install the tool and we're going to uh create a notebook and I will show you more details to install ml flow simply go to your git bash run pip install mlflow after it is installed you can run mlflow UI to launch a local version of mlflow okay so this will run it on your local computer which is Local Host colon 5000 so I'm going to copy this URL and run it here so this is how it looks so there are experiments then you will see some models here this is my previous model so you can ignore it so now that this thing is set up let me give you some understanding of the docs so if you click on docs here you will see a detailed document mlflow guys have done amazing job in creating this documentation so feel free to go through it you can click on getting started and uh let's see so see here you'll see some screenshots and so on when you go to uh getting started guide you will see installation instruction and also another way to launch the mlflow server is by running this command okay you will see a detail documentation so feel free to uh refer to this document in the next section we are going to do some experiment tracking in MLF flow we will now record some experiments in ml flow I have this sample notebook where I have done classification on a synthetic data set and you can see there is some kind of imbalance in this class where zero class has 900 sample one class has 100 samples so I I've trained a basic logistic regression model using uh all these parameters and also printed a classification report when you pass output dict equal to True argument to class ification report you can get that report in a dictionary format and we will use all this metrics to log them into MLF flow the first step is to import mlflow Library once again if you not installed it P install mlflow then mlflow do set experiment and I will call my experiment first experiment okay and then mlflow do set tracking URI so that it knows which URI or URL it can use to track our URL is this okay so I will copy paste this thing here now this is the same URI which we got when we ran mlflow UI okay so we are running ml FL locally and we are using that as our tracking URI now you can say with ML flow. start run and then you can start logging all the parameters the best practice is to train your model and once the model is trained you know you have all these matrics in report variable and then you track them so mlflow do log parameters so you can individually log the parameters or you can log the entire dictionary so our parameters in our logistic regression with this so I'm going to log all of them here then the other thing you can log is log Matrix okay so you are logging parameters Matrix and model these are the three main things that we will log so Matrix is nothing but the output of our model training you can log a dictionary of Matrix or you can log individual Matrix trick as well okay so let's do dictionary and I will say my accuracy okay what is my accuracy so if you look at our report dictionary accuracy is one of the key see it is one of the key so that is my accuracy then I would like to log recall for class 0 and one in anomaly detection recall for class one is kind of important same in fraud detection as well so report dict and in that dictionary you have to look at one okay so one and in that recall so one recall is 0.5 so that's what you will log similarly recall for class zero okay so I will do zero here and then class one here and also F1 score so F1 score uh I will use the macro average so I'll just call it macro so macro average is this basically you can do macro average or weighted average so here report de and then macro average fub1 score if you don't want to use log Matrix method you can individually log these Matrix as well okay they both are same thing so we logged parameters we logged Matrix now we can also log model and the way you can log it is MLF flow. SK learn. log model so it will also have MLF flow. XG for example it doesn't have like direct model log model function uh but that's okay so here the model name is LR and you can give the name here so logistic Recreation it is also called artifact path see or like the first parameter is the model and the artifact uh path okay so I think that's pretty much it control enter to execute it sometimes it gives an error that experiment ID XYZ doesn't exist in that case you can just rerun it okay so now looks like this thing is done I can go to my MLF flow UI and see here you're not seeing any experiment now I will hit refresh and I see my first experiment see it gave a random run name see I did not give any run name here explicitly like right like run name or run ID uh so it generated this name on its own but if you go inside you will see that it has a scal model it has all these parameters see accuracy precision recall everything is logged the parameters are actually here and here are the matric okay parameters that you pass to your logistic regression and here are the Matrix uh that you got after evaluating your model you can see them in a this kind of chart format as well and when you look at your artifacts you have your logistic regation model see this is the artifact so you can copy you can uh download that particular artifact see model. pickle so it exported it as a pickle file if you want to download it it will download that model you can package it up into your Docker container deployed it is giving all your RL files requirement. txt so that you can put all of this in your Docker container and deploy the whole thing as an atomic unit or an artifact to your Cloud environment requirement. text is all the libraries which are needed for this model and cond ml file is similar to requirement. txt it is useful for a cond environment if you're using Anaconda python environment is useful if you're using python virtual environment and so on I have this other notebook where I have run four different experiments okay so same make classification synthetic data set with class imbalance class zero has 900 samples one has 100 samples anomaly detection use case let's say fraud detection whatever and then I have trained these models explicitly the first model is logistic regression you can see it is giving gr numbers on class zero but on class one it gives pretty poor performance then random Forest improves the performance on class one however recall is still low XG boost improves the performance you know overall still recall is 80% and then you handle class imbalance using a SM Tomac technique where you see that the samples are you know new new samples are created using over sampling technique SM Tomac is one of the class imbalance handling techniques and when you train your XG Boost after that the recall improves a little bit but the Precision goes down okay so these are four different experiments what I have done is I have ran these experiments in a for Loop so I created this kind of array with tles same model okay same models that I used before same exit four models and then I am running for loop on that so it would be okay even if I don't write this code but I just want to show you that if you want to run multiple experiments how can you use a for Loop eventually the reports are stored in this reports um array so you will see the report dictionary basically so this is my first experiment second third and so on okay now as usual let's import ml flow and then send set the uh experiment so I will call it uh anomal detection also set tracking URI so your URI once again it is this one and then I will itate through uh this models array okay so we'll say before I uh element in enumerate I hope you are aware about enumerate method so when you do print I see it's printing just 0 1 2 3 4 and if you print element it will print all these elements right this is element number zero element number one and so on now I can also print report right so individual report is taken from this reports array so you can print the reports for each of those uh model elements so uh here I will say my model is the first element so you have a tle in that first element is a model name second element is the actual model okay so the first element is a model name second element is an actual model and your report now you can say start run I will give run name as model name okay so my model name is my run name you'll see the effect of this later on and then ml flow do log perms you could have uh stored these parameters into a you know separate element as well so I'm just logging the model name okay so I can say model name is this and then log um Matrix I will just individually log Matrix you can use dictionary as well and then if uh there is xgb in the model name so if xgb in model name then use mlflow do X boo do log model okay so give model in the else you will use a eSalon so now do control enter it will take some time for this to run but when it comes back you can uh just refresh it and you will see this anomal detection so when you go there there you can see all these different experiments now see when you go to this experiment in the parameters you will see whatever you uh logged I just logged model name you you can actually log the actual parameters okay so we can make that code change I will make that code change in Notebook basically so here you can put your parameters and that way you can see those parameters here you are seeing all the Matrix that you have logged here okay so model Matrix in the chart format system Matrix nothing artifacts is something you can download see model. xgb you can download it here you can also compare all these runs so let me select all and just say compare here I can use recall for class one because that's like my most important metric and recall for class zero and F1 score you can see that for logistic regression see this is a blue line it is .5 recall for class 0 is96 and F1 score is. 74 but for random Forest you can see it it's going up so this way in this uh visual when you compare all different matrics you will get an understanding on which model is best you may care more about one matric over the other so here let's say recall for class one is the most important thing so there for this particular uh model which is uh the smot you are getting the maximum recall for class one although recall for class zero is little little low compared to other models you may pick that model okay you can see all these metrics in a tabular format as well see this is SME this is regular then random Forest logistic regression you have all the times you know if you care about the run time see logistic run Reg ation took the maximum time to train and so on you also have a scatter plot here where once again recall for class one can be compared it is lowest uh you can also put F1 score so on that plot F1 score versus recall in my top quadrant which is uh top right whatever model is there is the best model so here I see xcb classifier and then xgb classifier with smot I will probably go with SM because once again I care about recall for class one in anomaly detection so this kind of visualization helps a ml product manager or a team lead or a data scientist to pick the best performing model you can go to chart tab to see all these metrics in a chart view you can maximize it you can just play with it folks these are all uh very like self inuitive see Evolution traces you can just play with all these tabs these two are new features this Evolution and traces so I don't have anything to show right now but this table is kind of the the main uh important visual you can also set filter see matrix. RMS less than one params model equal to three and that way you get again a good understanding or you can filter your results that's it for experiment tracking in the next section we'll uh explore model registration and general uh Model Management I would like to take a break here and tell you that this tutorial is prepared based on mlops chapter in my machine learning course if you're interested in learning machine learning please refer to this course it covers everything from python Basics to data visualization math and statistics Basics it has a lot of quizzes exercises very simple explanations that even high school student can understand it easily uh supervised learning n learning everything uh two endtoend projects which will give you an experience as if you are working in a real company as a data scientist or ml engineer it covers entire ml Project Life Cycle from data cleaning feature engineering tuning uh building streamlit app and even presenting it back to business so once again this tutorial was taken from this MLF chapter if you like this tutorial and if you want to learn machine learning in depth then please refer to this course back to our tutorial now now we will look into model registry and model registration I have made some changes to the notebook so I included parameters in my models array and in my for Loop now I'm also logging those parameters so see I'm using uh these parameters variable to set parameters on each of the models and then uh I'm logging them into ml flow using this log params function I went to MLF flow and deleted the previous experiments you can click on this delete icon if you see any anomaly detection here click on it delete it uh that way your environment is fresh and now I will come here I will execute all the cells okay so that way it will create a new anomaly detection EXP experiment and log everything fresh into mlflow UI all right it took some time to run this but eventually it came back up so now I see anomal detection four different runs which I had in this particular experiment now when you go here you will see each of these parameters see e metric use label encoder so EV metric use label encoder these are the parameters which we are logging through uh this log params function now as a next step we want to register our model so what happens is people will conduct their experiments okay they will compare the performance and here I care mostly about a recall on class one and I see that a recall on class one is let me just remove all this it is maximum on um XG boost with SM okay so if you look at this see 833 so now I figured that xgb SM is the model that I would like to go ahead and deploy so I need to register that model into my model registry here there is some documentation so let me open it when we were doing a run we could have uh registered the model using log model function itself so when you're using this log model function here itself you can register the model but what I wanted to do is look at all the models in my experiments and pick only one model and register it okay so to do that you can use this separate API okay so let me just copy paste this is the API we are going to use so here we'll say register the model create a new cell and use this API now this API expects the model URI and model name okay so model URI so this is model URI the first argument and then model okay so model URI the format for that is something like you want to provide okay runs column you see runs column what is this this is run ID so if you look at any uh model here let me just show you it will have a run ID it's like a long number which uniquely identifies that particular run so you will say run ID here and then uh model name okay so I will give some model name xgb SM that's my model name it's just a unique way of identifying that model nothing else now how do you get run ID well run ID you can take an as an input so I will just say input enter run ID and when you control enter it it will ask you so you can copy paste whatever you had there you copy paste here okay let me remove this and just hit enter so what this is going to do is register that particular model into our model registry so our model registry previously was empty now you see this xgb SM version one okay so when I come here I see the description you can add a description that okay this model was trained on over sampled data set uh and the classifier is actually whatever description you want to add which can be useful to your team you can also add some useful tags you can add um aliases so usually when you're deploying a new model you want to call it Challenger because the one which you have in production is called champion and this one is Challenger okay so I will add Challenger as an alas this is just a name folks you can add anything but teams will have some common understanding that champion means something that is deployed in production Challenger is uh challenging the position number one you know so uh it's like uh your next candidate which can be better than Champion so I'm just giving alas uh Challenger here and when I see it here see I get this if you run this again by the way let me just show you um it will just increase the version so I will so when I do this what's going to happen is it will have version two see so it it has version one and version two and Challenger is your version one okay so when you have two different versions you can like click here and copy them as well and look at the Matrix but we are not going to do all of that so uh version one was Challenger and now what I want to do is I want to make a version two Challenger so you can add that alas here uh adding it to right so then Challenger alas will move it's like my latest Challenger now uh let me let me do one thing let me just delete this whole thing here and run this thing one once again you got an idea on version corre I mean that was my intention okay now I have version one now let me load that model how about I want to load my Challenger model and I want to do some testing in my notebook so how do you do that so the way you can do that is mlflow doxo we know it is EXO model if it was Escalon you would have said Escalon and simply load the model so model URI so here model version is one model URI is models um\/ model name model version and I will say loaded model this this and loaded model. predict on XT what you get is y predicted and then you can print few elements so when you run it it will go to the model registry download that model and then perform the prediction I was facing some issues and uh the root C was a bit weird so I debugged offline and what I found was uh if I give model name here you know previously I was giving model name that was creating an issue so I'm now giv giving a fixed string model so when you are running this code make sure your model Ur looks exactly like this I have given this notebook for the download so you can download and look at it and then when I load the model see it uh loaded that artifact and it also predicted and see these are the values on my y prediction so this is how you can take a model you can load it and you can run it for the prediction you can also use this format where you can say Challenger right model SL model name uh at Challenger so at Challenger so if you remember we give this Challenger leas so what we are saying is give me xcb SM model with Challenger as an alas this tag this alas will be present only on one of the versions so this way we don't have to specify or remember the version number okay so when you do that that will also load the model and perform the prediction see so this way this version number is not required now how do you transition from Dev environment to production well for that you can use use mlflow client so you can uh do this mlflow do ml ml flow client and that in client you can say copy model version where you give your Source U whatever that is and then destination name now my Source URI is this people will usually use this tag Challenger instead of fix version number so this is my Dave model and my prod model right so Dave model Ur right and then I have my prodad model so my prod model will be anomali uh data maybe you can give any name actually so Brad model is this my D model Ur is this and hit control enter all right let's go to our UI now and see now I got this production version of the model you can give this an alas such as Champion all right so this is my Champion the other one is a challenger this is a truly prod model now you can download this uh champion model package it up into Docker and deploy that Docker container uh we have ways to do this in data breaks AWS Etc so if you go through the documentation of uh ml flow you will see like deployment strategy for uh data bricks for example so in data breaks you will uh set up this kind of registry and you know you can have this kind of URI so when you deploy an organized model see deploy uh you can also delete it there is API available for pretty much everything there is a way to deploy this in AWS there are many cloud providers who support MLF flow and what people will do is uh they will have all these versioning Etc like they will have this kind of apis and they can put this into their Jenkins pipeline so you have Jenkins file jenin is a cicd tool continuous integration continuous development tool and you can automate all these ml workflows using things like jenin now let me down download production model and do the same testing okay so here I will copy paste this and my model name is actually prod model correct so um here I have broad models and it is Champion instead of Challenger it is Champion I don't need a version hit control enter and it will see download that artifact and and it uh performed the prediction so it went to model registry it located this champion model download it locally and perform the prediction using predict function folks I really love ml flows documentation it is so detailed so you can go through it if you want to study it further you can as far as this course is concerned I just want to give you an overview uh if I start going over all the details it will become probably 20 hour long tutorial okay so you understand now I think you should have a good understanding of how to do uh experiment tracking in mlflow and also how to do uh Model Management using this model registry so far we have run MLF flow server on a local computer but what if there are two or three data scientists working in a same team and they want to uh publish their experiment results to some centralized cloud-based server dags Hub is a tool or a platform that will help us do that so we will use this okay first thing we need to do is create a GitHub repository for our notebook so I will create a GitHub repository here uh quickly for G ignore I generally use Python okay my repositor is created I'm going to clone it so copy this and go to your command prompt type in git clone that now I will go and copy the mlflow Model Management notebook that we uh created in the last section okay it's the same notebook I will go and copy that here here mlflow dexop demo if you do git status okay you need to go inside that and then get status see that file is showing up here so now I can do G add to add that file to my you know working kind of this is my working copy I will now do get commit m flow dehub notebook and then get push so it will push it to your main branch see here the main branch so if you refresh it see I have this notebook right here now I can go to dsub and create a free account I will use my Google account you can use your GitHub account too I'm not a robot obviously okay I don't know what happened there was some issue but anyways uh looks like like we created the account correctly so we can say personal you know whatever you just give some organization name uh just me and uh tabular and Text data whatever okay just basic details dehub can be used to version your code and data just like GitHub so Dex Hub sounds like a copy of GitHub it can allow you to do Version Control additionally it can allow you to do data control see in Version Control we have a control of the code right like versioning of the code but what about our data set I have a data set which is also going through updates so data set version one version two dags Hub allows you to do all of that it has additional features such as annotating data generating training data set and track experiments okay so track experiment is something we we are interested in so we can say track experiment next and if you look at this UI folks see this looks like again this looks like GitHub UIC see you have pull request you have all these repositories and so on now I will say create new repository so just like GitHub you can create a new repository and check in your code here I did not do it because you all are familiar with GitHub so we will connect with GitHub by doing this and it will ask you for GitHub credential I already gave it before so it did not ask me but you will just follow those obvious steps okay don't ask me okay how to do it if you are becoming data scientist you need to figure things out on your own that's a very important skill so you are at this stage and then you can search for your ml flow dhub repository this repository and you will connect okay so now my repository is connected once again see this UI looks just like a copy of GitHub okay so if you don't want to go to GitHub and create a blank repository and check in your code here you can do that it's the same like git clone git checkin same thing instead of GitHub you have dsub like a different website we connected with GitHub pull the code here so now here in dags Hub we have a copy of GitHub repository see this is my Repository it is connected to this particular GitHub repository now on the steps are very easy you can just go to remote and click on experiment and just check this code okay so this is the code we will need so I will now go and look at my notebook so my jupyter notebook is open I in C code directory I have to go where mlflow dsub demo correct and I will open that code okay I will control enter and run um first few cells so that okay smt is not defined okay what is the problem okay R this will be r s and you will run all these code you can run this also and now we will create a cell for dags hub setup and how do you do dexop setup well copy these two lines okay so I will copy this you have to import Dax Hub module and then call in it you are specifying your owner and repository name you can do pip install dsub to install it okay so from command prompt run pip install dsub to install it once it is installed control enter and now you have connected there I will commend this line because this was publishing to local repository now with this code we will be publishing to our centralized dags Hub server so control enter I think I'll have to copy this uh urri as well so I will just copy this particular URI and copy paste here okay so this line is commented now it should be publishing my metrics to this particular URL control enter when I turn this code initially I was getting this error API request to endpoint fail with whatever error that was happening because my permissioning was not set right it is possible that when you run the code for the first time it will just work for you but in case if you face this kind of error what you need to do is this see first of all these two things are not needed okay so you need to set some environment variables previously I was connected with a different dsub account therefore I got this error as I said when you do this it will kind of take care of doing this but because in my case I was connected to a different account and maybe those old values were cached in memory it was not able to connect so if you're facing this error just add these three lines okay your username is this your password is basically something you can get from here so public key right so you can just view it click on viewing and then copy like copy that is your password and your tracking URI is nothing but whatever you see here so if you just click this copy paste this so you're saying this is my username this is my password this is my tracking URI now whatever I publish it will go to this URI so I rent this cell it took some time and eventually it published the metrix so I can click go to nflow UI and look at this folks I'm seeing my experiment here this is not a local server this is running on the cloud C db.com learnpython language and this is exactly same thing that you noticed previously when you were running MLF flow locally you can uh compare experiments once the experiments look good you can uh register the model by going to models and registering it you can do it through code see this particular code that we have here you can do it through code or you can do it manually as well you can say create a model this is my XG boost and no Mal detection and then once the model is created you can go to your experiments and you know register that model under this so now when I go here see I see this as a version one but let's say later on you find that my random Forest is better so again you can register a model under same and when you do that it will displayed as a version two I mean I should not call it XT boost I should just call it anomaly detection Brad candidate or Dev candidate Etc and from here uh you can deploy the model just to summarize what we did uh in this particular section is we set up DX Hub repository we published our metrics from our notebook to this dhub centralized ml flow instance and now if I have a data scientist friend working in the same team they will also use the same kind of URI right here this will be team specific credentials and when they publish their metrix those metrics will go to this centralized server you can do the same thing in data breakes in AWS u in variety of ways I what I have shown you is one of the ways and daub is free so from your learning perspective it kind of helps if you do these things on Dex sub hope you enjoyed the tutorial the code is available in the video description below uh if you like this video please uh share it with your friends who are also learning mlops [Music]"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Management and Versioning",
        "Topic":"Model Registry and Versioning",
        "Video Title":"MLFlow Tutorial Part-4 : Model Versioning and Model Registry using MLFlow | MLFlow | Karndeep Singh",
        "URL":"https:\/\/www.youtube.com\/watch?v=iIiPo4qv97o",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/iIiPo4qv97o\/hqdefault.jpg",
        "ID":"iIiPo4qv97o",
        "Publish Time":"2023-01-16T07:41:01Z",
        "Channel":"Karndeep Singh",
        "Channel ID":"UCswr5O1wBDdyULYqIfgtY4Q",
        "Transcript":"hello all and welcome to my YouTube channel so today in this particular video we are going to talk about Tamil flow model registry so this particular tutorial is in continuation video of my previous ml flow tutorials so if you're going to my channel and if you type mlflow tutorial playlist you want to get all these three previous tutorials on ML flow so I have I have discussed lot of things of about ml flow in detail like what are the important terminologies involved in ml flow and while tracking the experiments and also how you can uh use ml flow in your own uh ml ml pipelines and then how you can hypertune your parameters of your ml model and then how you uh store it in your mlflow and experiment with those hyper parameters in your ml flow interface so all these things has been discussed in in my previous videos you can just go into the channel and and just go and follow up this particular ml4 tutorial playlist you want to get all those previous videos and then understand the the ml flow in in much detail so this particular tutorial is in a continuation video for my all those previous videos so ml flow a model registry is a part of ml flow it help us to take up that trained best model and register it as a production or a staging or any kind of archive mode so actually um there are three things that we need to consider while uh taking up a model right so let's suppose we have we have we have followed all of the steps that we were looking for training up model so we have got it best to train model available on our ml flow interface and then from there we are going to choose our best model based on the size Orbison the parameters always on the Matrix so if so we're going to select a best model out of those experimented models right and then once we choose those particular models we are going to register those models as a staging model or production model or archive model so to so to do that ml flow modern registry will help us to tag those kind of model whether to give that a particular best model in a staging or in a production or you can archive is also right so this is the way that you can productionize or or you can say the you can you can make the process very clear and understandable for the deployment Engineers uh before deploying the model like which are the models they should consider uh before deploying it whether it should be staging model or production level model right so this is what the importance of the ml pro model resistance comes into a play so it is not going to help you deploy a particular model but it's gonna help you to tag a particular best model for certain for certain staging area or production areas so once you have this particular staging or production models tagged then you're gonna then deployment Engineers or ml Engineers can take up these particular models and start deploying it based on the tagging that you have provided so this is the flow that uh one should follow uh before proceeding for deployment all right so this is how the ml flow actually help us uh in in particular tracking and setting up the or registering of the particular model for the deployment and then from there once once you have tagged the particular model as a production level model or switching model then Engineers or or deployment agents can take up those kind of models from here and then start proceeding for the deployment so this is the way that you can take up and it's a very uh crucial uh steps that are important when you are trying to develop an ml pipeline right so now let's let's look into these things um in ml flow UI like how we gonna do it so in my previous videos uh we have trained a lot of models and we have tuned a lot of models by using hyper parameter tuning right so these are the models that are available and then uh based on some criteria I am going to select some best models out of this particular print model so let me just uh take rmsc as a metric and whichever the metric of will be lowest so we're going to take up that and and choose a criteria to select a model right so let's suppose I'm going to take up rmsc as in metric right for for uh for choosing up my model criteria like which are the best models available right so based on the rmse I am going to take up rmsc with the lowest error uh and take up those models which has lowest rmsc so I am going to take up those models so let's suppose this is a model and it has 0.67 to rmse and Then followed by 0.73 and then 0.75 right so this is the model I'm going to take up and once I've selected these models maybe you can open this particular model and understand the different parameters and artifacts that are that are there in this particular right and then you can take up these models and then you can start checking uh the different categories that you can take up right so I can also see the size the duration it took uh for for for training of the model that could also be parameter for selecting a particular model like how many times like how much time does a model takes to train and then you can also check up on the uh size of the models uh like what is the size of a particular model before you consider it for deploying you can also check the parameters Matrix and then also uh different criterias on which you can take up but or you can consider a particular model for your staging or productionizing right so based out of those criterias uh you can select some models so um I'm selecting this model uh right so I'm going to select this model for my staging so once you select this particular model once you click on this particular model you're gonna see uh you want to click on this particular artifact and then model you want to see a register model options available so once you click on this register model you want to provide a new model so you have to provide a name so let's suppose I am just testing it on uh wine quality data set right so I'm going to provide a name called wine quality and then it's going to register the model with the name of wine quality so once I click this model register so if you go on to this model section right now we were in experiment sections now we want to go into the registry model section right so if you click on this models you want to see that there has been a model that is registered with as a version one so this is the model that it that you took from the experimentation area and then you then you realize the that that this particular model is best for the deployment or staging it so you you just select those model and move that into the registry Model area right and then from there you can see that this is the version one model and once you click on this you want to see uh the details of it uh what are the inputs and what are the outputs you're gonna see and then you can also see some some tags you can also use some tags name this particular thing and you can also use the description tab to see the information and if you click on this run you want to see the details of all the details of particular model right so this is the information that you can see over here and then uh let's suppose you have this version one model and now let's suppose I'm going to take up another model uh for my for my for my analysis so just let me pick up this particular model and then register it again so I already have created a particular model with the wine quality so I'm going to select this select the same model with the same name and then I will register it so if you go into the model section you're gonna see there are two models available right not showing version two uh yeah you can see that there are two models are available that is version one and version two so version one was the recent uh the the previous model that we selected and the version 2 is the second model second best model you can see that I have selected and then you can just uh use your version one and then you start tagging it to a production area or changing area so what you what you're going to do is you just open this particular version one and you're gonna tag it to Let's suppose staging area so I'm going to put I'm going to put this particular model into a selecting area and then that's how you're going to tag it and then and then I'm going to use version 2 model for my productionizing area so I'm going to put this particular model for production so what is going to happen is if you're going to this model section you want to see that there are two models available which is version one and version two so version one is in a staging area and version 2 is in production area so what will happen now from here is uh you're gonna expose this particular API uh to the to the ml Engineers or to the deployment engineers and they're going to look up but look up on this particular interface and they're going to understand okay this is the uh production model that you have trained and tagged it as a production model and second best version model is a staging model so what what MLG is going to do is they're going to take up this version two models from the production that you have tagged and then they're going to take up this model and they're going to put it up for the deployment from here so this particular ml flow moderate registry doesn't uh deploy any kind of model it's just a part of or tagging a particular model whether it should be in a staging area or it should be a production ready model so that's how it will help you uh to productionize the things in a proper way and ml Engineers will get in more understanding once they click on this version 2 and they start writing this particular interface so this is how they're going to understand the uh the flow and you know take up this particular best model for the production ready right so this is how uh ml4 HD works and you can do the same things whatever I did just now you can do the same thing with CLI commands so let me just do that thing uh if so if you are familiar with ML flow there's also a ml flow client uh API that is available in ml flow so what you can do is instead of going through the ml flow UI you can also interact the same things whatever you're going to do here in the UI you're going to you can also interact everything using the memory flow client so this is how you're gonna do it so from uh so from ml flow you can just import ml flow client and then you can mention the ml flow tracking UI and then you can start using it for the interaction with your ml flow database right whatever you have created so now let's suppose I want to interact with the ml flow UI so what I'm going to do is I'm going to just try to list out the experiments what are available in my ml fluoride so with this ml flow client you want to provide the ml flow tracking UI which I have provided above and then you want to use the list experiment method to list down the experiments that are available so you can see I have two experiment available which is 0 and 1 so 0 means default so if you're going to ml flow UI you can see this is a zero one and one quality is one so you can see there are two experiment which is ml run slash one and ml ml run slash zero so this is one is default one and this one is a our custom wine quality experiment so this is the one that I am talking about you can see experiment right is one right so this is the these are the experiment that are available in our ml flow and then uh you can just interact with the client like I just want to use a client dot search run which will exit so what it will do is it gonna take up the inputs so let's suppose I'm going to use uh metric dot rmsc um greater than 0.7 that means or whatever the models with with root mean Square greater than 0.7 I am going to just pick up those uh particular models and then I'm going to use an order by two uh to arrange this particular ordering in a descending order right so this is what you can do and the same thing you want to do it over here in the ml flow UI if you type all this information here in the search section you want to get the same results right so this is how you can also do it here in the client so if I run this particular cell and if I print out the information you want to see I'm I'm going to get all these run IDs here right even I can also do it here so if you see uh I'm going to use rmsc with the sending order uh if I type metrics Dot rmsc or greater than 0.7 so I may get the results that I am looking for so if you see these are the result that I got and then if you compare it I I'm going to see I'm getting the same results right so this is how you're gonna get and interact with a particular ml flow with a particular Amazon client API right and now once you have this uh models with with the with the exact rmsc that you're looking forward now you can take up those run IDs from here and then you can start interacting with ML model registry so uh you're going to use this uh uh run ID and and you're gonna mention this in a form of URI so you're going to mention the location where the URI is setting so this is uh this is the particular model you are able to take up and then ml flow has register model method which which gonna take up model URI and then the name of the uh model right like how we created the model over there in the UI the same way we are going to create it here by using the ml flow client so this is how you're gonna take up the Run ID filtered from here and then you're going to put it up in a model URI and then you're gonna register the model uh using the ml flow client right so this is how you're going to track it so you're gonna see uh I just took this particular first run ID and I've registered it in the wine quality uh model right so you're gonna see the the version three has been created right now we had only version one and version two model available over there which was already in a staging and a production ready right but here you can see I have registered a new model using client ml flow client and it has registered it as a version 3. so if I go into ml flow I'm gonna go into models you want to check now we have three versions of models available you can see version one version two version three so version three is just created by this particular client right now right so this is how you can register the model by using this client and now what you can do is you can just check up all those things like what kind of versions of models are available on that and for you guys so just this particular cell will help you to do so so you can see version what is model is in a staging area version two model is in a production area and then version 3 Model is is has not been decided like which area it should be going through right so we have to decide it here try to we have to transition that particular model that we have just did it here right you have transitioned that particular model into a particular uh region so that it can be helpful for ML genius to take up this model and start uh preparing the deployment part right so now what we're going to do is we're going to decide it to the production area for this particular version 3 Model right so uh we're gonna take up this uh version 3 Model so I'm going to specify version three and then I'm going to specify new stage so so it's gonna take up uh this particular version 3 model from none to production area right so so to do this uh the client has ml flow client has a method called transition model version stage so what you want to do is it gonna take up this particular ml model name which is wine quality uh if you see it here and then you're gonna take up the version which more which version of this particular model you're gonna take up to a different version staging area right so this is where you're going to provide provide the version three and then you're gonna take up a new stage so what is the new stage you're going to transition into so I'm going to transition this particular model into production stage and then this particular argument help you to uh decide on which like what what what you have to do it on existing versions so let's suppose I have this version 2 model already running on production now I am I am transitioning version 3 to the production then what what I have to do with this version too so that that's how this particular argument will help you to Archive the version 2 model into the archive section and it will put version 3 into production section so if I just make it true so let me just run this particular cell and you're gonna see that uh there are different versions that are that have gone into uh different uh production area so let me just run this particular cell again just copy this and then paste it here and then just run this so you want to see that uh there's a change right so you're gonna see that version one is still is in a stating area a version three gone into production from none and then version 2 got archived so this this particular argument helped you to to switch that particular version 2 to Archive and move that version 3 into the production ready so that's how you're gonna decide it uh like how you can move it uh from staging to the production and project production to the archive region right so that's how you can tag a particular best models and then once you have those models right available tagged then what is the uh what is the use case for the particular ml Engineers would be to take up this production ready model and start using that production ready model to start tracking and start analyzing the things out and start getting the prediction out of this production model so let's let's stimulate the same um environment like how ml Engineers would be taking up these products production model right so for this what we can do is uh let's suppose M Engineers got this version 3 as a production model tagged right so a production Engineers will take up the data and they they want to get the ready data like train test speed so this is what they're going to do it and then uh they're gonna have a ml Pro function ml flow function uh python function and load module function from the client and then they're gonna specify the name of the model and the stage of a model so and and also they're going to take up this uh X text for prediction and they're going to get the score of this particular model so this particular function will will actually help you to load up a model from a particular production area reason right and it's going to help you to predict the results by using this particular model so this is how it's going to help you to do so so this this particular uh python function is available in ml flow registry also so if you go into this particular version and you're going to explore this you're gonna see there's a documentation provided so you can go and copy this particular function and it is available in the ml flow so you can just take up this and load up a model and then you're gonna use the prediction function novel prediction function operate on your test set and you want to get the result out of it right so this is how you can stimulate the environment of a ml flow or an an mlgener so this is how you're gonna stimulate the environment of a ml engineer like how he he would be doing how he would be interacting with the production uh tagged model right so he gonna load up this particular data and then you want to prepare the particular uh function ml flow function and then from here you're gonna just have to pass the model name so model name is wild wine quality right and then which model you're gonna he gonna take up so it's a production model right and and the text he a text text data set and test uh real values right so you're gonna take up and just test the rmsc code you can also take up the staging model so let me just change this as a staging model right so it's going to take up this switching model uh which is version one model and you can also take this archive model for for the functioning so that's how an ml genius can test out all these models once you've tagged it in a particular model registry so this is the whole flow that an ml flow can help you to provide and you can you can develop all these things in in an integration with the cicd pipeline that all so all these things Works in a flow of our ml flow and then it will help you to organize the things out in a in an ml pipeline so that's how uh the ml flow uh the implementation looked like and I hope this will be helpful for your iteration of ml pipelines and I hope you enjoyed this particular video thank you"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Management and Versioning",
        "Topic":"Model Registry and Versioning",
        "Video Title":"Simplifying MLOps with Model Registry",
        "URL":"https:\/\/www.youtube.com\/watch?v=l2YGTiA-yCU",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/l2YGTiA-yCU\/hqdefault.jpg",
        "ID":"l2YGTiA-yCU",
        "Publish Time":"2021-03-04T20:09:03Z",
        "Channel":"Verta AI",
        "Channel ID":"UClJKA9nhsFNpxK5ACRKjzbA",
        "Transcript":"hi everyone thanks for joining us today the topic for today's webinar is simplifying ml ops with model registry before diving into the presentation here is a quick intro about me i am heading product at verta and what whatever what we have built is a platform that essentially helps solve the last mile challenges for machine learning what we do is we enable data scientists do better experiment tracking versioning of models and eventually deploying and monitoring ml models at scale and today i'm really excited to talk to you all about model registry and how it can simplify taking ml to production but more importantly i'm really curious to talk more about how this process can empower data scientists and give them more control in the process and throughout the presentation we'll answer some of these questions first of all what is a model registry and how is it relevant in context to mlaps why do we need a registry primarily from a lens of how it can help a data scientist next we'll talk about what functionalities and features you should look out for in a registry whether you are buying or building one we'll also see a demo of virta model registry and finally we'll recap followed by q a now this is one interesting statistics in next three to four years 75 percent of the companies are planning to graduate from pilot to operations for their ml projects that's really exciting right and of course then businesses are seeing a lot of value and they're really gearing towards moving forward however here's a reality check we still have a lot of challenges when it comes to turning posis to products that are truly production ready and and that's the worrisome part and i don't want to preach to the choir here i know that you all go through such pain points every day in your ml initiatives in fact we have come a long way uh with the really advanced ml algorithms we have very mature training or workflows but unfortunately when it comes to productionizing them the state of the art looks something like this again there are too many ad hoc processes we use spreadsheets we use tools that do not simply work and also we end up using tools that are completely misfit for data scientists or our ml workflows that's the reason why now you're seeing a new breed of tools that are focused on mlaps that helps deliver ml much faster and as we see the entire mloffs lifecycle i kind of put them into these two buckets uh there is ml and then there is ops when it comes to ml we are talking about how you plan your projects create test and package them and for ops it's all about how do we release operate them scale them and monitor and between the ml and ops processes model register really fits in the middle and i would say it's a linchpin to make mlaps work all right so model registry is key to mlrs but what is model registry now the model registry is a system that allows data scientists to publish their production ready models so once you are ready with your experimentation phase and you are ready to share you are using model registry to collaborate with other teams and stakeholders and if you step back and think about how we work as data scientists we are starting with experimentation until we decide on a model to deploy and register really fits between the experimentation and the deployment phase where you're staging your model that's ready for the release now let's draw some inspiration from the software universe right how do software releases work there we have solutions like docker hub artifactory pi pi and many more taking docker hub as an example with the container registry that dockerhub provides developers can they can store and distribute images they can tag different versions so the releases are version control and everything is happening from a single hub so they go and build and publish one image that's how that's happening once and it anybody can download that image and it runs seamlessly and it also helps them collaborate and work with the other stakeholders be it qa be tops and other peers and in the end model registry but how we see it it really takes a very similar approach for machine learning now let's talk about why model registry is so important now you have spent so much cycle in building an algorithm let's say that works amazingly well and you believe it has huge potential to essentially impact your business outcome maybe it's uh going to help increase um the conversion rate in your website or improve the product recommendation engine and at the end you want to quickly release it and finally keep it rating on it what frustrates you the most rolling out ml to production is painfully slow we have spoken to teams who have taken somewhere between one quarter one year to sometimes two years to release one model and that kind of timeline could potentially in the past would have worked with hardware but that's not going to cut it for machine learning and again i want to bring another interesting point here the job attraction rate for data scientists is very high in the industry and again there are countless articles written on it and again i wonder if these broken processes are another contributing factor to it but overall like having a slow production rollout process and things not working out hurts both both the data scientists as well as the businesses now what slows down ml rollouts you are running a bunch of experiments you are trying on different algorithms tuning few hyper params and typically you are not comfortable sharing the tens and thousands of iterations that you're working on with the entire team but rather you go for sharing the best fit model this ad hoc process sometimes makes it harder to identify which models are production ready and even if you know what model is ready and you want to hand it over the handle process is sometimes broken when you are handing it off off to your engineering or your ml infra team the handle process is generally not elegant and this is where things break down what we lack is better transparency and a way to collaborate with other team members ultimately the goals of both the teams are the same right we want to move forward and release the model but the approaches that we take and the process is not that streamlined which kind of creates a more barriers but instead what if we have one central repo where you are staging all your release ready models so your deploy meant counter parts can just go and take the model download it from that repo and you keep it reading and adding of the future versions to the steam same repo that kind of would streamline the entire release and handoff process and that's where model registry can add a lot of value another aspect of ml is how finicky ml models are which we do not see with software again with the slightest change even a library version or an or one environment variable a model will produce very different results and in reality what we have seen development qa production environments could be very different and you don't want to create unexpected last minute issues when you're handing off your model to another stakeholder who is will end up working in a very different environment and again we have heard horror stories from teams who have spent weeks figuring out why a particular model is giving a different prediction on development versus production and all of those could boil down to a small change in the version of one of the libraries or it could be that the training was done on cp gpu versus cpu and nobody was bothered to know document that now model registry um kind of comes very handy in this process to help eliminate these kind of inconsistencies and undesirable delay because what with the model registry what you can do is you can ensure that anytime you are making a model ready for release and logging into registry you are also logging the key model ingredients where that the stakeholders can access when i talk about ingredients the key things that i am talking about is data any configurations which could be your hyper params the environment variables which are the kind of the underlying libraries used including the version number of each library your model code as well as the documentation so your stakeholders do not end up making any assumptions so you have everything in one place next we'll talk about lack of governance and security the this issue is present in many industries but i'm just picking on healthcare for now we have heard stories about how united health used an algorithm that was biased towards some certain race so they the what that algorithm was essentially doing was prioritizing uh the patient care the amount of investment that hospitals need to make based on the level of illness that the patient has and when it was audited it was found that black patients are assigned the same level of risk even if they are sicker than the white patients and interestingly enough the algorithm was predicting health care costs rather than the actual illness so this was completely biased and again this is just one example of the extreme lack of you know governance that we have when we are launching ml products and it ultimately bites us back because we kind of that slows down adoption and then we go back to the white board to think okay what went wrong what should we do it right and again another interesting take on this is how ml is used for pandemic responsiveness and ml is showing a lot of potential right to be used in things like contact racing syntax tracking but again there's a lot of questions around um how can we ensure that ml is used correctly without really impacting any human rights and these questions are being asked by legal team and other stakeholders now the owners comes backward data science teams and the impact is the amount of risks we are putting on businesses brands and human rights until it's done right ml governance is um that's that's imposed is today is very ad hoc i'll give you some simple examples for example uh you have to worry about uh for example you're using so many libraries right we'll end up using many libraries how do you easily track and maintain a list of all the underlying ml libraries maybe it's numpy sci-fi you'll have so many libraries is there a place to track everything and also manage the specific version that you are using next you have to think about vulnerabilities is there a way for you to track these libraries their versions and also scan for vulnerabilities and this is mostly missing in the release process again most of the ml applications whether you want or not will end up dealing with pii or phis personal and health information and you need to ensure that you are anonymizing the data or the data is handled appropriately based on the rule set by your company now third is how do you review and ensure that um the open source product libraries that you're using any libraries you are not crossing any license constraints or barriers these are just few examples but the this could be much the list is much bigger than this but all these questions for all these scenarios um there is no formal process everything is very ad hoc that's why things fall through the crack and create such kind of nightmare that we saw in the other slide now with a model registry you can manage the life cycle of a of a model from development staging to production to end of life you can also implement a formal model governance and approval process you may have your legal team your business team your technical stakeholders may be your security and they need to review and approve prior to deployment right now that unless you have a proper approval process it's very difficult to put all these workflows in place and but at the end of the day it's your company-wide model inventory that lists all your ml models all their associated data their usage how dependent they are on each other they are assigned risk levels so everyone is on the same page with the visibility and collaboration you are seeing with ml projects are somewhat broken and this is a code from head of data science in a u.s bank we have half a dozen teams using half a dozen different ways of building models as many places to run them we have no central place to get visibility at an org level bottom line data scientists are siloed you don't want to reinvent the wheel right you you'd rather much prefer to take the work of another data scientist and um just build on it work on something novel versus um working on the same thing or same problem that has already been solved right how often do you have that luxury because the organizations are so siloed even if there is a solution there is no way for you to find that what you need is one place where you can find all your models no matter no matter where it is staging production or end of life anyone can discover anyone can iterate and collaborate on it so you are creating value versus creating silos and model registry in essence provides that kind of capability uh you can think of it as a like a data warehouse for all your models and the artifacts all right so model registry helps with a faster rollout of production models b improve governance and security that gives you peace of mind and see it creates visibility and collaboration now let's switch to features and capabilities let's say if you are in lookout for a model registry if you are building or buying one what features and functionalities should you consider i'm looking at our lens of again how we will proceed through different ml stages you are starting with an experimentation so it's really important to make sure that the model registry integrates and works well with your experiment management system so if you're running multiple experiments and you're pushing one experiment to your staging and release process all the associated information should flow into your release pipeline i'll quickly go through a demo of vertex platform to showcase how that works so we'll switch back and forth between this presentation and the demo to talk about features and functions um i'm starting with um notebook and this this kind of gives you an idea of um how the experiment management works with inverter and we are using a python library which is simply a library that that works with any of your model training environment and you'll start with the model development phase so you are instantiating a client and you can have your data in aws or somewhere else but you are essentially preparing your data and then configuring different hyper parameters and training your model and this is how you'll run multiple experiments with different hyper patterns information that you will end up logging could be your different attributes your hyper parents the accuracy and different metrics the deployment artifacts as well as your training data and for example your code or git information so there is a bunch of ways that you can track different components and artifacts and information about your model as part of your experiment run i am not running this experiment in the interest of time but i've already run this so if we go back this is a view of our web app where you can track all your experiments and review them so the thing that you are seeing here is a development phase where you can go and look at a specific project and in this project we have logged multiple experiment runs so in this case you have logged maybe 20 or 30 different experiment runs and in real world in ideal scenario you'll be running hundreds of thousands of them and you are looking at different metrics and hyper parents to to compare different experiment runs you may go to our charts and see how these runs are performing and compare to different accuracy or you can compare and contrast with different hyper param groups but at the end of the day you decide that this is the best run this is my best configuration and i want to go ahead and release this to production and in this case i'm just sorting by accuracy so i want to go ahead and register the run that has given me the highest accuracy and if you're a ui person you can just go ahead and register through our ui but instead what i'll do is i'll show you how to do it in the notebook so in the notebook once i'm logging all the experiment runs i will go ahead and retrieve the retrieve the best run and in this case i'm looking at the metrics value accuracy and ordering it in terms of descending order so that i can get the um run with the highest accuracy so let's run this and see that's giving me the validation accuracy of 0.7945 which is the best model from all the yards which is same as what we are seeing here in the top of the list and i will then take the best run model and train it with a full data set and finally i'll calculate the final accuracy and that comes out as 0.7904 which is fine with me and this is when i move from development as you can see in our web app from the development phase to registry which is your staging phase when i know that this is the model that i want to release and i want to get it in the hands of other stakeholders and what i'll end up doing is i will go ahead and register this particular model version you can either create a new model and in this case we already have the model sensors demo as a registered model and we already have a previous version that is already logged so i am going ahead and creating a next iteration so let's go ahead and register this particular model version so as you can see i got that there is an existing registered model census demo and it has created a new model version which is v2 let's go ahead and again this is our model registry and you have the census demo model register here and you can see that there was an already a v1 registered model which is in archive state and now i just moved a new registry to development which is version v2 and if you go to the specific model you will see that this the stages development and it is associated with this particular project from my development cycle and this particular experiment run and in addition to all this information you'll also have information about the attributes uh the package as well as different artifacts that is being used in the model for example you can go to the level of what library is used what versions have been used so everything is tracked here as part of the registry so your stakeholders can easily access all the information at one place and again speaking of the feature that we just discussed about having seamless integration with your experiment management system if you go here and click on the experiment run you are actually going back to the experiment run that you've run during your development phase so this is the run id the timestamp all the hyper params use metrics attributes the artifacts code versions what was the git code that was used data sets what training data sets and validation data sets that you have used as well as it gives you back a link to the model registry where you have locked this model you can go on back and forth but both the systems are talking to each other so you have all the information flowing automatically next let's talk about the other features so once um you move from experimentation staging you need to worry about features like sharing and collaboration so you need to make sure that the teams and your stakeholders can easily you know access your registered model as well as you know edit and update on it so that's the reason you have you need to have a system where you kind of have the full control on the type of access that you need to give to your users so in this case like um you can go and decide the different permission levels by default for example everyone in the or would have read write access or you may have very custom access level where you can say that everyone has a read-only access and i will invite a specific user in this case jane alice and give her read write permission so i'll have specific groups of teams who have more elevated privilege so that's all about sharing and collaboration and then again that links back to security and access control because you are kind of managing who can do what with your registry and finally ability to configure governance and approval workflows and we have discussed that already that you need to have more controls in place of um who can approve your model and um and then you can move the model through the life cycle so i'll show you how that can be done um for example let's say in this model of v2 we can move this model from development to let's say i request to move this to staging please approve and i can request approval so immediately you know that the state change request has been made and i would see that yes it's in pending approval and someone needs to approve this and anyone who has approved authority can go out and do that and as well as there is an activity lock that tracks um all the activities that has been taken so you kind of are auditing the process throughout now let's i'm just making another user and going ahead and seeing what's the view of another user who has approval permission what do they see so in this case i am i'm jane and i go here to this version and i can see that hey there is a request to change from this model from development to staging i can go ahead and review all the artifacts and decide that yeah it looks like it's good it's good right i confirm and all my activities are logged here and if you go back to the view of the user who has requested now as you can see this model has been promoted to staging and at the end i can gradually promote from staging to production and so on and another thing if you go back to the presentation here is about automation so in addition to moving and promoting the models through lifecycle stage and the proven workflows another important consideration model registries to ensure that it helps with automation and you are not really adding more ad hoc processes in the middle so one thing that you need to worry about is having the right apis or web hooks that can trigger downstream actions so for example if you have moved a model from development to staging you should have the right controls in place that your deployment pipeline can pick um that model right so they they they should be able to be in a stage where they're looking out for any stage change so if um if something has moved from staging to production let's say uh the the downstream application should track and automatically deploy the model for example i'll show you how a quick view like here i'm just looking for the latest version model here i'm looking for which is the latest model that has been registered and that's model v2 and these are the attributes and i have all the information and i can go ahead and get the associated artifacts as well if i need to go ahead and deploy it so again for going back to the features automation is really important as well as the right configuration to promote the models so for example if you have let's say different clusters you have a different dev environment different staging and production you want to make sure that when the models move from one stage to the other the right clusters are picking up the model and deploying it and also performance so i just shared how you can fetch all the model artifacts and if you have a four gig five gig or even bigger models you need to make sure that um the system that you're using is performant enough so that um it really doesn't negatively impact your deployment pipeline and finally deployment uh integration into your deployment systems so you may have your own ci cd pipeline and you want to make sure that the registry talks well and integrates with your deployment system so that you don't end up creating another fragmented workflow so this this ah kind of concludes the demo that i wanted to share with you and hopefully it gives you a picture of how you can integrate with your experiment management system push your model to a registry move it through different life cycle stages and manage all the information about the registry in one place what are the benefits of model registry first of all model registry helps managing production ready models second it enables automation of your ci cd pipeline and it works well with your existing deployment infrastructure third it enables more centralized governance of models so all your models are in a single place and you're managing them well fourth it enables clear handoff between data scientists and your ml infra engineers and finally it provides executive visibility into what models exist and their current status as the next step i recommend you to try this yourself so we do offer a free trial at verta and what the features that you can try with the free trial is model versioning experiment management model registry that i just shared as well as deployment and monitoring so you can go to https www.water.ai slash trial and sign up for the trial the other option that you have is db 2.0 which is open source version of our product and that doesn't include model registry but you can play with horses system for model versioning and management so you can go to github and fork and play with the open source version as well thank you and really appreciate your time today and i'm open to questions now great thank you so much uh we have some great questions that have come from the audience samina i'll go ahead and get you started uh first one is could you give one example of how model registry has helped with deployment automation yeah that's a good question so um i can give one example of one of our customers how they have you know used registry in their deployment process they have taken completely two different approaches based on their use case and how fast they want to move so in one scenario what they have implemented is a very stringent an approval process and then the releases come in the back end is fully automated the user like our data scientist moves the model to staging uh the next step their data scientist fears they they they kind of review make sure that things are looking good and approved those changes are captured through you know in the back end through api notifications and automatically uh deployment is triggered in their staging cluster and once the deployment is done they trigger a batch test job and again this end-to-end process is fully automated so the back gesture runs and based on whether it's a success or a failure let's assume it's a success the model version label in registry is automatically updated that the test is successful um and the tag it as ready to move to production so that particular version is scanned as production ready and again from staging to production that's um they follow a very similar approach so again as you can see there is a human oversight here because of the complexity of the model and how critical is it for the workflow they want the data scientists to be in the loop and review and approve but the entire backend is automatically you know the rest of the things are taken care through automation and the other approach that they have taken for some of their batch use cases is um they they do not want any human oversight they the they want to make sure that things are visible to data scientists but the batch job just automatically registers a model uh it triggers testing once the testing is successful it the the model moves from staging uh to production and um and subsequent clusters so again everything is fully automated in that scenario and but then they made all the activity log and everything is maintained so that a data scientist they could go and you know review everything and what's happening but uh nothing is blocked on a map on any manual approval process so again depending on the use case and um what type of approval and workflows that you want to introduce you can you can have different deployment uh options and you can automate it accordingly and one thing that um these guys have achieved is like i think they are deploying uh releasing almost daily so this type of automation really helped them a lot you know to achieve this level of efficiency um i hope you know that answers the question i can go to the next question and um i'll try answering it um i think we have a question around them what is the definition of a model in a registry um do you include a model binary or the inference code or everything um the that's a good question so as i mentioned in one of the slides that um for us uh we feel like model is just not the model code to really you have to ensure that the model when you are putting it in in an experiment phase or in registry it includes so many different artifacts so one is of course the model code but then there is data there is um all the environment variables uh and and also the you know configuration so all these things together really make sure that you know ensure that the model is reproducible and you have all the components in place so for us in model registry when you're registering a model it's just not the code it comes with all the model artifacts and data and everything so when it comes to someone reviewing the model someone downloading it they have everything hanging so that that's the philosophy that we have followed with the registry um and hope you know i answered the question and happy to follow up later as well um and welcome to the next question um we have models running in different places and we have a complex cicd process how should we approach model registry so so um i think uh the i covered like what the difference you know must have features and things that's uh really critical for a registry um this is a very relevant question that if you have a very complex uh cicd process and um you have a much more mature release cycle um how the model registers really fit in and for us i think i i kind of see three key areas that you should look for one is like around how agnostic the model registry is right so what you've seen is um in a typical data science world you'll have both batch and real-time deployment you may have deployment and serving infrastructure all across the board you don't want to uh the data scientists don't want to be restricted to one platform right so we at verta we offer a deployment option as a module but then we have seen customers using virta uh you may be uh you may have spark stage maker azure you may be running your inference in multiple places again that's another factor that should that should be considered in registry as well as you may have a bunch of old models that are that if you cannot just ignore them that may not be running in one of the platforms somewhere it may be some old you know legacy platform and then you may have different environments your test staging so there are so many parameters involved here so i feel like when you are using a registry of building your registry regardless of where the model is running and um what the model is doing which stage the model is everything should be captured in the registry the registry implementations would be very much agnostic to all these parameters so i think that is one critical factor when it comes to a very complex you know infrastructure that when you are dealing with uh the other factor that would be important here is how easy it is to integrate right you have your own cicd process and we don't want you to rip and replace it um and um and then you also be looking for a fair bit of automation and this can only happen if you are using a registry that has very open and flexible apis and even notification system so you can just easily tap into the registry and integrate it versus creating a lot of custom um work right for yourself and uh the third factor that is really important here that um that you should consider is performance and i briefly touched upon that in the presentation as well because one aspect is here is the number of models that we are releasing and also the size of the model and in some cases what we have seen is the size of the model is fairly big and since registry is kind of a data warehouse for you where you're storing and retrieving all the models it should be like it should be done quickly and without any significant overhead on your system i'm highlighting this more because that's one pain point that i heard consistently um about some of the other registry options that that are out there so something that for you to consider again uh agnostic easy to easy integration and performance as long as you are hitting those three bullets um if you have a complex cicd process a registry can really help and help you move faster and integrate much quickly uh hope that that helps answer the question um let me see i think i have i see one more question so i'll quickly jump to that uh we have a lot of focus and compliance and auditability not sure if and how model registry helps uh i'll put this into you know two parts one is about the like we covered about the approval workflow and auditability of the model release process and definitely registry uh provides that option and it also makes sure that you are not using a model without a horror review process across the board so that's a process aspect but the second part is um how to build a much more compliant registry feature that works in your you know as for your company's you know architecture and technical implementation and i say here the key things that you should look after is of course access control and that not everyone has equal access so maybe some folks would have read the access some would have read write um you have to control who cannot prove who can release so having that uh granular access control really helps in your you know at the end of the day for your compliance and something that can integrate with your company's identity management system um and it's fairly easy to implement that that's also important here the other aspect is um like uh we saw in the demo the activity log so that really is also important from the compliance aspect because you are kind of logging everything where the model is who is approving the model when is it releasing our time spam so you're kind of capturing all that in the activity log that also at the end helps secure compliance and the third aspect is the activity log is great but it's kind of it's um it's a part of your audit process but um what's also important is having a very detailed trail of actions which and with the right audit log and you need to maintain that so that you can go back and look at the time stamp and see who has deployed the model at what time what happened to the model and what actions have been taken so again with right access control logging everything and a very detailed audit trail is another approach to me to just to make sure that the registry feature is um is compliant and its health it is helping with your compliance needs um i hope that helps with the question and let me know if you have any follow-up um you know uh questions around this so i i don't see any more questions so i think we are good for now you have my email address you have more details about um vertex trial options so please feel free to reach out to us i will hand it over to stephanie next to wrap this up and thank"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Management and Versioning",
        "Topic":"Model Registry and Versioning",
        "Video Title":"MLflow Model Tracking  and Model Registry",
        "URL":"https:\/\/www.youtube.com\/watch?v=3Jiduh4VIrU",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/3Jiduh4VIrU\/hqdefault.jpg",
        "ID":"3Jiduh4VIrU",
        "Publish Time":"2022-11-10T22:04:39Z",
        "Channel":"FourthBrainAI",
        "Channel ID":"UC1UoddLXvBg2nb3k6_-ZeMQ",
        "Transcript":"this is a brief overview for the full machine learning life cycle so usually we'll Begin by some data preparation visualization and then the data scientists were coming to develop different models and here you can see ml flow is greatly helping starting from the model development phase we can help log those parameters metrics metadata in a programmatic way and we can push the model to different stages from your Dev to staging to production and eventually it will help with the deployment and model serving as well there's a mlflow tracking server so I can easily store the parameters Matrix artifacts including like model or even a image or some CSV files metadata and model information on here I can connect it through notebooks some local apps like Jupiter notebook and also some Cloud jobs I can easily build the results through UI API or query it programmatically through spark data frame so what is ml for model registry it actually is a central Repository you can create those unique named register model for Discovery across the data teams and the workflow is usually like provides specific UI and also API for the registry operations on the model registry tab you can also see different model versioning you may sometimes change the model by just a little bit like some parameter change so you can see model version one two three so that allows multiple versions of model in different stages you can push your model transition from none to staging to production or archived it's also great with cicd integration it's easily specify a version and you can just load it in to do your unit test and do some additional inspection it's great for model lineage provides a model description lineage and also activities"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Management and Versioning",
        "Topic":"Model Registry and Versioning",
        "Video Title":"Iterative Studio Model Registry Demo - A GitOps Approach to Model Registry",
        "URL":"https:\/\/www.youtube.com\/watch?v=EmD6itL96nY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/EmD6itL96nY\/hqdefault.jpg",
        "ID":"EmD6itL96nY",
        "Publish Time":"2023-03-18T11:00:09Z",
        "Channel":"DVCorg",
        "Channel ID":"UC37rp97Go-xIX3aNFVHhXfQ",
        "Transcript":"yella is a front energy near here at iterative and the dri for the development of iterative Studios new model registry today he will be giving us a demo of model registry and showcase the improvements that have been made since its release in July and so with that I will pass it on to Yellow thanks Annie and thanks Rob for uploading I hope you can all see my screen on the other uh like Jenny said we've been working on model registry for quite a while made a release in the summer and we've added some new features that I want to show you today I'll keep it pretty uh short and sweet I'll quickly set the scene explain what model Registries are and why you would want to use them in your team and we'll quickly go into iterative Studio and I'll show you around show you how you can get started and we'll wrap up with a brief summary show you what's next for uh our feature development yeah so let's get started a model registry is a registry of models and that's not very informative but a model registry provides you with a way to organize your model Development Across your teams it provides you with insights into the improvements of models that you have and there are a couple Solutions out there today what makes iterative Studios model registry a bit more different is it's very closely tied to your software engineering teams so in some other model Registries your data science teams are pretty isolated from what happens in your software engineering teams Creator chip Studio model registry really brings it together it's based on git it's open the the core package that drives all this GTO is open source you can check that out later if you'd like to uh but we yeah without too much further Ado let's head into the demo it's part of Studio of course so if you sign in you're still being yeah you'll still be redirected to the Project's dashboard there's a new tab on top which says models and that brings you to the models dashboard there's already some stuff here what's actually happening is the projects that are connected to Studio those have repositories behind them git repositories and studio parses all those commits and tries to find any model information that's present there we have two projects that have some model information in them and on this model dashboard you can really get a quick sense and overview of what's happening with the models that you're uh developing so yeah you can see the name of your models which repositories they belong to the the greatest Center version that's currently registered uh and then there's all these colorful columns which we call stages stages are in the context of model registry to be seen as deployment targets these are completely customizable so we've taken a bit of a traditional approach and added three stages called Dev stage and production these can be anything you like GTO provides a way to configure What stages are allowed to be used but this is the setup that we give it shows we go one level deeper will end up at a model detail page we're currently looking at the random forest model and specifically at version 2 of this model on the left hand side you have a very detailed view of this specific version and information that's specific to this one and on the right hand side you have some tools to get a sense of what's happening on a Model level so you can quickly see which versions are deployed for this model specifically to which stage and below that there is a History Section which provides some sort of an audit Trail if you if you would there's a couple of sort of events here stage assignments version registrations stuff like that um yeah it's a chronological timeline so you can see what's been happening here on the left hand side um there's like I said detailed information for versions we're looking at version two you can switch to different versions and get information about them there's ways to describe them there's labels to identify them we show the path to the model file as you can see by the get up icon is actually a model file that we've committed to git but this can be a remote model as well so any Cloud vendor you like to use as S3 or any other option you can use them to connect to your model registry and the section below here is actually a little special because this model has been safe using mem a different tool that iterative also you also develops Emily M is able to inspect the model object and provide you information about the packages that are required to run it you can even provide a quick Glimpse here in what kind of methods are available on this model and what you can expect to receive in your turn when you're calling it so we took us a quick look at something that's already there um we have this demo project on GitHub which I forked the original one is connected to all Studio users so if you'd like to try it out you can go to Studio this project should be part of your user ground user account by default and Jenny will provide a link to this so you can get started as well it's a yeah it's a demo repository um but let's argue that we've been developing with a new model and we would like to add this to our model registry we're on the light GBM model Branch you made some changes uh currently it's not part of our model registry so let's add it there's a big blue button on top which says add a model and this takes you through adding your model to a model registry quite easily we're working in this repository so the demo Bank customer churn and my model is a local model and if you're model would be in a would be would be a remote model maybe saved in a cloud storage somewhere this is where you would specify that it's a remote model the second section is um a way to describe your model we can add some labels um description and the section afterwards kind of reveals what's happening we're creating a commit that adds some information to the repository which lets iterative Studio know what's happening we've been working on the live GBM branch and we can either choose to directly commit to the branch create a new branch on top of that and make a pull requests but let's go for a more straightforward option of directly committing to the branch um and you'll see that it's being added and we now have a new model here below if we take a look at GitHub there's actually a new commit on this Branch uh that's this one and as you can see a file called artifacts.yaml has been edited and this is basically this is a file that's in a root of your repository that iterative studio uses to um make up what's part of your model registry after that we could develop the model further but I'm actually quite happy with uh what I've got right now and so we can choose to register a version versions are milestones in your development or maybe patches to fix small issues that you've had with them but essentially a version is a git tag so to tag a git commit you need to choose a commit and that's what this form is for so you can see the the commit that we just made 50 seconds ago is currently selected I'm fine with that um we use somewhere versioning here so you can provide a version name here and the default message is uh is fine for now once that's done registering you can see version 1.00 is now uh is now here and when you open this model the model detail page shows you whatever information is there currently okay let's move again back to GitHub and what's actually happening is a new git tag has been added to your git repository so this says um light GBM model which is my model name um the version number and a counter sign at the end of it which we use to build up the model history correctly as you can see it's my avatar there's the message that was that was sent there and this way it's actually stored to your git repository this action another action that's comma is assigning certain versions to stages so let's say that we really like this version one and we want to assign it to depth to test it out we can specify the stage here and the model version that we would like to deploy to it and add a custom message if we want to and when we assign that stage we'll have to wait a little while it oh it uh it goes to the top because it's the the table is sorted on latest stage assignment but if we would go back to our tags again refresh uh you can see there's a another tag that's been committed to the repository which basically States this light GBM model should be assigned to the stage Dev and again a counter to um to help with Chronos uh yeah to help with the order of things so yeah you can click on uh a couple of nice UI elements but what actually happens uh now that we've created versions or assigned stages that's actually entirely up to you so an example that we have in the demo project is a GitHub workflow that's triggered this is also using CMO and you can see that um this is a this is a very simple script but uh on the event of tags being pushed into the repository with the dev hashtag Dev content being part of the git tag this job is being run which potentially could deploy or pull the model from storage and deploy it to somewhere um yes using GitHub actions uh it's currently running as you can see but like I said whatever happens when you assign a stage is entirely up to you please take a look at the demo project if you're interested in in the example that we that we made up for you I'll quickly wrap up there's also a couple new actions that we've developed um to Signal Downstream that a certain action should be negated one of them is unassigning stages so if you want to Signal Downstream to whatever systems is listening to these git tags you can on assign a stage which will clear up the stage here in a similar fashion you can deregister versions if you no longer want to treat them as a version that you want to work with which is yeah sometimes the case these two actions also create new tags and you can see they follow a different format they have this exclamation point which signals GTO that this version or this stage assignment is no longer to be used uh if we go to um the model page right now you can actually see this entire history also displayed in iterative studio and lastly if you made a mistake or you don't want to track a certain model at all anymore uh you can deprecate it um and after that finishes up it is no longer part of your model registry so this row should disappear in a couple seconds yes all right um so that's all I have for the demo right now so to wrap up we um we provide a models dashboard which shows you in a quick overview of what's hap what's happening with all your models um when you go a little deeper you can get some in-depth information about uh specific versions of models um your history is represented in the studio UI but it's also part of uh of your git history so it's always there um kind of circling back um what makes irritive Studio special is that it's all git based the inner or the the package that's used under the hood is GTO and that's also open source so you can take a look how it works there are some ducks in the work some documentation in the works so um we'll be sure to let you folks know when they uh when they are released and yeah as with most iterative tools it's completely modular whatever happens after you add the git tag is completely up to you you can use something like CML but if you have something else that you want to use use that and um there's no vendor lock-in so this works with Google Cloud Amazon Azure that all those providers some stuff that you can expect in the near future is uh we'll be adding metric information and parameter information to the model detail page and if there are spots to be displayed you can eventually find them on the model detailed page as well so if you're looking at what makes version one version one you can get even more information on the same page besides that we're also happy to hear what you guys think if you have suggestions or feature requests there are a couple uh ways you can contact us if you're part of this Meetup Community you're probably also familiar with some of the channels that we have one I want to highlight in particular is the studio support GitHub repository you can file an issue and I'll be probably the the first one to respond other than that there's Discord there's this community uh this Meetup community so yeah thanks for your time hope you learned something if you have any questions I would love to answer them and uh yeah thanks is it possible to compare accuracy results from different models versions into in studio oh that's a great question um currently that's not possible one thing I did not demo uh I I'm still sharing my screen I think one of the older or you know one of the first parts of studio is the project table as you can see models are part of the project table as well those are these green columns uh and let's take the X GB boost model and um take the version registration display setting you can see that this row here is version 1.01 and this is 1.00 and if you have certain metrics they're the same here it's kind of a that's part of the demo project I guess but in this way you can quickly compare them like I said we have plans to bring specific metric information to the model detail page so maybe there's a yeah maybe it can be useful to also compare those versions easily at some point so that's it yeah that's a good suggestion thanks for watching if you enjoyed this product update video please like And subscribe thanks Stevie and feel free to post comments and questions below on our YouTube channel we share videos on product updates tutorials and how members of our community use our tools as they solve problems in their domain spanning a wide variety of fields in the machine learning and AI space don't miss out on any of it see the description below to find links in the docs for our tools they are an excellent resource for getting started also visit our blog where you can find tutorials on our tools as well as product and Company news join our Discord server to get support help others grappling with the same issues as you connect with other like-minded folks and discuss our tools or other topics in the ml app space it's a great way to learn and get to know others worldwide who share your interests we also have a job Channel where you can find relevant job opportunities in the space finally if you're really serious about taking your ml lab skills to the next level we offer a free online course that is designed to help you understand the iterative philosophy and achieve your ml Labs goals our mission at iterative is to deliver the best developer experience for machine learning teams by developing an ecosystem of open modular ml tools thanks for making it to the end Devi and I will see you in the next video foreign [Music] foreign"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Management and Versioning",
        "Topic":"Model Metadata Management",
        "Video Title":"Webinar: Data Modeling &amp; Metadata Management",
        "URL":"https:\/\/www.youtube.com\/watch?v=jyva44uHoR4",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/jyva44uHoR4\/hqdefault.jpg",
        "ID":"jyva44uHoR4",
        "Publish Time":"2017-06-02T18:47:37Z",
        "Channel":"DATAVERSITY",
        "Channel ID":"UCeTLK-PN9ZynubJ5UIxzk-Q",
        "Transcript":"and welcome my name is Shannon Kemp and I the chief digital manager for data our city thank you for joining the latest in the monthly webinar series lessons in data modeling with Donna Burbank today Donna will discuss data modeling and metadata management just a couple of points to get us started due to the large number of people that attend these sessions you will be muted during the webinar and for if you'd like to chat with us or with each other we certainly encourage you to do so just click the chat icon in the top right hand corner for that feature for questions we'll be collecting them via the Q&A in the bottom right hand corner of your screen or if you like to tweet we encourage you to share highlights or questions via Twitter using hash lessons DM as always we will send a follow-up email within two business days containing links to the recording of this session and initial information requested throughout the webinar now let me introduce to you our speaker for today Donna Burbank she is a recognized industry expert in Information Management with over 20 years of experience helping organizations enrich their business opportunities through data and information she's currently the managing director of data strategy limited where she assists organizations around the globe in driving value from their data she has worked with dozens of Fortune 500 companies worldwide in the Americas Europe Asia and Africa and speaks regularly industry conferences and to let you know um so with that I will turn the webinar over to Donna to get us started hello and welcome hello Shannon always a pleasure to do these and thanks everyone who joined um so as Shannon mentioned today's topic we'll be talking about data modeling and the ever favorite topic metadata management which always tends to draw a crowd here at dat diversity um so Shannon mentioned a little bit about me already so I don't have to go talk too much about this uh just a few notes as Shannon mentioned today we do have a hashtag lessons DM so if you're a Twitter person and want to continue the conversation online please do I am online on Twitter um Don birbank which is easy to remember um and a little bit more about me in terms of metadata I'm one of these few metadata nerds said proudly that have been doing this for over 20 years so if anyone remembers Platinum repository way back showing my age um I was a metadata consultant they actually wore those titles way back when um across us Europe Africa um and Asia helping some of the larger companies in the world manage their metadata I went into the product be become the product manager of that product both the main frame and distributed um and then kind of spent some time with some of the data modeling tools in the market you may be familiar with the your studio Irwin um and really flushed out some of their metad and modeling as well written a couple B books on the subject was also if anyone knows or familiar with the object Management Group which they had the acronym OMG before um before sort of online twittering and that sort of thing uh so it's sort of funny now um but I sort of worked with their metadata strategy things like the information management meta model and things like that also a big fan of daa um and very been involved in that for many years if anyone is part of the data management association on a great group to be in so that's a little bit more about me a little bit more about the series um some of you which is thank you very much so we do see some of the same folks joining every month that kind of be part of the series other folks kind of join in as a topic pequs their interest um and we tried to set it up that way so you'll notice last month we talked about the evolving role of the data architect um that one was very popular partly because when you see the different topics it really is an evolving Market out there and it's not just plain old ER modeling anymore there's there's so much more that just the role and the the practice of data modeling is so broad it's really hard to limit it so um we kind of bro broadened out the topics this uh year and you'll see kind of a broad range from agile to data wrangling is next month kind of what that means in the world of data modeling uh quick call out and I think Shannon will be putting it um in the followup usually there's an email with kind of some of the questions and and topics we talk about we're actually doing a survey on data architecture um and we're as curious as you in terms of it was actually a hard survey I'm doing it with the diversity and some of their writers as well um there is so much changing and what's going to stick you know who's using blockchain who's using agile who's using traditional you know Legacy technology so kind of want to hear from you as well so if you have some time do take that it's a bit long um but we should have needed to because there's so much out there so that's a plug uh for the series um if you missed any in the past they are all on demand um and you can catch them because I know everybody has busy day jobs and I personally hardly ever I'm able to catch a webinar when it's live so I'm always doing it weird hours of the evening so we get that okay so that is the past and the future and today we'll be talking about metadata um how data modeling and metadata are kind of uh nice cousins and how they fit together and kind of what that means for your organization um and Beyond so um talking about surveys we did a survey last year on again merging Trends in metadata management was hugely popular still available for download that'll be in the links as well and no surprise to me hopefully it's not a surprise to anyone in the call um but in the survey over 80% actually statistically speaking actually a little higher than that um metadata is more important now than it was in the past um and it's growing in importance and and I don't I'm not seeing that as a surprise because we see a lot of requests most my customers are either asking for metadata as part of a larger data strategy or as part of a governance strategy or just a pure play how do I man metadata so um it was nice to see that confirmed in the survey I guess is what I'm saying but it's a growing Trend it's hotter than ever and hopefully this webinar will come and help demystify some of that for you in terms of what is metadata if it's new to anybody here I know this is kind of simple but I think we can often over complicate metadata and say things like data is metadata is data about data which of course I had to say that once in this webinar but I won't say it again because I find that a bit frustrating in terms of a definition but really it's just data and context it's the business business and Technical context around your information uh to be a little more specific about that one way I like to talk about it is the who what where why when and how of data almost the zucman framework of data in a way um so we won't go through each one of these but I think as you look through if you're not familiar with metadata it sort of starts to make sense right who who created this data who owns it is if we have data stewardship in place what you know that's I think often what we think of when we think of metadata what's the business definition what's the technical structure where it's stored you know why is one we don't always think about and I always start with that when any project with any project I do you know what's the business purpose why why are we managing this anyway um there's so much information across the organization you really have to prioritize and and focus on the stuff that's going to have the highest value when you know is it going to be current um and then how how is it formatted and that sort of thing um so you can read through these but I find that helpful especially folks um I've used this for some sort of business stakeholders and that kind of get the n heads okay now I see what you mean because metadata I'd love to say it's sort of a funny word but it is sort of a funny word so we don't want to overdo it they already think a lot of us techy folks are too techy anyway um but the key thing about metadata is it really is part of a larger landscape and we don't do metadata just for the fun of it um although maybe it is fun um but you know this is a this is a framework we use in our practice all the time and I think it we've gotten some good feedback but it really kind of sums up the challenge we have as data management professionals and we always start with the top down why are we doing this anyway um and I should have alluded to it on the beginning of the call but I'll just talk to it now I mean we often used to get in my we on a Consulting practice there's a lot of this stuff and we would do metadata there's always been metadata right but it's sort of been hidden we're going to do governance we need better metadata to govern but I'm having companies come to me not only with data strategies how do I make my company more data driven I've actually had folks come to me and say I have metadata and what's my metadata strategy and how can I become more competitive using metadata I think people are getting it that metadata is kind of that glue and the the magic uh dust that makes everything happen so I'm seeing that as a positive thing and these are business people they understand you know metadata and the importance um but you know it it does help manage the stuff on the bottom too because we can have a great strategy but unless you understand whether it's unstructured data or structured data or big data or what documents uh there's metadata in all of that um we won't go through all of those today um we kind of we'll focus on the relational side of things um but they really are kind of the glue that fit all those other things together in the middle you know Master data data warehousing bi data quality you can't get any of that without metadata um and then the governance is kind of the people processed policy we have a couple other uh presentations online if you're interested on that we've done in the past on governance and really how meditated makes governance actionable so it's great to have policy um about data what's pi and what's you you know what's our information policies but unless that's actionable through the metadata and the lineage and that sort of thing it's not so helpful so again that's another way a lot of my customers come and say I've got governance but how do I make it how do make these POS how do I make these policies real and actually link to those physical systems on the bottom so hopefully that's a helpful framework to kind of put what we're talking about today in a larger context the other sort of larger context is really metadata and that exists not only within your organization but also beyond the organization when we think about open data um you know the first thing I look at in open data is the metadata around it when we think about who creates it why what was the purpose of this data that's a big part of open data that you might have been scientific research but what was the context why how were they tracking it um and what was it used for then um so we'll be talking about data models but I think there's a lot of other pieces of metadata that exist within your organization Beyond it and really that's the CR Crux of of trying to get that full context of your information another uh slide from the survey we talked about early on uh this emerging Trends in metadata I found this really interesting I was whenever I do these surveys with with with the diversity I'm I'm a big old nerd and I was curious myself what some of the stuff would come out um so we did a a question not only what are people using uh metadata for now and that's you know probably not huge surprises things like data models and relational databases and data warehousing glossies those are kind of the traditional ones but I was curious what are people going to be using it for in the future um and again not too many surprises there when things like big data are becoming bigger um no squl some of those some of the fun things I did found in find interesting is one of the most popular ones in the future was Legacy systems things like Cobalt and JCL and again that's probably not what people are building their new platforms on but when you think of as people retire metadata is that that that's the documentation that we wish we had when when Joe retires and what was in Joe's head right so I think metadata is is that magic glue that helps not only different types of Technologies but also past in future and helps you leverage when you're trying to get to new technology it's the metadata that stays con constant really the definitions around your data so I found that kind of interesting in terms of what people are using metadata for and what they're planning on using metadata for um and data models are a good source of metadata and I want to put this in context they not the only Source um kind of the focus of this series is sort of data modeling and how all these other areas kind of relate back to that so don't want to limit um I don't want to limit metadata just to data models but so many when I was a metadata consultant years years ago um and we would go into some of the largest organizations on the planet and do their entire landscape and they would literally spend millions of dollars on this robust system and really when it came down to it some of them were really just using it for relational d data bases and glossies which is great um but a lot of that was in a data modeling tool and so if when you do your inventory which is we'll talk about that later but that's one of the first things you want to do of what what type of you the who what when where why what who what where when um what what are what type of metadata are we tracking key thing to think of and if the majority of what you're doing already is in relational data models a lot of the tools in fact most of the main tools on the market today have wised up to that and do have some sort of metadata component that or sort of metadata repository light I guess I'd like to call it um because if you're building these models by definition they sort of tracking your technical metadata what's the structure of that those systems and as well if you're doing modeling correctly the business metadata what do I mean by customer and so the beauty of it is that you're you're already doing it as quote part of your dat job if you're a modeler so leverage that and and publish that out to other sources so that's kind of the Crux of what we'll be talking about today nothing wrong with the full metadata repository in fact many times you will need something more broad um but as a starting point or if the majority of what you're working with is relational and and you're already doing modeling leverage what you have I guess is the point so a little bit more in terms of what we're talking about in terms of metadata if this is a New Concept to you data versus metadata and meta metadata we could keep going U but if you think of a spreadsheet which is kind of a rough approximation of a database if we want to think that way you know it's sort of the rows of the data the fact that Joe Smith is in New York and he purchased something in 1970 that's the data the metadata think give it as the column headings the fact that Joe is the first name and Smith is the last name and city is New York right so it kind of put some additional context so um you might feel sorry for my friends because you know at parties I actually talk about things like metadata and I was trying to explain this to a friend and there things like well yeah but that's pretty obvious why would you need a whole you know city is pretty clear is it you know we just know this stuff well maybe not what if your column headings were something like this right I'm sure we've all seen this okay so somebody has some sort of logic behind it there's some strings string one string two text 2 3 text 127 a date field so yeah that's metadata you can probably infer some of it but if you didn't see the data there you probably would have no idea what those felt Fields meant um so the the names don't always have that additional context so that is why these other pieces of metadata can be helpful uh the other part of that that even if it were named like the first case you know first name last name City if if you're been doing d you probably know there's a lot of subtleties that just think of city is is that the city where the person lives is it the city where they purchased the item where the store is located um you know where the billing address is you without that extra metadata you don't know or even something like first name last name it could be a business rule that in the Asian market what we call first name is really the last name you know so there's a lot of different subtleties even something that seems very simple um so that's where some of that metadata adds the context and definition around seemingly simple things we'll talk more about that as we go so just some ex other examples of that you know this idea that especially in a data model the beauty of them is you both have that technical which describes the structure and the format so if you're familiar with databases that sort of some ddl or data definition language on the left however create a table employee that had a name and and first name last name social security number uh that's kind of the technical data structure and then the business metadata is what do I mean by an employee versus the customer you know the ever elusive what do we mean by customer and there's so many subtleties to that another day is the actual customer themselves and we often can forget that um but think of things like gdpr now or the um European J general data protection regulation and there's many more think of privacy you should be thinking of that actual person who's John Smith you know we sort of obate people in a way we start thinking of it just in terms of you know bits and bites of data but there's an actual human being there or if you're doing something like Master data management you're trying to get that single view of customer well that metadata actually represents a human being out there so always good to remember that um and then the next slide is just a few more examples of what we mean by business and techical metadata we can keep it very simple in terms of you know column structure or but you know even that gets more complicated things like keys and validation rules and nullability rules and permissions and ETL and and all of that and on the business side you can add you know end levels of detail there in terms of definition and stewardship or you know speaking of privacy and security a lot a lot of companies are using metadata for that very reason um Etc so there's some examples there um and speaking of the business side you know we often if if you're a technical person on the call we kind of think Med is ours right that's a techy stuff so but business people do get metadata and they need metadata um and so in the survey that we referred to over 80% of the users are from the business and there's a quote there that you know we really help that's what helps us understand the data um and I often find my little color commentary I think sometimes the business quote gets meditated more than it does I think sometimes it sees it as a burden oh yeah well yeah I just know what that table means well you do but does everyone else in the organization um if anyone reads tan I had sort of a CM several months ago on you know metadata is actually the marketing for your system you know you wouldn't write a book and forget to put a title on it and tell anybody about it right so you know the the databas is your art you know make sure you publish it and let people know I had one one of my favorite quotes from a business person is we were trying to start a metadata project and we went we're trying to explain the benefits of understanding the technical structures and the linkage and the lineage and the business definitions and this project and she looked at me and said you mean you're not doing that already she kind of figured that's what you guys had locked down and you know if we couldn't get away with that in finance not knowing the lineage of our money and where it came from and how it was stored um so I think uh the more business eyes you think of self-service bi we had a webinar on that a couple months ago if you wanted to catch that um more people are looking at the data and want to see the metadata if I want to write a report or yeah I kind of want to know what this data means so but business folks get that if you are a business person on the call you're probably nodding your head um and it folks remember that they can often be your best sponsors um I've had some great marketing people be my best Champions on a project because they get the need of data and they get the need of metadata um so I always throw a few data modeling cartoons in these cuz I have them and where else can you use data modeling cartoons and you've probably seen this one before and it's not that funny um but it sort of is if you've been in the business you know okay we've built this application we've done all the testing we're ready to roll out just one small question what do we mean by customer right and I'm sure you've all had various flavors of that I've worked for many Fortune 100 companies around the world that were remain nameless because you don't need to because it's happened to probably everybody where you do something like send renewal notices to people who don't have your product or send um you know purchase notices to people who already have your product produ or you know etc etc etc and I think that seems the quote easy stuff um so people forget it um but don't forget it because that's really the Crux of what you're doing is you know what do we mean by why are we doing this anyway if we think about who what where why when right um the other you know piece of that is the I like to call it you know avoid the I just know I kind of alluded to that earlier I think so many of us on both the business and technical side you know in the business side you could say well yeah it's part number how hard is that that it's the number of a part right but there's a lot more context to that um so avoid that I just know think of if you were here and training what would people maybe want to know you know maybe this guy Joe who wrote the Cobo program and he's retiring that used to be called component number before we were required and all these different other pieces of information that may be obvious to you or probably not so obvious so write it down take that extra 3 seconds that it takes um to write it down all right maybe 10 seconds but you know what I mean it's not that hard and I think the value um could be great when we're thinking of business definitions like that um and it can be expensive I mean there are real world costs I mean we could I mean of when I do a data modeling or or metadata class if we start out as one of those ice breakers tell us one of your horror stories or or success stories about you know poor metadata having real world implications and I've never been in a class where no one had anything right there's always something um and here's some actual numbers and and you can use these they're they're quoted um in some of your own projects that you know the US economy can lose what $3.1 trillion just when it comes to data quality or things as simple as male being under under delivered because the the data is wrong and and data quality really comes from metadata um you know we think of things like data science or self-service Pi I mean the quote there is that people might spend 80% of their day getting the data right what you want to do is find the discovery um so the clean metadata really helps you you get to that easier probably one of the more famous ones that we love to quote because you know this one actually was directly metadata if you've heard of the um the NASA Mars climate Orbiter um where they lost it so that was $125 million mistake very easy to quantify it and it literally was a metadata issue so they were sending the data for the thrusters that kind of sent it up um and it was in English units rather than metric unit so they kind of missed the mark uh started off course and it got lost and that was is a very embarrassing thing so not only did they lose a $125 million asset um sort of was embarrassing so you had that brand and reputational damage you'll think of your company something similar you you you send the wrong mailing to the wrong person there not only did you waste that mailing you doubly waste it because now they kind of think you're not that professional um and probably less likely to buy from you and the thing you know it's easy to sort of forget is the Lost opportunity they were sending that up there for a reason to do this great uh research and we never got to do that um so that's kind of a very very popular one and really it was just something as simple as and someone probably thought well I just know it's metric of course it's metric we only do scientific stuff in metric well somebody didn't know and they did it in English units right so that's that avoid the dreaded I just know um to give NASA a little credit because it's always easy to pick on people but um a couple of the other uh presentations we give I talk a lot about open data and actually NASA has some awesome open data sets and their metadata is great so they've sort of learned their lesson and generally they say who published the data why it was used um and summaries and and they have actually some great examples of good metadata for some of their data sets so they're not all bad but that wasn't a very embarrassing example but that sort of is the summary right just like your parents said when you're a kid you get into trouble once at school they remember that they didn't remember all the a you got on the test they remember you got in trouble so um that's kind of how it is with metadata too so um getting this right on the other hand can offer some great not only efficiencies in terms of knowing your information but in terms of reuse and that's you know one of the biggest challenges of most organizations is a you know if we think of that idea of um kind of that single view of customer do I even know where all my customer data is across the organization so what some companies do as a start is kind of do that inventory based on the metadata you know with the data modeling tool they kind of these scanners or you know reverse Engineers or whatever you want to call them and you can get that uh source of the information and then you can start to do the rationalization you know based on some of the matching rules is this the same version of customer is it different and how do we kind of get that standard reference metadata so as you think of kind of the golden record in MDM of what's the golden record of customer you can also have those golden record in terms of standard metadata so part of the problem sometimes um we're all human we're trying to get things done but I need to create a customer database if there's nothing out there that I see I'm just going to build it myself um and and when you build yourself there's always going to be a slight difference I mean in my company we kind of had a shared directory I I just the other day I saw this and we are data people right so what do we do we do data standards and we were just creating a directory uh structure and it was for a conference we've gone to for the past three years and everybody for those past eight conferences all named it something different it was London conference X conference in London it was it was actually hilarious how many different versions you could have of the same conference name and we data people even did it so I think publishing a standard um had I published a template and said this is how you publish the name of a conference that wouldn't have happened so the same thing if someone's going to create a database and there already ddl they can reuse or there are standards that are published people are more likely to use it so that is the beauty of metadata really does help with that consistency that does lead to more consistent data itself so part of what can help with these are these idea of um metadata Discovery tools or scanners or parsers or every tool kind of has their different name for it but the beauty of that um is that they're smart enough to kind to go through and read these systems either the data dictionary if we're talking relational databases and kind of understand that um structure um and then populate them to some sort of storage repository so I put kind of repository in parentheses there so if you're the full metadata repository they're probably going to have that store with a lot of other sources but if we're talking data modeling repository a lot of them have kind of broadened up and they store what we call met model I think of a meta model as a data model for metadata um always talk about the meta levels but when you think about it you know a table from Oracle and a table from cbas one from db2 and a data model and a spreadsheet and a cobal copy book they all kind of have some similar things things that are kind of like tables and kind of like columns right so that's what the idea of some of these meta models can kind of do that rationalization these things look a lot alike we're still talking about customer first name can we kind of either link them together or kind of rationalize them together and that's the beauty of storing it in a s singular place you can kind of do that linkage between them um I do give a call out at the end there we have a whole at dataversity uh class on Med online course um and we do go in a little more detail on things like meta models and how this all works so if that little tiny piece of it was it all interesting too we kind of go into more detail on that course um but for now just think the kind of store one plays like like your data model there's a metadata model behind it and then once you have that kind of model that can do the linkage you can do things like data lineage and a very common example when we're thinking of things like data modeling tools um is relational databases and warehousing and bi so your data modeling tool probably has all your data structures in a physical data model or could if you wanted to so I have a customer table in Oracle one in SQL Server one in db2 I may have created a staging area um what a lot of the tools now they kind of have um built in they there's one popular company uh based California that kind of does um metadata scanners for some of these the same one everyone uses which is kind of a benefit because everyone's populating the same way and so if you're using an ETL tool you can use one of these pop scanners and it can populate your data moding tool usually there's some sort of source of Target mapping and it can get either the table structures and sometimes the mappings as well into something like a staging area or into your Warehouse so again if you have models there as well if you're logical and or physical and then you may go to a dimensional model in your warehouse but again the lineage and the metadata and those relationships can be pretty well tracked um and then a lot of the tools again they can do things like par your bi reports and kind of see what Fields have been used so that elusive I want to see the field on this report and kind of how we can go back um some of the tools are better than others but many of them can get a lot of those pieces um or the other way around some can kind of push back out to a bi tool um I had one customer that was kind of uh taking all the definitions from the logical model and cut facing him into the bi tool and they found out there was kind of a bridge between them that just did that automatically and saved him about a month of time not having to go do all that so there are inter relationships between all of these things and when you think of it they're things that kind of look in some very simplistic way of a entity and attributes or table and column and and the different permutations above it around it um and you can link them together and I know that's an oversimplification but not really I a lot of these things are more similar than maybe we think about I mean the other piece is kind of an impact analysis or where used right so I'm going to change a field I'm going to change the length of a column what else is going to be affected you know the dbas and the column can probably know their heads right just don't I don't break anything so we can't just go randomly changing columns going to be affected and what's going to be affected on the front end um so again if you have all that in some sort of repository you can better do that sort of impact analysis and see what's going to change or I have something like Pi you know I have everything in a repository like this something like gdpr comes up and you need to show the lineage be doing customer data you're a step ahead because you have that lineage other people are playing ketchup so this not only helps you be more efficient but you know once you have this the amounts of the amount of usage you can use is kind of your worlds your oyster there's a lot of different things you can see from the metadata itself part of the reason you can do some of this if folks are using different model design layers um so if you think of conceptual models as kind of your business Concepts so kind of your you know business this definition what do I mean by a client what do I mean by a customer is it different from a client um and it's a logical model and again we could we could debate this picture all day the people have slightly different definitions but you know handwave um you the conceptual is kind of your business Concepts The Logical is still at the business level but you're creating kind of rules between it can a customer have more than one account what the attributes on customer and then the physical and and many of the tools you can do it from all these layers and Link them together um so the elusive I have this term called customer customer where does it live on the different databases is it cust on Oracle is it cable 16 on db2 and how do I do that lineage and kind of the mapping between them and so if you have this design layer relationship that's yet another way you can do this lineage again think of something like GDP and where's all your customer data well I have that because I've been forward engineering from a single logical model for example or some of them can do this after the fact or you can create kind of mapping rules um you know that I know that when I of C that means customer right or I I know that you know these are different kind of mapping rules you can do that uh some of the more P playay radata repositories are kind of doing more kind AI you know and they can imply well GE this this looks like a email address I'll link it the things called email so some folks can do it that way but you more traditional data modeling you kind of do those mappings so you kind of create your naming rules or proactively that I know that when I have customer it's always going to be abbreviated as cust you know not C x96 or whatever you want kind of proactively create some of this L use um so we've been talking a lot about relational and I kind of hinted at other things like bi tools and things like that um but kind of wanted to talk Beyond relational databases because any was any of us in the business know that that there are things Beyond relational especially nowadays there's so much um and most of the data modeling tools again out there do support some of these other sources and I'm not going to list everyone on the planet again that data modeling course uh that I talked about we kind of do go into a lot more sources um but just to kind of keep things in scope you'll get the idea and I also don't want to be too redundant on some of these um one of the big ones uh where you can get some benefit and there's some kind of add-on tools that can help with this some of the data modeling tools and have that module when you think of something like a CRP uh CRP CRM or Erp system your Salesforce that kind of thing those really have relational databases underneath them and when you're trying to do things like create an MDM Hub or a warehouse and you're trying to you know a lot of your company's most important information say about customers or employees are in these systems um and if you ever just try to reverse engineer them which I've done and two days later it came back and it looked like something in the left you know all this kind of random tables with funny uh names that are very technical or even in German and Technical or you know very hard to understand um some of the tools out there can actually translate that to more of business logical metadata and you can see that mapping so if you want to integrate these with your other systems you can kind of get that metadata behind it not only the technical metadata but some of the business layers as well then when we think of things like no SQL which you know means a lot of different things there isn't just a no SQL um but document databases for example um they you know some some of these quote no SLE have better metadata than others it's probably my quick summary for this webinar um but documents kind of document databases have a little bit more structure than others um and a lot of the data modeling tools can kind of support that so you see that it looks a little different and they have things like collections you know it's a different Paradigm but you can get some of that metadata as well and what what the the important thing about this is you have customers remember that back to the picture we have Joe Smith who's a customer he exists he has all this information about himself the fact that it lives in a mongodb database versus um sbas database is s of beside the point right you want to get all the data you can about this person wherever it is so the more that some of these data moding tools can support that it really helps you get that bigger 360 view um some no SQL is not as friendly when it comes to metadata things like key value pairs kind of by definition I mean it's great that they're super flexible they can be super fast if you're trying to do something like get all my session information you click someone's using on a web page or something awesome if I'm trying to store long-term you know customer names and addresses little you know it doesn't have the more traditional metadata that we're used to if you just scan it often that's on the application code um so you can get some things um but it's not necessarily as robust in terms of we're used to kind of a relational land um motoring on through apologies but just to kind of give the idea a cobal metadata so for probably most of you on the call who've never had to De code these um think of it um in a way that's you know a bit like your data structure so here's a picture of a Cobalt copy book and if you've never seen one you can probably start understanding okay I see first name last name date of birth there's some kind of data types here I I sort of understand well a lot of these um data moing tools can kind of translate some data mining tools are better than others some are are starting to natively um natively model some of these artifacts the newer ones probably not going back in natively modeling cobal um but for things like a cobal or for things that they don't support often they kind of well I'll say sort of fake it right they kind of map it to a really Rel ational model which often is good enough I mean I think often we worry too much about the details so you know I could argue all day on this we have things like classes and we have things like tables at the end of the day if we're just trying to get simple things like the fact that we have first name last name and date of birth across these systems maybe we don't need it stormed as natively as um you know exactly how it looks in the native structure maybe we're just trying to get the high level metadata in some cases you do and maybe that is why you want a full metadata repository that many of them do keep it all in the native format go back and forth um and the reason I put cobal in here is not CU I am a dinosaur um but because it's actually growing um and partly because there's no people um to do this anymore they're all on the beach with a margarita somewhere laughing their way um that they coded this years ago um but that you know when it gets back to I just know I mean that's sort of the reason of of the beauty of metad that it should go beyond human beings that coded this and kind of leave the Legacy for other folks that might not remember how you coded it um XML is another kind of classic one and again um there is some metadata in XML itself um but when you think about if we're going to simplify things we're just trying to see where name address is used um a lot of the data modeling tools can get that in now and again depending on the tool some model it more natively which is more of a hierarchical structure um and some kind of map it to a data model so everything kind of looks like an ER model and there's pros and cons to each um but literally we're trying to do the broad Rush of where's all where are all my customer names um it could be helpful no matter what the format similarly a lot more folks are kind of moving to the Json kind of World um and again if we're thinking of I want to see everywhere where price is used across the organization Json has its own structure as well so again I could I could have could spend a whole day on the different types of metadata um but I guess just quickly just to point out that um either you have data models doing things like for Rel databases that you awesome sources of metadata um structural business metadata if you've been putting it in a lot of the data moding tools do support some of these other systems um most of them have some of these Bridges you can look and just take a look at them you'd probably be surprised at some of the sources between bi tools ETL tools some of these Legacy tools and often it's free as part of your license so look for it or ask for it um you might be pleasantly surprised if you don't have a budget for full metata repository and you kind of want to use what you have um there might be some hope there for you okay so I have talked a lot about tools and Tech and all that and big fan of that also a big fan of the bigger picture so uh just like you may do a data strategy or trying to understand how to use data metadata needs to be treated just like any other kind of data and really planted out so like anything who who's using it right so we talked about before some of the biggest users are business people um but also techie folks as well so you know could be a business person saying how did you define Regional sales in this report that's business metadata it could be a developer saying I'm going to change this field what am I going to break or what are the source of Target mappings if you're a data warehouse etc etc etc um the more data metadata you have the more folks are going to be using it I mean the weird thing about metadata it seems that we all hate creating it and we love consuming it right I mean just look I'm a big fan um if I haven't used the word nerds describing myself yet I'm going to go ahead and do that but yes I will on a Friday night be looking up open data sets or something um out there there's so much exciting information out there and when you find a site that has great metadata documented and you know who created it and why it was created and what the fields mean you just want to hug that person also I've seen stuff that's just crap it's just some field with x963 and you have no idea and you don't use that right so it's sort of your um you know Legacy to others that if you've built this data set please document it so other people can use it does not take that long um so please do it um so because other people do want to use it so when we think of publishing this information out to other users again one of the common artifacts um for metadata when we think of the business side is a glossery um so that could be the business terms their abbreviations who the steward security levels a lot of that a lot of the data modeling tools out there again I don't want to overstate you know there are beauties of a full repository there also a lot you can kind of do with what you have but a lot of the modeling tools have kind of figure that out and when you think of a say a logical model if you're doing logical modeling correctly you probably have something called um you know employee number or what interest is rate is and what you know first name means all those definitions kind of be published out to the glossery so if you already have some of this information you can help you know publish that make sure it can be uh gotten out to a wider audience so to kind of summarize that because again each of these tools in the mar I mean part of what could make anybody's head explode in the market is that especially with metadata there's so many different options but again when we're talking about medad tools that kind of was the topic of this conversation to kind of put it in context here's a little chart right so what data modeling tools and there's some examples of some of the names I'm very careful not to mention them you can figure that part out yourself um what they're they can do so I kind of have the big X for what they do well and the small X for kind of lesser support um so of course they do data modeling well they can do some of that lineage they can do some of the metadata storage you know they're probably not going to let you customize the medad model fully you're probably they have some glossery but it's not a full-fledged glossery if you want to do full-fledged glossery maybe something like a data governance tool like a cbra or diaku which is now in fortica or you know metadata repository sort of by definition folks like the SG and an Adaptive and data Advantage group they do metadata great they're not going to do is data modeling that's not what they were meant to right that's what a data modeling tool does um you know a spreadsheet I put there CU it's probably the number one competitor for any data management tool in the market you can do almost anything in a spreadsheet big fan I do a lot of stuff in spreadsheets especially when I'm testing things out I'll do it in a spreadsheet first so you could um they're really not designed for that so you could have a gloss scen a spreadsheet probably wouldn't be an a prise scale um so again there's no right or wrong answer to this so but so if you're already doing data modeling and you need a lightweight glossery think of that or you're doing data modeling and you want some lineage between bi tools that's fine if you're doing Internet of Things um and a lot of application coding and you have you know 17 different data sources and you want a true full lineage maybe you do want a metadata repository I guess is what I'm saying so but just you know this is kind of a helpful when is what I have uh good enough and if I'm doing a lot of data modeling I can probably leverage a lot of the tools I have um so again when we think of metadata management itself there are components to really make it successful we've been talking a lot about the tech that's only really tiny piece of it so you should have a strategy why am I doing this anyway right have I talked to all my stakeholders what are their pain points is it they need better definitions is it the technical structures what what are their goals what are they trying to do with it how am I capturing and storing it um so that's kind of what we were talking about already what what are my sources what is the best storage and publication me mechanism and then when we think of publication who who are those users those little you know people figures I had in the previous is it mostly the business the there and therefore maybe something like a glossery is it mostly Tech um do they want it in the data model can I just publish ddl out to a DBA um you know can I send code Snippets to somebody so again a lot of what these metadata tools can do is is implement it natively in somebody's day job tool so they may they don't have to go out to a separate place and look it up but if I can just import it into the tool I'm using and there there are the definitions sure I'll use that so give that some thought before you build the glossery but it's all dbas and all they want is the ddl I'm not going to go look up a glossery I want the code I want the actual data structure so to give that a little bit of thought before you start building your system and then how are you going to govern it so data governance is hot I think every company is looking at how they can have better governance of their data well you need to think of how you govern metadata so do you have roles and responsibilities specifically for metadata and there might be overlap um between some of your met uh metadata governance and data governance roles but it may be something completely different and do you have standards for metadata and do you have life cycle manag do you have metadata quality statistics right so we talk a lot about data quality and data profiling but are you doing that for metadata is for every data element is there a definition you know defined is it a business definition do you have privacy tags on it how are those tags cascaded across your organization is that metadata integrated with other tools or is it Standalone so again give that some thought because metadata is a thing in of of itself um so when you think of a metadata strategy kind of some of the components to look at is canist went enough business drivers and motivation so um I just did a big metadata strategy uh for a couple customers late last year and they were both probably on the opposite Spectrum one had one of the big Med repositories technically had every everything it was amazing they had full lineage they had glossery that all that but you know when they started tracking music user statistics nobody was using it they were spending a lot of time populating it but they really hadn't done that due diligence and talk about what people wanted to see or or publishing it or getting the word out another customer um had done a great job selling the need for metadata had talked to the governance team i' talked to the business team I before I walked in people were saying the word metadata really had a lot of ideas we really had to spend time prioritizing what we could do first because everybody wanted it and even though that second company technically was probably level zero on the maturity curve they didn't even have a medat repository I gave their chances of success a lot higher because they spent the hard time before they even did the tech um really figuring out the why and the how and getting Buy in and prioritizing and in a year when they have a small repository everyone will be using it and they can show ouri we built kpis and all that the other folks they had had a big expensive project and now they were trying to sell it and that was kind of putting as they say put the cart behind the Flor of the horse so awesome technical I don't want to knock them for that um but don't forget the business part because that's almost more important you can have a great one of my first metadata repository successes and I can't give myself S Credit it was my project manager at the time for a huge Bank in New York um we published a glossery and and I I remember being the techy whiz kid and I wanted to do so much more and they said no you got to get the Buy in and everybody used that glossery and then we built the SE messages and the lineage and all the stuff behind it but we got the Buy in first um so a bit a bit of a a tangent but not really that should be the number one thing um assess your med management maturity there on the right and I kind of mentioned that already where are you so it may be that everyone in the business wants full lineage and you have nothing so set expectations and we need to build that out or it could be hey we do have this great Med repository guys we could use it for this or hey new project B do you want to use what we have you know so give give that some thought um look at your sources and technology and when you look at a tool I mean that does seem so obvious um but when you are looking at you know vendors find what you need first so you know and and make sure to be clear with that when you're doing an evaluation do you support these six sources these are six sources I have I don't care that you have 400 scanners I have six sources do do mat these and be a little um be a little tough there because that is important and then consider your stakeholders and audience who's using it why are you publishing the information to the right uh people in the right way so when we think of publish we always sort of think publishing literally a web page a a newsletter that kind of thing but for some like I mentioned it might just be importing into their development tool again they see quick definitions I had one customer that built it into the agile life cycle so when they're using things like jira they kind of had their metadata or data questions around there so again think of who your users are it isn't always a business user it isn't always Tech it could be a you know a combination of those so gives that some thought and and like anything talk to people get direct feedback um find out what those pain points are you know some of these when you read through you they kind of are obvious you know where are the data structures right but but some are um a little more you have to give that some thought of you know we're spending this much money to clean up the data was because there was no metadata so have to make that connection for people of if we have metadata it'll help either problem um so then when you go back and solve the problem you can tell the people you know you didn't have any data structures now you do you don't know where the data standards are now here they are kind of helps you in selling it down the road um and I'm a big fan of probably because I started out modeling uh everything I do is a model and I'm a consultant so we have matrices and for everything but they're actually very helpful keep making complex things simple instead of our art and it can be really helpful so I'm a big fan of kind of here's one template we use of what are the business drivers write them down are we trying to get um to a digital organization can we explain to management that you can't be digital without metadata behind it no one knows where to go um and then why can't we get there well you'd like to be all digital self-service we don't have our data integrated to get there for example or it's too expensive to do it because the data quality is bad and then map that to the things you're trying to do often I do even a heat map of these six things are solved by these two things um to kind of show why you're doing it because it's often if anyone's ever been in one of my classes I generally have people do an elevator pitch right so if you go up to management and say you know we're going to rationalize all your data sources with a great metadata lineage you know they're probably not going to jump for joy that sounds really nerdy um but if you can say I'm going to help with your journey digital uh sales and help you sell more because we can have a single product catalog you know that might be more interesting so again link everything you're doing to some sort of business driver when we do talk about the tech again big fan of having some sort of um heat map not in ter only in terms of again if you're evaluating tools you're probably doing this already but if not write it down make sure these are all the sources I'm using when I did the in inventory you know my relational databases my bi tools I'm using open data and make sure you ask the vendor you how you support those um and they support it and then kind of do a heat map of who's using it right so here you'll see that everybody's using Oracle um and so again you may be looking at a tool and maybe they don't support we have one here that no one uses they don't support the open data but really only three people are using they don't use MySQL but only marketing is using that and we can probably live with it through some other means um so again sort of obvious but kind of create that heat map sometimes and I've done this myself you get so stuck on the say oh my gosh they don't support this and stepping back and saying is that really important or could we get 90% of the way with 90% of the coverage that we need so um again metadata roles and responsibilities do people have either specific job titles around metadata or as as is with things like governance is it probably a part-time role but they understand that metadata is part of their job so do you have um you know things like a metadata repositor administrator is probably the full-time metadata job um but folks like who are the consumers um are they that SS odd to be sort of compensated against that but when you think of it are you actively using the standards that are out there we have standards are people being held to using the glossery are you publishing metadata if I'm creating code am I documenting it um and then do you have someone like an executive sponsor that kind of gets why you're doing this and helps either carry the stick or talk about the benefits and that kind of thing for you um and then as I mentioned metadata quality and metrics so we often think of data quality metrics is it complete is it accurate is it up to date metadata is and just another type of data and so to keep track of that yeah I I documented this code last year well is it still current right so um I know when I mentioned open data several times but that's one of the first things I looked at is when is this data set published is it new is it from 12 years ago it really depends when you're doing analysis on it um so monitor that it could be as simple as are people using it are we showing hit rates is it entered um or you can get um very complicated I'm working with a customer now that's actually being very detailed U when they put a definition different components of a definition that have been filled in not a a part number is a number for a part right but actually there they have different metrics on how complete those definitions are so you can go crazy um you can be very simple but do do track it uh do give some thought of the quality and the metrics around your metadata so in summary um metadata is for the cool kids it's more important than ever it's always been important but I think it's I think more and more people with things like open data with data driven organizations with more especially with more business people looking at data they want to know what it means crazy idea um and data models if you're using them are a rich source of metadata so leverage that um you can probably do a lot with what you already have in house and you can augment it um especially if you're using some of these other sources um they can kind of fit nicely together and again while I'm the biggest fan of a full Med repository um if that's not in your scope and a lot of what you're doing is some of the things we've already mentioned often these meditator repositories can be just enough or even just the data models themselves um and don't forget the organizational considerations why are we doing this is everybody bought in are we tracking it um and you might laugh but have fun I mean there's so much out there and even metadata is evolving um and hopefully some of these webinars are showing I mean the new technologies and how they track it and um it's it's really kind of a fun time be in the biz um little bit about us we do this for a living so if you need help let us know um here's my contact information and Shannon will send that out I mentioned the white paper several times this is free for download not only on the data Verity site but on our global data strategy site as well I think uh Shannon will be putting a link to that in the followup um and again if you are so entrenched with metadata that you would like more uh we have a full online training for that as well so without further do I am a terrible multitasker but I did see a lot of questions coming through as I was speaking um but just quickly before that we get to that do try to join us next month as we talk about data wrangling and data munging and all of those new terms out there as well um so Shannon if we could open it up for a questions I think we've got about five minutes absolutely and we got a lot of great questions coming in um just to answer the most commonly asked questions just a reminder I will be sending a follow-up email my end of day Monday today with links to the slides links to the recording is not to mention links to the additional things that she's mentioned so far and anything else that comes up um so diving right into it Donna you know to understand the horizontal data lineage you need a a vertical lineage connecting the physical to conceptual the seems to be brute a Brute Force exercise and quite timec consuming any ideas how to shortcut that effort um it should not have to be um that Brute Force so uh there had two questions in there and maybe I misunderstood but the horizontal lineage um some of that again these scanners or or interfaces can kind of pick up um so some of it it can do the mapping with certain naming standards um some can kind of do best guess B based on the table matching so some of that can be automated um having those good standards in place always helps um the earlier you start so things like uh the top down that the questioner mentioned that can be a little hard to do after the fact um and so sometimes getting that into your best practice um early can help that a fact if I have a logical model and I Ford engineered from that um so and in some cases it's it's enough um to have a separate uh logical model and and kind of have as long as you have those definitions it doesn't always have to be fully linked to the physical um you know I'm a big fanal lineage but sometimes search works right so if I have that all in the repository and I see the definition of customer the logical level and I see the customer you know sometimes that works as well so I I would say get it all in there and then some of it can be added as you go or or some of it needs to be human mapping but a lot of it is kind of the Google approach right so at least getting it out there I would say don't be Afra uh don't be afraid to do a phased approach and the other part is you need full lineage for everything so what normally we I had a slide on it that I didn't show but you know if it's a behemoth effort to link that just pick the most critical stuff what what is Mission critical is it the C customer stuff for the customer CMM migration just pick that the 90% that we don't care about don't try to boil the ocean here it would be a behemoth effort so if if it's hard and it's just pick the critical ones and make sure that it's the stuff people are using sure so um in an organization you know how does metadata management and data quality how does metadata management and data quality affect the data model especially exploiting the data quality results of validity and consistency and do you have any use cases examples um they are very well linked some folks U link that actually in the data model they kind of have a um a field um for data quality specifics um some folks can use some of the metadata around the the model to look at whether it is um so you have the structure and you can kind of Base off the structure of the definitions can kind of and then sometimes data quality um is less the set of data quality that's the true profiling so I have some data quality I have some domain rules that the social security number should be in this format you kind of Base your your rules off that right or I have these domains that that customer name has to have 10 characters and once you have those rules sometimes you can be proactive and maybe the application at the front end can have a drop down with those domains and kind of nip it in the bud to begin with um and so a lot of these rules either can be done after the fact or proactively even better that if we know that the name is required or name has to be 10 characters or there's only a drop- down of three genders and you want to put those in then you can do that proactively the other piece of data quality which is often the bigger one is what does the data mean and sometimes it's been populated because people didn't know that you know what I mean by state it's the state someone was born in not the state they purchased in or whatever so sometimes it's the business metadata can help with quality in even a bigger way because you can have all the bits and bites right but if people aren't using it right it could be a bigger issue all right well I think we have time for one more question and I will get these questions over to you Donna that we've we've had a lot coming in uh one of the biggest challenges is tying all the metadata that lay scattered in multiple platforms databases data models ETL mapping what is the best way to bring this metadata together are there tools to help with you to help you integrate uh uh all this metadata there are tools so there's kind similar to one of the previous questions there's kind of different levels of the tools so most of the if we're just talking the data modeling the metadata rep proc stories do this too but data modeling tools have this idea of reverse engineering and many of the quick wins I've seen at the customer side is just do that reverse engineering against some of these sources and do that scan and just see the structures and in some cases say if you're thinking of something like a legacy system nobody even knew what those structures were and that could be a huge aha moment that can be a quick win just getting them all in one place and again to that previous maybe just searching across them is enough for now the rationalization part of how do I link these together in some cases the tools are smart enough either because the names are similar or you can create matching rules or there's some metadata linkage because your uh your tool supports your ETL tool things some of that can be done automated and then some may need to be more manual effort um in that case I would prioritize like I said to the previous user so it's probably a phased approach some of the stuff you may be surprised how easy it is to get in with something like just reverse engineer it you may say wow I didn't know that um some of the tools out there can do autom linking and then and some of it is probably you know there's always that kind of manual cleanup after the fact which sometimes a valuable step is kind of looking through seeing what you have all right well I'm afraid that is all the time that we have for Donna thank you for another fantastic presentation just love it as always uh thanks all of our attendees for being so engaged in everything we do what a lot of great questions that are coming in don't I'll get you those if you want to take a look at those questions uh remaining open uh and just a reminder again I will send a follow-up email by end of day Monday with links to the slides links to the recording of the session and all the additional info that Donna mentioned throughout including links to the white paper are the research paper from last year on metadata and uh links to the survey that's currently open for next Friday on data architecture and as well as her um online training courses that are more in depth on metadata so again thanks everyone I hope you have a great day Donna thank you so much thank you it's always fun cheers bye"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Management and Versioning",
        "Topic":"Model Metadata Management",
        "Video Title":"Metadata Management &amp; Data Catalog (Data Architecture | Data Governance)",
        "URL":"https:\/\/www.youtube.com\/watch?v=8i2tPWXpgMQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/8i2tPWXpgMQ\/hqdefault.jpg",
        "ID":"8i2tPWXpgMQ",
        "Publish Time":"2021-11-21T11:12:45Z",
        "Channel":"Software Architecture Academy",
        "Channel ID":"UCwlJazSg1PqeAVEPg1OyllA",
        "Transcript":""
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Management and Versioning",
        "Topic":"Model Metadata Management",
        "Video Title":"modeling and metadata management ea",
        "URL":"https:\/\/www.youtube.com\/watch?v=EePGjr9ddp0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/EePGjr9ddp0\/hqdefault.jpg",
        "ID":"EePGjr9ddp0",
        "Publish Time":"2013-11-14T15:52:41Z",
        "Channel":"MLT Creative",
        "Channel ID":"UCkLZhf-3I5sqTHeHisbfiBg",
        "Transcript":"hi i'm david dickman senior product manager for design tools at scibase and today i'd like to talk to you about modeling and metadata management for enterprise architecture to begin i'd like to first take a look at what is enterprise architecture fundamentally enterprise architecture is about these three things first it's about getting business and i t alignment so we can have agility for effectiveness in responding to change second it's about regulatory compliance now not just being able to ensure that we've met the regulations but also defending against donald and having the trace ability to do so enterprise architecture is also about preparing for whole business transformation here we're talking about big block change moving from today's infrastructure to the infrastructure to be tomorrow now let's take a look at how power designer breaks down all the things we need to know to be able to achieve this vision so here we have a view of all the different types of models that power designer manages for us and where they belong in the enterprise architecture now first let's take a look at these different layers for example here we have the business layer where we're understanding the business in pure business terms then we have the conceptual logical and physical layers where we start reflecting the business back to itself in technology terms and drive down to the detailed implementation we also have these different perspectives we have our information perspective for our information architecture our process perspective for our business architecture and our technology perspective for our application architecture so these different layers and perspectives start to make up a grid pattern this is something a lot of you may have seen if you studied enterprise architecture frameworks things like the zachman enterprise architecture or the architectural framework under togaf give us an idea of what types of things we need to put and where and when we're going to do that now this is really a generalized simplified view of those frameworks for our purposes and easy understanding so let's take a look at what power designer does for each of these places we've got things like the conceptual data model this is where we'll reflect the information concepts back to the business in business terms but we'll use those structures to drive into the logical understanding of how we'll start to organize data for storage and the physical implementation will go to the database these different transformation layers will give us the idea of how to relate business down to information technology we offer a similar set of models for the process perspective we have the business process modeling at the analysis level which understands the business processes and business terms but again those structures go into a platform specific model a business process model that starts to understand how process automation will be managed by technology and then we have models that understand things like ws betpel the web service language for business processes to actually automate and orchestrate the ac the business processes themselves we also have for the technology the use of the object-oriented model an analysis level object-oriented model defines the key technology pieces and how they relate to each other and that again can also be used to drive the more technical views the what the omg would call the platform independent model and the technical generating from the code platform specific model so all of these different models here represent different parts and different reasons for describing different aspects of the enterprise now tying that all together at the top here we have the business requirements model power zones business requirements model allows us to import and export from microsoft word and defines purely what are the business requirements and the motivations for developing the architecture and then we have the enterprise architecture model itself which is an abstraction a comp a series of composite models that understand how all these things roll up together to make the whole cohesive enterprise now just having the right place to store the right kind of information at the right time is is a good start but we really need to do is show how all of these things relate to each other so the first set of the technologies that power designer brings to bear on this is the linking technology what that's all about is taking what we know about the data information being able to relate that to the processes that use it this allows us to take the knowledge of what data is used by what process and store that intersection directly in the repository we can also understand how the data model and the application relates so where is the data used by the application and what data does that application need as well as from the process perspective what applications are used to fulfill the services required by those processes so this link technology allows us to connect within each of these layers the different pieces that need to relate so now when we need to see what the impact of changes on within any of these layers we have the visibility to do so by the way all of these links are bi-directional so when i know what process uses what data i also know what data is used by what process so now that we tie all these things together the next problem we want to solve is how do we go between the different layers so here we have the idea of being able to generate a conceptual data model into a logical data model the logical data model into the physical data model and these transformations the data in those transformations is also stored by power designer so we remember the heritage i can take any piece of implemented data and tell you what logical and conceptual models its definition was derived from these are bi-directional and also many-to-many relationships so i can have many logical models from the conceptual many physical models from the logical and this allows me to have a lot of freedom in describing defining and describing the whole of my enterprise we also offer a similar set of transformations here and here and this is starting to tie the entire framework together so now we have link and sync technology the last piece of this is being able to take the business perspectives and link them anywhere at any level that's appropriate for us to be able to take our architectural perspective and understand where it was derived from at any other piece of the architectural framework so power designer lincoln sync technology takes your enterprise architecture framework it makes it more effective and more efficient to manage change whether that change from comes from competition from regulatory compliance or from technology thank you very much and for more information or to download your evaluation version please feel free to visit us at sybase.com powerdesigner"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Management and Versioning",
        "Topic":"Model Metadata Management",
        "Video Title":"Metadata management",
        "URL":"https:\/\/www.youtube.com\/watch?v=KkC1Bj3Kt5k",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/KkC1Bj3Kt5k\/hqdefault.jpg",
        "ID":"KkC1Bj3Kt5k",
        "Publish Time":"2020-07-05T23:00:06Z",
        "Channel":"Google Cloud Tech",
        "Channel ID":"UCJS9pqu9BzkAMNTmzNMNhvg",
        "Transcript":"SPEAKER 1: Kubeflow pipelines are made up of a series of components that give you an ML workflow. Naturally, you'll want to improve those workflows over time, like their performance, efficiency, and accuracy. Kubeflow metadata is a central thing to help you do just that. If you want to check out the documentation directly, check out the link below to read more about metadata on Kubeflow. The goal of the metadata project is to help Kubeflow users understand and manage their machine learning workflows by tracking and managing the metadata that the workflows produce. Not only is it useful to improve your results, but it's important for audit and compliance reasons so you can know which models are in production and what their characteristics are. Here, metadata means information about executions, or runs, models, data sets, and other artifacts. Artifacts are the files and objects that form the inputs and outputs of the components in your ML workflow. Kubeflow comes with a metadata component installed by default, which is used for storing and serving metadata. You can write to the metadata server from your own code. And in addition, some Kubeflow components log metadata about their activities automatically. Then the Kubeflow UI lets you view the logged metadata. In particular, Kubeflow pipelines automatically log information about a run, including workflow artifacts, executions, and lineage. You can see this pipelines feature in action with the Cloud AI platform's Pipelines Installation. Models and data sets can then be visualized in the metadata web UI so that you can start tracking them out of the box with no extra work required. You just need to head to the Artifact Store in the left-hand panel. Behind the scenes, Kubeflow is handling the metadata through ML MD, a library for recording and retrieving metadata. ML MD is an integral part of TensorFlow Extended. But it's designed to be used independently. Kubeflow helps you use metadata for all components of your ML workflow. It comes with a metadata API and corresponding clients. And you install the metadata SDK, a Python library that makes it easy to write to the metadata server. Kubeflow Pipelines automatically logs metadata during a pipeline run. This lets you track the definition of artifacts for things like pipeline versions and when they were last updated, the metadata to evaluate an execution of your pipeline, like the run ID inputs and outputs, and the metadata about the pipeline and associated lineage information, so you can clearly see how artifacts and their associated metadata relate to one another. In addition to Kubeflow Pipelines automatic tracking, you can manually write to the metadata server to collect additional metadata like the metadata for a data set that forms inputs and outputs, like a description, path, and query; metrics used to evaluate an ML model, like the accuracy and learning rate; and the metadata for an ML model that your workflow produces, like the model type, hyper parameters and labels. Let's walk through an example of writing to the metadata server. I'm running a sample [INAUDIBLE] classifier model in my Jupyter Notebook server. After I execute a run, I can log a data set, model, evaluation of the model, and metadata for serving the model. Once I'm in the Artifact Store, I can see a list of items for all the metadata events that my workflows have logged. I can see an example of the model, metrics, and data set metadata with the name mytable-dump. Besides using the SDK to log metadata directly, you can also add your own metadata watcher to watch Kubernetes resource changes. It extracts metadata from some Kubernetes API objects and logs them to the Kubeflow metadata service. To learn more and try out this notebook and explore its metadata, check out the links below. And stay tuned to learn more. [MUSIC PLAYING]"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Management and Versioning",
        "Topic":"Model Lineage Tracking",
        "Video Title":"What is Data Lineage?",
        "URL":"https:\/\/www.youtube.com\/watch?v=Jar5Rr_7TOU",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Jar5Rr_7TOU\/hqdefault.jpg",
        "ID":"Jar5Rr_7TOU",
        "Publish Time":"2023-11-03T11:00:16Z",
        "Channel":"IBM Technology",
        "Channel ID":"UCKWaEZ-_VweaEx1j62do_vQ",
        "Transcript":""
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Management and Versioning",
        "Topic":"Model Lineage Tracking",
        "Video Title":"Lineage tracing approach | Tool and Method Development at Whitehead Institute",
        "URL":"https:\/\/www.youtube.com\/watch?v=ykM_Q_VMxkM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ykM_Q_VMxkM\/hqdefault.jpg",
        "ID":"ykM_Q_VMxkM",
        "Publish Time":"2021-11-30T14:59:05Z",
        "Channel":"Whitehead Institute for Biomedical Research",
        "Channel ID":"UCY9j_cXz6o5gG4h16sUdpUw",
        "Transcript":"hi i'm dan yang i'm a postdoc in the weissman lab i work on tumor evolution try to use this lineage tracing technology to understand how tumor evolves from one single cell to final stage like aggressive tumor we engineer cancer cells with the linear tracing machinery and follow their metastatic behavior so we try to understand how metastasis happen and we can track how these cells like leave the primary side and go to different tissues what's the kinetics and what's the roots of this process understanding how tumor evolves from this early stage to this late aggressive stage will help us to develop better therapies to treat cancer the mutation gives humor certain features and these features help them to adapt to the environment better than the other cells that's why over time these cells will expand more than than their neighbors over the past few years the weizmann lab have pioneered this lineage tracing technology which take advantage of the crispr cas9 technology when calcine bind our guide rna it can recognize this site and cut this site it leave a permanent insertion of deletion which can serve as a barcode because it's like a scar in the genome we on purposely choose the size that when you cut many different times it generates a diverse type of alleles in order to trace and label many cells we need a diversity of barcodes we integrate many different copies of the target site theoretically based on our calculation is about 13 target sites engineered into the genome we can uniquely mark almost every single cell in the human body and these cells will further divide and their delta cells will inherit all the marks from their parental cells this is basically when there's no cutting you're gathered very intact a nice peak the same size and when there's cutting you can see you start seeing this smaller size of peaks over generations we can based on the accumulated pattern of this insertion of deletions we can relate these cells back to the very parental stage and understand how all the cells in the at the end experiment we harvest all the cells based on these intel patterns we can we can identify how they related to the parental cells linear tracing technology can not only be used for understanding disease progression like in this case the tumor evolution but you can also use for any normal development for example how you derive from a fertilized egg to a whole animal how different tissue types are different cell types within each tissue are related to each other you"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Management and Versioning",
        "Topic":"Model Lineage Tracking",
        "Video Title":"TopBraid EDG Demo - Data Lineage Tracking",
        "URL":"https:\/\/www.youtube.com\/watch?v=og7IT-ULlB0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/og7IT-ULlB0\/hqdefault.jpg",
        "ID":"og7IT-ULlB0",
        "Publish Time":"2016-10-07T22:29:35Z",
        "Channel":"TopQuadrant",
        "Channel ID":"UC64kSd719Ld5LFZJAXVRnQw",
        "Transcript":"hello today we will share a demo of top rated enterprise data governance or top rate edge top right edge makes it possible for you to manage and govern a complete range of information assets different firms may have different priorities and starting points but with edge you can start incrementally for instance with business glossary or reference data and extend your governance scope to other assets when you're ready to do so beyond just supporting specific data assets such as catalogues of where data exists there's a need to connect the business technical and operational metadata and what's important is the top right edge enables not only the representation but also the connection of these data governance assets needed to answer questions such as where does this data originate top right edge supports an incremental approach to governance with the comprehensive suite of modular packages each is available is an initial configuration of edge and the packages you see here can be added in any combination towards your targeted scope of data governance today's demo focuses on an important aspect of metadata management the ability to trace data lineage in regulatory compliance recording it's critical to be able to prove to regulators where the data that you've reported came from so in our demo we're going to look at an example from finance the form wide 9c and we're using this form y 9c as a compliance example but the same kinds of lineage requirements would apply to other regulatory reporting forms such as the Federal Reserve's capital analysis and review see car and the data protection regulation GDP are from the European Commission data lineage tracking for regulatory reporting is not easy the form y 9 see that you see here itself is 65 pages long the instructions run to 556 pages it has a collection of schedules and number of supporting schedules and for this example we'll be looking at the schedule HCR HCR is important because it describes how well an institution is securitized and if you're held accountable for this data how can you know that you've gotten it right we'll show how to operate edge can help you trace values that you report and be able to prove where they came from so in our short demo you're going to see how easy it is to track individual data values and their lineage on the FR y9 see just by following the connections we're going to look at this item the s4 90 and as you can see in the screenshot at the bottom top rage edge models represent all of what's needed what's less obvious is how these various data elements are connected the form itself is composed of line items or amounts that derive from calculations depending on other inputs in today's examples we're going to trace this line item the S 494 the total on balance sheets securitization exposures and see where this data value comes from let's use edge now to find out so here we are at home screen of top rate edge you can see on the left that edge supports a variety of assets models and vocabularies for tracking discovery and reporting on the enterprise and what's even more significant is that these models and assets span the spectrum with connection from high-level models to the most granular data elements how does edge support data lineage for y 9 see well it's all about following the connections will trace the path and use this to visualize and understand and it's exactly this understanding that's going to help ensure compliance so we're going to select the bank lineage model to do this and going into the bank lineage model we're going to look at one aspect and that aspect is the regulatory lineage models here you can see that there's one about securitization and that one if we open it up you can see refers to the schedule HCR well this is good we could click on these and follow them to understand the connections better between the models the schedules and the data items and this helps us to be in to appreciate how things are connected but can it be easier and the answer is yes top right edge has a visualization engine that displays these relationships graphically and we're going to take a look at that now and see here that this securitization model actually produces the HCR as an information asset well where does it get the information to do that we can see that it brings the information in from a pipeline and what is a pipeline well it's a direction information flow and that flow can be fed by several pipeline steps as you see here and these steps are carriers of simple or transform data well something must feed the data into these pipelines steps and if we were to look we'll see that they're fed by a software program the software program will gather and ingest data from multiple inputs and transform these inputs into modified outputs so here we can see the several inputs and the outputs the result among them are the s4 90 that we're looking for we can see right away that the s4 90 is used on the reporting form 19 c and it gets there through this schedule HCR that's great what if we want to know where the data was coming from that was used to generate these outputs we can see these inputs here that we mention and look for example at one of them and we can see that input the exposure amount actually happens to be drawn from a column of a table called top rate bank securitization exposure and you can see the data type as well so what you've seen here is that schedule HCR is generated through an entire lineage of data that comes in from at schedule HCR pipeline that is fed by several pipeline steps each one using a software program to transform a variety of input data into the outputs that are needed and you can see the S 490 that we were looking for so what we've seen today is the top rate enterprise data governance is a new type of the agile data governance solution it uses a model driven approach to capture and preserve the meaning of the data and the data elements this modular approach helps organizations start with a simple focused area of governance such as managing a business glossary and still incremental e expand to a more comprehensive governance of critical business assets like data lineage with top right edge organizations are now finally able to connect their silos of data and metadata and to unlock the true meaning behind the business information if you're interested in seeing how top right edge can help connect your data please visit our website or contact us for a more targeted demo of top right edge thank you very much you"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"Model Management and Versioning",
        "Topic":"Model Lineage Tracking",
        "Video Title":"Machine Learning Data Lineage with MLflow and Delta Lake",
        "URL":"https:\/\/www.youtube.com\/watch?v=3-YytTKi5fk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/3-YytTKi5fk\/hqdefault.jpg",
        "ID":"3-YytTKi5fk",
        "Publish Time":"2020-08-19T15:52:50Z",
        "Channel":"Databricks",
        "Channel ID":"UC3q8O3Bh2Le8Rj1-Q-_UUbA",
        "Transcript":"- Welcome to Machine Learning Data Lineage with MLflow and Delta Lake. Hi. My name is Richard Zang. I'm a software engineer from Databricks ML platform team. Prior to Databricks, I was a Software Engineer in Hortonworks working on Apache Ambari. Before that I was a Software Engineer in Opentext Analytics building the company's BIRT iHub Visualization. - Hi everybody. My name is Denny Lee. I'm a Developer Advocate here at Databricks. I was previously a Senior Director of Data Science at Generic Concur Also Principal Program Manager at Microsoft. Also known for what is Project Isotope which is also known as as Azure Hdinsight. So thanks very much for joining us. Back to you Richard for the agenda. - Today Denny and I will provide a brief introduction to MLflow and its model registry feature as well as Delta Lake and it's time travel feature. Then we will show a live demo on how to use various versioning features from these two frameworks to achieve data lineage in the machine learning process. We know that Machine Learning Development is complex. To give a sense of it, this is a typical machine learning pipeline. You take your raw data, you do some ETL or featurise it or data prep. Then you want to do some training with this data to produce a model and deploy this model to production. Score head produce a REST API serving layer, run it through spark et cetera. And then when you get new data, you will reiterate the process again. Many organization using machine learning are facing challenging storing and burgeoning their complex ML data as well as a large number of models generated from those data. To simplify this process, organizations tends to start building their customized machine learning platforms. However, even such platforms are limited to only a few supported algorithms and they tend to be strongly coupled with the company's internal infrastructure. MLflow is an open-source project designated to standardize and unify this machine learning process. As you can see that MLflow has four major components; ML project, packaging formatting your reproduced runs and make it available in any compute platform. MLflow models allows you to generate model format and standardized deployment options. MLflow tracking allows you to record and query experiments and lock metrics and parameters. And finally model registry allows you to have a centralized repo to collaborate in the model lifecycle management. Most machine learning library lets you save the model file but there isn't any good software to share and collaborate on these files especially with a team. If you're working alone, you can probably check the file into a git repository. You may need to name the file somehow to keep track of your model versions and hopefully it's still manageable because you need to actually remember what you did to come up with these versions of files. If you're working in a large organization with hundreds and thousands of models and each of them has different versioning for many different reasons, this management... This management becomes the major challenge. You may ask that where can I find the best version of this model? How was it trained? How to add documentation for it? And also how can I collaborate with my colleagues to view the model? Inspired by collaboration software development tools like GitHub, we launched MLflow Model Registry which is a repository and a collaborative environment where you can share and work on your model. You can register named models and create new model versions for your registered models. You can comment and tag your register models and model versions so people can collaborate with you to quickly find the latest version of the model and relevant information about that model. It also has a built in concept of lifecycle stages. Like each model you can have versions that are staging production or archived and it provides a serie of API for you to easily interface with the model registry and you can do it automatically and test it with your CI\/CD pipeline. So the new workflow is that as a developer, you log your model into the model registry and work with any type of model alone as you can package it there then your collaborator can go to the model, manually view it or use automated tool to test it with the MLflow model registry API. Then the downstream user can safely pull the latest model after it's been reviewed and check it if it works and you can also use automated jobs or serving services for, of your choice with your latest model to do some inference. And we can see that the data lineage through the MLflow model lifecycle is as follow; it starts from training data set you ETL from the raw data. You may have different versions of the training data and relevant parameters and metrics as well as the model file can be logged in the tracking API. Then the MLflow tracking component... Then in the MLflow tracking components to run details page, you can register a new model or create a new version of an existing model. Finally you can manage different version of the model and their life cycle stage in an MLflow model registry component. That is part for me. Let's welcome Denny to talk more about Delta Lake. - Hey, thanks very much Richard. So we have many sessions throughout summit that talk about Delta Lake so let's just focus on some of the key components here. What you really need for proper model data lineage is reliable data and that's what a data engineer dream. Is that they're able to process data continuously and incrementally as new data arrives in a very cost-efficient way without actually needing to choose doing batch or streaming. So underneath the covers we're talking about Delta, what's Delta on disk? See, it's a transaction log that actually has your table. You see the del underscore delta underscore log and the action json files that you see there plus the parquet files themselves. As you'll note, there's the table versions and also optional partitioned directories that you're working with. The data files are actually your original parquet files that you're used to working with, a package together is now your Delta table that ensures ACID transactions. So that way not only do you have reliable data but you also have a transaction log that now we can go back and look at what the old data looked like in when you're modeling, watching your ML models as well. And so the key aspect of implementing atomicity is that you wanna be able to make changes to your table as they're stored as ordered, atomic units called commits, all right. You have your first file 000 json here. Then you have a second file, the 01 json. If I'm adding one or two parquet files that's recorded in the first json or the zeroth json, in the second json or the first and zero one json, that actually records the removal of the first and second parquet files and actually adding of the third parquet file, all right. What we wanna be able to do is solve these conflicts optimistically if the two clients are trying to run each other at the exact same time. For example you wanna be able to not the record start version, the record\/ writes, any attempted commits and if someone else wins, check of anything that you tried to read has changed as you can diagrammatically see here. So that's it for this the slide portion of the session. Let's dive into the demos. - In this demo we're going to show you how to use MLflow model registry and Delta Lake time travel to handle data lineage in machine learning process. We will also show you how to use various versioning features from these two frameworks to troubleshoot data versioning problems to achieve reproducibility for your experiments. Here is a notebook where we are going to run some machine learning code with the box with the Boston housing data set prepared by Denny. The Boston housing data set contains a bunch of columns like crime rate, number of rooms, percentage of lower status population. Our objective is to use this data set to train a linear regression model and use it to predict home values. We have few pre-run cells doing some data preparations and visualization. You can see that we create a data frame by occurring the Delta table and then converted to a pandas data frame. From the scatter plot matrix here you can see that the number of rooms and the percentage of lower status population are having positive and negative linear correlation with the median value of the house as shown here and here. But we can see it even more clearly in the following two separate scatter plots here and here as well as on the bar chart that's showing the correlation from all columns to the median home value. We then define a list of more readable column names and we drop all the rows without median home value for data cleanup. After reviewing the correlation coefficient matrix and scatter plots, let's choose features that have a strong correlation to the median value. Let's say we will choose the columns with the absolute value of correlation coefficient are greater or equal than 0.4. And then we do a train test split, 80\/20 train test split for our training and testing data set. And here shows that we're gonna try different learning rates and choose the one that yields the lowest RMSE. And we have two training session with ridge and lasso regression respectively and let's run the training sessions. As the training session is running, let's take a look at our training function. Our training function takes our training and testing data sets and the regression type as well as the learning rate alpha. At first it creates the MLflow run and initialize the linear regression object based on the regression type then it fits the training data set and collects all the training prediction outputs. Then it calculates our RMSE and r2 metrics and use MLflow API to log all the parameters and metrics. It also logged the linear regression object as a SKlearn flavored model. Finally it creates a prediction error plot plus a residual plot and log both of them as run artifacts using MFflow API. After the training process, we can see a list of runs showing in the notebook run side bar. Let's choose RMSE and let's sort ascending by RMSE and we choose the lowest RMSE and go to that run. Now we can see the run details page. In the run details page, we can see that the parameter and the metrics that we logged in the notebook and in the artifacts section, we can see the ML model file indicating the SKlearn flavor model we logged and the png files for the plot that we logged. Since this is the best run we have, let's register a model using this run. To register a model, we first select the model folder in the run artifact section and let's click register model and choose to create a new model and let's use Boston housing demo as the new models name and let's click register. And we can see that the first version of this model is being created. Let's wait for it to finish creating. Now it's finished creating the new version of this model which is like basically the version one of this model. Let's go to the model version page. So this is the model version page. We can go back to the registered model page and see that version 1 is the only model version we currently have. Since I wanted to collaborate with Denny on this register model, I'll give Denny manage permission of this model. So add Denny here and then I choose can manage at him and click Save. If you want to load this model in our notebook, we'll need to switch this model to the production stage. So let's say ship it, okay. Now our model is in production stage. We can go back to the notebook and there is a cell at the bottom that we pick a row from our data set, our training data set and to test the model and see the prediction. Let's run the cell. As we can see that the prediction is 23.7925 which is pretty good. - So one thing I had noticed when working with this notebook is that as you can see from Richard's model, if I go ahead and dive into it a little bit. He was actually using a different version of SKlearn. He was actually using 0.22.1 and I actually wanna use a different version of it. So what I'm gonna do is I'm gonna go back and rerun the whole thing and you'll notice that I can jump right into here and find the lowest RMSE. So I'm just gonna pop that open and I'm gonna quickly go ahead and jump this over to and deploy a different version of the MLflow model. All right, give it a couple seconds here. So I'm gonna grab this one, this model. I'm gonna register this model similar to how Richard had done before. I'm gonna register it and then now I'm on v2. It'll take a couple seconds for it to go through, just to make sure. I'll just take a look at real quick, transition that to production so which will automatically... Perfect. So now that I'm good to go, I'm gonna go back to my notebook, I'll close up the runs and I'll go ahead and rerun this particular cell again. And as it goes through you'll see again a value of 23.79 and based on the original median value of 17.8. But let's say now I want to go ahead and replace the null values that we actually had. So remember that this particular Boston Housing kaggle data set, there are 504 rows but 333 of them actually has values, a median value. Well the remaining 180 or so don't, are null, okay? So I'm gonna go ahead and instead use this particular model to update my table. So I'm gonna actually fill this with new values and so here you go. So right now what I'm doing is I'm updating our Delta table with, in this case it's matching by ID, I'm gonna update that value with the new values that were calculated using this particular model. So if I scroll down and take a look at the values, you'll notice that basically I have not only the original values inserted in... Perfect. Apparently scroll to the right here, okay. You know so with either one or two decimals or none for the median value but also the new values if I scroll down. There you go. These are the new values that were predicted by our model and we've inserted the back in. Now fortunately for somebody like myself, this' probably a bad idea but fortunately because I'm using Delta Lake, I've actually saved all this information because we actually kept the transaction which Richard's gonna show. But meanwhile I'm just gonna go ahead and hide the fact that I did this and delete the cells, okay. So now I've got the updated data which is sort of incorrect using the updated model, all right. Oh! And then let me go ahead and run this all over again and I'm gonna register the new model based on the updated data which probably isn't the best idea but again I've got a new RMSE value so let's just let it run through and we'll see what ends up happening. I'll have the new results coming in and I'm gonna register this as a third model. All right, perfect. So we're almost finished. Excellent. So now let's go back to this. I'll go back and choose RMSE. Now I have an even lower one of 4.331. I'm gonna make this my new model. So I'll go back here, register this as a third model of our Boston housing demo, transition this to production as well, click on okay. So if you look at the models, you'll actually see the three models. There's the first one that Richard created using a more recent version of a SKlearn. There's a second version which I ran which actually has a older version of a Sklearn and the third one in which I've went ahead and re-updated, updated the median value data. So if I was to go back and run down , I'm gonna keep this cell for the purpose of understanding what's going on. I'm gonna run this one but this one's now against the new model against the new data. And it helps if I actually write the correct name. And here you go! You'll notice that actually same something point but instead of 23.79, I have a value of 23.13 now. All right. So that's it for this part. - Now I'm back to the notebook. What I want to do this time is to retrain the Boston housing model and see if we can reproduce the exact same result. However, I noticed that Denny has rerun the notebook and created a new prediction. The code looks the same but it generates a different prediction value. Is he using a new model? Let's go ahead and check. Let's go to the model page and search for Boston. Here we can see that there's already three versions in the Boston housing demo model and the version 3 is now in production. No wonder that prediction is different. Let's check what's different in version 2 and version 3. So in version 2 Denny says that he switched to 0.20.3. This looks like a SKlearn library version. I think if we want to reproduce the the same result we just need to use the previous SKlearn version which is like a newer version of SKlearn and that's fine. Let's check version three. In model three, Denny says that updating to include predictive value for median value. This doesn't look very correct. Wow, if I understand it correctly, he probably has updated our training data set in the Delta table with some value from the prediction output. That isn't sounds like a good practice in machine learning. So let's check our Delta table and see what's happened to it. Let's go back to our notebook and then you can see that now I'm using a cluster with the latest scikit-learn version and let's check our Delta table here. And in the Delta table history, we can see that there's a new version created by Denny and from the operation metrics, we can see that there's 173 rows updated and the two number upper row is 506, okay. Let's check zero version and like V 0 and V 1 version respectively and see what's the content of the two versions. So the V zero looks pretty legit and it has a bunch of rows that with the median home value bin no and the second one, it looks, everything's filled and wow, these looks like what Denny says that the predicted value right. This doesn't sounds right. So if I wanted to reproduce my training and my experiment, I'll need to overwrite the tables back to the V 0, okay. So I'm gonna go ahead and do that. Okay, done. Then let's check the Delta table again. And we can see that we have another new version which like me roll back the previous v-0 and that will like revet back our data set version given us the data lineage on the data set, okay and this is pretty much what we need to do for getting the data set aligned and then we have the same data set and the same library environment and now let's run the training and see what's the output. And to have a clean slate for the training, I'll need to clean up the previous experiment runs. So those are the previous runs, just for convenience of reproducing with a clean slate, let me delete all those old runs. Okay, deleted. Let's go back to the notebook and refresh the run sidebar. All the runs gone, click save. And then let's go ahead and rerun the training process. So all the way to here. So let's run everything above the prediction, run all above and after this training, new training session with the exactly same dataset and exactly same secular inversion, I will register a new model version with the same way that we did earlier which is like to select the run with the lowest RMSE and then use the run artifact of that run to register a new model version and looks like it finished running. Let's open the run sidebar and then choose RMSE and store ascending, we'll see that this is the lowest RMSE and let's go to this run and register it as the new version of our Boston housing demo model. And you can see that the v4 of the model has been created. Okay, it's finished. This is the v4 of model and let's make it to the production and let's do some prediction based on the v4 of model which is currently in production state. And let's copy the prediction code here and let's run it down here. Mm-hm, 23.7925. Looks like the same value we got before. To confirm, I will go back to version 1 and make version 1 production and then try to run the prediction again because the prediction is always based on the current production model version. So I wanna see if version four and version one can output the same prediction value. Let's run it. Nice, exactly the same, 23.79. Cool. So to recap in this demo, we first train a linear regression model from the Boston Housing dataset and create the model version v1. Then we messed up with our library version and original training dataset in the data table and after we found the problem, we used Delta Lake's time-travel feature to switch back to the original version our training dataset and rerun the training process with a consistent SKlearn library version. We end up reproducing the same result we had in our previous experiment training session. This is the end of our demo. Thank you. - Well, thanks very much Richard for those awesome demos. Thank you very much for attending our awesome session today. If you want to go ahead and dive in more, please join us @mlflow.org or Delta.io for more information. Thanks very much and have a great summit."
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"CI\/CD for Machine Learning",
        "Topic":"CI\/CD Pipelines for ML Projects",
        "Video Title":"Build CI\/CD Pipelines for ML Projects with Azure Devops | #MVP Connect",
        "URL":"https:\/\/www.youtube.com\/watch?v=xbgMqCuWgzs",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/xbgMqCuWgzs\/hqdefault.jpg",
        "ID":"xbgMqCuWgzs",
        "Publish Time":"2023-04-13T01:51:16Z",
        "Channel":"Microsoft Reactor",
        "Channel ID":"UCkm6luGCS3hD25jcEhvRMIA",
        "Transcript":"hey everyone how you all are doing hope you all are doing great my name is paru event program manager for Microsoft reactor India good morning good afternoon and good evening to you all who all are joining us globally and Welcome to our MVP connect before I bring in Sonia who is our guest speaker for today's session let us go through our code of conduct we are all here to learn together so please please be respectful of other people views understanding of differences being kind and considerate in the way we engage yes obviously we do encourage you all to participate please drop all your questions in the chat section and then we take questions from there yes I now like to bring in Sonia for uh today's session hey hi Sonia hey hi par hi hello everybody hey folks Sona is in a ghost mode today my network is um a dodgy so uh so I won't be sharing my screen uh sorry I mean sharing uh video um but I hope uh you're able to hear me properly yep we can hear you and we are live yeah yeah so Sonia what do we have for to disconnect um so as you can see on the slide we're going to discuss us um uh we're going to have some architectural discussion around uh how to build cicd pipelines uh for uh sample machine learning project with the Azure pipelines right so um actually uh those who have joined I just want to set expectations um so that you you you will figure out if the session is for you or not so so what can you expect from the session are these three points right so first you'll be uh we'll be discussing uh some architectural U um nuances of U build and release design for machine learning projects and then we'll do a endtoend demo um on the same thing and then I'll lastly um walk you through um uh the demo generator so basically using that You' be able to um demo the entire thing by yourself right uh so I'll share some of my learnings and discoveries that I made while doing this project so this is what you'll be getting from the session today um but at the same time these are some things that you should not be expecting because we just have very limited time so I won't be able to cover the uh the basics of uh Azure devops or Azure ml um and we are not going to do any Deep dive of the code right so there is a code base we have scripts but uh um I would leave that uh for you to um go in depth after the session I think that would make a lot more sense um and it will give you a lot of time to reflect as well so yeah that this is what we have B I hope uh yeah please please the floor is all years you can go ahead and start the session viewers pull back we have a fantastic session coming in now what to you yeah thank you P thank you um okay so uh before I begin the session um just a short intro about uh myself um I I'm Sonia mea and I am currently working with EXL um EXL Health Care um and uh I'm helping them with ML mlops and data engineering as well so I worked on um I worked on computer vision projects NLP projects uh building Cloud native applications um um as well and uh this is my LinkedIn and uh GitHub um references here so if you if you guys want to connect uh please I'll be happy to happy to um connect with all of you so yeah let's um let's jump into the session right so um so to just to just to give a brief like um like a warm War up before the session itself so we're going to talk like let's just like walk through the ml application life cycle so from um a bird's eye view we all know that uh it has three steps right so first we we prepare the data data is basically the entire thing driving the machine learning project so there are a lot of steps uh involved in cleaning the data uring we have selected the right features we have fine tuned the features and we have created a perfect data set okay and once we prepare the data we come to the experimentation stage and this is where we try um a plug-and play of different models um and usually we use a jupyter notebook or we use vs code we choose our favorite ID we build the model and we tweak a lot of coefficients parameters here and there learning rate and all of that and then we we we figure out our best model uh so experimentation is the only thing that drives so once we figure out the the best model uh we test it with some new sample data see if it's generalized well enough and now comes to the deployment stage right so this is where your ml application becomes useful to the end users and one of the ways to deploy it is as a web service okay it's one of the most common um methods so web services okay before going there um um what we generally um do is in in in any um uh corporate setting if you look at how we approach ml projects is we take the model and because it has certain environment dependencies we bundle all of those environment dependencies into a Docker image right so a Docker um or any kind of image uh building uh tools that are available in the market so you use that technology to basically bundle up your dependencies and create a container right now this is basically uh the backbone of the model so that this can be run anywhere on any kind of infrastructure and once we have that Set uh then we can deploy it in any way we want it right it's a bat service or a web service if you want to create an endpoint so web service is basically and uh creating an API uh basically an endpoint so you hit the endpoint which redirects it to a server which is hosting a model and we basically send a Json input um that is our sample data and the Machine learning uh model that is deployed will take in that data Churn it give us the result back so endpoint helps us to coordinate all of that activity so that is a web service right so a lot of tools available engin uh gunicorn flask Django so these are some of the tools that we have in the market which helps you put your web service together right so within this this uh application framework some of the uh common issues uh that um Architects face right or even the developers face is firstly running experiments right so we all know that in the experiment phase we we would um there be a lot of times when the infrastructure is not sufficient we want to run a model on a better Hardware um um so we basically want to test it on different VMS with different CPUs and RAM um configurations and at the same time we also wanted to uh we want to keep a track of all the models that we're running right so we need a mechanism in which we can uh basically um um log the model um the model metrics and the version of the model right and the changes that we made to the model so it's basically keeping a track of all the versions so that lack of that creates a lot of issues right and then we have validating solution which is basically becomes um how good your solution is and and you come up with the discussion of um you know which metric should we be looking at to validate and thirdly it is deploying Solutions a lot of times we face this issue of how do we um make our deployment very seamless right so every time you train a new model how do we create a docker image and push it as a web service very seamlessly right generally if you look at um the um um the way um n uh the naive approach to this is there'll be two teams right there'll be one team which will be developing and giving us the pickle file and then there'll be an operations team which will sit and make the image and then put it very it's a very manual approach involving a lot of teams and manual effort right so there is a lack of iterative deployments and uh obviously because we have two teams and all of that there'll be a lot of long chain of approvals as well and another issue is retraining model right so as in when we want to we get some user feedback and we want to go ahead and change something in the model um there are lack of reusable scripts correct and we have also um we also have this um um this uh inability to decide which thresholds should be used for triggering this the training even right like let's say I make some changes but the model which is already in production if it's good enough why do I have to retrain it right the train okay even if I train it I don't have to deploy it right so we need some thresholds to make that kind of judgment that evaluation right and then we have Monitoring Solutions so even that's a big challenge common issues are not a lot of teenss are able to track the data that's coming into the model the they're not able to capture the data trends they're not able to capture the performances the performance of the model uh so these are the common issues that uh are faced right so uh today what we're going to see is um basically um these two Services right and how they both combine to solve these multiple issues that we discussed in the previous slide so Azure ml is a service which um provides a cloud-based environment okay where you can you can do plethora of activities you can develop you can train you can test deploy manage different experiments version them and track them as well right so basically Azure ml has these several apis or sdks right so you just have to call that function and you'll be able to you'll be able to register it track the model you and it has all these inbuilt functionalities you don't have to um um sit and uh install the third party TOS to do it yourself right and we have Azure devops so Azure devops is again the service as I've provided the definition it is basically a service which provides Version Control uh project management automated builds testing and release management capabilities so it covers the entire application life cycle and enables devops capabilities right so what we're going to see today is the amalgamation of both these services so from azure devops we're going to have a CO a code base right we we've written a code base in that code base we are using Azure ml SDK and that SDK is going to basically um develop the model uh sorry it's going to create the workpace we're going to train the model we're going to uh create a Docker image we're going to deploy it and everything will be done using Azure mlsd K okay so um so this is what we're going to see um so yeah let's do um a deep dive right into um the first part which is the build build pipeline so um when it comes to normal applications like net or something the build is a little bit different from how machine learning works so in machine learning build you have to see it from the perspective of training and building the docker image okay building the scoring image so let's see how to automate training and building scoring image on Ado so i' I've just put up um um um like an architecture diagram okay now this is going to make it easier for me to explain uh when it when we come to the demo um so okay so let's look at this image so what do we see here um we have a Linux VM agent okay so let's imagine you're giving a you you you've been uh given a VM okay it has Linux um Ubunto installed on it right and um now what you have to do is your task is you'll have to create um you have to automate the training of the model and creating the scoring image right so um so one of these designs I've captured in this uh this diagram is um you ask Azure devops to give you provision of VM for you okay and you ask uh an Ubuntu version VM and on top of that VM we're going to install some dependencies right so obviously we need python so you define which version of python you need right so Azure devops internally has certain tasks okay where it can install python for you you don't have to do anything right you don't have to write a script pip install anything it will it has a inbuilt module you just have to call that right so first um you set your environment on the open to layer on the host machine so you put the python layer and on top of that you install asure CLI for authentication purposes and for other purposes right so Azure CLI then you put your Azure ml SDK then you put other libraries that your model would need right for training like psyit learn numai pandas whatever so you put all of them right so once the layer is ready and all the environment is completely set okay now you go on to the next step which is as you can see the first step is create get or create workspace okay so basically basically what this script is going to do is um it's just going to call um an API right through the SDK layer it's going to call an API which will um um okay okay before we even go to 1 2 3 4 five I want to cover this part right so what you see here is basically the volume which is mounted on the VM right so this volume will have your source code okay we have your configuration files B basically that configuration file can have your um can have your subscription ID whatever Resource Group name that you want to give and uh and uh um you know the workspace ml workspace name that you want to give so you can uh establish those configurations here and each of these steps can generate some intermediate output file right so it can write into some file and that file will also get stored in the volume now this volume is a source of truth because once the build is completed this is the volume which will become an artifact okay so artifact think of it as a um as a as as a package like that you want to share between teams that you want to share between projects okay so after this entire build is completed it's going to have along with the source code it's going to have a several intermediary files which is then going to be used for our continuous delivery pipeline okay so this pipeline look at it from the continuous integration perspective right okay so let's go back so we have the VM we have installed all the dependencies on top of it now what we're going to do is we're just going to create a workspace first now why have I put the Azure ml uh thing logo here is because this is this is basically going to create an Azure ml workspace for us right now when we do a creation Now creation activity will happen when this script internally will check if on Azure for my subscription do I already have a workspace created with this particular name present in my configuration file if not let's create a new one so it's either get or create reuse the same one or create a new one so let's say it doesn't find something so it'll create a workspace and internally Azure ml will take care of deploying all the uh Services right so internally it will it will deploy a keyal storage it will deploy um so keyal is for basically all the access permissions that are needed then it will have its own um um um Studio ml Studio that's going to create it's going to create a storage container as well for you right so it will take care of all the uh things right so once we have that set up and a resource Group is created now it's going to go to the next step okay now in this step the way I've designed it is it's going to train the model now okay so once we have the workspace ready we have a script in the code base okay and we're going to instruct Azure devops to run that script on the Linux VM itself now this Linux VM is basically the host agent okay so we are not provisioning any other machine because I'm running a very um um a very simple model which doesn't need a lot of uh RAM capacity or um CPUs so I'm going to run it on Linux VM directly so it will call a training script okay and it's going to run it now uh this training script again it's going to um um because it's running on the local environment it needs some way to register it within the Azure ml workspace right so we are again going to call an API say Azure I mean indicate to Azure ml that hey we have started an experiment on a local machine and once the experiment is over we're going to say yes the experiment is over right so we're just going to register and EXP experiment in Azure ml workspace with a with certain parameters right so once the training is complete it's going to say experiment completed nothing is happening on your Azure ml Studio no VMS are created here it's all running in your uh host agent okay so once the training is completed okay so we have the pickle file now okay now what we're going to do this intermediary step the second step is going to write some output okay to our volume okay so the the way I've designed it is this step is going to write the details of uh the the experiment name and uh the the model name the model pickle file name and everything is going to sto be stored here and output pickle file also is going to be stored in some output directory here okay now we come to the third step now this third step is basically going to check it's it's basically an intelligent check okay of whether you should deploy this model or not whether you should even continue with step number four four or five now how do we take that decision so basically before um so once you have trained the model we know certain metrics of the model right so it it might give us so if it's linear regression we might have um um msse okay least um um uh we will get some errors and metrics of the models so once we have those uh rmsc or these metrics right um we can use those metrics and compare it with the model that we have in production currently okay so imagine that I ran this build pipeline before and it worked and it is registered a model already in my workspace so that will is basically the model that is gone into production so I'm going to take the latest model that is running in production I'm going to compare the metrics of the latest model the model that I've trained and the model deployed and upgrade this model uh the the latest train model if the metrics are better okay and I'll upgrade it okay so this is basically an evaluation checkpoint and once this is cleared and we know that yes we want to upgrade it it's going to move to step four if the metrics are not any better it's going to skip four five and it that's it right the build process is going to be completed it's not going to generate any intermediary flights later on right so let's say we have a good model with us so we go to step number four now this is again going to call an Azure ml uh AP and it's going to register the train model and Aur now why do we do this step why do we need this step now this is so that we have a model versioning in place right we know that this is the model which was deployed on this particular date it was registered on this date and these are the scores these are the tags and we are letting you know that okay this um um this has been basically this has been registered on Azure ml now right and the pickle file has also been uploaded there so that's why the Azure ml symbol here because we are calling an Azure ml API now once we have registered it now we come to the final and the most important step which is creating the scoring image now why have I put a Docker symbol here and why have I put a Azure ml thing now generally when you want to create a image you know we all uh manually install Docker then we do a Docker we create Docker file then we do a Docker build and all of that right um if use Azure ml that entire process can be um outsourced you don't have to bother about anything Azure ml will just need certain parameters from you so you'll have to uh point it to the you'll have to give it uh a couple of things right first you'll have to give a scoring script to Azure ml so you have to say that this is my execution script which needs to be the one deployed as a web service okay later on I'm going to show you in the demo how the scoring script is written right and uh so once you have the scoring script ready so you just call an API to say Azure ml that create a image for me with this execution script with this particular um um image name uh and these dependencies right so we can create an we can create a text file with all the dependencies uh like similarly like all these dependencies that I have shown in uh for the uh on the host machine right so we can create a similar um cond dependencies do yaml file and we can have that along with the scoring execution script and everything we can send it to a jml and it will internally take care of deploying the image for you okay so once the image is deployed we are done right so now this will also write some intermediary output this will also write some intermediary output this will say what where is the image located which container registry it is gone GTO and all of those things right so it's going to communicate and it'll write everything into the volume now this volume has become our artifact now this is an end to endend build process okay um any questions here like any anything that um obviously I'm going to show you the Cote scripts later I'm going to show you the end to end demo but this is just a rough architecture of how things are going to be moving right um so just just like to take a pause yeah even I'll have a sip of water okay um so um this is the build pipeline right so let's um hey Sonia sorry to R in this we have a question yeah can we manage VM Source image with external sources like uh mask or CNN can you repeat the question can we can we manage the VM Source image with external sources like mask or CNN oh you want to manage the VM Source uh VM image itself so you're talking about this virtual machine right which is provisioned by Azure devop so there's a doubt here like which VM are we talking about this VM or the VM that we can potentially train the model on so where are the security controls Locker image creation can they be locked down so only the ml model updates into the docker yeah into the same VM this thing yeah okay so I'm not sure what mask rcnn generally does for VMS I not sure about process but what I believe is azure devops will only give you set set of images right you can either have Mac OS installed Ubuntu installed or Centos I think so you have these three four kind of images but on top of that you have all the flexibility to um to install uh other things right like uh so as I mentioned you have flexibility to do anything on top of it and once you have that ready oh yeah so you can have your mask rcnn installed if that's what you mean and um T of flow whatever it is um installed on Azure devops VM yeah and coming to the docker scoring thing see the security it works in this way right Azure ml internally it will it will require certain parameters so I'm going to show you that script and that exact call actually so initially I didn't plan to do the code Deep dive but you can when you go to the code repository you would see it to yourself right so in that one score that scoring script alone you will see that Azure ml once it gets those parameters of where um what is this what is the script and what um what image needs to be created it's only going to uh do that for that particular model the model that has been registered right uh it's not um um yeah so basically if you're saying that how do we restrict it to only certain machine learning model um yeah there are there are parameters that you can change in that API call to restricted I'll show you that um as we uh go deeper into the session right so we have 30 more minutes yeah so but I'll move on and we'll take the questions the a few more questions later on okay yep yeah so so what we saw was the build pipeline so far right now let's see how to release the model so once the scoring image is ready um we are going to we basically have to deploy it on some kind of an infrastructure right right so that we can um consume our model as a web service and now how I ideally want to do the release is in a staged way I want to First push it into QA okay I want to push it on a very small container probably um with okay so once you push it into that container we want to test if everything's working fine and then after we do all the checks and approvals then we want to push it to prod so on then we can go ahead with some other strategy right we can we can uh create um a kubernetes cluster and deploy our images uh image um uh in that fashion right so that so basically this is a staged uh deployment uh design so okay so this is um this is the diagram for that so basically if you look there are two stages uh the release has been planned in both of these stages we're going to provision a new Linux VM from Azure devops and um as as we did the build we're going to install the same dependencies okay now why are we doing that because we want to call we want to call two scripts right we want to First deploy our model on Azure container instance and we want to also test do some integration testing and functionality testing right so to run these two scripts we need python we need CLI cdk other packages so we install all of that and once we do that the first step is we call a script which will which will internally call Azure ml SDK that SDK will deploy our image on a Azure container okay now Azure ml will internally take care of take care of which container it should go to it will create the container registry to uh to to look uh to to save your image it will create a container instance for you and it will push your image there and it'll take care of all the things right um all you need to mention in that API call is what is the CPU uh that you want to configure CPU RAM and all those things that how you want to design your container basically and you give the image location right now how do you get the image details and all of those things which were defined in the previous build process so that's where the artifact code based comes in so I'll take you back to that diagram right so this is where our entire code depository was stored and created so the build process created a repo and this gets carried forward to the release right so the moment build is completed Azure devops has this capability that it will publish that volume as an artifact and that artifact the moment it's put into a location it's going to trigger a release Pipeline and when it triggers a release pipeline my QA deployment will start and my production deployment will start later on okay after all the approvals so we're going to see that in action okay some in in few minutes so so the QA stage as I told you it will go install uh so it will go and create an Azure container instance and uh get all the image details from the artifact code base and let's let's say we have deployed it there now second thing is to ensure that everything is working properly we will have a functionality testing script now this script is basically a simple python script it will go it will fetch the web service uh from the Azure ml workspace it's going to take that it's going to pass some sample data and get some result back if it gets the result in a satisfactory way it's going to say testing successful and our QA deployment is over now to move it to production Azure devops has its own internal uh features right you can add in reviewers approvals predeployment checks gateways so there are multiple things you can do and I would love I would I would like you all to go and explore that yourself and even in the demo later you'll be seeing how that happens so let's say I am the approver of uh pushing this model to production so I'm just going to approve this and I'm going to comment that everything works fine let's push it to production so once it goes into production it's going to do the same thing it's going to ask Azure devops please provision a VM for me it's going to install everything and now we're going to deploy the same image on multiple Azure uh um on multiple containers and we're going to have a kubernetes layer sitting on top of it right now I don't have to manually do any of this Azure ml will take care of it right so you just have to call an API pass the image details pass the Azure kubernets cluster configurations that this is the kind of VM I'm looking for these are the number of Agents I need and all of that and you give that uh to Azure ml it will take care of the deployment and once it's deployed and we get the endpoint successfully back we are going to do another functionality testing on top of it see if it works fine and that's it we are done with the deployment on production as well okay so this is a simple stag release that we can U design for a RL projects so um so these are just simple design uh ideas right you can add so many more layers and um functionalities to this like before um uh number two um you can um you can have many more scripts to test your model uh how is it reacting if I give it a very weird kind of data right if I give uh let's say negative um uh numbers as features how is it reacting is it rejecting it properly or not so we can do a lot of other testing um um in an automated fashion right uh so I've just done a simple uh functionality testing but there's a lot more that you can do here okay so we're done done with the release as well um okay so now let's just see all of this in action right so we're going to open Azure devops now and um I'm going to run this end to end and let's see okay are you all able to see my um screen P can you confirm if you're able to see my uh azure yep you can see the screen okay okay uh okay so so this is azure devops in from the UI perspective we have a lot of things here right the but the services that I have preliminary I have used is repos and pipelines inside pipelines have used build Pipeline and release pipeline these two things so let's come to the repo this is basically the entire repo that I have for um my project right it has configurations it has helper files um so basically all of the code that we saw training uh evaluating the model registering the model deploying the model everything is uh done um these are the codes inside this this folder so you you can check this out later I'll tell you how to access this code repository now let's see the end to end run for this okay I'm going to run the pipeline so when you're running this you Azure devops will ask you to um select which um VM do you want to run this on right hosted by Azure pipelines or default something that you have provisioned so I'm going to select theer pipelines and I'm going to give ubun to 20.4 as agent specific and um this is the master branch that I'm running it from right so I have all my codes sitting in the Azure jit repo um and from the master Branch I'm going to create my um pipeline build okay so I'm going to start the run now so basically now going to wait for the VM to get provisioned and it's going to um it's going to do multiple uh tasks here right so it's so you can see from from the list of tasks here right so you can see what are the different tasks that we saw in the architecture diagram you'll be able to correlate it now so first we are checking out the repository so we are basically copying the entire code onto the VM volume then we we are uh installing python okay so we go and we install python 3.6.1 5 this is the version that we're installing sorry and uh so once the python uh environment is installed then we're going to go ahead and install all the other dependencies right so Azure CLI and um I've configured some uh requirements and you'll be able to see that in the repo so let me open this in a new tab okay so it's going to take some time here so it's going to uh it's going to um collect all the libraries uh Azure ml SDK and it's going to deploy all of that uh sorry install all of it on my VM right and then uh we're going to go step by step right it's going to create the workspace and start the uh training let's wait for this e right so now we have finished installing all the requirements this is just going to call an API and create the workspace first it's going to check if the workspace exists um so in my aure environment I have uh no resource groups created so far so it's going to go and create a resource Group for me so as in when it does this um I think I can show you this as well I can so what I'll do is instead of taking you through the UI route I'm going to um and let me go here let me download the yaml file export to Y um okay so I'm going to just switch the tab share this tab instead yeah so so let the the let the build pipeline complete right but what we are seeing here is so I've just created this new yaml file now this yaml file is basically whatever you see in the um whatever you see here right how do you even Define this entire build there are two ways to do it either you do it from the UI route right so you um um you configure all of these jobs these tasks within the pipeline uh on the UI or you if you want to track the changes you want to do version controlling and everything you will um you can just uh create a yaml okay now this yaml um has basically a structure to it so you're going to um create a job you're going to uh Define which Ubuntu pool that uh you're going to use right and um after you define the VM image these are the steps right so you define each of the task here right uh so using a python version you don't have to do anything by um in um you don't have to write any code Azure dev has its own internal module that you can use to run a shell script you just have to call the shell script to and it will in install all the requirements that you have now this is the um this is the script that we're going to run right so this is available inside the root directory environment setup if you open the install requirements Dosh this is what it's going to run here like that Azure CLI um this task is going to basically run the python workspace dopy file this is going to create the workspace for me this is going to start the training for me this is going to evaluate the production model with the new train model right this task is going to register the model so every task is basically running a python script correct and it's also so um once it finishes the scoring image we're just going to copy all the files from the artifact directory to the build um sorry from the source directory which is our root directory um to the artifact sving directory these are basically logical um bations that we have and this will be uh done on the VM level right so on the volume we going to transfer everything from Source directory to artifact directory and once it's all done we going to publish it so let's go back to um our um job Okay so we've seen that okay it's finished creating the workspace and we have uh our training has also been completed so let's go and have a look at the training so we are running the training script this is basically the alpha value that you've decided then we create a pickle model sorry we we create a pickle of our model then then what do we do we register um we register the experiment into a aure ml right we submit all the experiment details now aure ml knows that okay training has been completed and we have the model ready with us right then we evaluate the model uh once the evaluation is complete and it says that oh there's no no model in production currently so why not go and register the model so it's going to go and register a new model now um and once it finishes registration it's going to create the docker scoring image um let's wait for this to complete so while this happens I think it'll take another 3 4 minutes let's go to the code right so so we saw and the entire environment setup on the VM is taken care by the script right so we have all the requirements established here then let's let's go to the ml serice so so all the zero scripts starting from um till 30 right these are all the scripts that we using in the build pipeline when we are training our model sometimes we don't want to train it on an azured VM right we want to train it on a separate VM that we want to configure we want to it can be a GPU it can be anything so if you want to create your own VM okay then you can uh attach basically you can create um uh provision of VM in aure ml workspace right so there is um there is an option in Azure ml uh there's a module called remote uh compute so you can go and create a remote compute so you can Define the VM name VM IP password all of that all the configurations here and create a VM and that VM then can be used to train your um model okay that's another way to train your model so there are different ways to train it I have just trained it on the aure devops um agent itself um so you later on after the session is over you guys can check these three scripts so these three scripts basically uh train the model on different environments okay um and evaluation script basically looks like this it's just going to um so all these are all the intermediary files right so it's going to read from the Run ID J it's going to read all the model experiment name run ID and everything it's going to create a new experiment and it's going to check for the latest model that has been created and from that latest model let's get all the metrics and we're going to compare uh the msse right so we're going to compare the old the the latest MC with the production MSC and we're going to see which one is better okay and if it is better we going to create a new um uh run ID Json file right and we're going to replace the old run ID with a new run ID and once we do that it's going to trigger the next script so like that there are several code scripts here that you can go and check it out after the session we won't be deep diving into it similarly I uh there are scripts to deploy the model on ACI as as well so Azure container instance so as I told you all of these scripts are basically calling an Azure ml um API internally right so it's using for creating uh accessing the model we have the model module we have the to create Docker image we have the image module to create a web service we have the web service module right so you can just use these and uh go ahead um Outsource all of your work to aure ml right so this is how you create your web service all you need is tell it these other configurations and create my web service from these configurations so I know I'm jumping in uh on the code but um yeah I'll um because once the pipeline is finished so as you can see our pipeline is done right our entire build pipeline is complete now so we have created an image and everything now the after it's finished the way I've configured it is if you go to release now it is automatically started a release because the last step of the pipeline is basically pushing the entire code repo into an artifact directory right so the moment something gets added there it's going to trigger a release pipeline now let's look at the release pipeline it looks something like this right this is the artifact devops for aici this is my code repo 32nd um uh release uh sorry 302nd uh artifact that we have and this is starting the first stage which is deploying on the QA okay now within this there are several steps right work items um so we have um we have multiple steps here we have initialized job so you'll get your entire um I'm not going to go into the logs but let me show you this right let me go on tasks yeah basically it looks something like this so this is how you configure it so release pipeline the first thing we're going to gen create a new VM okay on that Ubuntu VM we are going to use a python 3.6 version we're going to install that we're going to install all the requirements CLI SDK everything then we're going to call the web service ACI script that web service will uh generate um an endpoint for us that endpoint is going to be consumed and we're going to test it using our python script our local python script right so um this is how it looks like similarly the the production one will look will be the same just these two scripts will differ this it will call the kubernetes API okay let's go back to the release and you can see it has been successfully deployed okay so that means the testing is also been completed correct um and um now you can see it has not pushed to the production yet uh because it's waiting for for an an approval right so I've configured it to have an approval so let's go and approve this right so I can I can approve this and I can also defer the deployment for later saying that let's schedule it to a later time because I want to continue my testing further so okay I'm just going to approve this right and it's going to cue my production uh deployment and it's going to start this AKs deployment as well now let's go into our azzure um um um portal and see uh the resources that have been created oh just to give you heads up the AKs deployment will fail because um the subscription that I have uh my Kota is only um uh it only allows me to deploy four uh um four cores on a particular region and AKs has a minimum requirement of having six cores so it'll hit a quota isue and something it'll fail right but this should ideally work for um um the subscriptions that are paid right I have a uh so so this uh so let let's go to the Azure ml portal so I hope you're able to see it right um let's go to Azure machine learning so you can see we have um Azure ml workspace that has been created okay and let's launch the studio okay it's opening in a do different page share this tab instead okay so as you can see we have so okay so an experiment got registered with devops AI demo name it got completed if you look at the models our model was also registered here right version one and if you click on this it's going to tell you that this is the version and um the other details of the model right so let's go to endpoints let's see what has been deployed we have an ACI web server is deployed okay so you can see it's a container instance and our container instance is healthy uh if you go to consume this is the rest end point right so if you look at the python script here it it also gives you so basically um if I uh hit this particular endpoint and give it sample data it's going to um it's going to run the model and give me the results back okay so this brings me back to this um so if my Azure kubernetes also had worked it would have created another instance kubernetes instance and deployed it over there okay so um let me so this is I was just showing you the um Azure machine Learning Studio right if you let me just open my Resource Group and um share this tab instead okay so this is basically um these are all the resources that have been created in my Resource Group right so we have a container registry this is where the image is um is stored this is the container instance internally Azure ml takes care of creating all of this for you this is the workspace Studio this is application insights for web service uh to track all the API calls and everything this is the keywall it has all the access permission stored and this is the storage account right this has all the logs intermediary logs um everything that has been stored in the storage account so when we create the workspace all of this is uh um Azure ml takes care of creating all of this for us okay [Music] um so quickly let me just share the last part of the uh the slide um so we saw all of this into action right we saw the build pipeline we saw the release Pipeline and now what I want you to do next is um I want you to log in into these two links um I'll have path share this with the team so um basically when you uh when you get into those links you'll be able to generate a demo uh the same whatever whatever pipeline that I just showed you right um if you follow the steps written in this blog you will be able to have the same thing in your Azure devops organization as a project you'll be able to create it there and you'll be able to run it all by yourself right you don't have to uh just follow these two uh links and you should be good to go and once you do that then you'll be able to Deep dive into the code see the nuances there how the apis were called what parameters were passed and then uh a lot of it would make sense and before I end the session these are some of my key learnings actually some things that I want you few things that I think will save uh a lot of time for you guys when you sit and run this uh project so first is uh um if you have a free subscription and you are planning to run any Pipeline on Azure devops you won't be able to do it uh without getting approval from the Ado team right so uh basically there there have been a lot of cases before where people were misusing the uh VMS that were available on Azure devop they were running uh very heavy tasks on that so Azure devops came up with a restriction uh now if you want to run any uh project on it first you have to go tell them that this is the project that I want to run then they will approve some time um so basically you get some minutes uh free minutes to run your jobs so you'll need to get that approval and only then you'll able to run this Pipeline on your Azure devops account uh second thing is um um the PIP version right so in the code repository if you look at the scoring image uh right the the docker image that we're creating the docker image needs some dependencies that needs to be bundled up right so in that dependency file there is a little change that you have to make there um because this project is built on python 3.6 you have to downgrade the PIP version to 20.11 as well so in that particular file if you change the PIP version this way it's going to save you a lot of troubleshooting and debugging so it took me a lot of uh effort to figure that this was the missing link so once you um downg gr it uh you will be successfully uh you'll be able to create the docker image successfully and third one is I saw this problem that um if you have let's say you have your resource Group uh name you have decided it to be ABC okay now you deploy it once using Azure devop pipeline you see that something is wrong you delete the resource Group and you try to repeat the process again now if you do this repeatedly like on the same day if you do the deletion of the resource Group and repeating the creation again and again like more than 10 times then what happens is the Azure ml right somewhere in the back end it's going to uh act very erratically it's going to create some permission issues for you so because of that your training and all of those might uh fail right so this also took me some time to uh figure out so please be aware of that don't delete your uh uh resource groups uh again and again um what you can do instead is if you want to go for a new deployment change the resource Group name all together uh or you can go to the key Vault uh copy paste all the permissions that have been created in the first time and ensure that the same keywall permissions are U passed on to the repeated uh um I mean the future deployments as well right so these were some of my uh learnings after the project yeah so once you once you figure out um uh how to um run uh I mean take all the code and run it uh the the blog has a very clear uh instructions on how to do that and then I think all of this um this entire session would uh son can you just uh send me those links on the studio chat so that I can uh send it here sure sure sure yeah sure let me do that yeah but uh yeah this is um this ends my session I'll I'll share the links shortly uh just to summarize right um uh we we looked at um some of some of the architectural design perspective so I just shared some of uh some some designs on how you can configure your build and release pipelines but obviously there's a lot more that you that you can play around with and uh for every model that we train there'll be certain um uh different nuances some of you guys might not want to go ahead and Outsource things to aure EML you want to you would want to do it yourself so in in all those cases you know that you have a plain Ubuntu machine so you can basically do install anything on it and uh run um a task using Azure devops yeah so this was basically an architectural discussion so free feel to ask me any questions or email email me um uh the questions please do ask you all questions in the chat [Music] section oh yep so yeah final call to ask the questions meanwhile I get the uh links I'll post this links yes I have the links okay I've shared those links please take your learning further from there and feel free to reach us reach Sonia anytime and any questions or we call for a rap okay looks like we are good thank you thank you all thank you Sonia thank you so much for joining us today and for the wonderful session and thank you everyone and enjoy the rest of your day thank you so much PA for organizing and putting all of this together thank you everyone who joined"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"CI\/CD for Machine Learning",
        "Topic":"CI\/CD Pipelines for ML Projects",
        "Video Title":"DevOps CI\/CD Explained in 100 Seconds",
        "URL":"https:\/\/www.youtube.com\/watch?v=scEDHsr3APg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/scEDHsr3APg\/hqdefault.jpg",
        "ID":"scEDHsr3APg",
        "Publish Time":"2020-03-12T17:45:46Z",
        "Channel":"Fireship",
        "Channel ID":"UCsBjURrPoezykLs9EqgamOA",
        "Transcript":"DevOps a set of practices to build test and release your code in small frequent steps one of the core practices of DevOps is continuous integration which has developers commit their code to a shared repository often on a daily basis each commit triggers an automated workflow on a CI server that can notify developers of any issues integrating their changes when a repo evolves in small steps like this it prevents what's known as merge Towel imagine Mary you're back and developer builds a new API for your product shortly after Jane your front-end developer starts work on a new UI a few months later when it comes time to merge their features we find that they're completely incompatible the build fails and we now have to spend a bunch of time and money resolving these conflicts let's go ahead and build a continuous integration pipeline now to see how it prevents issues like this here on github I have a node.js web app in order to deliver this out to my customers I need to run three commands test build and deploy I can automate this entire process in the cloud by using a CI service like github actions first I create a workflow and then I tell it to run on every push to the master branch the event triggers a job that runs on a Linux container in the cloud and we tell the container what to do as a series of steps first it checks out the code in this github repo then sets up nodejs installs my dependencies and runs my tests build and deploy commands now anytime we commit code to the master branch in this repo it will run this workflow if any of the steps fail the bad software won't be delivered to our customers and will automatically know there's an issue that needs to be addressed at the end of the day CI CD offers two main benefits it helps you automate things that would otherwise have to be done manually by developers that will increase your velocity but it also detects small problems early before they can grow into major disasters and that results in higher code quality this has been C ICD or DevOps in 100 seconds make sure to hit the like button if you want to see more short videos like this and stay tuned tomorrow for a full dead loss project using github actions thanks for watching and I will see you in the next one"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"CI\/CD for Machine Learning",
        "Topic":"CI\/CD Pipelines for ML Projects",
        "Video Title":"MLOps Tutorial#1. Continuous Integration (CI\/CD) for ML Pipelines with Github Actions",
        "URL":"https:\/\/www.youtube.com\/watch?v=9I8X-3HIErc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/9I8X-3HIErc\/hqdefault.jpg",
        "ID":"9I8X-3HIErc",
        "Publish Time":"2021-03-25T00:06:33Z",
        "Channel":"Data Science Garage",
        "Channel ID":"UCLhx5RCmUl3cOti6vJq0DPA",
        "Transcript":"hello guys this is the first video on my channel dedicated for mlabs and mlaps is machine learning operations and in this first video i will try to cover continuous machine learning it's an open source library for implementing continuous integration and delivery it is ci cd in machine learning project and continuous machine learning is cml i will mention it many times in this video you can use similar to automate parts of your development workflows including model training and evaluation comparing machine learning project across your project history and even more so let's start our journey to mlaps and in this video i will use the simplest tensorflow regression example ever to cover the main aspects of continuous integration and delivery in real project so you can do the same action on your site i will do my action on my site and in the end of this video you will realize what github action can do with your machine learning project and how you can to save your time in development so don't waste our time and three two one and let's go okay now we on my github and in this github repository we have a very simple project uh which consists of two files it's model.bi and requirements file in this repository i have one branch so far so let's take a look what is the model.bi is about it is a tensorflow regression model which generating training data itself and so if you have uh external data sources that you need to use it's not a problem you can use it by loading it into the neural network so we are going to train our model and 50 epochs and we'll see if it's enough or not enough in in the next in this video so in the end of our code we are having a print statement that will output the result in the terminal so it's also very important and so let's come back in here and what is the requirements file is about it's uh it is the list of uh libraries that we will need it's uh tensorflow numpy and matlab live for visualization so and when we are having this kind of uh structure in our github repository we are ready to integrate we are ready to implement a continuous integration part so let's do it okay and for continuous integration part we are going to use a special tool it's called cml and we can find it in google and you can do it the same as i'm doing right now it's a cml github um iterative yeah uh let's go into this github repository and we'll take a look what is about yeah this is a full repository that provides you a cml it's for continuous machine learning so let's go down and here you can find the template that we are going to use so let's come back to our github repository to our project and in here we need to create a new branch but the main idea behind we are going to create a new branch where we run our experiment and during our experimentation cml will create a docker file that will run a full machine learning pipeline and deliver us the result that we expect to get so let's do it um yeah add file create a new file yeah and in the directory that we need to create it's very strictly rules you cannot change the directory so it's a github not a car it's a dot and workflows and the name of the file is cml dot i am so in here we need to copy and paste the our template i will copy and paste only this part of the template because what is going down it will be a customized our machine learning pipeline so copy and paste and what we can do in here uh we need to change our name it is ml ops example uh tensor flow regression yeah and then you see the push event this pipeline this cml file will be triggered when we will make any comments and push into the github repository so let's keep it at this and then as you see we are using a docker to creating a docker file that will run the full uh machine learning pipeline inside and and what is the pipeline the pipeline is begin right here yeah you see your machine learning workflow goes here the first is the install requirements file and remember in the requirements file we have the all libraries that we will need so then we will uh run our training script in our case it's not a train dot pi is model dot pi okay leave it as at this right now to get a better intuition step by step what is how it can be customized in your project and we need to create a new branch it's experiment yep we are creating a new pull request and simple like as this this and you see something is happening let's go to details yeah and we see that something is happening we are set up in the job we initiation the containers and then we will run the actions and train the model and stop the containers okay let's take a couple of minutes and just take a look what's happening okay finally we finished our job and let's take a look what happened we install all dependencies uh in our container we trains we train our model in 50 epochs and in the end we have some results about our training progress we have we have this line from the print statement in our code okay we see how our result from the training is in terminal but how can to improve so as you can see the simple line and the terminal is not very efficient way how we can to share our results with the team members because it is interior terminal it is not very user friendly and we need to think how to make it more user friendly and in more efficient way um i think that a good idea should be if you if you write a result is some kind of text file and grab this text file into some kind of readme and share this readme file to the team members let's do like this i think this idea we will implement in the next couple of minutes and come back to our branch and let's go to model file right here yeah and in my code i am going to write my result into txt file it is a metrics dot txt and inside this file i will write the final result of my model performance so i commit it to my branch then go to cml file and define my machine learning pipeline so what i'm going to do when my model is being finished i'm going to create a new readme file and write some information inside this readme file so iho and in hitline model metrics yep and create a new readme file let's say it's a report.md it's a readme file for report and inside this readme file i need to write the information from the text file so i open metrics dot txt and write it down to readme file report dot md and in this moment i wrote some information in my readme file but i need to decide what to do with this readme file in order to make it accessible for the team members in this branch so i need to send it as comment that will be available for all the members so cml has a special function is cml send comment to report.md and that's all so again i install the dependencies of my model i run the model with predictions i create a new arrhythmia file i put some information in this readme file and make it accessible for team members so i start to commit into my experiment branch right here go to actions select the most recent one okay let's wait a couple of minutes and see what's happened okay i see that my machine learning workflow uh finished successfully and then i can to check what the result is so let's go to pull request and see what is the result and yeah in pull request i see that comment is my result is mean absolute error is almost 13 and mean square error is 168 and it's working so and one thing on the top let's uh check your email oh i got a new mail from github and let's check it yeah i got a email from the github with the result that means that you can get immediately the result from your training job to your email and the results are available in github and in your email also so how you like it uh it is more efficient way that you saw in the very beginning but let's have a situation you need to get not only the textual information in in the comment but for example what if you need to get some graphical information from your model for example trading performance so it's also doable it's also doable and github actions and continuous integration part so so let's do it and let's try to implement it um let's come back to our model dot pi file in in our experiment branch open it up and edit okay and in here i have a function that will plot the results and save it as a model underscore results dot png it is external file that will be saved in our repository so um i need uncomment this function if you have your own real project you have you can to um you can to write your own code that will plot your results in similar way and save it as external png file with matlab lib or seaborn or some kind of python libraries okay and i commit change in my experiment branch then as i did before let's go to workflow file cml and and add some additional steps in here um as you wrote some information before we can do the same we can do the same uh with the new data let's say we need to new line we need a new line and make a some hit line model perform performance yup and ejo model performance metrics are on the plot below okay and maybe you guess that the next step is to somehow to insert the external png file into our readme file let's do like this cml publish and now we need to publish a png file in our readme file so remember the file name is model results is png and mark the markdown flag and we write it in report dot md i think that's all for this pipeline i commit my change and push it to experiment branch go to actions and select the most recent one yeah and my workflow is starting and let's see what will happen in here okay let's wait a couple of minutes and see what's happening okay and i see that my action job is finished again and let's check what the result is open my pull request whoo and and i got my graph in here that's perfect it's working you see i got information in textual format and below i see that is it is the graph from my python script in my comments that is available for the team members but one thing i missed um you see there is no additional line uh that training and prediction result on the plot below um okay let's go quick quickly to um [Music] cml file yeah i forgot to add report dot md and that's it and if you check your mail again we got a new mail in my gmail and let's check it what is inside yes i get a result in txt line and on the below i see that the modal results are displayed on very nice graph in my email directly from github actions and it's very interesting house look like in my mobile device let's check it i have my mobile device and if i open it up you see how it's look like and and i think it's a very good way how you can to share your result and get a very fast feedback from the model immediately in your mobile phone in your workspace anywhere okay we are on the finish line of our video and let's have a simple situation for example we are working on the model improvements and we need to make some changes in the code and see how it affect our model performance so for example let's have a situation and make changes in our code so uh in this example to make some improvements i think that one of the quickest way to see changes is to increase number of epochs and add one more dense layer in our neural network so i think we can commit our changes and see what will happens and see what the result we will get in the end then we can to compare the result after our improvements and what the result way before so we can do it by github actions and mlaps so let's start our action for continuous integration and let's see let's wait a couple of minutes i see my github action finished job and it is a time we need to check the results let's do it right here let's go down for the recent one yes and you see the difference mean absolute error is almost true while on the previous version we had almost 13. you see how it changed and mean square error is 5.5 and on previous time we had almost 170 you see how how increased our model but this is not a point of this video this is how you can track the model performance on github actions you see you can check what is the performance on different scenario on your project um i think it's a good time to check our email and see if you got any feedback from the github or not but i think that we should have something yeah you see and we got a new mail from github let's check it and yes is it it is the same result in my email and that means that whenever you be you will get a feedback from the github to your email to your github workspace anywhere and i think this is very very beneficial and saving time tool that's all for this video what i wanted to say and you can see the screen from my mobile device right now how it's look like i think it's user friendly and one more last time let's open a workflow file and this is the final version of the cml file that i implemented during this video and if you want to do the same on your real machine length project you can check my bro project on my github the link is in the description and i hope it was useful for you and i really hope to see you on the next video never stop learning and bye bye"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"CI\/CD for Machine Learning",
        "Topic":"CI\/CD Pipelines for ML Projects",
        "Video Title":"Build CI\/CD Pipelines for ML Projects with Azure DevOps",
        "URL":"https:\/\/www.youtube.com\/watch?v=OVghJ_nQ3_I",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/OVghJ_nQ3_I\/hqdefault.jpg",
        "ID":"OVghJ_nQ3_I",
        "Publish Time":"2024-01-30T04:41:27Z",
        "Channel":"Cloud Guru",
        "Channel ID":"UCnBemNcYRBCZI6BsHlhrIVg",
        "Transcript":"[Music] hello everyone in this video I will walk you to the process of creating a continuous integration and continuous delivery pipeline for a machine learning project in aure first I will show you how to set up a local or Cloud native development environment which is connected to a gab repository then we will use giab actions to set up a continuous integration pip P line and aure pipelines to set up a continuous delivery pipeline finally we will use aure up services to serve or machine learning API all the steps described in this video can be found in the GitHub repository I will put the link to the repository in the description of the video without further Ado let's get it started first you will need to fork and clone this repository I have already forked the repository so I will go ahead and clone it I will copy the link to the repository and then go over to the aure portal and open a new terminal here I have created a folder for this demo so I will go ahead and clone the repository inside of that folder next we will need to create a virtual environment this project has a make file that will help us to do so we need to run the commands make setup and then source next we will use the command make all to install all the requirements in this environment now we can go ahead and run the command python app to deploy the application in the aure command shell we can verify that our application is running by opening a web app and opening the port 5000 we can also verify that the machine learning model is working by opening a new terminal and running The Bash script make predict provided with this project as we can see the prediction is working now we can go ahead and test that the continuous integration Pipeline with GitHub actions is working to do that we will make a small change in the welcome message of our application we will go ahead and open up the py and add the word demo at the end of the welcome message then we will commit and push the changes now if we go back to our GitHub repository and open the tab actions we will see that a new continuous integration pipeline has been triggered we can open this workflow and see that giab actions is running a series of tests to make sure that our code is always in a Deployable state we can also go back to our web app and verify that the change has been applied now it's time to set up Azure web apps and aure pipelines to set up our continuous delivery pipeline first we will need to create a new Resource Group and configure it as or default Resource Group for the subsequent comments then from the portal we will create a new resource we will search for web apps and create a new one let's select the resource Group that we created in the previous step we will give it a unique name and select python as around time choose a region that is close to you and finally a a Linux plan if you don't have one go ahead and create a new one then select review and create and finally create this will take some time to be deployed once the deployment is ready go ahead and open it in a new tab as we can see or your web app is ready to be used so let's go ahead and create a continuous delivery Pipeline with aure pipelines first we need to go to aure devops I will leave the link in the description of the video then we can go ahead and create a new project we will give the project a name and a description and click create next we will create a new service connection we will select aure resource manager and service principal automatic we will select the resource Group that we created in the previous step fill up the information for the service connection name a description and Save The Next Step will be to create the aure pipeline so I will go ahead and click create Pipeline and select my gab repository I will select python to Linux web app in aure and select the web app that I created in the previous step finally I will select validate and configure you can use this ml file to configure your continuous integration continuous delivery pipeline I will leave a link in the description of the video on how to do that for now we will just add an additional step to run link test and then save and run we will need to add a commit message and this will trigger or continuous integration continuous delivery pipeline now we should see under pipelines that a new release has been triggered and we can see it a step by step go through the deploy first it will build the application once the build jab is done it usually takes a couple of minutes then it will deploy the application and then at that point if I go to the app service application URL we will see that the application has been deployed and or changes are actually reflected here so this is very powerful and now as a final step we will go into the cloud share environment and do a final modification that will trigger the whole process first I will make a small change again on the welcome message of the app then I will go ahead and commit and push these changes now if we go back to our GitHub repository on actions we will see that a new continuous integration workflow has been triggered this pipeline again will run all our tests and make sure that our code is in a working State now if I go back to aure devops under pipelines you will see that a new continue delivery pipeline has been triggered and this pipeline will again build and deploy or changes once the build process is done I can go back to the app URL and again check if the changes are reflected and they are the app is also available via a predict API and I can look at that from this file make predict aure app and you can see that I'm going to do a post operation using this script so I'm going to go ahead and run that to verify that the service is serving traffic correctly and it is from here I can go ahead and run this commands to subscribe to the log stream of the service great that works so in a nutshell we've been able to do end to endend continuous integration and continuous delivery using it have actions and Azure pipelines at the end of this repository you will find ideas for enhancements this is a very simple machine learning application but it can be easily extended you will also find helpful resources from Microsoft thank you for watching this video [Music]"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"CI\/CD for Machine Learning",
        "Topic":"Automated Testing for ML Systems",
        "Video Title":"How to Use AI\/ML Systems to Revolutionize Testing and Test Automation",
        "URL":"https:\/\/www.youtube.com\/watch?v=GtD02_kSf_k",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/GtD02_kSf_k\/hqdefault.jpg",
        "ID":"GtD02_kSf_k",
        "Publish Time":"2019-03-04T18:20:27Z",
        "Channel":"Bython",
        "Channel ID":"UCGTxz2nU-LVbyxtVq1C_MWQ",
        "Transcript":"how to use AI \/ml systems to revolutionize testing and test automation hello my name is John and in this video we will discuss artificial intelligence and machine learning and how AI capabilities have helped to automate the process for testing and testing automation and we'll also discuss how the pattern of recognition allows machines to learn and better predict what is going to happen and then apply it to new data let's take a deep dive further into how artificial intelligence and machine learning can be used in testing and test automation what exactly is artificial intelligence simply put it is the opposite of the natural intelligence shown and exercised by humans and animals it is basically the study of entities that perceive the world around them and makes plans to achieve their goals machine learning is basically a subfield of artificial intelligence which aims to allow computers to learn on their own with no explicit programming most experts agree that software testing will be hugely impacted by machine learning by applying artificial learning machine learning provides systems the ability to learn automatically testing and systems automation would then improve and will automatically run tests and access data while learning from the results and making improvements to the testing cycle d convergence is tech Mahindra's property tool and uses a machine learning algorithm to obtain actionable insights from aggregated data from testing tools by analyzing defects it can predict the root cause of issues and reduce the cycle times for tests by using machine learning for test automation we can dynamically write new test cases based on user interactions by data mining their logs and their behavior on the application and service level for which tests are to be written I hope you received some great information in today's video to read more about AI and machine learning systems and how applying them can be beneficial to your testing and test automation click the link below for more information"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"CI\/CD for Machine Learning",
        "Topic":"Automated Testing for ML Systems",
        "Video Title":"Automating Mismatch Detection and Testing in ML Systems",
        "URL":"https:\/\/www.youtube.com\/watch?v=VWuGgvkZGfQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/VWuGgvkZGfQ\/hqdefault.jpg",
        "ID":"VWuGgvkZGfQ",
        "Publish Time":"2022-11-04T11:59:28Z",
        "Channel":"Software Engineering Institute | Carnegie Mellon University",
        "Channel ID":"UCrmnnE3yzpAyAuX_hRqyLdg",
        "Transcript":"foreign s often struggle with moving machine learning components into production systems because different teams work independently during different stages of system development for example data science teams develop a model these teams then pass the model to a software engineering team for integration and testing into the machine Learning System and finally an operations team deploys and monitors the ml system because these teams often work independently and without any system or Mission context they tend to make incorrect assumptions about decisions that other teams and stakeholders have made leading to what we call ml mismatch as DOD and organizations adopt machine learning to solve Mission critical problems the inability to detect and avoid ml mismatch creates delays rework and failure in the development deployment and evolution of ml systems in this presentation I will discuss and demonstrate Tech a tool that detects ml mismatch by using a series of descriptors that Define information that teams must share to avoid mismatch I will also show how we are extending and using the descriptors to support testing MMO components for production Readiness which is a major challenge for fielding production ml systems that meet Mission and system goals you don't want to miss it [Music]"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"CI\/CD for Machine Learning",
        "Topic":"Automated Testing for ML Systems",
        "Video Title":"Using ML to find value in your automated tests | Dmitriy Gumeniuk | #SeConfLondon",
        "URL":"https:\/\/www.youtube.com\/watch?v=Dx2gHpoacFw",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Dx2gHpoacFw\/hqdefault.jpg",
        "ID":"Dx2gHpoacFw",
        "Publish Time":"2019-10-21T18:17:22Z",
        "Channel":"Selenium Conference",
        "Channel ID":"UCbDlgX_613xNMrDqCe3QNEw",
        "Transcript":"[Music] like you know I really love this size of the clicker they like the first time I see this thing really really tremendous I even kill someone if the questions will be not that good so about the questions you can see this link on the screen so you can just scan it with your iPhone Android and ask the questions online in the app at the end of my talk I will like open up the screen and walk through this question so please go ahead check this I also will show you this little bit later but still you will be prepared so today I will talk you about the machine learning usage and how we use within our tool which is called the report portal I also show you what is the report portal itself how I highlight the main issue of issues month fishes and how we use it how it grows so let's switch to the presentation a few words about myself and what I do my name is Dmitry Dmitry gumenyuk I am from the open systems it's lucky that in Minsk Belarus but the company is really huge around the globe so I already thirteen years within the practice I started as the Java developer from the ground up from the junior developer to the team lead and I had the experience to work with the test of the mission at this scale of 120 thousand of the automated test cases nightmare but still all these experience brings me a lot of analog insights ideas and what I do right now I'm heading the development of the tools and accelerators within testing competences Center within my company helping out to grow the test automation expertise and like overall practice we have around it I also like how on conference which is called del X which stands for the delivery excellence and focuses on test automation and DevOps well you may ask yourself where's the Belarus itself it's not really far away from here it not even the small country itself so just like two hours two and a half hours to flight from here and the best thing like few words about our country so we have own kind of the silicon veil in our country which is gross pretty first and I would say that right now the we do pretty big amount of the Commonwealth for our country doing the IT themes and also like we have pretty super special conditions for the different startups for instance we have the zero tax for the blockchain companies will also have the zero tax for the any crypto currency earnings so basically you can earn as much money as you want within with crypto currency and pain zero taxes in our country so basically they we have like lists of 2200 more special conditions for the startups and IT companies within our country so like there's that's just a few words about my currency hopefully the guys will let me in on the border after this presentation just kidding well I bomb itself so does anyone heard about the bomb yeah I see the hands that's good so we already pre big one we have more than 34,000 engineers in 27 countries and the best thing which I laughed about upon we have the competitive centers so what is what is the competitive Center you know that's the thing you can find in Google which and we do the same we actually have the kind of the Kibler collaboration of the experience and the experts engineers who works in the practice worked in a hands-on with the test of the nation and we haven't a thousands of the project collect the best experience worst experience false gaps and like all the information can collect from the production projects what we do we aggregate this experience in order to create tools accelerators and something which will help fill the gap and help you to speed up the delivery and test automation and their ops so we'll also focus on like but everyone speak about the AI or sometimes we just call I the III in intestinal intestine and test automation so like we also to have the information to understand how it works to handstand what's available on the market already we make some researches audits to understand what's available and shortly we can summarize that these 13 points are usually addressed by the like competitors on the market by the companies on the market where they try to build the AI e but the main issue about that is like the AI itself is pretty big we have like AI machine learning and deep learning inside of it and basically in most cases which we found like not every company use it in a way how they should use it they just do the regular precoded or rule-based decisions and solutions and call it as shin learning which is not really true so but still we can find and based on my own experience I can say if like for instance here we see the self-healing algorithms for the desktop dimensions for the test automation like two years ago only one company of ten can claim that they have the self-healing albums nowadays almost like every single company has own self healing algorithm as well like we do but we don't just don't call it ml or AI because it's like basically the comparison of the Dom structure you have this thing also will be open-source pretty soon by our company so stay tuned so you can just add this small thing into your selenium base test and it will heal your test during runtime so basically if you have the wrong okay which was broken this thing will find the proper one and replace it in run time good I think really useful so going forward the report portal so he have we have this story it started seven years ago with the idea that guys we need something to implement in order to fill the gap tube for each of the projects to have the robust and standardized reporting so it also was like outcome of my experience with like hundreds on thousands of the test cases how to collect and understand the results of your all the attested formations so what we did we started to implement a small thing which collect results in run times store it in your database and help you to visualize and track with the things you have so three years ago we have like we did open source this tool completely it's completely free to use no any restrictions or limitations all the functionality is available and it still continue to grow pre-first within the community so at the moment we reached almost three hundred thousands of the downloads for the docker and it still continue to grow so like this is our landing page you can find the information about it here and shortly I will describe you what it does so we understand it is a single entry point for the test of dimension results and you can put like send any type of team results into its basically it can be the UI tests API test in unit test any kind of the selenium based test the first selenium I said today because we had selenium camp so even the functional test can be reported into the report as well with some workarounds so it also like can basically aggregate and acquire all the old type of three results from variety of the available agents and I say like commonly used engines the list of them is growing you can finally see on our landing page some of them made by the contributors some of them supported by our team but still like the list is growing and please welcome you can control into the open source if you want and the one of the benefits like the key benefits we have a need it's a real-time reporting so what it means as soon as you start your execution you will see the results within the report portal just in seconds milliseconds and in case you have pretty huge amount of test cases we should they executed for like 6 10 9 hours there is no need to wait you have the ally reaction because you will see that your test is test for example started to fail in a second not in an hour's not in the next day and it like it really saves the time also we have the open API which is used for the seamless integration with your pipelines with your test automation you can grab any data from our database for the API you can do variety of the Curie's selections and use this information for example the form go and not go decision I was in your pipeline looks like I should speed up hold on guys well fasten your seatbelts integrations we have the integrations with the JIRA and like different types of the bug tracking systems we also have the integration with the source lab for the version 5 for the report portal high markers like where you are you should love it and having all this information in database will help you to keep the eye on the you a matrix so basically you can visualize the status of your test of dimension you can visualize how the things is going on and this is this thing is really large by the managers they don't need your numbers they need like widgets and graphics to see how the things is how things are going on and the report portal helps you to make it out of the box like nothing to make like just you do your regular stuff you'll know as your results you make your test cases and the report will show it to the manager in visualized way so like the managers I excited now I would say the chair on top and the main topic of this particular presentation is the machine learning so machine learning helps us to save a lot of and reduce the effort of the test of the mission-team in result analysis so how in how we do it how I understand the flow everyday when you execute your results you have the past and failed test cases what the report portal does it mark of all the failed test cases as with the to investigate flag which is means that engineers should pay attention to those particular fails what the test of dimension engineers do they analyze those fails so basically they open up the failed test case look through this stack trace and can see you like okay that's a product back that's automation issue and here may be the system issue like your environment was not ready your virtual machines was unstable you still do not use cells lab and have unstable environment so once the engineer made this analysis they create the training set for the machine learning and we use this training set in order to make this analysis instead of the engineers so how it works once again we have failed test cases which is analyzed by the engineer stay mark the reason of the fail link the bugs have all this information and within the tense or like a couple of the weeks of the executions we had enough data to start use the machine learning which will mark your fails automatically so once you have the new execution with the fails it just will mark up what are the product box what are the test automation issues and where are the system issues so there is the juice do you live it so the the best part is trained by the humans trained by the engineers it it uses the lazy type of the training so basically that means I will show it like a bit later but it means that it uses only special conditions which are related to your project not to any other projects so even you have like its results from Ruby Ruby Ruby Ruby and Python are different from results of the Java so that's why you have have to have separate training set for this and in case we use lazy learning we'll train only on your particular results within your project so here the short over you the this how the report portal looks like just like a few highlights what do you see on the screen so we have the project spaces so basically every single project separated one from another with the levels of permissions of the XS you see the section for the dashboard launches filters the bot mode and right now on the screen you see two launches for one particular set that's the demo data but still this is a particular set of the test cases executed one by one in sequence so basically you can understand it as the yesterday you have a executed section number nine today you have executed Testament from execution number ten and what you see in the line nine that those issues which was failing previously was analyzed by the engineers and you have knows no items to in the state of investigation let me start it so what I show here that I point out that okay we have execution nine ten the execution nine is analyzed we see the distribution of the issues like once again was made by the engineers and no item to investigate in previous one but for the latest execution you have 70 items failed and now what I do I will just go within the particular test case execution so here I can see that the report portal stores the results in the same structure like you have values you substitute subfolders and the final element will be the test case so once you have executed and report the result you will have all the related SEC trace locks screenshots the binary data anything which is related to your execution even basically like payloads which is sent from the a browser to the server can be also added within these results and right here you see the line we call the history line which shows you what was the states of these test cases within the previous executions so no need to spend the time and find it somewhere in Jenkins who just like in one click and accept the previous results and see what was happening previously so what I do right now I copied the name of the test case just to find it within our latest execution so we see that it's currently have the flag to investigate and open up I see the logs for the latest execution I also can see the stack trace for it right here and like this stack trace is pretty equal to the fail what's happened in within the previous execution and as I told you with this history line I see what was the state for the previous execution which was number nine and we see that it's marked with the CRT which stands for the critical black critical product back so what I do right now I will go to the level of the all executions show you that we have execution number ten and what I do I just call the analysis function just go so I tried to stop it and say you that nothing nothing fancy happens only wife or the engineers in this case because the only thing you do you just click the button analysis and all the magic happens on the background machine learning like start to work analyze the fail compare it with the existing multi-dimensional space of the vectorized text to find the item so let's back to the presentation so now I just executed the analysis refresh the page and what I see that 39 of the test cases was analyzed and marked as a critical issue related to like to this particular latest execution so while doing right now I just go back to this particular test case showing that it's like this save name opening up and see like right now it has it marked as a critical back as well it has the flag AAA which means that this item was analyzed by the Machine Loraine it also has the history and it tells you that the Machine ring have analyzed set this particular issue details his little link to the item which is considered like the equal item of feel sorry sue once again we we are in the same test case and what did report portal it analyzed the latest fail based on the history of previous executions now when you have all this information basically like when alas all this fails we have the distribution and all the fails are marked by the engineers which one related to which type of the fails what it gives to us we have the so-called distributions of the failed what you see right now on the screen like on the right side is the regular view of the majority of reporting systems you can find so they just show you that you have green and red test cases sometimes gray but if you have those red test cases to have the idea why they failing most likely not because you have to spend the time to understand what's the reason of the fail and with report portals it's the engineers can do this categorization and later machine learning can use this information to categorize that those fails you have the trend on the Left which shows you the distribution of the reasons why a test automation is failing so basically you see the light red line which related to the product box and that means that that's the value of your test optimation because why do you need the test cases which are failing because of the system issues as they do not test anything like the same the test cases which are failing because of the automation issues because of the locators I know they still do not have any value for you because they do not test so those tests which have have been failing because of them product issues those are the fails which have the value for you for your project and for your automation so as I said we have a lot of information in database and we can build variety of the widgets graphics like show you the most flakiest meant the most failed failing tests I'll grow paid according to different rules summarized to basically prepare the best information for your managers so they will be super excited like I have the reports the best thing for it like you just created once create this dashboard share the link with the manager and forget about it case he can open up in see results even appearing here in runtime so that was the report portal I can like have a time I will click it more but now let's switch to the [Music] technical part hopefully I will squeeze your brains right now but let's see how it works so as you see we use the text for the machine learn we use your stacktrace we use your failed stacktrace turn the issue so how we can proceed with it how we can use it and there are different types of the way how you can analyze and work with the logs but the main steps which you definitely should take and which I will show you right now I will show it based on this little fairy tale about the little bunny which lives in the forest so initially when you have the your stack trace when you have your logs they are not really prepared for the machine learning because they have a lot of variety of the words different articles preconditions and all the different words which makes English harder to study and what you should do with these looks you should normalize and this term is called stem stem the logs so basically clean dry and normalize your text before you start to proceed it so how it looks like first of all we remove all the preconditions all the articles like everything which makes the English once again have a hard to study and next we'll go with the columns semi columns dots like remove all the those dots on the screen then the next we'll replace the capitalized letter with the lowercase in order to like simplify the text minimize the variety of the words minimize the variety of the terms and now this text looks like more like the russian-speaking guys trying to speak in English with you even worse for the next step because we replace the variation of the word like Bonnie will be replaced with hair hopefully I pronounced it right and for the next step we even do the normalization so here it means that we replace such a forms like M is our with the form B so basically we replace the all available variation for particular verbs and words will fit just one single and pretty understandable form of it so in case you have this kind of the text we also place all kind of the wards where and blah blah blah so even if text became even worst and for the next like right now we have the clean text which is I would say pretty ready to be vectorized so what we do for the next step we transform this text into the vector because machines they like not really fond of work the text they can work with the numbers so if you want to machine to make something just give the numbers for it so that's the approach how you work with the text just for instance here I'll have this short piece of this stack trace which appeared after the test fail and since we have in the beginning the part which is java.lang assertion exception there's a pretty common term for the job related issues it basically just makes the noise within our text so this part will be removed and all the rest will be normalized vector I like normalized cleanup and at the end of the it will have this line which will be converted into the array so now we start to convert the this array into the vector so how it happens for instance we have the first word from this line invalid in with respect to the fairytale we have it should be have so we take the term free tf-idf index term frequence and inverse document frequency so what it means turn frequence that means how many time your particular term appear in this particular piece of the text this piece of the text called document so we see it like we have two of them right here so that means that TF index will be equal to two the IDF index is inversed index of how many documents you have with this word in holyoke collection so basically just you can imagine the library with the books and the TF means how many words how many word hate you have in the book and the IDF means how many books about the head you have in whole library but in order to to here have the inversed index we have to take the logarithm of it and basically the value will be like that how you can calculate it just like with these simple things easy but at the end of the day we need to just multiply those numbers and now we have the ratio or scale of tf-idf index for this particular term does anyone understand what I'm talking about easy now Alexei Alexei that's for you easy so hopefully you are still with me hold on like we have 50 more slides to go but it's simple well so now we have the ratio for this particular term which can be easily set it into the vector so how it looks like I have the axle for invalid voice term all for the hair and the ratio of it or size of it should be like 0.06 for the next term in this line we do the same and have the value as quarter of like 1 so now this term like now this vector or basically vectorized representation of our text should be that size with each next term we add additional axle with add additional dimension and basically reach it against the other axle and is much more words you have in your text more and more dimensions will appear within you mouths multi-dimensional space are you getting these guys so but it's still much simpler to understand it in two dimensional space so let's just simplify it so we have two dimensional space which based on two words and for instance we have this type of the vector placed in within this space so once we have like let's correlate it with test automation so once we have our failed test case which is marked by the engineer as a product issue that means that this particular size of the vector in multi-dimensional space belongs to this particular position and organize this dot which like equal to our product issue with the next and every every single execution and with every next single analysis more and more items will appear with this multi-dimensional space around the vector you build so that comes like because in majority of the cases test automation related issues they have the particular set of the terms which is related to test automation product bugs to product system to system so one at one day you can find that those fails are grouped around the particular spheres or like in machine learning it called clusters so you have this section for the like area for the product issues they have the area for the test automation issues and for instance here you have the area for the system issues so what happened happens when we have a new test automation field fail who can guess what we will do yes we will take the new test automation lock new fail normalize vectorize it and place within this multi-dimensional space and most likely it will appear for instance just for instance somewhere here which means that with particular predicted tivity this fail belongs to the system issues so what we do we will take the items of vectors which related to a particular test case fails will take them and from this small sphere and start to compare it based on ranking which is more closest failed to the one which you currently asked us to analyze so this algorithm called the qiyan analyst learning the Kenan stands for the key key key nearest neighbors which is which means that it tried to find X nearest neighbors of the vectors around the one you currently analyze and the laser learning that means that you have no pre trained model for the whole process of the analysis which means that the pre trained model made and happens at the moment of when you Curie the machine running to make the analysis and what this approach gives us the possibility to train and train and train and train your machine learn with each next step of each next analysis which was made by the engineer okay some more slides to go so how we made it we actually took the report portal API an elastic engine to save the indexes and implement it one small service analyzer in between of them using the gold language why the goal language because it's like consuming really small amount of the memory so how it works in what we do first of all we need to put our vectorized form of the text within the elastic so here lassic helps us a lot and we use this like mapping to put the data into elastic so named main objects we have here the relation to the launch name so basically that relates with the scope of test you execute the test item itself so the link to the particular test case the issue type is a product of dimension issue blah blah blah in log level it's the level of the error which appeared so the main thing which helped us to prepare and make this nice fairytale ugly piece of the english words that's the analyzer part which helps us to make the church' filter make the recognize the text technology terms recognized means the break the your words into separate terms according to like specific rules and have the filters for the those tokens basically remove the articles remove the freaking preconditions yeah and semi columns and all the commas so for the next step we have like in case when we need to analyze we ask elastic like give us the reply for this particular Curie and this Q we have three main sections in it so it must not include item within response must not include items which are not analyzed so basically if they have still have the to investigate flag that's not our thing it should include and boost the priority for the items which have the equal lunch name so which means if it still belongs to the same sequence of the test a group of test or lunch of the test the purity for this item should be higher at the same time we have some boost priorities for the items which was analyzed by the engineer instead of the machine and some other small tricks and the main one is must so what must be in this response it should like first of all you should have the logger it should exceed like the issue type should exist within it and we also ask you like to give us more like these fields with the questioners distance as 10% which can be translated into the minimum minimal should match 90% so this request returns us the list of the items grouped by the priority and we just take the first one with the highest rank and put it Internet analysis done [Applause] okay so here's the link to our landing page you also can find the link to the github right from this screen also you can find me in majority of the social networks as gah Dimitri and ask the questions and for the current presentation still like go to this leader ask the questions and I will switch to them in a couple of minutes I know after my four minutes I have 15 minutes for questions right who can help me with that no no just for five minutes okay so let's keep the rapport portal walkthrough I guess you you saw it already but we can switch to the questions yes Olli digitalisation I would say okay so I open up the same on my phone and see curious created through the UI I have in max 100 results running sent bla bla bla bla bla this is configurable looks like somebody uses the report portal right by the way who using report on Reza raise your hands okay who knows at least about the report portal now yes like now everyone knows come on so yeah that's definitely the question from the users so yeah for the version of of the report port so we have limited it to 150 launches just because we have the MongoDB on the background and it consumed a lot of memory so we need to limit it sometimes in order to save the performance the build the good part for the version 5 we switch from the MongoDB to the PostgreSQL where we have notice limits and most likely this number will be increased to 600 lucky you are the next one so I should mark this one is done now let's just keep it all tonight this does not work in default installation described in this see in this side you to elasticsearch issue could you add more info elasticsearch setup well yeah the the elasticsearch is not that easy to set up sometimes so for it we have this special instruction on our landing page which stay States like you have to make some special preparation steps in order to make elastic ready for production usage so yeah just follow with instructions come on that's that's the thing that which Simon said when you work with the open source and with the big community your skin became too thin to like to accept all these questions because we still have the instructions which completely describe this issue and the steps you should do but we're all engineers nobody read the instructions I will do it myself your thing doesn't work so once again you should put attention to the how to set up the production ready elasticsearch so in this case it just will work once again we have these steps fretted at landing page how you can compare the report portal real-time capabilities we've continuous testing in a shoe devops portal i have no idea what their portal who asked this question please find me after the talk we have will have the discussion about this is there a way to push test results in a predefined test execution which is created by api yes it is and for the version 5 will simplify this approach to make it super duper easy to send results into particular execution and even merged it on the fly with the retries and all this stuff so like we point into this not like issue but request with the first price right now how many manual analysis online should be done before start using your ought analyzer good question I would say that it really depends on the quality of your loss because if you have the looks which states like should be true but found false even Engineers cannot understand what's debated about so the trouble here that as more information you have within the logs the more information can be used by the machine learning so just created good looks and in more in majority of the cases after like 10-15 executions it started to give the best results like super cool results will be after one thousand executions yeah oh yeah I have to say I still have these questions will can continue out of the stage so thank you well when this version five will be released we plan it for kotoba 15 thank you just a few words I have some pretty good stickers you can grab one and the t-shirts which stays like we saw you're testing parameters Airport portal so grab one who can cook up who will be the first thank you Matt - Thank You muchly 3 [Music]"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"CI\/CD for Machine Learning",
        "Topic":"Automated Testing for ML Systems",
        "Video Title":"Learn How to Test Machine Learning Models from James Bach | ML in Testing\u00a0|\u00a0Worqference",
        "URL":"https:\/\/www.youtube.com\/watch?v=chEPjbULh4Y",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/chEPjbULh4Y\/hqdefault.jpg",
        "ID":"chEPjbULh4Y",
        "Publish Time":"2023-02-02T07:29:20Z",
        "Channel":"The Test Tribe",
        "Channel ID":"UC09Myr77xB6vAOZ7qky3MdQ",
        "Transcript":"how do we go about AI in software testing should testers Learn Python and eventually machine learning uh AI is a big topic lots of people are asking me about this they didn't ask me about this last year did the uh ideas are uh coming through uh just just this year so it is uh it's a hot new topic in fact I've been asked a couple of other questions that are related to this and I'm going to answer them all at the same time uh naven asks hi sir what is the latest exciting thing in software or testing that you want to share with us and I would have to say that the latest exciting thing although it's not exciting in a good way for me um but the latest exciting thing is has to be the fact that so many people want to apply artif icial intelligence to software testing or they need to test artificial intelligence systems and sharro asks what would be your approach towards testing machine Learning Systems so I'm asking a cluster of questions about this and now we'll deal with it first let me talk about testing an artificial intelligence system and testing an artificial intelligence system what what I mean is testing a machine Learning System I mean I'm talking about um uh neural networks if you're going to test a system like that the number one thing to keep in mind is that machine learning systems are trained on a set of training data and so you got to ask how might that training data be biased there is a tremendous danger that data used to train machine learning systems are biased that these data uh have built into them uh essentially mistakes and that those therefore could turn around and uh be terrible biases that will cause Injustice to be done whenever these systems are used so uh the first thing that I would uh do is try to question where did the data come from and how might it be biased for instance there are uh been systems that are designed to decide whether to give loans to uh people that are systematically biased against black people and we've got to be hypervigilant about that sort of thing the second thing that I am I would be concerned about is something called adversarial data or adversarial images now these are uh images or data that is uh designed to fool the machine Learning Systems because machine learning doesn't work the same way that learning works for you or I machine learning is not like human learning uh what machine learning does is it is basically an elaborate system for stat statistically associating input with output and how those statistical associations work are poorly understood once the system learns how input is connected to Output it it can't explain that to you or I this is a big problem in the industry because the machine just decides based on essentially its intuition and it it can't justify what it's doing and a problem with this is that sometimes there are not sometimes but all the time I think there can be bizarre side effects bizarre relationships between input and output where a picture which is it looks like a bunch of junk and garbage to us the machine will say oh that's a kitten that's a dog and it doesn't look anything like a dog to us and uh the images which look like garbage but which are identified as a kitten or a dog those are called adversarial images so if you are going to test a machine Learning System you need to learn all about ways to create adversarial data here's another example there's a a machine learning system used to identify hate speech that was created I think by Google or Facebook one of the one of the big guys and researchers have uh discovered that you can say anything you want as hate speech and the system will not identify it as hate speech as as long as you erase all the spaces between the words and add the word love at the end of it that's pretty ridiculous now a human can read uh sentence even if there are no spaces and a human can tell that something is hate speech even if it has the word love at the end of it but the researchers found that this was the uh it's called it the love attack it's the most effective way of doing hate speech and getting around the automatic detectors so you've got to study the different ways that machine Learning Systems can be spoofed [Music]"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"CI\/CD for Machine Learning",
        "Topic":"Continuous Training and Deployment",
        "Video Title":"Continuous training and deployment MLOps pipeline demo",
        "URL":"https:\/\/www.youtube.com\/watch?v=MvRFLqVjRyY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/MvRFLqVjRyY\/hqdefault.jpg",
        "ID":"MvRFLqVjRyY",
        "Publish Time":"2023-09-04T14:41:20Z",
        "Channel":"Empirical Software Engineering, University of Helsinki",
        "Channel ID":"UC5fXWiZVFSTTFe_U1EfpU-Q",
        "Transcript":"hello and welcome to the demo of our mlops pipeline in this demo we use the open wine quality data that includes two data sets a red wine and a white wi data set each data set contains 11 input variables which are some chemical attributes and one output variable which is a wine quality School in this demo you will see two types of models one for predicting red wine quality and another for predicting white M quality all models used in this demo are simple escal and elastic net models so we have already deployed all the pipeline components to a local kubernetes cluster beforehand and as you can see here we also uploaded the kfp pipeline used in in chaining to the main iio storage service in advance so uh this KY pipeline will be used to retrain both the red and white white models and uh as for the pipeline configuration the monitoring is configured in such a way that we monitor data drift and predictions mean absolute errow uh the configuration is written by using the curry language Pro by PR the like uh this one so uh this expression means that for each model if promethus detects the mean absolute arrow is larger than 0.3 and data drift occurs then an alert will be pending if uh such a large mean absolute error and the occurrence of data drift lasts for at least 30 seconds then an alert will be sent to trigger a retaining round as for the AP testing the same service level objective and reward metric are used in AB testing both the red and white wi models uh the service level objective requires that the 95th percentile response latency of the inference service should be smaller than 5 seconds and regarding the reward metric the smaller main absolute errow the better the model performance the progressive traffic split method is used in our AB testing so this means that if both the Old and retrained models satisfy the service level objective then traffic will be shifted to the model with a smaller mean absolute error the shifting ratio of traffic will be decide will be decided by iterate using its traffic spading algorithms and each AB testing consist of three three iterations and each iteration lasts 2 minutes and now as we can see from the dashboards a red and a white y model are already running in kerf and uh this figure can describe the progress of the demo so at this moment we are in this stage one so there are two models like two initial models running in Cas of the red wine model was trained using white wine data and the white wine model was trained using red wine data so in other words we intentionally trained these two models using like the wrong color data so their performance will drop when they receive the correct color data and now we start send in like red wine data to the red wine model and white wine data to the white wine model and the ground truth will be sent with like 10 second latency and the sending rate is one request per second now like after a few minutes we can see that promy the has detected that for both models the main absolute arrow is larger than 0.3 and data drift occurs so two alerts one for each model has been triggered and this two alerts will be sent to trigger a retraining run like to trigger two retraining rounds in Cube flow pipelines and then now from the dashboards we can see that two retraining rounds one for each model are started and by clicking the cube flow running ID we can and like we can see how the corresponding retaining Cube flow pipeline running is progressing so now here we can see that the retraining is completed and we go back to the dashboards here by clicking the model version the model version we are in the ml flow U and in this mlflow UI we can see like some more information of the retrained models such as the name of the cerve inference service where the model is running as well as the training data used uh in the ret training and we can also see like the parameters using model retaining and uh the evaluation Matrix so if we go back to the uh to this figure that describes uh the demo progress at this moment we are in stage two where we have two models for red wine prediction and two models for white wi prediction and now we uh start sending red wi data to both the red wi model and white y models and as we can see again from the dashboard now uh AB testing for both the red wine models and for both the red wine and white wi models are started so now we look at this figure again so as we remember that uh the the old red white model was changed using the white white data and the new red white model was was trained using the red wine data on the other hand the white the old white wine model was trained using the red wine data and the new white wine model was trength using the white wi data because now we are sending red wine data to all of the models then we can expect then uh we can expect that this new red wi model will be better than its predecessor however uh the the old white y model will outperform uh the new one now if we try to check the changes of uh the Ingress rule for example for the white white model then like when uh the reain new models are deployed in the first place the traffic expion uh was like 50 and 50 so 50% of the user traffic will go to uh the the old models and another 50% of the traffic will go to the new models so as the AB testing is progressing then at some points we can see that now for the white y models the old white y model is receiving 95% of the use traffic while the new white y model is only receiving like 5% because the old one uh is better than the new one and then finally as the AB testing uh is completed we can check the results so for example here for the red y model uh the new is better than the old and so here like we can see that the Winger is the new version which is this version two and again for the white wi model here we can see that the wing version is uh the old one which is uh this version one and so now if we go to the graph then we can see that like for the uh red white prediction uh the arrow was kind of large in the beginning because of the data drift so like uh the arrow is larger was larger than the 0.3 threshold then after the retraining and AB testing the performance of the red white prediction uh is becoming better"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"CI\/CD for Machine Learning",
        "Topic":"Continuous Training and Deployment",
        "Video Title":"Continuous Training and Deployment with ZenML and Seldon",
        "URL":"https:\/\/www.youtube.com\/watch?v=_rFZcNCJQnQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/_rFZcNCJQnQ\/hqdefault.jpg",
        "ID":"_rFZcNCJQnQ",
        "Publish Time":"2022-05-13T09:00:10Z",
        "Channel":"ZenML",
        "Channel ID":"UCi79n61eV2sVyYxJOqk_bMw",
        "Transcript":"zenometer recently released integration into selden um there's this really nice diagram i'll zoom in on this so you can see it right oh maybe not that much um but the idea behind it is you've used nml typically lots of people using it right now to kind of um automate training evaluation um all of that kind of stuff so it might be like you know importer steps and then regularizers and different training parameters and so on and then evaluation um but now xenoml have built deployment um like a deployment i don't know what the right term is for it um but it's it's basically like a deployer um concept and what that does is allows you to define an endpoint that your model will then get sent to after it's been trained um so again you'll see more of this in the example will probably make a lot more sense but this nice little graphic kind of shows you you might pull data in from a feature store train it evaluate it deploy it to seldom and then in this case you know run it on top of kubernetes um cool so uh i think the easiest way to show you this is just to show you some code so um why cool and hopefully you can see this i'll make it a little bit bigger because i know when screen sharing is not always the best um so if you're you're totally new to xml um the way it does it is with these really clever little decorators for your python function um so the two things you will probably need to know about most are the concept of a step and that's essentially like any you know kind of step within a pipeline um it is kind of what it says on the tin but um you know it could be in this case we've got an importer step which is going to take you know import the data split into test sets and training sets um the second step we have here is the trainer so that's going to go away and train uh in this case he's going to train a very simple circuit learn model um and then another step might be an evaluator so once i've trained a model can i evaluate it against you know some sort of test set or cross validation set um and see how it's performing uh so so those are steps right it's kind of like individual tasks that you might do as a data scientist or machine learning engineer um and then the way you bundle these together is in something called a pipeline uh so again i define this with this pipeline decorator and then i simply list my steps in there so i've got an enforcer trainer evaluator and then tie them together in terms of you know the variables that i might pass through so in this case you know i'm gathering my uh training in test sets and then i'm gonna run you know find my model as being the output of my training step and then i'm gonna run the evaluator on the model including the test set that i had for my importer um and then i simply define that and i can just do pipeline dot run and so i can let's actually show you this happening okay so uh i can just do python will now run that pipeline for me um so what you can see is it says using stack default we'll talk about stacks in a minute um but then it's running my importer step it's timing it it's running the trainer timing that doing validator and putting out the output of that which if you noticed in the code down here my evaluator prints the the accuracy for my model cool so that's a very simple pipeline with the xenoma within xml and the idea behind this is these are like super reproducible and just by adding those python decorators it adds tons of cool functionality to it so all of the inputs and outputs of each of those steps automatically logged tracked so if if i go in and let me just change something in here so you'll notice i disabled the cache on my pipeline if i run it just by default and run the same thing again and this should run much quicker right and you'll notice what it's saying here is using cache version of the importer so like it's going to say oh you don't need to import that data again you've already done it before automatically caching the steps within my pipelining that i need but so there's a kind of very quick um rundown of what zen ml is um so how does that work with seldom so the this example here um and again you know kindly built by the guys at xenml so you know give them a big shout out for this um really awesome that they build these examples for pretty much every integration that they work with um and what we're going to do is we're going to run through the the fashion mnist data set which is just a bunch of images of clothes um if you're not familiar with it it's very much like mnist right it's a very simple uh classification problem uh and then we're gonna train models either using tensorflow or scikit-learn um and then we're gonna at the end of that pipeline it's actually going to add a deployment set which is going to automatically deploy the model straight to seldom core running on a kubernetes service so um i guess this is where i might need to explain a few more things so we've talked about steps and pipelines but then there's also this concept of a stack so if you look in the middle here this is the pipeline right and i appreciate this diagram looks a bit confusing when you first see it um if you look straight down the middle you've got kind of source data split pre-process training deployment right that's pretty simple that's our simple pipeline that we're going to run um but if you look over on the left hand side there's in this big box which is um you can see on the left hand side everything in there is what's defined as a stack and a stack is essentially your reproducible endpoints or like configuration of tools that you would use to run pipelines um and why i love nml is that they're totally agnostic of the tools that you use right so it doesn't matter whether you're using you know dvc for data versioning or sell them for deployment or you know feast as a feature store right it doesn't matter at all and what zenmel does is it kind of decouples you from all of those and allows it to be totally reproducible regardless of where it deploys um and and that's what's really cool so the way you do find that is in something called a step um and i think i might be able to show this here actually so let me just move to my other example it's interesting this um again probably i think alex from general is on here so uh what have i done wrong there so i'm all stack this maybe cool yeah so so what you can see here is that there's like this default stack which is just runs everything locally um and xenoml will automatically track star you know stick in its own artifacts or have its own metadata store and orchestrates things um but you can also define your own stack so maybe it's like you have a local stack that you use for development uses a bunch of you know dependencies on your own machine and then you have a production stack that actually you know uses s3 for your artifact store um you know uses some sort of um you know maybe ml flow for your metadata store uses kubeflow as the orchestrator and then deploys it on top of you know seldom on in this case like you know eks cluster or something like that um and you and you can chop and change all of the tools that you replace in the stack and then deploying stuff or running the pipeline looks exactly the same xenoml handles all the different integrations to those tools so whether i run this you know if i change my stack to local or default here and just run it everything runs locally and then i just do send ml stack set you know in this case i've now got another one with like local aws right then it's now going to run in this case you can see the artifact store is now sitting on hands so whenever a model is trained it's actually going to get pushed to an s3 bucket um and then i also have a model deployer that i've um configured in here i've called it seldom eks actually in this example i'm going to run it locally on a kind cluster um but you can see again how you know it doesn't really matter what the uh where the cube cluster is where it runs you just define the thing in the stack and then it um the whole thing runs from there it makes it's find a bit smaller okay uh when i got to any any questions at this point i realized there's a lot of information to take in um cool all right well i'll continue to run through some of the stuff from this example um i had a bunch of notes to remember that i should go through and then realize i don't have a second monitor here because i'm in a meeting room um so apologies if i have to tab back to here every year and then um cool so we talked about stacks um i've shown you the list the stack what i didn't show quickly was the in integration is literally like as simple as this you go integration install um and then i'll in my case it won't actually do anything right because i've already got the dependencies in there um but you know that works for anything right if i wanted to do you know integration install ml flow right then i xenoml is going to install all the required dependencies etcetera um and all of the relevant deployers and so on like if i want to deploy it locally on mlflow or if i want to send my metadata to the you know flow metadata store all of that kind of stuff cool so um the next thing i'm going to do is uh okay facts let's run through the example quickly um because one of the really cool things that this seldom call integration does is it it doesn't just kind of spit out a manifest and then run a cube ctl apply right which is kind of what i'd assume you know would be the easy way to integrate with zeldin it actually automatically integrates with the rclone secrets so if you haven't heard of rclone that's the tool we use underneath to pull um model artifacts from things like you know an s3 bucket or you know gcp or azure storage or file store or whatever right um it's it's a tool that supports like all the different clouds um and underneath if if any of you've used zelda and core in any kind of like production setting you'll have played with it already because you have to define a secret in there that allows seldon to go and connect to your object store um and so the cool thing with xnml is that they handle secrets automatically within the stack and then when you run a deployment it creates that secret for you on your kubernetes cluster allow seldom the access to it to be able to go and talk to your artifact store so again in my code here right where you see that my artifact store is now aws because i'm using this stack um that secret has been configured automatically um okay any questions cool um fine so let's actually run some stuff then so i'm to do a yeah cube cc i'll get pods this is in uh what was the name space called cinema workflows i think okay and we'll just watch that um yeah make it a bit bigger um so i'm what i'm doing here is just watching the pods in that name space on my cube cluster and the reason i want to keep an eye on this is because you guys want to see the actual deployment so they get spun up um and then before we run anything in the seldom example let's jump into the code very quickly so you know what's actually going on so there's this pipeline file and it's a you know very much like you know the example i showed earlier this is just slightly more sophisticated but you'll see there are still these steps in here you know in fact this data importer looks very similar to the one you saw earlier in this case i have a normalizer i don't have a tensorflow trainer but and and again one of the cool things you can do is is add like configuration parameters in there that you can set at runtime so if i want to rerun the whole pipeline but i want to change like the learning rate or you know the number of epochs or whatever i can do that without having to go in and alter the code and xenon will track it all i've got an evaluator and in this case i actually have a you know a psychic line trainer and a psychic line evaluator as well so maybe i want to swap in different types of framework and see what types of models are going to perform well on my data um and then i have this step deployment trigger um and then a prediction service loader so what what i'll show you the important bit is these this pipeline down the bottom so this example actually has two pipelines the the main one is this first one uh so this continuous deployment pipeline and you can see what happens is the data gets imported then there's a normalizer trainer evaluator deployment trigger and then model deployment the deployment trigger right in this example is basically testing against the output of the evaluator and saying you know if the accuracy of the model is above a certain threshold we'll deploy it uh but if not don't bother and that's really important because obviously in you know in a reproducible pipeline you don't want to just train every model and then deploy them straight away you maybe only want to deploy the one with the highest accuracy or ones above a certain threshold um and then finally there's a second pipeline in here and that one's defined to run inference um so this there's pipelines just to to show you how you can actually run dynamic inference against a live endpoint which which we will do in a minute okay so i jump back over here um i can just run this so i can go type in apply boy and then i think it's i can set things like uh yeah epochs equals 13. so let's see i might have gone wrong um but what should happen is i will run through my pipeline and it should train a tensorflow model um with 13 epochs in here okay which hopefully be quick i do have a gpu on here so you won't be waiting too long okay good feel free to ask questions over the next seven epochs if you're fast how does the model get into the um into the blob storage does the the zelda deployer component have the credentials to put it there and the model has the credentials to pull it from there okay so that's actually what's just about to happen it's a very good question um is that when this model is trained uh it will then get so as i mentioned xenomel like keeps track of everything and stores all of the the artifacts automatically for you um but because i've defined my artifact store for xenoml to be an s3 bucket um what's happening here you see in fact there's a line here um xenoml is going to automatically write the output of my model so my trained model file to my s3 bucket and then so that's now in a remote s3 bucket uh the way that seldom then accesses that is it will get given this url you know like we're used to passing to sell that is you know his here's my model uri um and the bit i was mentioning earlier is that i don't need to go in and configure seldom to use the same secrets that i have done within xenoml because xenoml automatically passes them to the cube cluster for the deployment to use um so it's pretty neat on that front okay cool so um what you see is my yeah my model finished training it got written through an s3 bucket um and then the deployment trigger okay passed because it was accurate enough this 92 accuracy and now it created a new zelda deployment for me um and if i jump over to my tab over here you can see that you know it's actually spun those pods up and i now have a seldom deployment running on my kubernetes cluster um and that was all within this reproducible python pipeline uh from zen ml um which is really cool um and then what i can do is you see it's like it spat out the the prediction url again another kind of neat feature that it does um rather than you having to go and dig in and understand what the the um the makeup of that url is it's all done automatically um but i can now run let's run the prediction pipeline and we'll actually run some live inference against that deployed model okay cool so what you'll see here is again it's you know um using this dynamic uh data importer where and what that does is it goes and like grabs a bunch of like test data um for this fashion and this model um and then it's gonna do some pre-processing fire it at our model and you can see that it's spout a bunch of predictions so this batch request head i don't know whatever that is like 50 different um test samples that we've fed through and if we wanted we could go and compare these these labels with the actual classes to check um that they had made their predictions correctly um so that's pretty cool right and then i guess something i wanted to show was that xenomorph the reason you design these pipelines is because they're like totally reproducible so if i wanted to i'm just going to get my here we go oh come on um so if i ever wanted to rerun this whole pipeline but this time swap in a scikit-learn um classifier instead then i can do that right i can run in this case i'm still running the same thing the deployment pipeline but maybe i'm setting the threshold a bit lower because i'm not sure how accurate this model will be and then i'm running model flavor psychic land and that will what that will do is that swaps out the tensorflow trainer and the tensorflow evaluator and swaps in the psychic lan one instead in fact you can see it didn't even bother training the model because i ran this earlier and so where's this training yeah uh here you see this highlighted section saying using cached versions of cycling training so that typically would like take about 25 seconds it didn't even need to do it this time because it said you know all your data is exactly the same you're training with the same parameters um so i'm just going to use this cached model i have earlier um and then it's what's going to happen it's going to run through it and it's going to replace my deployment with that because the accuracy sits above this threshold that i've i've defined um so and we might see a change and we're going to change over here yet um cool any questions just making sense it's kind of hard when i like i can't i can't see anyone other than myself and my screen so uh you know shout if i'm if i'm you know making no sense at all that makes sense it makes sense um cool so the final thing to show is some of the the actual um like xml integration is it has this like server models thing so i can do xml server models list uh so now i've got a model deployed out there uh i've replaced it with my psychic learn one i can rerun these pipelines with different parameters each time um and then but the integration also like will pull back the live status of my deployed models which is really cool um so you know you can see here my status is green it hits the unique identifier for that um deployment um the pipeline that was used to run it because you might have lots of different pipelines it kind of depends on the use cases and what type of models you're building um and then my model name itself and even better i can do a serve models describe and then if i give it this id um it'll actually give me like a ton of information on myself and deployment um you know one of the things that we hear from people a lot it's like it's a bit of a pain to figure out what my prediction url is well xenoml has already like automatically built this url for you um because it knows where you've deployed it and how the route is built up um so you you can pull that straight back with this ml cli um which is really cool and it's showing you know again uh somebody asked a second ago i think was it fabio about like where's that model getting stored um so xenoml when it finishes the training is automatically pushing into my s3 bucket which is called cinema seldon um and then this was the output of that that pipeline run so you know whatever that was run number 196 um and then this is the model uri that gets passed to the seldom deployment um when when the actual manifest is created and then applied to kubernetes um but the cool thing here is that i can i've kind of got all the traceability all the way back to show like here's the pipeline run id that was used to frame that model that's now deployed on this endpoint um over here uh so like if i needed to i could trace it all the way back to the exact version data set that was used to train the model so like a super powerful way of uh adding auditability to everything we do and then finally i could do a you know let's do senate mouse delete so i can actually you know it can delete seldom deployment straight within xenoval now too um and what happened here is it will go and remove that deployment service and we should see if we jump over to look at our kubernetes pods over here yeah it's it's now terminating those uh three pods that were running you know for seven and a half minutes or whatever it was so we're down to two et cetera um cool so i've like whistled through all of this i appreciate it nice tons to take in um but the cool thing is you guys can do it yourself so this um github repo talks you through how to do this um call example and in fact another thing i love about zml is you can even pull examples like this so you can do like uh example full uh selden deployment or something like that i can't remember the exact this is probably wrong but um the exact terminology you use oh no there we go cool um and it will pull down these examples for you and then you can go and like um you can go and run them locally so you don't even have to go and clone the right pieces of github uh it's all handled within cli um but yeah kind of cool things i've done um i guess if anyone has any questions you know please fire away uh i don't know if alex is still on"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"CI\/CD for Machine Learning",
        "Topic":"Continuous Training and Deployment",
        "Video Title":"Continuous training and deployment pipeline for robotics",
        "URL":"https:\/\/www.youtube.com\/watch?v=8LhJ28TQGls",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/8LhJ28TQGls\/hqdefault.jpg",
        "ID":"8LhJ28TQGls",
        "Publish Time":"2024-07-31T20:04:25Z",
        "Channel":"Inception Robotics",
        "Channel ID":"UC2pUHoK7Te7ebJm6s9xZ0xg",
        "Transcript":"hello everyone this is a demonstration of inception robotics continuous training and deployment pipeline tailored for robotics use cases most of the robots that are being deployed use sophisticated AI algorithms that require constant training to improve navigation or perception abilities the entire process is very involved complex and time consuming it involves data collection data management label extraction training performance evaluation and then deploying train model to a real robot imagine doing all of these in just a few clicks that's where our pipeline comes in to streamline this entire process in this demonstration we are going to show you how easy it is to retrain and deploy an open-source outdoor navigation algorithm terapen based on the newly collected data using our platform we will be using a clear path Jackal robot with a Vine VP 16 ligher Intel 515 liar camera and Intel nuok let me walk you through the steps we will first create a new project I will specify the project name as pipeline demo and the project type is going to be continuous AIML pipeline for this project connecting to real robot as required we will specify the robot name to be clear path and and the algorithm to be terapen and the training frequency is going to be daily so we need to copy this uh following command and run it on the robot we'll do that later next we'll click on the setup project and then launch button to execute the project project has initiated successfully we'll just click on launch button to execute the project now now we will SSH into the robot to run the call command that was provided during project creation so what the script will do is it will enable robot to collect the rosback files upload them to the cloud and also enables the ability to deploy the latest train model from Cloud to the robot after running the command the robot is now ready for some data collection we will need at least 15 minutes of data for this algorithm so let's quickly collect some new data let's jump back to the portal and take a look at what's happening we can see the data collected on rossback dashboard on the left side we'll see select a rossback file and you can you can see the collected data on the right side we have integrated web tool to visualize the sensor data as you can see we collected some data of asphalt grass surfaces let's see what what is happening in the pipeline we can see the progress on the main dashboard so once these back files are collected the next step is merging these back files and then extracting the labels for further training and once the training is completed the platform will Auto automatically fetch the latest train model and Trigger the terapin algorithm execution in multiple High Fidelity test environments that we designed in Nvidia ISM the training and test executions will take some time so let's fast forward to the end so now you can see the retrieved test results are highlighted in green that means test execution is successfully completed let's take a look at execution dashboard where we can see all the test execution utions and their results on the left side you can see all the test executions and their Associated trained model weights so on the right side we are providing the quantitative test results and in addition to the quantitative results we are also providing the recordings of each test cases so that you can evaluate the qualitative performance improvements let's take a quick look at the video of one or two test cases so this is as fault and grass environment and since terapin is trained on this data you can see robot chooses a smoother surface over grass to navigate and in these environments we have combination of different outdoor surfaces for example in this like we have Mulch and crass and the surface properties of the simulations replicate the real world surfaces so that we can compare the IMU sensor variations and this is concrete and grass after evaluating the quantitative and qualitative performance if it meets the desired performance we can choose to directly deploy to the connected robot with just one click let's click on the deploy button to initiate the deployment we can see a prompt once the deployment is completed as you can see the deployment has initialized we'll just wait for the prompt here you go the latest train model is now deployed Ed on the robot this is the end of the demonstration please feel free to share this with someone you think might be interested in this if you want to learn more about this pipeline please feel free to reach out to us at team@ Inception robotics. thank you"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"CI\/CD for Machine Learning",
        "Topic":"Continuous Training and Deployment",
        "Video Title":"Continuous Deployment vs. Continuous Delivery",
        "URL":"https:\/\/www.youtube.com\/watch?v=LNLKZ4Rvk8w",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/LNLKZ4Rvk8w\/hqdefault.jpg",
        "ID":"LNLKZ4Rvk8w",
        "Publish Time":"2019-10-01T13:00:36Z",
        "Channel":"IBM Technology",
        "Channel ID":"UCKWaEZ-_VweaEx1j62do_vQ",
        "Transcript":"hi I'm Eric Minich with IBM cloud I want to talk a little bit about the difference between continuous deployment and continuous delivery now for both of these practices you're looking at trying to take new code valuable move that as quickly as we can out to production that's where that valuable code is useful so what you're basically going to end up doing is you're going to take your code you're gonna build it with a continuous integration tool deploy it to a test environment or two or three run all of your automated tests and then it's going to move into production now the idea of continuous deployment the term was really coined by Timothy Fitz back in 2009 in what he described was a process by which a company called I in view deployed to predict production 50 times a day the code came in and then no human touched it again until it was in production how do you do that without it being completely irresponsible right and and that was what was really interesting here well you have the automation to move things they had one core test environment so they brought these things together but thousands of tests really robust testing suite that ran across a large number of servers so that you could run thousands and thousands of tests in a short period of time and know that your new code was probably pretty good because you had run these robust set of tests that you could depend on and because you believed in your testing you could put that out into production now this was a consumer facing app that wasn't taking credit cards and so you could just put that out to 2% of your users or 5% of users do what's known as a canary deployment and make sure that the servers weren't falling over that people were still interacting with the app as they should so you had a robust testing suite and then you were also testing in production right you were monitoring really robustly and if things were going badly out here in production if like two of your servers keeled over okay you're gonna pull that change out of the load balancer you have the rest of your servers working fine you can go back and fix it in code it'll go right back out the door real quick nice so this is really an extreme approach someone writes code it ends up in production no one else touches it where you have a little bit of regulatory approval and you need to eyes on anything for it gets production back here in code review back into code you can do a code review and another developer can provide that check and then the testing provides you with another safety net now this approach this continuous deployment continuous deployment tool production is really what it means that's still extreme and something a lot of organizations both don't have the testing for nor do they have either the kind of stomach or the regulatory approval to do that without some sort of approval in here right some sort of human decision sign-off check that the move from staging to production from your last test environment to production is okay right and so this sort of thing requires you know another check and it's typically that just what distinguishes continuous delivery is that in continuous delivery you're automating as much as you can kind of code through end of testing but you're gonna have a decision point here at the end for what we do with the approval and so this is just one step a little bit more cautious than deployment that's what most organizations that I see are trying to get to today they still need to do a lot of automation in their testing area in order to achieve that to even get to the point where they can just go straight to a business decision if should we release this or not but this sort of automated as much as I can make a decision to go or not that's what we see with continuous delivery that's where we see people going but this extreme approach continuous deployment it works we've had documentation that it works now for over ten years it can be a reasonable approach if you're in the right situation so that's continuous deployment versus continuous delivery thank you if you have questions please drop us a line and if you want to see more videos like this in the future be sure to LIKE and subscribe"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"CI\/CD for Machine Learning",
        "Topic":"Infrastructure as Code for ML Systems",
        "Video Title":"Infrastructure as Code with Cisco",
        "URL":"https:\/\/www.youtube.com\/watch?v=SzeIvqkVS8M",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/SzeIvqkVS8M\/hqdefault.jpg",
        "ID":"SzeIvqkVS8M",
        "Publish Time":"2021-05-07T19:57:32Z",
        "Channel":"Cisco",
        "Channel ID":"UCEWiIE6Htd8mvlOR6YQez1g",
        "Transcript":"as infrastructure changes so does the way to manage it the answer infrastructure as code replacing old manual processes with code to manage and provision infrastructure infrastructure as code is more scalable reliable and agile than manual methods automation reduces human error and processes can be endlessly replicated tested and incrementally improved by people anywhere in the process for faster delivery and app deployment to keep up with your business with less downtime for years to come the world's changing fast but with cisco solutions you're always up to speed that's infrastructure as cisco can deliver it to you with infrastructure as code"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"CI\/CD for Machine Learning",
        "Topic":"Infrastructure as Code for ML Systems",
        "Video Title":"Infrastructure as Code EXPLAINED \ud83d\udc69\u200d\ud83d\udcbb (in 30 seconds) #programming #technology #software #devops",
        "URL":"https:\/\/www.youtube.com\/watch?v=klnBwQBlDuU",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/klnBwQBlDuU\/hqdefault.jpg",
        "ID":"klnBwQBlDuU",
        "Publish Time":"2022-09-20T15:27:19Z",
        "Channel":"Coding with Lewis",
        "Channel ID":"UCWI-ohtRu8eEeDj93hmUsUQ",
        "Transcript":"you can create an it infrastructure in a text editor how infrastructure as code is where you can manage and provision data centers through files that you write there are a lot of pros one costs navigating through user interfaces makes it hard to understand what is running and what isn't seeing it all from a top-down view makes it easy two is sharing you can share with your teammates so you don't always have to be on call and three is to automate the beauty of coding is that you can automate every part of it rather than forcing yourself to write scripts there are many open source tools like terraform and ansible that can do all this for you now i have to figure out what i'm doing"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"CI\/CD for Machine Learning",
        "Topic":"Infrastructure as Code for ML Systems",
        "Video Title":"Machine Learning Training &amp; Deployment using Infrastructure-as-Code with Python\u00a0 |\u00a0 Scott McAllister",
        "URL":"https:\/\/www.youtube.com\/watch?v=8DjqdfAAtGc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/8DjqdfAAtGc\/hqdefault.jpg",
        "ID":"8DjqdfAAtGc",
        "Publish Time":"2021-03-25T14:00:02Z",
        "Channel":"Data Strategy Professionals",
        "Channel ID":"UCO6JE24WY82TKabcGI8mA0Q",
        "Transcript":"all right um so yeah uh this evening i'm gonna talk to you all about machine learning model training and deployment using infrastructure as code um in other words how to get people to actually use the wonderful models that you're you're building and i'm really gonna start by just spending most of the time talking about an actual problem and the solution to it and actually talk very little about actual infrastructure as code so the problem statement starts with the task of automating a manual process and this is real world too um from the government space where contract analysts take about two weeks usually to review end user license agreements or eulas it's those things we all blindly accept we get a new operating system update we're downloading new software it would take them 14 days two weeks to review those things and they were wondering if a machine learning model could use natural language processing to parse through all those eula clauses and then predict ones that are potentially non-compliant for federal law and then an additional requirement was like could that model be invoked by a web application like some kind of ui that would automate the process all these contract analysts were doing with like emails and excel and things like that nature and so we actually we built something these are some screenshots here it's just a web app where they would upload documents you can see on the right hand side there's a bunch of pdfs and word docs that are uploaded and those all get scanned and all the clauses of these end user licensing agreements get broken out like you can see here for one of the docs and then the idea is like in this web app it would be calling a model to predict whether or not each clause was compliant or not if one was potentially not compliant it'd be flagged like you can see in the background here is yellow and then they can click on it get a little modal that might tell them you know what particular words or phrases flagged it is not compliant so it's not just a black box model it's a little bit interpretable and then they could of course provide feedback like agree or disagree with the prediction and that just would support model retraining to prevent the model from drifting in the future so behind the scenes though is where i want to talk about the this model being deployed and using infrastructure as codes the idea is to have a model as a service so the architecture for this application here on the right side is that web app just built with python and docker and using redis as a service worker to communicate with that model which was an api over https everything on the left hand side here which looks a bit complicated is what you could deploy in amazon web services using infrastructure as code and although it does look complicated it's really not when you use infrastructure's code it's when you're writing your code in a language like python or typescript for example but the idea it really just starts down here on the bottom with that little brain looking icon the idea is you're going to write some infrastructures code to give yourself a remote compute environment where you could train a model or models as with as much cpu or gpu as you want and then amazon will just take care of the process of taking that model say your best one and putting it into an object storage for you and then handling the task of deploying it to a web server putting a reverse proxy in front of it in a load balancer and then giving you the ability to write some additional custom code to pre-process post-process input and output of that model making it all available as an api that's a little it's definitely complicated it's a high level diagram but to make it as simple as possible the idea is you want to train and deploy a model so that you can use it as an api so for example here i've got like python code pseudocode but then also curl the idea is like you've got an endpoint and you want to send to it some text like a clause like for instance it's just the phrase this is a test um this would actually be like a euler cause and you want to be able to get back as a response from this api what the prediction was what the likelihood of that prediction being what it is and then finally an explanation which is really just base64 encoded html of the uh table that you saw here explaining how the prediction was made so that's is really like this is what you would actually want to solve that problem that the government had and it might seem daunting like how would you build something like this stitch all these different things together to be able to provide basically this functionality to a web application a web client and the answer is just with infrastructure as code so like i said earlier the idea is you want to use all these different cloud resources like you can see down here like there's got to be a space a box for you to host models there's got to be a like an api service to give you that end point and amazon you would have something called lambda functions where it's custom code to control how that model is invoked and that's all going to be used by the client application setting all that up you can write python code or typescript or java or i think even c sharp to create that infrastructure and this is the actual code i use to make it's like not all of it but at a high level it's what provisioned everything that you see back here and the benefit is that it really like this tool it's called the aws cdk the cloud developer kit really democratizes for us data scientists or people that know language like python what is otherwise a really difficult task of like setting up all this infrastructure because you can find an example of this exact workflow which is a dime a dozen on their github for free it's just there you've just got to tailor it to your own needs i do have questions this is extremely cool so um i have had uh i've had some experience working directly with um the aws sam sort of specification or with um or with cloud formation directly just writing their yaml files and to provision these resources um do you find that like um do you find that working with the python cdk is a better experience yeah i i'd say definitely um the cdk like sam does use cloud formation behind the scenes so you can actually run commands to just generate the equivalent cloud formation templates gamma or json for your python or typescript code i think the additive benefit here is that you can be much more concise you can write 10 lines of python to generate like 150 lines of yaml but then you also get the ability to have intellisense in your ide and code autocompletion and you can also then write unit and integration test for the code that's provisioning your infrastructure so you can write assertions to make sure certain conditions you want in place like say a database using like secrets manager for a password or something or having a certain password policy you can bake those into your test um and then like just like you could with cloud formation templates you could source it all in a git repository too that is fantastic yeah it is a pain to put those templates together and then just hope you've got everything together so that when you go uh sam build sam deploy okay they didn't work sam build step deploy again please yeah okay that didn't work yeah that that's that's fantastic yeah the one downside is there's not uh one like a hundred percent of the stuff you can do in cloud formation isn't supported yet in the cdk um with like their higher level constructs but you can get really low level um with the code and do anything you could do in cloud formation but at that point it's like you might as well be doing it in cloud formation yeah i guess uh one more question um do you find that lambda is enough for um most use cases or do you ever find that like the amount of space or the time limit is an issue and like i i imagine anything where you're where you want it to be really accessible to users and respond in a timely fashion you wouldn't want it running longer than 15 minutes anyways but for most of the things you have to do do you that's what you reach for yeah it's like the swiss army knife uh in my mind although with the with aws sagemaker the models when they're deployed they'll put them in an ec2 instance for you that's always running um but you could like forego that and put the model in a lambda function now that they even support docker containers but um it's harder to get the compute resources you need sometimes for a model than a lambda function um sometimes an ec2 instance or a fleet of them like an auto-scaling group could be better so useful thank you so much"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"CI\/CD for Machine Learning",
        "Topic":"Infrastructure as Code for ML Systems",
        "Video Title":"Unlock The Power of MLOps with Infrastructure as Code (IaC)",
        "URL":"https:\/\/www.youtube.com\/watch?v=UVSFLbJWbo0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/UVSFLbJWbo0\/hqdefault.jpg",
        "ID":"UVSFLbJWbo0",
        "Publish Time":"2024-07-25T01:59:18Z",
        "Channel":"Dr. Carmenatty - AI, Cybersecurity & Quantum Comp.",
        "Channel ID":"UCSXI3SJ7k0nt_MNZ9ecTYjw",
        "Transcript":"Welcome to our Channel where we explore the fascinating world of technology and its practical applications today we're diving into a crucial Topic in machine learning operations why use infrastructure as code iic for mlops we're excited to have you join us on this journey of Discovery before we begin we invite our friends and viewers to share their thoughts comments and suggestions for future videos in the comments section below your engagement helps us create content that truly resonates with you you so don't forget to like And subscribe to stay updated on our latest videos now let's embark on our exploration of iiac in mlops to understand why infrastructure as code is so important for mlops we must first break down these Concepts and explain them in simple terms let's start with the basics and build up to the more complex ideas what is mlops mlops short for machine learning operations is a set of practices that aims to streamline and automate deploying and maintaining machine learning models in production environments think of it as a way to bridge the gap between developing a machine learning model and using it in the real world Imagine you've just baked a delicious cake mlops is like taking that cake from your kitchen packaging it properly delivering it to a bakery and ensuring it stays fresh and tasty for customers to enjoy it's all about making sure your Creation in this case a machine learning model can be used effectively and efficiently in the real real world now that we understand mlops let's talk about infrastructure as code IAC what is infrastructure as code IAC infrastructure as code manages and provides computer systems and networks using Code instead of manual processes in simpler terms it's like writing a recipe for your computer infrastructure traditionally setting up servers networks and other it infrastructure involve much manual work clicking through interfaces typing commands Etc with IAC all of this is defined in code much like writing a computer program let's use our cake analogy again if setting up traditional it infrastructure is like baking a cake from scratch each time following the steps in your head ISC is like having a detailed written recipe that anyone can follow to bake the same cake every time now that we've covered the basics let's dive into the 10 benefits of using infrastructure as code for mlops one consistency and reproducibility one of the biggest challenges in machine learning operations is ensuring that your models perform consistently across different environments from development to testing to production IAC helps solve this problem by providing a consistent way to set up and configure these environments when you use IAC you create a blueprint for your infrastructure this blueprint can recreate the same environment multiple times ensuring that your machine learning models have the same resources and configuration wherever they're deployed for example you've developed a machine learning model that works perfectly on your local machine without IAC moving this model to a testing or production environment could introduce inconsistencies different software versions configurations or missing dependencies these inconsistencies could cause your model to behave differently or even fail with IAC you can Define all the necessary components the specific versions of software the exact configuration settings and all required dependencies in code this code can then be used to set up identical environments for development testing and production ensuring your model behaves consistently across all stages two Version Control and collaboration when you use IAC your infrastructure setup becomes code and code can be version controlled this means you can use tools like git to track changes to your infrastr structure over time roll back to previous versions if needed and collaborate more effectively with your team imagine you're working on a complex machine learning project with a team of data scientists and Engineers without IAC coordinating changes to the infrastructure can be challenging someone might make a change to a server configuration and forget to document it leading to confusion and potential issues down the line with IAC all changes to the infrastructure are recorded in the code team members can review the these changes suggest modifications and understand exactly what's different between versions this level of transparency and collaboration is crucial in mlops where small infrastructure changes can significantly impact model performance three scalability and flexibility machine learning projects often require different Computing resources at various stages during training you might need a lot of powerful gpus while during deployment you might need a different setup optimized for inference IAC makes it easy to scale your infrastructure up or down as needed you can Define different configurations for different stages of your ml Pipeline and easily switch between them for instance you could have one IAC script that sets up a high-powered environment for training your models and another that creates a more streamlined environment for serving predictions you could switch between these configurations with just a few commands saving time and resources four Automation and efficiency in mlops automation is critical to efficiency IAC allows you to automate the entire process of setting up and tearing down infrastructure this is particularly useful in machine learning where you might need to run many experiments with different configurations without IAC setting up a new environment for each experiment could take hours or even days of manual work with IAC you can spin up a new environment with a single command run your experiment and tear it down just as easily when you're done this level of automation saves time and reduces the chance of human error it allows data scientists and ml Engineers to focus on improving their models rather than wrestling with infrastructure setup five cost management cloud computing has enabled access to vast amounts of computing power but it can also lead to unexpected costs if not managed properly IAC can help with cost Management in several ways first by making it easy to spin up and tear down Environ ments IAC ensures you only use and pay for resources when you need them for example you can automatically shut down expensive GPU instances when they're not in use second IAC allows you to Define and enforce policies about resource usage you can limit the types and sizes of instances that can be created preventing The Accidental creation of overly expensive resources finally because IAC provides a clear view of all the resources used packing and optimizing costs over time is easier six disaster recovery and business continuity in the world of mlops where machine learning models might be critical to business operations quickly recovering from failures is crucial IAC provides robust Disaster Recovery capabilities because your entire infrastructure is defined as code you can quickly recreate your entire environment in case of a major failure instead of spending days or weeks manually setting up servers and configurations you can run your IAC scripts and have your environment back up and running in a fraction of the time this minimizes downtime and reduces the stress and potential for errors in High Press recovery situations seven compliance and security many Industries have strict regulations about data handling and modeled deployment IAC can help ensure compliance with these regulations by providing a clear auditable record of your infrastructure setup you can build security best practices directly into your IAC scripts ensuring that every environment you create follows these practices this might include setting up proper Network segmentation configuring firewalls or implementing encryption correctly moreover because IAC allows you to Version Control your infrastructure you can easily demonstrate to Auditors how your setup has evolved and prove that you've maintained compliance eight enhanced testing capab abilities in mlops it's not just the machine learning model that needs testing the infrastructure it runs on must also be tested IAC makes it much easier to implement comprehensive testing for your infrastructure you can write tests that verify your infrastructure is set up correctly that all the necessary services are running and that your model can be deployed and run as expected these tests can be automated and run whenever you change your infrastructure code catching potential issues early this level of testing helps ensure the reliability and stability of your ml systems which is crucial when deploying models that may be making important decisions or predictions nine knowledge sharing and documentation one often Overlook benefit of IAC is how it serves as a form of documentation your infrastructure code becomes a living document that describes exactly how your system is set up this is invaluable for knowledge sharing within a team new team members can look at the IAC scripts to understand the system setup rather than relying on potentially outdated written documentation or having to ask colleagues for explanations it also helps with troubleshooting when issues arise having a clear code-based representation of your infrastructure makes it easier to identify potential problems and Implement Solutions 10 future proofing and adaptability machine learning is evolving rapidly and what works today might not be the best solution tomorrow IAC provides the flexibility to adapt to these changes quickly as new tools Frameworks or best practices emerge in the mlops world you can update your IAC scripts to incorporate these changes this allows you to continuously improve and optimize your ml infrastructure without starting from scratch each time in conclusion infrastructur as code is a powerful tool that addresses many of the unique challenges in mlops it provides provides consistency reproducibility and scalability which are crucial when working with complex machine Learning Systems it enhances collaboration automates tedious tasks and helps manage costs it improves security enables better testing and serves as living documentation for your systems by using IAC in your mlops practices you're not just making your current workflows more efficient you're also setting yourself up for success as the field of machine learning continues to evolve and grow we hope this comprehensive overview has given you a clear understanding of why infrastructure as code is so valuable in mlops adopting IAC is a journey and implementing these practices in your organization may take time however the efficiency reliability and scalability benefits make it a worthwhile investment for any team working on machine learning operations we'd love to hear about your experiences with IIA in mlops have you implemented these practices in your work what challenges have you faced and what benefits have you seen please share your thoughts in the comments section below if you found this video helpful don't forget to like And subscribe to our channel for more in-depth explorations of cuttingedge Technology topics your support helps us create more content like this thank you for watching and we look forward to seeing you in our next video"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML System Monitoring",
        "Topic":"Monitoring ML Model Performance",
        "Video Title":"ML Model Performance Monitoring with WhyLabs",
        "URL":"https:\/\/www.youtube.com\/watch?v=8Tupxt0ciRI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/8Tupxt0ciRI\/hqdefault.jpg",
        "ID":"8Tupxt0ciRI",
        "Publish Time":"2023-04-14T11:36:48Z",
        "Channel":"WhyLabs",
        "Channel ID":"UC9lRj988vfMTuUryc0JMPvA",
        "Transcript":"hey everyone uh thanks for thanks for joining we're gonna give it about a minute for folks to be able to join uh while we're waiting just a bit of housekeeping uh please feel free to throw comments in if you have any questions I'll probably answer them uh as I go if possible if not I'll save a few minutes at the end and we can hopefully go through some of them uh if I can't get to them on this uh webinar uh I'll I'll make a note for folks that want to talk more definitely get you some answers afterwards so like I said we'll give about a minute and then we'll go ahead and get started thank you all right so again thanks for uh joining our webinar today uh the the focus for today if you join the last one we're going to be focusing a lot more on the monitoring aspect uh of of managing models and Productions uh so last time we talked a little bit more about data quality and data drift uh this time we'll be talking about performance and performance doesn't just mean the performance of a model uh itself but really the performance of a whole system so uh let's dive in and we'll go into our agenda so what we're going to talk about today uh so we're going to start with you know what does ML monitoring mean uh what are the three pillars of monitoring uh and then we'll talk about those three pillars for different types of applications we'll also talk about what you should monitor so for those three pillars what makes sense what are the scenarios that we should be looking for and then lastly how can we use y labs to help so again if you have any questions any thoughts uh please feel free to throw them into chat uh as we're going along too I would love to know you know what you're curious about especially when it relates to uh monitoring what's most important for you what are the types of problems you're solving also if you have any models in production like how many models in production do you have from up to encourage folks to to put their thoughts on what's most important to them and also what kind of problems they're solving so a bit about me uh my name is Andre Elizondo I'm a principal Solutions architect here with ylabs I'm based out of Seattle Washington and uh I uh I love coffee so it's a little bit about me I tend to to take it in some form of espresso form so it tends to be the Americano I would love to know what kind of coffee you like if you like coffee at all some people like tea that's okay uh so that's a little bit about me and like I said I'm in Seattle I would love to know where folks that are chiming in uh where you're coming from what area of the world are you uh are you watching this stream from today all right so what is ml monitoring we're going to spend a minute or two talking about this because this tends to be a really important topic once you focus on getting the right data training your model uh getting the right model and deploying it into production it really becomes important to ensure that your model is continually doing what you expected it to do uh and that tends to be a much harder problem especially as your model becomes more important so when you think about what are the core pillars of monitoring uh an ml system uh it tends to come down to model inputs so looking at data drift identifying data quality problems identifying schema changes this is incredibly important right because unlike when you're training the model you're kind of handling everything you might be working in a notebook doing a lot of things ad hoc typically when your model is in production you don't have as much control over exactly what data is fed into it and that might be different for everyone maybe you are doing a little bit more of this ad hoc maybe you're running your batches by hand but for the majority of people especially if you're doing any kind of automated batching or you're doing streaming or real time this becomes an incredibly challenging problem just ensuring that the right data is actually feeding into your model the other thing is model outputs so not just what's feeding into your model that's incredibly important but also what does your model actually doing uh for instance if you're doing something like uh fraud detection or some kind of binary classification if you see that also your model is doing something insanely different than it was either when you trained it or over the last week or over the last month uh you want to be able to have that level of insight even if that's uh something that's expected uh it might be you know a scenario where you need that level of auditability and when that change happened and if it's unexpected maybe that'll prompt you to do something like train retrain your model uh or you know go make a change in your data if there's a problem with your data transformation that's impacting your outputs um also in the concept or in the topic of outputs not just what your model is creating but uh folks tend to have things like business apis additional metrics things that aren't just direct output to the model that they want to be able to track over time and that kind of bleeds into the last pillar here which is model performance so when most people think of model performance they tend to think of this last column here so they're looking at things like accuracy precision recall uh you know if you're working with a regression model A classification Model A ranking model they're going to have different ways that you measure performance and also the business kpis like I mentioned before uh there might be a certain percentile that you're calculating a certain count that you're calculating that you're reporting back to the business on how your model is actually generating uh Roi and so typically that becomes a critical factor in measuring performance and like we tend to say when it comes to ensuring you have visibility into all these three pillars is you know 10 it tends to be that bad data happens to good models so even with the best intentions we want to make sure that we have the right level of monitoring and observability here so a few different scenarios here like I said this might look different depending on the type of use case that you're running if you're working with text you know there's a number of different things you might want to be able to extract from your text anything from the amount of end grain engram excuse me topics character statistics or any specific metadata about the text or another common one is measuring the embeddings uh the the transformation step when you're actually creating those embeddings and the distance in between different clusters uh and things like output so what's actually being produced by your model you can capture a number of different things here as far as what was the output length what was the confidence any kind of categorical fields uh and then performance you know tends to be the kind of model that you're actually using with that text-based uh application uh audio similar story where depending on you know what type of application you're you're processing audio for uh you might be wanting to extract uh spectrogram features if you're working with spectrograms any kind of metadata so any kind of like QuickTime or exit file metadata tends to be really important to understand over time as well as things like unique outputs so the number of words detected number of speakers there's a lot of different things that we can derive or extract from your Audio models to help you to understand maybe why a particular behavior is happening or why a particular Behavior has changed uh in the last one uh images uh we tend to see things like image features extracted so brightness Luminosity uh maybe entropy any kind of metadata similar as before as well as you know a number of different types of outputs so things like bounding boxes uh categorical output properties uh and then you know potentially some Custom Performance metrics there depending on how your how you're measuring its success so let's look at a few different scenarios here I mentioned that those three pillars tend to be common even if you're not just working with tabular data if you're working with really any kind of data it's important to ensure that one you have quality data feeding into your model this can help you to identify things like external changes so any kind of bad bad data that's gotten into your pipeline data source has changed uh you know if you're working with computer vision maybe your camera is pointing in the wrong direction or all of a sudden a uh you know a change has pointed or a change has happened that's impacted the quality of your data or your schema has changed or really you know any kind of pipeline bugs that might impact the quality data that's actually supposed to be fitting into your model uh drift is another one that we tend to see is a common way to monitor your ml systems again talking about those inputs you want to make sure that not only is your model getting high quality input data into it depending on the type of use case that you're putting your model to work for um but you want to make sure that that data is the closest thing to what your model actually understands right that could have a number of different impacts across a number of different factors so uh maybe the the your model hasn't been trained in a number of months or weeks or years uh you know that you could be impacted by things like changes in seasonality changes in new categories uh or just changes in kind of external conditions that you can't really control and what that tends to do is impact the model itself so when you're looking at you know external changes this is something that can change the input of data it might be something that can can introduce a drift um and you tend to not only see the drift in the inputs but also something like a concept drift where you're actually seeing that maybe your model isn't producing the kinds of results that you expected maybe if you're doing like a fraud model maybe you're denying a lot more people than initially you saw as a distribution a month ago a week ago or anything like that or maybe you're approving a lot more people that you didn't expect uh some scenarios that we see right or things like changes in the housing market coven 19. there's a lot of different external factors that can play in that can really obtain a picture of why it's incredibly important to understand a drift in concept and understand adrift in what your model is outputting so a couple things that highlight here there's different ways to look at this problem so when we're looking at those three pillars uh what should you monitor what are the critical things to ensure that you have coverage for um we can look at it from a few different angles so one is looking at it as you want to be able to understand your ml application so you want to understand your features your model endpoints your predictions your ground truth and any kind of kpis you might also be tracking as well you want to be able to understand when something changes at any point there so when you think about like where do I start if you're trying to think about what's a a monitoring strategy that I can Implement either for myself my team my company we tend to see as a starting point to really just focus on monitoring your inputs and monitoring your performance if possible uh understanding that you have high quality data feeding into your model it tends to be a really good first step and that tends to be where we recommend people start if you don't have a good sense of what data is being set into your model is it close to what you trained it on is it close to uh you know the the same time last week last month if you don't have that level of visibility it's really hard to start measuring some of the more Downstream like those trailing indicators of things like you know your accuracy went down or a kpi went down if you don't have visibility into how your data forming and then the other thing is like I mentioned looking at it from a second angle here is understanding when changes happen so for instance if your model is trained on a certain data set at a particular point in time you want to be able to understand how your data is changing how you know you're seeing things like trading training serving skus if you are seeing them how you respond to them how you maybe trigger a retrain how you can maybe fall back if you see that your model isn't performing very well like understanding how these changes happen and not just over time but over specific periods uh it becomes a great next step to really understand how your systems are performing so doing things like ensuring that you're capturing a profile of your data set or really understanding your data set at the point of train or at critical stages when you want to understand how things compared today to those particular points in time that tends to be a really good next step to really just start tracking how your model reacts to change and how things change over time and then lastly the the final stage of really understanding and analyzing change is is the analyzing part so being able to uh once you have uh visibility into your model inputs and performance uh once you have an understanding of what your model is trained on you have visibility into the data over time it becomes possible to be able to understand things like distribution drifts or when you see a bug in data quality when you see a degradation and then ultimately what your customers are going to be impacted by so this tends to be where uh you get to after you have those first few things implemented so you can actually understand you know if your data set is seeing a sudden shift in distribution a pure models are seeing a sudden drop in accuracy or F1 or any kind of measurement that you're using there and it tends to be a good way to kind of take a trailing indicator look at your leading indicators and form an opinion on what you need to do to resolve that so let's talk a little bit about why labs so we talked about uh what you should be monitoring uh why you should be monitoring that and also what are the different kind of phases of monitoring or what are the uh the different ways that you can look at the problem uh let's talk about why Labs if you're not familiar with the platform uh we come at it with kind of a two-pronged approach here one is we have y logs which are is our open source uh data logging library for being able to help you understand your data we can profile massive amounts of data capture what's important and exclude the raw data and actually create a profile of what your data looks like at any point in time we could do that at the point of training we could do that continually for a lot of different types of workflows streaming batching uh you know really anything that you want to be able to capture a point in time on what your data look like we can use while logs to create that consistent profile and then what we do in ylabs which is the platform is we have a SAS where you can actually aggregate those profiles over time uh look for anomalies identify any kind of outliers uh you can look for uh you know really complex Trends or very simple Trends right hey I expect my accuracy to be above a certain threshold or hey I expect my uh you know drift to be under the certain threshold or I expect maybe there's distributional uh statistic to be kind of within this bound over a very complex thing maybe like seasonality or something even more complex than that um really just helping you to understand your data over time and then allow you to react to change so for instance if you do see a certain amount of draft three do you see a particular degradation uh maybe you want to either fall back to a previous model version maybe you want to trigger a retrain of your model maybe you just want to notify the right people uh through maybe a web hook notification slack notification trigger pagri duty if it's Mission critical uh but really just let the right people know uh when something has happened to your model or your data pipeline or anything that you're monitoring the platform so we'll go ahead and take a look and see you know really how we can put y labs to work here and understand our data understand our data set and I'll walk you through a couple examples I'm going to check chat I don't see any questions just yet uh if you do have any questions please feel free to throw them in all right so uh one particular example I'm going to look at here I'm looking at a text-based classification model uh and to do a little bit of setup this is actually a tutorial you can run through yourself I'll put a I'll put a link in the chat uh what we can do here is we're actually we're uh capturing a a reference point of not only uh the the text the tokens but also our embeddings and we're going to be looking at a few different factors on how we can identify uh drift in our model how we can identify a quality problem how we can identify a performance change uh and you know it's as simple as just uh capturing the right information through something like a like a y DOT log uh so why why Labs is really easy to to instrument here like I said we can just capture for instance a particular point in time when I really care about uh understanding what my data looked like right so um we'll we'll dive into this we're not going to go super in depth here but I I'll I'll point out a few different kind of unique things the one thing I did want to point out here is that I'm using segmentation to be able to understand the different groups of performance uh cohorts so when I'm looking at performance here I'm going to drill down into a certain prediction targets so I can look for you know what happens when I predicted a particular article to fall under this label and then I'm also capturing like I said uh the informations about the tokenizers information about the raw inputs as well as information about the outputs here and so we'll go ahead and jump into the platform and you know I I uh what I did here is I created a uh a an expected performance drop for the sake of the demo here so to to start off I can see that white labs immediately notifies me uh on the summary page on my main on my main page here that something's really gone wrong I see that there's a 58 drop in the accuracy of my model I see that it's 27 which maybe on its own is maybe uh if that happened immediately that's probably something that's uh incredibly important for you to be notified of uh it was a it was a slow degradation uh you also probably would want to understand how this change happened over time so we'll drill in here we'll start here and we'll see that you know for instance here I can see that uh even though the volume didn't change drastically it's kind of within the same trend line that I see I can see that an accuracy drop has been happening kind of slowly over time here and like I said this is expected in this scenario here I I actually introduced a particular translation problem which I'll talk about in a moment here but I can see what the trailing indicator that I'm looking at here which is the accuracy ultimately went down uh the the false positive rate went up uh the X4 went down so I can see there's a number of different indicators here uh and I'm not actually alerting on these which I'll talk about in a moment here but that might be a first step maybe I want to ensure that in the future if this does happen I want to get that immediate notification I want to be notified so that I can actually respond to it so coming over here I want to see you know why is my model seeing such a drop in accuracy uh well the first thing I might want to understand if I'm looking at this purely from getting notified from a performance perspective is you know who's impacted are they equally impacted again if I'm doing some kind of classification if there's certain uh labels that were impacted other more than others so in that case you know I segmented on the labels so I can use the tracing view to actually understand uh which segments or cohorts of uh predictions actually were impacted more than others so again I'm looking at that accuracy metric here I'm looking at kind of a general aggregation over the time period that I'm looking at here and I can see that for instance the the for sale category actually had the the most drop in accuracy or the kind of the worst accuracy during that time period uh versus you know the the medical documents the baseball documents uh you know they're they're a little bit above the curve and then I can kind of see where other things are relatively in line but I can see that the for sale uh group here actually had the most predictions so that's the the uh light blue bar in the back and then I can see that even though it had the most predictions that had the lowest amount of accuracy so I can do that on a number of different uh metrics here right if I wanted to understand Precision false positive rate F1 score to really understand how they all compare against each other I can also do something like maybe I want to drill down into a particular point in time where I started to see that drop happen to understand uh how did this compare not just for uh the overall whole but how did it compare for a particular point in time compared to the to the window that I'm looking at here yeah and so like I said uh drilling into particularly segments or cohorts is incredibly powerful I could do that on the performance page as well so maybe I want to see how you know the baseball categories comparing to the other ones like I said the accuracy tends to be pretty higher uh compared to the rest on that one uh versus something like the for sale category actually seems to be pretty low pretty uh you know representative of that drop that I'm seeing here so the next question is what changed uh so I can see for instance uh the outputs of my model what's changed here I can look at things like the overall drift in maybe the prediction uh you know what are the categories that have suddenly shifted and I can see here for instance the for sale category had that uh spike in the amount of uh predictions that it was it was ultimately resulting in and then I can also see things like the inputs that fed in that ultimately created those predictions so a couple ones that I'll highlight here I mentioned that I was capturing information about the raw document as well as information about the tokens as in addition to uh the actual embeddings that were being created so first off I can see that you know wildlabs is tracking the direct distance here uh and I have a monitor set up where I'm looking at that drift distance and I can see that Wildlife sees the biggest change kind of in this two these two features here uh and we extract these features automatically using a resolver uh so I can see that you know for instance in the frequent items uh I saw a pretty dramatic change and I'll drill into what that changes in just a moment here I also see a change in the closest clusters of embeddings so without having to stare at a umap or any kind of complex visualization that could actually see that while it's making it really easy to understand my embeddings for a lot of different use cases here identifying closest clusters identifying distances within clusters automatically identifying centroids in my embeddings distribution so a lot of different out of the box things that we can do here to to give you a ton of visibility like I said the frequent items was an interesting one that I want to drill into a little bit more here uh if I drill in and I go ahead and select this particular point I can see some pretty interesting stuff I want to really compare against uh this period of time kind of earlier on where I saw some pretty good performance compared to what I'm seeing today uh and if I drill down into this particular feature this frequent terms uh extracted from my tokenizer I can actually see that right uh right at the point where I saw that that sudden drift where I see that drop in performance uh I can actually see that my documents change from English to Spanish so I can see that the frequent words change I can see that the uh they're completely different in the latter profiles and that was a pretty sudden shift but you can imagine if there are certain categories that your model is expecting uh if there were certain frequent strings that you were expecting before you pass it into maybe your tokenizer you would want to be able to understand how those change over time so you can drill in and see you know for instance what change in between this particular time period or you know know during this window uh and then you know the The Logical question is you know how did I get to the point where uh I have a monitor that I got notified of this problem how do I ensure that if that performance drops again in the future I'm notified so I can actually respond to it and that's really where Wildlife becomes incredibly important in not just looking at a point in time like what happened during this particular point in time where we saw you know this data set captured and profiled uh during point a or point B but really helping you understand how this happens over time so for instance here I really want to understand how I can create a monitor on that performance metric we make it really easy so I can come over here and say you know I'm working with a classification model I want to measure accuracy great uh what do you want to measure right maybe I have a static threshold that I want to ensure if it goes below this value I want to get notified of it maybe I want to make it a little bit easier and I really just care about looking at sudden changes or changes over time maybe from a percentage basis um or I could do both right you can have different monitors that are covering that so you have full visibility into those changes uh and then you also want to ensure that you're choosing the appropriate trailing window right or um Baseline excuse me uh because for instance maybe you want to look at that trailing window for how the change happened over time maybe over a few days a month a couple months maybe a longer period than that or maybe you want to compare it to a particular range where you know your model was acting pretty normal um so you know whether that was when you had a particular you know model running you were doing some a b testing and you know that during that time period it pretty much operated exactly how you expected it to uh you can say great that's my that's my reference point and I want to go ahead and compare to that or I don't have it in this particular demo here but we can also capture that that training set like I was talking about so when I'm looking at you know how does my performance look like today well I can compare that to maybe what it was uh trained on so I can actually see how much uh change happened you know in those accuracy metrics in those uh different like performance metrics for the type of model that you're working on uh compared to a particular point in time right so uh lastly I want to be able to Define like where where I want to be notified of or what kind of workflow I want to trigger if something goes wrong right maybe I want to go ahead and trigger a slack notification trigger page view notification maybe you want to go ahead and set up a web hook or an email uh you know we make it really easy to tie into other systems and again if it's something like a web hook maybe you want to trigger an automatic retraining maybe you want to trigger something in AWS like an sqs topic or or if they're an sqs queue or eventbridge or something like that where you're going to take an action afterwards and why lives really just helps you to get the right notification education to the right person at the right time so you know we covered a few different topics we talked about how you know wildlabs can help us to look at performance not just from a performance perspective from understanding things like accuracy or comparing targets to any kind of ground truth but also helping us to understand individual cohorts of of uh of my predictions or of my data set um also being able to let me drill into particular profiles at particular times to understand you know how does point a compare to point B and then lastly uh allowing me to easily set up monitors or you know even more complex monitors if I have a very unique use case uh where we can look at things like seasonality or complex Trends and really help you to to identify those changes in your data or in your predictions before they cause a problem and ultimately impact your customers uh so coming back over you know we talked about a few different things uh just to recap three pillars of ml monitoring when you think about what should a complete system look like you need to be able to understand your inputs you need to be able to understand your outputs and you really need to have a system in place where you can create those performance metrics and understand the performance of your model uh and so where do you go from here I would highly recommend we have some great examples on measuring performance on the doc site you can get started with ylabs really easily you can go to wildlabs.ai free start off with an account and if you want to know more please reach out I would also highly recommend joining our slack community and also if you uh if you feel uh in a good mood today please feel free to give us a star on our y logs data logging library on GitHub uh thanks everyone for joining the stream I will go ahead and end the stream now foreign"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML System Monitoring",
        "Topic":"Monitoring ML Model Performance",
        "Video Title":"A simple way to validate and monitor the performance of your ML Applications.",
        "URL":"https:\/\/www.youtube.com\/watch?v=D1CHLhjRAEk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/D1CHLhjRAEk\/hqdefault.jpg",
        "ID":"D1CHLhjRAEk",
        "Publish Time":"2023-08-22T03:50:25Z",
        "Channel":"PyCon AU",
        "Channel ID":"UCS9sdEyduD9K83K3GkvQlOA",
        "Transcript":"foreign okay uh last Talk of the day so um I have here nins who's uh software engineer that's transformed metamorph metamorphosized into her machine learning engineer and uh recently finished his masters of research with a focus on agriculture and image processing um so he's going to be talking to us today about a simple way to validate and monitor the performance of ml applications so round of applause please thank you hi uh hello hello can you hear me okay it's nice to be back on a live event um hi everyone I first want to thank Mike on Australia for organizing this event um definitely good one for all of us right so for today I'm here gonna I'm here and I'm gonna talk about a simple way to validate and monitor the performance of your machine learning applications okay a little bit about myself I do mountain bike I love DOTA 2. you can talk to me about software engineering data science machine learning I love to lay on grasses as evident by the picture and I like wrestler buns you know custard buns are really good anyway let's go so for today I hope I can impart some learnings to you guys and I hope you learned a lot from this session we're just gonna briefly go through and talk about these points right here so first one let's try to identify um what is model validation in the context of an application or a machine learning application to be accept right so how do we identify Trends why do you need validation specifically right now people are always into most of us are most AI subject to PT and stuff right so how do we choose the correct validation metric what are the available metrics that is in store for us um let's try to build a simple model validation module in Python so I'm going to show you an example of one of the simplest ways that we approach this problem in my previous work and I'm still using it as of today uh and then we're gonna briefly talk about retraining and model maintenance so stability versus training and then how long does your model stay in production and the maintenance cost of those models okay now every time every time every time I start a machine learning project or I talk with data scientists I always ask these questions these questions right so we always discuss if there's any ground truth validate validation data available going forward how can we validate the results of the model what metrics do we use as I mentioned we're going to go through this later is there going to be a change in the distribution of the features or are there going to be changes in the relationship between the features and the Target right so when I deploy the application will the deployment environment change over time so lastly we always talk about with the data scientist I'm working with we're always like how do we return the model how do we adjust the model how can we make sure that the model is always you know performing at its best now let's talk about a performance change I believe there's going to be a very specific talk about this tomorrow so we're just going to cover some of the basic and like um the underlying Concepts with this so model performance change it decreases over time it's almost always the case so it can be a result of several factors mainly um if it's a concept drift so the nature of the problem changes or if it's a data drift then the distribution and the relationship between the data changes it happens most of the time it can be an actual deployment setup problem right so someone accidentally pushed a new version of numpy in the requirements of the text the performance changed models are always going to be performing at its best right after training once you deploy it it's that's its peak right unless you're retrain it because it's going to encounter a new data set eventually and sometimes these changes can cause unintentional bias which is not bad because in some context or in some problems um putting in bias is actually quite good right it depends on the context it depends on the problem now just a brief preview about concept drift it typically involves um the problem that the model is trying to solve and suddenly it changed right so there's I I drew that graph I'm not sure if it's good or not but it's cute um so here we can see that initially the model is predicting the data points in blue correctly the data points in Orange are the ones that are used for training and then suddenly something happened right it's misdetecting everything so these things actually happen in the wild or in the development environment in most development environments uh we have before right one example would be probably 1920 prediction right when a variant change or if there's a new variant essentially it's going to have a different interaction right transmissibility will change other factors will change so essentially you're not predicting for the same problem anymore this could happen suddenly like this one or it can happen gradually incrementally or it can be recurring sometimes share occurring um chain uh concept drift happens and like weather data set you know it's quite common but I think the most common one is the gradual because sometimes you don't notice it that it's happening and then suddenly the entire problem changes and your prediction changes as well right there's also data drift so I think this is easier to understand it basically means you created the model using a certain set of data range or data distribution and then as you go forward new types of data distribution occur right so this is a very common in financial models or in demographic type of data right so if you're dealing with this problems right here you can expect to encounter this and it's usually easily fixed by just you know retraining the model again so we'll discuss more about retraining later now why is it important so you know Morty if you're I'm a fan of Rick and Morty um detecting and identifying this change or this Trends in terms of the performance of the model is actually quite important because it's a good indicator of your machine learning application it's a good indicator how it behaves right people talk about trying to make sure that their machine learning applications perform really well over time detecting these changes are a good indication of you know what's happening in your application so understanding these changes also lets you efficiently make decisions in terms of free training adjustments in your model especially when we have this project before and we're trying to do a factory line so you need to predict really fast and then you need to retrain really fast something like that so if you understand how the performance shifts you can easily retrain based on the parameters that you want etc etc and one of the I think one of the most interesting things that can happen is that you are able to understand more of the data set right if you're able to detect this change in the model performance then some new patterns arise some new features can be extracted and stuff like that so this is quite interesting this is part of feature engineering this is quite interesting that um it shows up once you understand how your model performs right so as I mentioned earlier here we have an example of fixing a data drift so the initial training data is limited and then as we move forward and encounter new predictions what we can do is we can adjust the training data to include the new data set that we encounter in order for us to correct the model so here you can clearly see that you know the model adjusted if you include more training data set right now how do we measure the model performance let's just go through this briefly it depends on the problem right so I'm sure at least most of most of you might be familiar with regression and classification type of problems so depending on the problem you might want to use this like mean square error root mean square error and you know similar approach if you're dealing with regression problems so if you're dealing with classification problems then you typically use your accuracy precision recall F1 Roc AUC and stuff like that but what does it mean right let's say for example I have this performance Trend right here I'm measuring two metrics which is accuracy and recall for example um initially the recall started really high but then at some point it went down while the accuracy went up which metrics should you use is it accuracy because you know over time it goes up or it is it recall it depends on the problem and on the context of the problem right there are some instances that it's very important to know uh like for example when recall it's being used for medical data set when you're not you know it's okay for false positives and stuff like that so accuracy there are also certain types of problems that you might want to prefer accuracy or F1 right so it depends on the circumstance uh and in our use case and in my experience typically when we're trying to monitor um the performance of a model we use two three five metrics at a given moment at a given model so it's not just one it's not just two like minimum two yes but then the more the merrier in this case or you know the more you use the more method you use the more information and the more you understand the behavior of the model right now how do we build a simple that's the key term a simple validation module how do we integrate this in our application so there are several ways for you to be able to detect data drift and concept drift um there are statistical tests those are quite technical right and some of us don't want to deal with those kinds of numbers and those approach there are several drift detection algorithms but typically they happen on uh you need you need lots of data set to be able to understand it we want to be able to detect these changes on the go as it happens right it needs to be simple so that it can be integrated into any kind of application that we want and the model validation function should be easily understandable because for developers we want maintainable and easy to understand code so you know show me what you got if you from share with the reference anyway um yeah that's the thing we need to catch the early signs so if you look at my poorly drawn graph right there we need to catch those instances the one that's in circled uh those data points right what we learned when we're building this machine learning applications we need to regularly check for performance change there are applications that we have I don't know maybe every five minutes we detect for we try to see if there's a performance change because it's surrounding 1000 predictions every second so you know and then one of the things that we learned is that let's just assume that the outliers are wrong prediction okay I have no bad blood versus outliers but you know in this instance um it's safe to assume that there's something wrong with these outliers right here and we want to understand why are they outliers so yeah just assume they're wrong predictions okay The Next Step would be to prepare a good validation data set I will show you a simple diagram later but essentially it's different from your testing or training data set these are data set that you can anchor to the model that's the term that we're using or that you can attach to the model to the deployed model so that you have a consistent understanding of their performance I'm gonna see that later and ideally you should have a secondary model stronger more powerful model for auto my automated validation or sometimes we do actually most of the times we do manual validation of um sample incoming data so that we're sure that the application is performing really well and it's very useful in terms of retraining or creating a new version of the model now here's a basic overview of what typically happens um during you know model development deploying it into an application so initially you have the model development which is the data scientists are involved the machine learning Engineers are involved and they're using training and testing data set to be able to create the best model or to be able to produce the best model after that we're going to use that model in an application and typically we record or no no you should record the results in the database or you know you should keep track of the results of your model that's the key part right there okay and then lastly we have two ways in most of our applications that we built we have two ways of model validation or you know checking the performance of the model so one is using the validation data set and the other one is double checking the incoming or the new data that we encounter or the model or the new data that the model encounters now let's just you know um take it slow maybe use this as a good story telling stuff that um we're gonna go each step so first one um data scientists machine learning Engineers typically create a really good model using testing and training data set I think most of us are familiar with that step after that what we typically do is we prepare a separate validation data set okay and then we place that model into a controlled environment that's very similar to production ideally and then we run the validation data set several times on the model maybe if you're using k-fold or any kind of sampling whatever um it's up to you so we run it we run it we run it and then we record the results after that we have this like uh we have this acceptable result metric that we're basing or that we're recording so in this instance for example this application this application setup paired with this model paired with this validation data set should produce an accuracy of 80 to 82 percent and an F1 score of 80 to 81 with 100 predictions in 10 seconds so you see our trying to figure out the constraint of this model in this given environment so take note of that um take note of that metrics right there so we typically record it right and then we deploy the model in the application production setup so the model does its job users use the application the model does the prediction it stores it into the database and you know after that we want to identify if the model is still doing well as we use the application so again we're going to go back to the initial approach right here inside the application we have the validation data set and we will apply that validation data set to the model and we will check if the result is still consistent with the recorded results that we have from our testing right so here is a very simple fast API example some of our micro services are built in fast API so we're just using the repeat decorator and first thing that we do is we load the validation data set after loading the validation data set we perform the prediction on the validation data set so it's here oh it's cool you can see it anyway um we record the start time and the end time just to get the duration right here and then we basically get the F1 and the accuracy or whichever metric you are using now after this we're basically just comparing if the accuracy F1 in duration is still within this range right so every time every time that this validation runs in this case it's running every day because um the frequencies per day so every day it should always fall into that bounds right if it's not there or if it decreases if it increases something might have gone you know wrong or something might have changed from the setup Okay so what's it for right initially when you look at this setup it seems pretty trivial because you're just validating it against itself every time every time what are we gaining from this right first is it's quite simple you can implement it in most of your applications easily straightforward and it's flexible enough to support multiple validation metrics okay however what we discovered is that implementing this very simple function can catch or can identify if there's a change in library or dependency version numpy for example um one specific example is that we have an application deployed in an ec2 instance and for those of you who are familiar sometimes Amazon role you know they sometimes update the VMware that's underlying the is it it's quite annoying so you think that there's no update but they and they update the underlying architecture of the VMware which changes some of the libraries especially when we are using deep learning and stuff like that so our results suddenly change and we're like what what's happening what did something change did someone push something so that's that's one of the use cases it's actually very useful to detect if the setup has changed um there are some instances that the parameters of the model change when someone push a new code or when someone creates a bug fix and stuff like that sometimes they accidentally change the parameter of the model so we can detect that change as well and when you have a device set up if you're using uh Hardware in your prediction this is definitely useful because you don't know if the hardware is broken and if it's deployed somewhere if someone tampered with the hardware if the resource is not enough so that's the goal of the first validation to detect these kinds of change okay now let's go to the second step right so we have our application right here and our really good model didn't do so well on some of the data points right so he's like I'm not sure maybe I'm just 50 sure of this maybe I'm 70 sure of this I'm 63 sure of this and stuff like that so in classification problems there is a predict probability which is this one so this is an example of some results using a classification problem right and then our little model right here is not sure I don't know but maybe this is true or maybe this is false so we we record the results right even the probability we we record it every time it performs a prediction now we have the probability we have two ways of going around this problem the first one is we have this big bad new model right here right so this is often our secondary model uh in our deployments we always have two or three models in place right so the first model is the one that we're using the one that's actually Taylor Made for that specific problem and then we have another model right here who's probably a lot stronger can classify better but it's quite slow right so there's a trade-off but they're still useful right if they can just validate some sample data it's still useful so that's the model that's the validator model right here that's what it's doing it's trying to figure out if the results of this main model that we have if the low confidence course it's trying to identify or it's trying to classify it with better confidence so the goal is this classifications right here should be you know it should have a solid classic or it should be at this 80 confidence uh maybe even higher right and if you're using this you can do an automatic retraining if you want well I'm going to show you some code Snippets later uh in some instances we're working with several research groups for example um and then when they're using their model in the application what they wanted is they want to see the sampled data set right here so they want to be able to see it and they want to be able to verify it themselves this is very crucial especially when you're working with them medical type of problem or maybe research type research in biology or you know so they really want to see their data they want to be intimate with their data so in this case what we do is after we build the application we sample this data set right here and then we give it to them through a report right and then that's it they validate it and then they can retrain based on it so this is sample code of manual retraining so we just right here we get the sampled result that has very low confidence score and then basically send it to the user do whatever you want or we can go in the automated route in this case after we get the sampled results right we have a second validator that predicts the flag data set and then we compare the accuracy or whatever metric you want to use if we want we can just directly overwrite the results using the more stronger model or we can decide on what to do maybe if doctors is this overwrite and then retrain okay now this is really good for detecting early signs of drifts model drifts concept drifts and possible changes in the data distribution so this is what we usually have in place so that we can identify if there's a problem or if there's going to be a problem with the performance of the model now lastly we're going to talk about model maintenance um here we have the concept of model stability versus retraining sorry so models are more stable if you don't need to retrain them from time to time like this one in the first model and it's less prone to model and data drift right however they might require more time to develop because you need more data set you need to gather more resource to make the model more stable unlike the second model right here where you need to retrain periodically if the performance goes down however you need to understand that if you retrain the model on a longer period of time it seems that it's actually quite stable right as long as you have a good retraining process and it benefits from the new data set that it learns now for model longevity like it depends on the context and the expected inputs if it's just a small problem then typically you're gonna have a model deployed somewhere over a long period of time because it doesn't require any change similar with the dynamic relationship and like if you need to retrain models from time to time the data that encounters is dynamic so you need to replace the model and most of the time you need to retrain them and lastly this is my last slide so the cost of Maintenance you always need to consider this if you want to produce stable models you might have more upfront cost again as I mentioned stable models are more expensive to develop right and you know it's always good for models that need constantly training it's better to automate them to reduce the cost and you can always use transfer learning if you want in the sale of my slide questions hey thank you nins very very interesting very insightful now we probably got time for some questions I can't see anything from here because someone over there I think hi um you mentioned uh retraining the data as the model accuracy dips over time and I just wanted to ask how do you like what are some techniques you have to avoid model overfitting as you retrain particularly with the things like unidentified seasonal data and um uh yeah just how you would avoid especially when you automate the retraining process shouldn't and I think we get the question a lot right remember in the earlier part when I told you to use more than one metric use five use 10 metrics to be able to fully understand the behavior of your data and your model right so for example this is a real life scenario like um we have a model deployed in production and then the F1 score continually goes down right it continually goes down however if you look at uh recall and the lag loss function the performance is still okay right if we follow the F1 score we constantly need to retrain the model and as a result the subsequent models became overfitted however we did not do the retraining because we have the log loss and we have the other metrics that allowed us to understand that okay it's okay if the F1 score decreases because it will be over fitted if we train it's not the target metric that we want to use anyway so you know that's one of the way that we approach the problem and I think um you should have a Hands-On or you should try it Hands-On so that you can experience it yourself right because it's uh it's something that um you'll definitely see in the patterns of the data once you try it yourself so thank you use metrics use as many metric as you want questions thank you any other questions oh I can see this one over there um I've got a bit of a two-parter um so you mentioned the test train validate split uh do you have preferred proportions for splitting your initial data set into test train validate ah yes they vary with the size of the data available the second part of my question was it sounds like you're picking local low confidence predictions and you and labeling those samples and using them to bulk up your data set in production if you're picking actual samples submitted to you have you ever come across privacy concerns with that okay so I'll answer the distribution problem first um typically uh we for example let's say we have 100 data points right so typically what we do before is uh somewhat 60 training data set 20 um testing and then 20 validation however uh we discovered that ideally we would want that validation data set to be sampled differently from the training and the testing data set so if we have an option to ask the customer hey look this is the initial data that you provided we can train with this data set and then we can use some part of it as a testing data set however would you be willing to extend your data set maybe to another month like for example if if we're building an application for them they're gonna give us another month worth of data after we finish the model and that will be the validation data set so we figured that or at least the data scientist in my team they figured out that it's more organic that way they were able to capture the relationships of the data better because let's please if someone gives you a training data set and a testing data set there will always be some form of bias going on in there right so they will this is our Target problem and oftentimes they will give you a really clean or a really standard relationship between the data so that's what we typically do we separate that training and testing data set to the validation data set right so if you can do that then do that right I'm sorry what's your second question uh it sounded like you were picking low confidence predictions for production use case of the model and then including them in your data set if you are doing that have you come across privacy concerns or are you just not processing data where that's a problem oh yeah so we're picking the low confidence predictions just for validation purposes right it's still up to the data scientist if they want to include those data points in the prediction right we just want to understand why these guys have low predictions scores and if it makes sense to include them in the training data set or if it makes sense to create a new model out of them then of course we will do that right so it's not it's not we just you know pick them and then include them right away uh there are some instances like what I've mentioned where if we're if we're really confident with the model like if we have a big red model you know getting ready for those data points we just overwrite them completely but in most cases there's always another layer of choosing which of those low confidence data points should we include in the next training cycle right so there are other statistical tests that we perform in those low level data points to make sure that we're including a data point that represents or has a good representation of the actual problem that we're facing so this is that answer your question I think it just sounds like you're not processing data that's personally identifiable so you're free to do that which is great yeah yeah basically yes so okay thank you I have a stupid question perhaps no no questions stupid all right wait until you hear it um if you've got a super brain model why aren't you just using that in production which one if you've got a super duper model that's like more accurate why wouldn't use that one in production well yeah that's the thing I mean in my opinion people are obsessed with how you know this very generalized model that can perform everything really fast very accurate it doesn't exist not yet okay it doesn't exist or at least not yet right based on our experience there will always be some sort of trade-offs like uh you need to communicate this to your customers or to your partners one example would be the factory line set up right we need to perform prediction on a two megabyte device so it's very difficult to put in a really powerful model inside that device yeah it's small memory so you know it depends upon the context and maybe someday maybe someday hopefully hopefully soon enough or or not we will have that really big AI that can generalize and predict most of our problems maybe maybe thanks okay thank you thank you nins and here's a token of our appreciation thank you round of applause thanks guys [Applause]"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML System Monitoring",
        "Topic":"Monitoring ML Model Performance",
        "Video Title":"Evidently AI Tutorial-Open Source ML Models Monitoring and Observability",
        "URL":"https:\/\/www.youtube.com\/watch?v=cgc3dSEAel0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/cgc3dSEAel0\/hqdefault.jpg",
        "ID":"cgc3dSEAel0",
        "Publish Time":"2023-08-21T11:22:35Z",
        "Channel":"Krish Naik",
        "Channel ID":"UCNU_lfiiWBdtULKOw6X0Dig",
        "Transcript":"hello all my name is Chris Naik and welcome to my YouTube channel so guys I hope first of all everyone is doing amazingly well in their life uh you're learning well whoever are interested in data science I know there are many challenges that may come in between but definitely continue learning right uh now in my channel I've probably uploaded so many videos regarding end-to-end project implementation right we had followed a pipeline of data ingestion to data transformation to model development to model deployment right and I also used to say about model monitoring right but I never showed you one practical approach right because I was looking for the right kind of Open Source tools right and I love open source tools guys until now the videos that I've probably uploaded right like like that or DVC GitHub GitHub actions like this in short are like kind of Open Source tools that help you to perform lot many things and that is where ml Ops activity comes into existence right specifically with respect to deployment and all so in this video now I'm going to focus more on the model monitoring part right uh probably I've never uploaded a video in my Channel about model monitoring but today I will definitely show you one open source tool which is basically called as evidently Ai and I will just give you an example like how you can probably monitor your model monitor your data you know and what all main key terms are there with some good practical examples but just seeing this practical example I think you can also easily develop or you can develop a model monitoring pipeline in your specific project so guys uh if you are new to this particular video please make sure that if you are new to this Channel please make sure that you subscribe the channel press the Bell notification icon and one more good news uh I'm I'm about to reach 800k subscribers uh 24th is my birthday 24th August is my birthday so I would definitely like you to press subscribe button make it eight lakhs at least 800k uh I've been working really hard with respect to my YouTube channel and many people have also appreciated the content that I have actually put so yes if you like this content please make sure that you share with everyone let's target as a gift to me and let's reach 800k subscribers okay so let me go ahead and share my screen over here so we are basically going to discuss about evidently AI now what exactly evidently AI is as I said for model monitoring this will be an amazing handy tool and it is also an open source tool right so uh over here you can see the open source ml Observatory platform I and don't worry guys make sure that you watch this video till the end because I'm also going to show you some practical implementation which will be super amazing so here it will evaluate your test and monitor ml models from validation to production from tabular data to LNP to uh to NLP and llms right so it covers almost everything in this video I'm going to probably talk about one machine learning project itself where I will probably be seeing this we will see one practical example and as we go ahead uh what we are basically going to do is that we are also going to check for llm models also so if you have not seen the tutorials definitely check out my playlist I've created videos on open AI llm models Lang chain and many more things so this is the idea about evidently AI and if I probably go more it is always good to have a look onto the documentation all you need is to rely on ML systems in production because see when you are running in the production right it is always good to continuously monitor the model if you are probably getting any kind of wrong results at one specific point of time it can really lead to a loss of business right so here you will also be able to build reports you'll be able to test your pipelines you'll be able to monitor it all right mainly when I talk about model monitoring at mostly uh has this four important steps with respect to data quality data drift mod model performance I will explain each and every time what is data drift what is model performance just to give you an idea about data drift right let's say I have created a model I've trained with some kind of data but after some point of time you know the distribution of the data may change the mean of the data may change the data may look something different and it can be with respect to time you know for a specific product let's say if I say a company has come up with some specific product initially the feedbacks may be bad later on the feedbacks may become good right so something like this this kind of data distribution will be there even in tabular data where you have numerical data there also the data distribution may change so uh evidently AI actually helps you to track this using Data Drive techniques we'll see that model performance also it will be able to see and each and every example is also given over here but just to show you today I will be taking one machine learning problem statement which is basically called as bicycle demand monitoring and if you don't know this uh problem statement obviously is also the data set is probably taken from a kaggle itself I will try to see and we'll see that how things are you know each and every line of code I will execute in front of you and will I'll try to explain it just by seeing this the prerequisite is that obviously you need to know machine learning some amount of stats and then you will be also able to understand it so let's go ahead and let's have a look so guys now let's go ahead and see step by step so to begin with what we need to do is that we need to import this evidently and if it is not getting imported uh what we'll do we'll do a pip install Okay so evidently is also available in Pi Pi so if you just write pip install evidently uh you will be able to get it uh so I will just go ahead and execute this uh it will probably take some time with respect to the installation so here you can probably see the installation is basically happening and then we can go ahead and do the importing of the evidently itself so uh initially we definitely require this Library this is the entire thing you can also join the Discord you can ask questions because there are many people with respect to this particular open source tool right you should definitely know how to use it right and I am really focusing on lot of open Source tools as we go ahead you know I've discussed about DVC as I said dags Hub and it is quite amazing right so here you can basically see that we have imported we have installed it now the next thing that we are probably going to do okay this is super important okay we are importing pandas numpy request zip file there is a reason why we are doing zip file I'll tell you uh because I am going to extract a zip file from a different location and then probably download the data set uh in evidently will specifically use something called as column mapping okay why this is used I will just let you know in some time and then we will also be uh importing reports in evidently dot reports which will be responsible in creating the entire reports itself and there are also some metrics inevidently which is basically data drift present Target drift present and regression preset okay so uh each and everything has some importance right this is specifically to understand the data drift this is specifically to understand whether the target variable has changed or not and this is specifically for a regression problem statement what are the various parameters that we may have like performance metrics we may have like mean absolute error root mean Squad error So based on that if that is changing or is that is impacting the entire model that also will be able to see it so these are some of the libraries that we are going to import it so here I'm going to just import it over here so it will take some time so the importing is done okay now if you don't know about this problem statement guys bikes bicycle demand uh data right so here is the kaggle data set uh kaggle link so here uh you'll be able to see that uh I will go to kaggle okay what is the problem statement okay you are provided with hourly rental data spanning two years right of how many people ordered a bike or ordered uh uh bicycle sorry for this competition the training set is comprised of 19 days and a test data is 20 end of the data you must predict the total count of the bikes rented during each hour so at every hour you really need to find out like what is the total count of the bike so this is the problem statement so now let's go ahead and first of all do step by step initially we will be reading the data set for reading the data set we are going to use this specific URL where that data set is available in the form of zip file and we can basically use request.get so once we get that entire content we can probably use zip file.zip file io.bytes content as Arc and this is what we are specifically doing we are extracting the zip file and reading the CSV file with some of the features like header is equal to zero separator par States index columns which are specifically required to see the raw data okay now once I execute this from the zip file we are going to extract it and these are something basic things with respect to reading CSV file we'll be able to see it okay now ah over here one more step we are specifically doing uh we are combining the row name uh with respect to the hours and the axis is equal to 1 let's go ahead and see with respect to the raw data head so here is my entire row data raw data so here you can see that I have all these particular features or what you can do if you don't want to do all these things also directly download the CSV file right download the CSV file and use it over here okay these are some of the additional things that we really need to do to extract that particular zip file and on which will also give you an idea about it okay so I have actually taken this code from the documentation of evidently AI now here we will probably start with something called as regression model now in regression model we are going to create some variables okay now this variables will play a very important role now I know what is my output feature over here my output feature is CND okay so what I will do my target variable I will create I will note down that variable name over here I will also create a prediction column right to just see like what prediction my model will be doing so that name of the column will be something like prediction then I'm also going to divide all my features into numerical and categorical features you will understand this guys why I am specifically doing this because evidently AI whenever you are doing the column mapping this is the first thing that is specifically required what is the target variable what is your prediction what is the numerical features what is your categorical feature each and everything is specifically required so that is the reason we are trying to Define in this specific way so here you can basically see a Target prediction numerical feature categorical feature okay so all this it is not necessary that you always need to write the name like this it can be your variable name itself but always try to make this division like this okay so one is the target variable prediction variable numerical feature and categorical feature now once we go ahead and execute this let's go ahead and execute it over here now the next thing as you know right in this particular data set you will be finding hourly basis data right everything you'll be finding over here hourly basis right now based on this hour the count of the demand of the bicycle may change right so if I probably consider this I want to probably check data drift or model drift right I always need to make sure that first of all I train my model on some data and on the upcoming data I probably check how my model is basically performing right so that is what I'm actually going to do over here also so if I go down over here you can basically see I've created a reference data I will tell you why is this specific reference data is there I'm taking from 2011 one that is January 1st from morning to L to night uh to let's say to 28th Jan 11 o'clock I have taken this all data as my reference data so probably I may use this reference data for training my model right and then this current data will probably be used for the testing purpose let's say for right now we will use this data for testing purpose so that we will be able to test the model so here I am going to probably Define this so that I will be getting my reference head you'll be able to see my data will probably start from this date and it will end till this particular date right 28th now this is done The Next Step I hope everybody knows about random Forest regressor we are going to apply random forest regressor with some estimators and some random state I'm going to just create this particular model and then I'm going to do the fit on this particular reference data where I use both numerical and categorical feature and this is my this is my training data right training uh like it's just like X Trail and this is just like my y train right this is my output feature these are all my independent feature these are all my dependent feature so once I execute this you'll be able to see that random forest regressor model is basically created right so I hope till here you have the exact idea what we are specifically doing now after I probably create the model what I'm actually going to do is that I'm going to do the prediction for this specific reference data so prediction for this reference data just like my extrane okay and here for this I will be using regressor.predict which will basically do the prediction part okay now similarly for the new data I can also do regressor.predict on my current data which will include both numerical and categorical features so through this I will basically be getting my prediction right so this is the prediction for the ref prediction current prediction right so this is very much good till here we are able to get all the predictions also so guys in The Next Step what I will do the prediction column that we have actually created I will try to store all those predictions over here that's it so here is all my prediction so I'm basically creating a new um new feature within the name of prediction on both reference and current okay so this is fine now this is where your model performance will be checked like how well your model is basically doing okay to start with the column mapping that we have imported on the top right if you remember we had actually imported column mapping over here right now this this is where you'll understand the importance of column mapping so in column mapping you first of all create a Target variable and you'll assign the target then in column mapping you basically create a production variable so this dot Target dot prediction is a variable inside column mapping itself right so here you're basically going to assign to prediction then there is one more feature which is called as numerical feature which will assign to the entire numerical feature and then column mapping dot categorical feature you'll entirely assign to the categorical feature right so this is what you initially do now I have mapped all the features over there now it's time we use those reports that reports library that we have actually imported which was present inside evidently right evidently dot report import reports we will be using this report and then we will be keeping this metrics as regression preset since this is a regression problem statement if it was a classification problem statement we could have written classification preset right so this is where we have basically created this particular Matrix okay and then we are just going to run where I am going to assign the current data see current data basically means what my model is currently trained on that will basically be my reference data right and my reference data I can assign it to current prediction also current data also right to my current data itself but right now I am not assigning anything so just see how my model performance is there right and this column mapping feature will get assigned to the same column mapping variable that I have so once I execute this see guys it is very much simple if you are probably following step by step like how I have explained you'll be able to understand it very much easily now if I do regression performance dot show here you will be able to see I will be able to get the entire report itself now this is what is the report we are looking at right right now we are not comparing with any model right right now we are not comparing with any model because I have just trained my model with respect to reference and I'm just trying to see the model performance now how do I see my model performance so the first feature with respect to model performance and the target is CND I can over here see that the current model quality is minus 0.03 m e okay so this is very very good because it is just minus 0.03 now if I talk about me 4.1 map 16.13 if I see with respect to predicted and actual right right now see this is this is what is my predicted versus actual uh reference okay this is how my data looks like and this is basically the current data I'm not comparing it with anyone right now because later on we'll try to compare the next seven days data or next one day data that I do and I do the prediction then I will try to compare okay so this is the second one now see predicted versus actual in time so see this red one is basically the predicted one and actual is the actual data and it is very much near to each other so that basically means my model is very very good with respect to what we have actually trained right now so that basically my model is not degrading right now whenever we get a new data at that time again we'll see this graph and we'll see that how it is performing okay so here you can see basically see that okay it looks really really good uh my data is over here and it looks absolutely fine now if I go and see error predicted minus actual so here also you'll be able to see this red line is basically my uh difference between the predicted and action it is very much near to this within this minus 5 to plus 5 standard deviation that basically means within this range which is again very very good minus 5 2 plus 5 in a regression problem statement it is absolutely good now if I go ahead and see with respect to Absolute percentage error then again it is very much less uh within the zeros line already you can actually see over here with respect to the red line okay this is good error distribution also it ranges between minus 20 to plus 20 it is it is symmetrical almost symmetrical so this is also a good indication with respect to this and there is something called as error normality in the error normality you'll be able to see all the data points right it should be falling on this specific line and if you have probably seen my video this is uh basically we have created theoretical quantiles to just understand uh whether my errors follows a normal distribution or not so if it is falling on this particular line it basically says that yes it is following a normal distribution now I hope you are able to understand guys why I really focus on theoretical Concepts right you have learned statistics Theory we have learned machine learning theories we have learned feature engineering this and all these things are super important when we discuss the concepts like this and obviously through this we are actually monitoring the entire model now this is fine then the error bias is also very very less uh the majority uh with respect to all the prediction only this much difference is there underestimation basically means how much I can underestimate so if I say uh the number of bikes ordered at a specific time let's say is 20 I can underestimate to 4 that basically means 20 minus 16 right 4 4 can be my lower point of prediction in those scenario in the overestimation if it requires 20 you know my model can predict up to 34 okay which is obviously the error is basically there so that is what uh underestimation and overestimation is done predicted versus actual group this is how my underestimation of overestimation looks like if you've probably seen the graph okay so guys The Next Step what you can probably do is that you can also save this particular reports in a kind of HTML file for that you will have this specific command now going forward in the week one let's say now in the week one what will happen the data will obviously change now when we do that specific performance check on that data can we find out some kind of model drift or can we some find out some kind of data refer can we check how the model performance will be right and that is what we are basically going to do so again I'm going to create this metrics for this particular report and now I will just do this regression performance run on the current data from this time to this time that basically means for my next one week from the reference data or the current data that I have right then the reference data will be the previous data that I have trained my model on that is basically the reference and all the column mapping will be same whatever reference predictions that I specifically have so what I'm actually doing I'm taking the next week data and then I'm comparing it to my previous model which is my reference model and I'll try to see that how my performance will look now so if I do this and probably dot show now you'll be able to see a very good defined metrics where you will be able to compare each and everything now this is where it is so much handy guys trust me it is very very handy so here the regression model performance Target now see how beautifully I get this entire report now the current model that I have current model which is through the reference data uh the current model sorry the current model which is basically trained with the new data itself right so new data I've actually given over here right the current data is this one right with respect to the current model quality ah that is based on the new data this is what is the meme and map and if I probably see my reference model right there you are able to see minus 0.03 4.1 16.13 right now if you probably compare both these values yes there is a slight difference before it was minus 0.03 now it is minus 6.14 13.38 4.1 40.8 16.13 right now obviously you can see that there is definitely some more difference with respect to the model quality right now let's see some more things with respect to predicted versus actual right this is right now my current model this is how my counter plot looks like right with respect to predicted and actual value if I probably map it and if I plot it if I see with respect to my previous model that was trained on the previous data this is how it looks like this is also almost same but here you can see that this width has got decreased right this width has actually got decreased so some some information you are able to get saying that model has really changed a little bit okay then let's go with respect to predicted versus actual in time okay so right now this is currently with the new data how my predictions look like with respect to the actual so the black line that you will be able to see is actual the predicted line is basically the red line so obviously there is a lot of difference over here right I'll not say with respect to all these points but here one point is having a lot of difference see over here over here over here right so definitely the difference between actual and predicted are also changing this was my previous one right it was very much near to each other right so obviously you can see that the model is degrading a little bit so what we should do in the tag we should combine the previous data along with this particular data and then retain our model this is basically giving you a indication that you have to probably train your model by right because here now your prediction is completely changing right and it is really having a lot of difference with most of the points it is fine but with some points it is having a huge difference and this is just an indication saying that the model performance is degrading now okay now if I probably see the error that is predicted minus actual before you could see it was ranging only between this to this but now it is ranging between 42 minus 80. so this is what you can see with respect to the difference and this is the same point that you can probably see over here this is the same difference right that is the error between this this you can basically say the error the error is quite huge and this also you can probably check in my previous model the error was less right now the error is more so is it good that I probably take this weak data and again train it in the model this is basically giving an indication and you should definitely do it right now with respect to the absolute percentage error again there is a huge change when compared to this error distribution before it was ranging between minus 22 plus 20 but now here you can see minus 50 to plus 50. there are some points but the current model shows a lot of error distribution right it has been increased error normality this was my previous one most of the points of falling in this particular line just to show that my error normality the errors follows a normal distribution but in this case you can see it has moved from this particular path right now if I probably go ahead and see more right how things will basically happen now error bias table here you can see current mean then predicted versus actual all these things are there and obviously it will give you a very good amount of understanding now this is also very important underestimation right before it was minus 16 now it is minus 64. over estimation before it is plus 14 now it is plus 28 right all these things are basically there now similarly what I will do I will probably take the target drift and again create a report uh this was with respect to the model performance right by taking the week one data but if you remember we had also imported the target drift Library package to just see okay now in this scenario you may be thinking Krish did only the feature changes or did my target feature also changed because whatever new data that I am getting the original Target feature also is getting changed it may be getting changed right so for that what you will do you will probably do something called as Target grift now for this target drift you will be taking the reports metrics Target represent and now you will be setting your current data to that specific data and your reference data to the previous one let's see whether my target variable has changed or not and that is possible with the help of Target drift over here okay so now if you probably go and see drift in the column prediction whether there is any Target grid drift or not okay let's see drifting column count okay that basically means I'm just checking that is my target variable changed so this is what is the current it looks like this is the value that we have okay ah now here you can basically see the correlation right current and reference so here everything almost looks fine only with this particular data on the weekdays it is getting changed the correlation is inversed on all the other data the correlation is almost positive right if you probably see with respect to Spearman candle right all these things it looks good right so this is basically giving an indication that the target variable did not change much right what about data drift see see how my data drift looks like it was uh basically giving with respect to the current and this is with respect to the prediction right this is my how my current data looks like if I probably see data distribution right current and reference right so current over here you can basically see the count is very very less right and this is how your data looks like correlation for column prediction you can also see how the prediction correlation is it also looks almost same spare man candle or already everything looks almost same so I feel that Target drift is not there only some amount of changes are there with respect to that but definitely your data is getting changed right then again you can probably do it for the week two data week three data uh and probably see keep on changing let's say that I try it for week three data now here you will be able to see now your model will be degrading more and more and this usually happens with this kind of data itself now definitely it will be degrading a lot now see see the see the degradation that you will be able to see okay current model quality is minus 22. see it has increased now M 24 here 41 right it is increased right current and this and this again this counter plot looks completely different with respect to this right here your actual and predicted there's a huge difference before it used to look like this right error minus prediction write this sorry prediction minus actual again a huge difference so definitely this is giving you an indication by you really need to train your model now see this error distribution it has gone to I don't know how much minus it is over here it is completely left skewed here my value goes between minus 20 to plus 20 error normality obviously it is not aware falling in this specific line so error normality is also there and this by stable C minus 111 so if my model is basically if let's say for a current time 20 is required it is predicting minus so many minus right so this is the time that you really need to start training your model right and again you can also check with respect to Target drift and all data drift is also an option if you probably want to check right so this is what is the power girls and by this you get an idea right whether your model requires training or not right so I hope you like this particular video I'll share you this entire notebook just execute line by line if you have just executed or if you just know how to train a model um in a Jupiter notebook you will be able to understand it so guys yes this was it for my side I hope you like this particular video I will see you all in the next video have a great day ahead thank you and all take care bye"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML System Monitoring",
        "Topic":"Monitoring ML Model Performance",
        "Video Title":"How to Monitor Machine Learning Models (Evidently AI)",
        "URL":"https:\/\/www.youtube.com\/watch?v=L4Pv6ExBQPM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/L4Pv6ExBQPM\/hqdefault.jpg",
        "ID":"L4Pv6ExBQPM",
        "Publish Time":"2021-08-21T07:33:57Z",
        "Channel":"Data Professor",
        "Channel ID":"UCV8e2g4IWQqK71bbzGDEI4Q",
        "Transcript":"in this video i'm going to review in this video i'm going to provide a quick overview of how to use evidently ai for your data science projects and so without further ado we're starting right now so prior to filming this video i've heard about evidently ai on twitter and also on linkedin and so in this video i'm going to provide you a very quick overview and i'm going to show you how i would approach using a new library and so let's have a look together and i'll talk about my thought process of how i adapt a new library into my existing data sets and so let's have a look at the website so this is the website of evidently ai and you could head over to evidentlyai.com and so let's have a look here so here the first feature that you see is that it will provide you some metrics that will allow you to look at the model performance and a unique part of this is that it allows you to have a look at the drift of the data and it will signify which features are drifting and here it will allow you to visually have a look at the target drift and if you have a look further there's also a visualization that will allow you to also perform like a quick eda so for one thing any data set that you're working with there's a high probability that you're going to have some missing data and so evidently ai provides a very quick approach for you to have a look at the missing data and so here the red boxes will represent the missing data however it should be noted that this feature for data integrity will be available soon same for feature analysis so you could also have a look at the change in the correlation of some features over time and another feature that will be available in the future would be the performance by segment so for this one you could stratify the data set and then you could look at the various statistical metrics as a function of the stratified data like for example here they mentioned about the data region so for example if you have a classification data set where you're classifying the iris flowers so it will allow you to segment or stratify amongst the different types of flower so for each of the three different types of flower you will be able to look at the various statistical metrics let's go further here so this provides a very quick overview of evidently ai so here it mentions that you would only have to import evidently.dashboard and also import evidently.tabs into your jupyter notebook and then the thing is you'll prepare your data as a dataframe using pandas and then the rest from there you're going to use evidently ai and here it mentions that at the end of your analysis you'll be able to generate a report which you could share with your team and for this one it will be available soon so you could run it as a service so probably they're going to make it into like a dashboard a portable version like a web application probably similar to that of radio or streamlits and to get started you only need to pip install evidently and so why don't i have a look at the documentation let's click here so let's figure out together how i'll use evidently ai on an existing data set that i normally use so let's have a look here what is it it allows you to evaluate and monitor machine learning models in production and it will generate interactive reports or json profile from your pandas data frame or csv files and so here it mentions that it will generate six types of reports and currently it supports tabular data and so the first type of report is the datadrift report and you notice here that there's a link so if you click on the link you're going to have more details about it and so the documentation of evidently ai is very nice and it's very complete as well so here they provide the tlbr they provide the summary it provides the requirements here and how does it work and it provides you a glimpse of how does it look like for the data drift table so here you could have a look at the distribution of the data for your reference data and also for the current data and whether there is data drift detected or not and the p-value and another way of visualizing such data drift by the features here is to have a look here and they'll also provide you data distribution by feature and so you have the reference data in gray and the current data in red okay so it's very detailed right so this is for the first report data drift and the second report is the numerical target drift so let's have a look here it detects changes in numerical target and the feature behavior so these are some nice visualization generated by the reports let's have a look at the third type of the report and it is the categorical target drift so here it provides a detection of changes in the categorical target and the feature behavior let's have a look at this so this is the example image generated by the reports so here you see the reference in the left panel here and for the right panel you see the visualization for the current data set so the target drift doesn't occur when it is comparing the reference data versus the current data okay so they segment the data and then they show you two separate visualization in order to allow you to see whether there is a data drift occurring or not all right and so the fourth report is the regression model performance and the fifth report is for the classification model performance so here they take the error and they show you the distribution in histogram for the reference data and also for the current data and so you'll be able to see the spread of the data for both and then they're gonna see that it exhibits different behavior okay so this provides a very useful diagnostic of the model and the classification model as well so you compare the reference with the current data in order to see whether there is a correlational difference that arises from the data drift and how about number six the report is for probabilistic classification model performance so here you're taking a classification model so typically the values here are generated by second learn and it's coming from the function predict proba and so it's the prediction probability and so you could use those probability to visualize here so they compare again the reference data and the current data and so you can see that the reference data there is a clear separation between the malignant and the other whereas for the current data here the distribution is not so distinct okay so they're starting to have some overlapping regions at the middle here all right so these are the six type of reports and they even provide some example jupiter notebook and tutorials let's have a look at the notebook here all right very nice it provides a very detailed explanation on how you could prepare your data as a pandas data frame and so here they essentially stratified the data into two portion so the first is the reference right and then the second will serve as the current data set so essentially they're comparing here is two data frame okay so the current data frame and the reference data frame so reference data frame could be a data frame for the model that you have already built and let's say that over time a year afterward that would then become the current data okay so you would want to see whether your model which you have built last year is it still applicable or is it still valid for the future data the current data okay so they use the reference data set and the current data set and then they use both for comparison and so apparently evidently ai will perform all of this automatically here using functions provided in the notebook and so whichever you would like to implement you could click on the link and you go to the reports description here and they also provide you some jupiter notebook as well let's go and have a look at some of the functions here generating the reports okay so actually we could follow this step by step so the first step here would be to prepare the data as a pan this data frame and let's see all column names are string all feature names that are analyzed for drift have numerical type okay pass the column mapping into dashboard okay so it is for mapping the properties of the columns okay so essentially they're creating a dictionary an empty dictionary and then here line by line you're going to populate the dictionary so the first key would be target and then the following value would be y so this is the name of the column which will serve as the target and so the prediction here is the name of the column with the model prediction so your predictions will go here into the prediction and then id is the id of the data set date time is the name of the column with date time okay so if your data set has date time your data set is a time series data set then you would have this column numerical features and categorical features okay so here you specify which of your columns are numerical and which of your columns are categorical okay so if you have categorical features and you're encoding them you would also specify it here okay so and then it provides you some detail of generating the reports all right so why don't we fire up a jupiter notebook so here it apparently saves the import dashboard and tabs so let me see let me see oh open up the examples here as well so why don't i choose the regression performance here i'll put the byte sharing link and so here is the trooper notebook of it all right so let me have a look briefly at the notebook here so here they're importing all of the libraries so they have pandas numpy they have the random forest from scikit-learn and then they import specific functions from evidently all right so here they'll read in the data sets and then they'll segment the data into the reference data and the production data so the current data right all right so they just have like the first 120 rows to be reference data and then the last few rows to be the production data and then they specify the target to be cnt and then they specify the numerical features and the categorical features and then for the features they had the numerical and the categorical combined and then they built the model made the prediction all right and then they generated the reports okay cool so why don't i simply just instead of creating a new one here i'll copy this link and let's see i'll open up a new one open notebook i'll go to github and i'll paste the link okay and i'll click on the bike sharing one right which is the link that i copied okay so i'll modify this okay so here is gonna be the solubility data [Music] so i just say look load data okay and so raw data here i'll put in the link of the data sets i'll also rename the okay so let me save it into my google drive so it's creating a copy all right so let me close that one then so here the link i'll put in delete the other ones okay so i'll read it in ask raw data let's see how many okay so i'll import pandas as pd first oh okay so i'll have to install evidently first so pip install evidently so i don't think i'm using numpy so i'll delete it and then i'll run it okay and then let's load the data and then let's have a look at the shape of the data shape so i have 1444 so let's say if i do a 80 20 splits how many would that be so i'll do one one four four multiply 0.8 so it's going to be 915 okay so why don't i do this 915 here 915 until the last one okay so let's have a look at the ref data shape and the broad data shape okay so now i have 915 rolls for the ref data and i'll have 228 rolls for the production data it's roughly 80 20. so let's have a look at the ref data here so apparently all of them are numerical so let's see for the target it's going to be the lock s so i'll change that to log f date time i don't have it i'll just comment it okay so categorical features i don't have so i'll delete that from here and numerical features so we have so why don't i let me see so this is graph data so if i do columns what do i get and if i do list okay cool and but i will not okay so why don't i do this let me drop [Music] i'll draw block s let's see what do i get okay so i have to add axis with one okay there you go so what i did just now is i take the ref data data frame and that i dropped log s which is the last column so that i have only the x variables and then i use axis equals to 1 to specify that i'm dropping the column so if x is equal to zero then it means that i'm working with the rows but then because it's column i use x is equal to one and then dot columns here will display the names of the columns right here and then i'll make it into a list so that it will resemble a list here so i'll put that in here and i have my numerical features all right and so we'll build a random forest model okay and here we use a random state of zero and we'll assign it to the model variable let's do that and then we'll build the model using the model.fit function so we specify the x and the y so x are the features and y are the target so the model is built and then we'll perform the prediction using model.predict and then the input data is the x for a reference which is like the training set and production which is like the test set and then we assign it to the ref data in a new column called prediction and we'll do the same for the production data so we'll assign it to a new column called prediction as well we'll run it okay so let's have a look at the data set again i mean a data frame with a new column prediction so it's right here prediction side by side okay and the production data let's have a look data and the prediction is right here to the right okay and let's see regression performance reports column mapping target is target prediction prediction date time we don't have it so i'll comment that out the miracle features yeah we have that um categorical features we don't have that i'll comment that out okay let's run it the column mapping and then let's create the dashboard all right let's see i think they left space here so let's run it so it's creating the dashboard and let's see in a moment we probably will see it here dashboard.show loading okay so if it's not showing then i'll generate the report then so probably we'll have to activate some options in the jupyter notebook to make it work but to save time we'll just proceed here so let's save it out as the reports so i'll call it solubility dot html and so it should be in the file pane here here installability let me double click let's see it's taking a long time okay so it shows me the html so why don't i download it and then i'll open it up for you okay and it's loaded right here or click on it solubility all right so this is the reports so for the reference data and the current data so let's see mean error this is zero this is minus 0.02 mean absolute error 0.21 and the current is 0.59 okay and so the reference data is shown here and the current data is shown here so i will look at the predicted versus actual okay so roughly it looks quite similar reference error okay and it's performing some diagnostic measures here and so roughly the error distribution are the same pretty much this is more smooth because of more data here and the normality is roughly normal okay so you will see that both are not really drifting and you might see that the variance is a bit loose here because there is fewer data all right because we have 80 here and 20 here all right let's have a look at the error bias okay so they're showing for each features and then you're going to see the reference and the current data okay so they're showing for each of the features so you could compare 2.42 with the majority and it's 2.17 for the majority of the currents so here have the majority and they have the overestimation shown in red and the underestimation shown in blue and the majority is shown in green okay so they are the predictions under or over the general trend line okay so pretty nice here allows you to quickly compare the two data sets and because we're drawing them from the same data set it really looks the same and so they don't really have a data drift but it would be interesting to actually use that on an actual data where over time something might have changed maybe the buffer i mean the reagent that are used for the experiment might change maybe they changed the brand and so that might also influence the measured property and so that would be an interesting way to observe such changes over time and so let me know in the comments down below how you found this evidently ai library and if you're finding value in this video please smash the like button subscribe if you haven't already and also hit on the notification bell to be notified of the next video and as always the best way to learn data science is to do data science please enjoy the journey"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML System Monitoring",
        "Topic":"Detecting Data and Concept Drift",
        "Video Title":"Diagnosing Concept Drift with Visual Analytics",
        "URL":"https:\/\/www.youtube.com\/watch?v=449t1pfeKq0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/449t1pfeKq0\/hqdefault.jpg",
        "ID":"449t1pfeKq0",
        "Publish Time":"2020-10-20T17:20:34Z",
        "Channel":"IEEE Visualization Conference",
        "Channel ID":"UCBJDy-9NtG3Db0YcuqNugiw",
        "Transcript":"the classifier trend on the historical data may fail to classify the incoming data when the data distribution is changing this phenomenon is called concept drift to handle the concept drift we developed the drift vis a visual analytics system to help the expert understand when where and why the drift happens and update the model to improve performance"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML System Monitoring",
        "Topic":"Detecting Data and Concept Drift",
        "Video Title":"ML Drift: Identifying Issues Before You Have a Problem",
        "URL":"https:\/\/www.youtube.com\/watch?v=uOG685WFO00",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/uOG685WFO00\/hqdefault.jpg",
        "ID":"uOG685WFO00",
        "Publish Time":"2022-01-28T17:43:36Z",
        "Channel":"Fiddler AI",
        "Channel ID":"UCKH8vjXzKzSAX4PpGqFBx_g",
        "Transcript":"[Music] hello everybody thank you for joining me my name is amy hodler we're going to talk a little bit about machine learning drift and how to identify issues before they become big problems i like to start with fundamentals or basic principles the assumptions when we do machine learning is that our the data and the logic that we're using for training somehow mimics the real world if we didn't believe that we wouldn't have a lot of faith in our predictions and hopefully we wouldn't be putting our models into production unfortunately the real world tends to change on us i sometimes the sometimes quite quickly sometimes gradually but when that happens that the data that we originally used that training data which was static can be very different than what's happening in the real world uh whether there was actually a data issue or whether the fundamentals change um the pandemic is a perfect example people are shopping more for yoga pants versus dress pants during that time frame so that our predictions um changed or rather they became less um less accurate and so machine learning model drift very simply put just means that over time our ml models drift away from accuracy and they make poor predictions so this is a very simple definition it sounds rather logical and maybe something that you would expect and wouldn't be a big deal but in actuality it can actually have very big impact and so we've had customers that say hey over a weekend i had a model drift that cost us five hundred thousand dollars uh whether you're talking about uh poor predictions poor recommendations whether you're talking about missing fraud or predicting too much fraud inaccurately those things can have very bottom line impact for a lot of high value models it can also have impact to teams so another customer says that it took them two weeks to correct and troubleshoot a problem and those during those two weeks their data scientists weren't doing other work and it was slowing down getting the model back into production which also has a major impact so it has a real impact and how we experience drift varies quite a bit as well we might experience a sudden abrupt shift uh pandemic again if things change really quickly i and it was clear you know maybe from one day to the next or from one month in to the next that the predictions were no longer accurate or maybe it's gradual changes or incremental changes especially with the value where you have kind of a slower change and it may not be apparent that you have a big problem for for quite some time it can and often uh models fail very silently or you might see something recurrently and you keep shifting for it and you either have a seasonal change or maybe it's a weekend change uh or you know something that happens um with some frequent um periodicity or you might see something that is a blip and the thing about blips uh they might just be an outlier not a drift issue but you don't you might not know the difference at first and so that i i like to think about that as well when you're you're evaluating for your machine learning drift now there are several key types of drift to be aware of um this picture is just showing training data with a little dotted line for your decision boundary so you trained a classifier to say orange dots or blue dots and it just gives you you know kind of a probability of one or the other depending on some input the first type of drift to think about is concept drift and this is where the relationship the probability of whether something is blue or uh orange actually changes uh and the or a behavior of people have actually changed um but not necessarily the input data so behavior change relationship change is the way to think about concepts uh changing we think about data drift a little accurate asterisk here um because we're going to be we're going to get a little more precise but here the data has actually changed either your incoming or your outcoming data has changed but the fundamental relationship the decision boundary is still holding valid it's just you're getting different information now to be more precise people usually like to talk about label drift and feature drift label drift is a drift and a change in the output data so your ground truth is changing your probability of y is changing feature is actually a drift in the input data so the um type of people actually applying for your loan um the probability of x is actually changing now these changes can influence how accurate your model is or they might not the last little picture here in the bottom is a situation where your data has changed in this case your little um orange circles uh have changed but the uh if you keep the decision boundary the same your model is still working now the thing here is your model might still work and it might still be accurate but understanding that you have had either a label drift or a feature drift is important for other reasons including being a signal of future performance issue so to give you an example of the three different types to make it a little a little clearer i'm going to give you a loan example or at least loan application model and in the first stance talking to concept drift again your let's say macroeconomic factors have changed and so you're in the uh an income level that you originally considered creditworthy is no longer um considered appropriate it's now considered higher risk for mass economic reasons and so in that case you may want to you know shift or retrain your model or perhaps label draft so in this case you're having a larger portion of applicants that are credit worthy have started showing up the macroeconomics haven't changed but maybe you've launched some major massive campaign in a more affluent area and therefore you're getting people who are more creditworthy applying that may be a great thing or it might be something your business isn't ready for if you weren't intending to make that many loans you might also have feature drifts so in this case the input features are actually changing so maybe the income levels of most of your applicants are either increasing or decreasing you suddenly get more applicants for one region that perhaps have different income or debt to income ratios and so kind of understanding that and seeing that happen ahead of time can help you understand why your performance is changing so some quick examples there of drift in a loan application and you might be asking okay what triggers these drift what we just talked about your triggers would either be real changes label feature has changed input out has changed concept might have changed or you might have a data integrity issue so let's say you have uh correct data coming in but your actual your engineering actually made it a mistake and they've swapped the values of two fields maybe your age value and your debt to income ratio value has has has accidentally been flipped would cause some problems or you're actually getting incorrect data or missing data so let's say you have your front-end loan application had an update and now it's allowing null values and that's a a key field for making a decision and so those are the two things that tend to cause machine learning model draft now at this point if you're asking you like okay but but what's actually important amy um i'm not sure what's really changed um do i have to know all these different terms it's important to be grounded but it's probably more important to understand how do i detect issues regardless of what we want to call them how do i analyze for root cause and then how do i fix it so if you're designed to detect drift issues you need to know whether you have labels and have them in the time that you need them or not if you do have labels performance monitoring supervised learning uh is the way to go if you don't or they're lagging you want to look for data drift monitoring and unsupervised learning are methods you can use now performance monitoring and supervised learning work again if you've got labels fantastic if you know your ground truth you can just put in some very easy metrics looking at statistical measures uh how accuracy you know how accurate is this overall what's my false positive rate precision rate whatever you want to measure there are a lot of tools out there including fiddler that has the ability to put in what i would call these low-hanging fruit methods to just monitor and know what your performance is at all times you can also decide to develop your own and there are a number of supervised learning techniques this paper that i've mentioned here a survey of concept drift adaptation as a really good overview of things like sequential analysis statistical process controls which is kind of interesting because you can look at the rate of change as well and also doing things like monitoring for multiple distribution changes which can be very precise but also have more overhead so you have some number of tools out of the box but also you can develop your own methods as well now if you do not have labels remember that the training was done on the static snapshot of the real world and that often shifts or the real world keeps shifting over time and so understanding what the distribution of the data is that's coming in compared to and how it diverges to what you trained on can be an early indicator of issues so i've listed a number of distribution metrics here popular population stability index is uh popular in the financial industry um but there are also a number of other methods like the kl divergence uh the jensen shannon divergence which is based on kl uh but has some changes to to make it um symmetric and always have a finite value which is a good thing uh and then if you have an unusual distribution or rather you know you don't have a nice average um distribution you can use something like the the komogorov smirnoff test the ks test that doesn't require a parametric value so number of different methods fiddler uses the jensen shannon method again there are out of the box tools that you can use or you can develop or work on your own if you decide to use your own or develop your own unsupervised learning techniques there are a number of unsupervised and semi-supervised learning techniques this paper an overview of unsupervised drift detection methods uh has a great list i've copied a graphic from that paper in here uh they can be very accurate especially the online methods that are looking at each instance but there's also a lot of overhead with that batch methods are you know tend to be more efficient these metrics tend to be or this machine learning models tend to be globally oriented which means they may miss gradual shifts or regional shifts they may have some sensitivity issues as well so just be aware of that the other and probably the biggest drawback is they're very difficult to explain um to different teams why why you're expecting or seeing this kind of drift versus a statistical method that is just kind of easier for for teams to wrap their heads around and uh and act upon so you found an issue now we need to get to the root cause of what's going on two things to look at data integrity issue um and uh monitoring there and then drift analytics what's actually going on and what's important so data integrity and outlier monitoring sometimes i talk about that in monitoring um but when i think about how i use this uh it's really a best practice you you want to be alerted on you know kind of a you know something that's maybe an error or beyond a threshold but you may have missing values you may have the schema mismatch maybe your business uh developed a new catalog and you've got new products you need you know that are coming in and you may not know right away but this is the first place i would check if you're if you're getting alerts uh and you you're either getting performance degradation or you're seeing some data drift i always check the um the my data integrity and look at whether maybe something's changed in the business or more often than not maybe there's a pipeline issue maybe there's a bug there's an api that that has changed because that just gets that whole area off your plate before you dive deeper into analytics and explainability so once you're ready to do that um you want to take a look at drift analytics and this is where you've identified there's an issue and you're now comparing that to let's say your data distribution you're looking at that drift divergence trying to understand what's going on and then you'll want to attribute you know for this time segment which features are more important which ones impact the global prediction which ones are maybe impacting just a regional um decision or a smaller slice of data and then drill down into that affected traffic as well and these are this process is where you want to work with your data scientist you want to bring in your your egg domain experts to help you kind of drill in and use explainability as well um so you can understand what you're seeing and monitoring and can kind of get to the root cause now of course you want to fix it once you've found an issue this may be updating a pipeline maybe you need to fix a violation incorporate new data maybe you need to retrain or adapt your model maybe you just need to refresh the data that you're training on maybe you need to weight different features maybe you need to collect different data and so looking at your model is often you know an area that that you end up drilling into you might also just need to do model management we talked about periodicity so you might find that you want to schedule models to be valid during certain seasons or change maybe your balancing of your ensemble model as well so with that quick overview of machine learning drift hopefully some tidbits if you want more information you can check it out check us out on fiddler or ping me if you just want to dive deeper or looking for more resources so i thank you everybody i appreciate your time"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML System Monitoring",
        "Topic":"Detecting Data and Concept Drift",
        "Video Title":"What is Concept and Data Drift? | Data Science Fundamentals",
        "URL":"https:\/\/www.youtube.com\/watch?v=jRM5_Z31y5U",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/jRM5_Z31y5U\/hqdefault.jpg",
        "ID":"jRM5_Z31y5U",
        "Publish Time":"2023-01-16T18:38:31Z",
        "Channel":"NannyML",
        "Channel ID":"UCtUUzHy9Xs5uVhjuC-PQu7Q",
        "Transcript":"areas in general two causes why a machine learning model can fail silently one is covariative and the other one is concept and with covariate shift we typically mean the distributions of the model input changing over time so if you look at a quick distribution here of on which type of people the model was being trained on we can take for instance the example of customer lifetime value estimation where we try to predict how much value a customer is going to bring to a certain company and then we can train this machine learning model and we see that there is mostly young people that we've seen in the past who haven't been using the service or the product and that when we deployed it a machine learning model in production that then there's also older people becoming present and the Machine learning model has to predict on a new type of population which it wasn't necessarily optimized for during training and hence prone to baking mistake and that's the first part so changes in the distributions of the mobile inputs over time and specifically production comparing it to training or testing and the other one is concept risk detection and that is when the relationship between the model inputs and the Target changes overdone so if we go again with comparing training and production we could see that here during training the younger customers actually had a higher customer lifetime value than when we enter production after a while of course and this typically means that the concept or the patterns that the models extracted during training that they're not going to be very relevant in production anymore that may slightly different because of the underlying changes that are happening and our model not being updated or properly refactored or re-engined and these two types of issues can cause our machine learning performance to deteriorate and our models to fail silent"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML System Monitoring",
        "Topic":"Detecting Data and Concept Drift",
        "Video Title":"Detecting Concept and Data Drift in Machine Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=lus1LLTfZzM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/lus1LLTfZzM\/hqdefault.jpg",
        "ID":"lus1LLTfZzM",
        "Publish Time":"2024-11-23T16:48:02Z",
        "Channel":"Giuseppe Canale",
        "Channel ID":"UCrBZKEFFdXiRjm2yuvyEjkw",
        "Transcript":""
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML System Monitoring",
        "Topic":"A\/B Testing for ML Models",
        "Video Title":"What is A\/B Testing? | Data Science in Minutes",
        "URL":"https:\/\/www.youtube.com\/watch?v=zFMgpxG-chM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/zFMgpxG-chM\/hqdefault.jpg",
        "ID":"zFMgpxG-chM",
        "Publish Time":"2018-12-28T23:47:52Z",
        "Channel":"Data Science Dojo",
        "Channel ID":"UCzL_0nIe8B4-7ShhVPfJkgw",
        "Transcript":"Hi and welcome to this quick introduction to A\/B testing. So what is A\/B testing? At a high level, A\/B Testing is a statistical way of comparing two or more versions, such as Version A or Version B. to determine not only which version performs better but also to understand if a difference between two versions is statistically significant. So why do businesses conduct A\/B tests? This is the way businesses are run these days and they have to take a data-driven approach. A common dilemma that companies face is that they think they understand the customer, but in reality customers would behave much differently than you would think consciously or subconsciously. Users don't often even know why they make the choices they make, they just do. But when running an experiment or an A\/B test, you might find out otherwise and the results can often be very humbling and customers can behave much differently than you would think so it's best to conduct tests rather than relying on intuition. So let's visualize. For example, in marketing, or a web design, you might be comparing two different landing pages with each other or two different newsletters let's say you take the layout of the page you move the content body to the right now versus the left or maybe you change the call-to-action from green to blue or or your newsletter subject line has the word \"promotion\" in Version A and the word \"free\" in Version B in order for A\/B testing to work, you must call out your criteria for success before you begin your test. What is your hypothesis or rather what do you think will happen by changing to Version B. Maybe you're hoping to increase conversion rate or newsletter signups or increase opens call out your criteria for success ahead of time. Also, you will want to make sure that you split your traffic into two it doesn't have to be 50\/50 but you will want to figure out what is the minimum number of people I need to run my A\/B test on to achieve statistically significant results you can do this with multiple versions such as two buttons that are blue and two that are orange one blue and one orange button say RSVP and another blue and orange button say sign up this would be called a multivariate test or a full factorial test since you are comparing different factors. So what are some factors we can test on when conducting an A\/B test? Changing the layout of the page and shifting where certain items are such as moving the content body to the right, the navigation to the left, or the call to action near the bottom you can change the call to action such as changing the color or the text or where the call-to-action is located on a landing page or email. You can compare two different images with each other to see if one has a higher conversion rate or a higher click-through rate. And what about on the back-end suppose the UX and the UI is the same but you update your machine learning algorithm to update the recommendations that are shown to people but what happens if something is broken or funky or the data is messy and the quality is off or there's too much noise maybe there's a sampling problem and you don't randomize correctly it could be a one to two percent impact but you should make sure that your A\/B test is being conducted properly first by setting up an A\/A test. Thanks for watching, give us a like if you found this useful or you can check out our other videos at Data Science Dojo tutorials."
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML System Monitoring",
        "Topic":"A\/B Testing for ML Models",
        "Video Title":"Simple explanation of A\/B Testing",
        "URL":"https:\/\/www.youtube.com\/watch?v=eiIhTbFP0ls",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/eiIhTbFP0ls\/hqdefault.jpg",
        "ID":"eiIhTbFP0ls",
        "Publish Time":"2021-09-15T13:30:09Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"You have two option option a and b and you don't know which option is performing better. A B testing is a technique that allows you to decide that. Let me give you example. AtliQ technologies is a software company owned by my brother. They have this contact button at the top right. Contact button allows them to generate sales lead because a website visitor. If they want to develop a software they will click on contact button. Now business manager in AtliQ proposes that we should move the contact button in the middle because that way it will generate sales leads. Now we live in a world of data and we don't use intuition. So what AtliQ will do is they will prepare an alternate version of the website where the contact button is in the middle and they will run both the versions in parallel for let's say 10 days where the traffic is divided equally between both the versions. Unless the website is getting 10 000 users every day now 5000 users are visiting version A where the contact button is at top right the remaining 5000 are visiting version B where the contact button is in the middle. At the end of the day they calculate the sales lead lesson 10 days version A got 300 sales lead version B got 400 sales lead. Now they can conclude that version B is better because that allowed them to generate more sales lead. Now it is not as simple as it looks because you need to consider number of factor. Factor number one is is there a sampling bias. what I mean by that is the 5000 users who are visiting version B maybe they were from the same area where AtliQ already has an established name and that would be a case of sampling bias you need to distribute your load randomly. So that you know people from the random region are visiting the participating in version Aand B in an equal way that way there is no bias. You also need to make sure there are enough samples. You can't run this experiment for one hour and let's say 500 user visits your website based on that you make a conclusion that will also not work. We'll cover all of these things in detail including what is called statistical significance in a separate video, but my point is you want to make sure you are not arriving at a conclusion just by a random chance. Now A B testing is something that is used by most of the website. Whenever they make change in their layout.. for example, Amazon repetitively uses A B testing and I'm going to provide a link of a research paper where they have shown how they use A B testing to design the tabs. You know the product category tabs in Amazon website, Linkedin, Facebook any website whenever they want to make a layout change they use A B testing. A B testing is used heavily in data science and machine learning as well Product recommendation on Amazon is a classic case. Let's say you are a data scientist working in Amazon and you have built a machine learning model that makes product recommendations. So if you're visiting one product you see some other product recommendation at the bottom. Now you are deploying a new version of this machine learning model. You won't go ahead and just directly deploy that you will do A B testing which means you will deploy your algorithm on a separate instance and you will have percentage of your traffic going to the new version for some days. It doesn't have to be 50 50 percent maybe 70 percent traffic goes to the current version 30 goes to the new version then you measure the success of the new version after certain amount of days and again you have to take into account the random chance and and all of that is done by hypothesis testing, which again will cover in a separate video. but at the end of the day now you can make a conclusion that version b is better or not and based on that you can deploy that to pretty much all the users Now you'd ask me why only A B testing? Why not A B C or A B C D testing? Well you can do it. The contact button that we had at the top right we can put it in the middle we can put it in a top left and maybe run three versions and call it A B C testing and that's perfectly valid but A B testing is more popular because when you have two option it becomes very easy to test things out as you had more options your experiment your experimental design gets very very complicated. Hence a b testing till date remains the very simple very powerful technique and people use it throughout the industry- Facebook ad campaign for example, social media ad campaign is another classical example where you will run your ad campaign. Let's say I have for example 50 000 budget for my product you know to spend on social media campaign what I might do is I will run maybe two ad campaigns on facebook twenty five thousand dollar each and or let's say initial face ten thousand dollar five thousand dollar each and I will design two separate campaigns which will have separate ad layout. Maybe my demographics configuration is a little different and by running this experiment, I can figure out which ad campaign is performing the best and then I can deploy rest of my advertisement money on my ad campaign. So it's a very powerful technique. I hope you like this video. If you have any question please post in a comment box below."
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML System Monitoring",
        "Topic":"A\/B Testing for ML Models",
        "Video Title":"Qwak Webinar: A\/B Testing Your ML Models: Why, How &amp; When",
        "URL":"https:\/\/www.youtube.com\/watch?v=58zLPx4knY0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/58zLPx4knY0\/hqdefault.jpg",
        "ID":"58zLPx4knY0",
        "Publish Time":"2022-11-21T19:02:28Z",
        "Channel":"JFrog ML (Previously Qwak)",
        "Channel ID":"UCLWsudAvgYwEgLT_OEwhGAA",
        "Transcript":"it hi everyone I'm Yvonne fernbach azamite said co-founder and CDO at quack and today we'll be talking about every testing your ml models uh why you should do that how you can do that and and basically when so just a few words about myself as I said I'm currently the co-founder and CTO of Quark quack is an ml engineering platform uh before that I was working for Amazon web services as a machine learning specialist over there and before that I was the CTO of IDF the Israeli Defense Force it Department so about the agenda for today uh we'll talk about machine learning models and some of the techniques to evaluate the Modern Performance then we'll talk about a few of the different deployment strategies available for machine learning models when you should use each one of them and eventually we'll connect it to will show how we can evaluate models in production and deployed models accordingly we'll finish with some of the advanced implementations available for a b testing use cases and use cases will deploy and evaluate your models and eventually I will save some time for your questions feel free as admitted to ask your questions during this session and I'll try to answer them during the session itself so first topic is about ml models performance evaluation or basically what are the standards how data scientists and companies evaluate their models so one thing that you know eventually every data scientist is aware of and usually do it part of the data science experiment is to run evaluation during training so basically what data scientists usually do is they take the data set and split it to different data set that they can use to test and evaluate the models and during the training itself data scientists extract metrics from the models for example metrics like accuracy or F1 score Precision recall drawing the RC curve and many other metrics that are available to extract from the model itself so this is a great technique and basically using this technique the scientists are can can understand if the models actually give them the right performance that they try to achieve or not but in many cases during just training evaluation is not enough using training evaluation you cannot really know if your model eventually give you the right performance that you expect to get and walk in production as you expected so second type of testing that is not that commonly used but we highly recommend our customers to use is to write also engineering testing not just data tests so basically model is is a software it's something that you want to put in production you want to make sure that this code actually works as you expect and doesn't break for example so during the the training and basically during the phase that you build new versions of models what you we usually recommend is to also write unit tests for different functions that you have in your code for example data extraction functions uh maybe even uh different uh data tuning and data processing functions second is regression tests So eventually like in every software you do have challenges you do have bugs and you want to make sure that the next version doesn't regress uh uh your previous Solutions so basically test every bug that you find test and try to reproduce things that are not broken as you expect and make sure that you fix these things and last but not least is integration tests so when you build a model especially if this model is going to be a real-time deployed model you want to make sure that the model works as you expect from and from to finish from calling the API and sending a bunch of bunch of features to getting the prediction that you expect to get and make sure that all the internal functions and everything works as you expect so using integration test basically you can test this flow but one of the challenges that this as well doesn't really solve the production part what happens after your model is deployed in production what happens with the data with the features with the outputs afterwards and the next part is is production evaluation and this is about a bit more tricky because eventually you need to track all the data that goes through a model you need to track their model predictions and based on that you can understand few things about the model first of all and this is the more uh simple things that you can query and understand and test is is your data quality good are you missing any features did someone for example a broken one of the features they're not sending anything something none or something like that so first production evaluation is just the data quality this is a really simple test that eventually you can reproduce by saving the data and just understanding what happens over there second production evaluation test data drift model drift basically understanding that data can change over time and making sure that your model was trained on the right data set so for example if I go into the Quark platform I can see in here one of the pages that allow you to see for example a few of those tests so things like do I have missing features and I can see that for each one of the features by itself things like what is the data distribution of each one of the features all my features still looks pretty much the same this is even before running model monitoring something like that just understanding visualizing what actually happens in your models what data your models get do this uh data change over time does one of the feature doesn't look as you expect it to look and you can see in this graph that basically you have the gray area this is the training data versus the yellow area this is the real-time data in this case it's the latest 24 hours of data for this specific model in this case the model is a real-time churn model basically a binary classification model for uh um churn customers but there is another test and this is a model performance test so I can understand if my features uh are as I expect I can understand if the data qualities as I expect but let's part is also understanding the models perform actually as I expect so this is a bit more tricky because to know if your model is perform as you expect if I talked about it for example before in the training part and I talked about uh things like getting Devolution data set and extracting features like accuracy F1 score Etc doing that for production data is a bit more complicated and if you do have this data if you do have this feedback of what actually happened you can actually use that to close this Gap to close this feedback loop and understand what is the actual metric of the model for example if I have a recommendation system then every time someone buy item or even every time someone clicked click on an item then I can take that as a feedback looks like my water workers expect and based on that I can track the model performance what for example we do in quack is we have a separate API that you can use to send this feedback data and based on that for example we can give you these metrics so in this example again this is a binary classification model so the metrics are as you see in here you can see the RC curve you can see um the confusion magic itself and you can also see some of the performance of this model in this case uh this model and their dates over here but I think it's last week of this model you can see the accuracy you can see the recall Precision F1 score and of course you can create different type of metrics based on your specific model so this is motor evaluation and once you have model evaluation once you have this data once you understand if your model actually work as you expect and when it doesn't work as you expect you can actually do many things but before we jump to that to what you can do with that let's talk about different type of deployment strategies how basically you can deploy a model what you can do with this model so this is an example for Quark but basically there are many other types of deployment strategies and you can Implement different types based on your infrastructure for example in quack will support three different type of deployment strategies one is a batch deployment basically running a batch execution getting the input data running inference saving the output somewhere else and you can of course automate the process of batch prediction second is a real time inference so basically um creating an API creating an API for a model getting back a rest standpoint sending a request getting that responses and and manage this deployment as any other microservice that you have and last is a streaming deployment so deploy the model not as a synchronous endpoint but basically as a stream consumer for example Kafka that produced to another topic and you can actually manage each one of these different deployment strategies in a different way so for example for batch execution and I'm going back to how you can evaluate model you can actually uh cross validate different version of the models on different data sets so because it's an offline process it's quite easy to run a batch on a previous version and run a batch on new data and based on that connect these metrics and understand how your batch is actually behaving but they're more complicated type of deployments where we actually need to manage your deployment strategies are real-time deployments it doesn't really matter if it's in real time basically an API deployment or a streaming deployment in both cases eventually you have an API up and running you want to have ways to change and replace and upgrade your models you want to have ways to test new versions because again with batch execution testing new versions is easy basically run the same batch on your data run our new model story and test it make sure what's your performance test it versus your label it understand if your model works as you expect but with real-time deployment you are talking about real traffic and for that you actually need to have more complicated uh Solutions so for example Solutions like Gradle deployment so replace your your version gradually to make sure that nothing breaks and if it does roll back immediately um Shadow deployment that's something that they're going to show as well but the ability to basically replicate your data replicate with traffic send the same traffic to different deployed models and test the performance of each one of the deploy models but only one model the previous version for example uh send back responses to the clients and third is a b testing and this is one of the topics that I will elaborate the most so AV testing and this is again the main uh topic for this session what is heavy testing what why I'm talking about AV testing and uh um how can you actually do that with machine learning models so first of all what is a b testing so a b testing is a technique that I'm sure all of you know from you know other places other type of of software engineering for example many companies run a test on their UI and try to understand which for example test the a or the B actually give better performance on which specific UI people for example click the most but eventually a B test is an optimization technique based on a b testing you can alter different versions and affect your audience affect your user engagement so it's actually applicable for models as well you have two different models two different versions and you want to make sure which model actually gives you better results even maybe which model gives better results to which audience right because in many cases audience can change and models can actually give different results to different audiences for example us customers versus EU customers or different ages of customers Etc and when you want to talk about daily test and you want to try to start implementing habitats then uh you need to go back to the evaluation techniques and basically first of all that's the question how do I know if a model is better how do I know to which model I need to send the traffic so this is first question and I talked about it a bit before but basically you need to measure your models second and this is an advantage of a b testing and by the way there are different types of AP testing I'll talk about it as well but maybe tests allow you to smooth the deployment process so instead for example uh on you know just deploying a new version to all of your customers you can decide how this deployment will look like maybe deploy the version only to better customers or only to Long play customers and maybe then deploy that to one or two or three of your managed customers and only eventually deploy that to your to all of your other customers so even eventually AV testing techniques or techniques that allow you to split your traffic between different models allow you to smooth this deployment process and by doing that reduce the fellow impact make sure that if there is a failure with one of your versions the impact of that is smaller and not basically impacting all your customers so how to implement an A B testing so first of all you need to choose a metric you need to have a way of understanding of measuring which model which version actually work better I've seen cases I've been customers where for example they're using a business metric and this is something that you can do uh for example if you have a recommendation system again I talked about this example before you can measure eventually the impact the business impact how many people buy from my website so this is a business impact you can do that this is actually can give you the the best indication if a model is good or not but one of the challenges with that is that in many cases it takes time till you get this specific metric it takes time from the model running an inference till this specific customer in some cases by the way not all of them but till this specific customer this specific user actually have a business impact so this is one of the disadvantages of that another option is the model metric and then I'm going back basically to the closing the feedback loop so if you can get a metric that eventually gives you the idea if the model walks as you expect and it doesn't have to be your model label your final metric so again let's say recommendation system and let's say um your label is that someone need to buy this specific item but your Metro can actually be if someone clicks on an item so it can be even before this eventual this this this final metric actually happens and based on that you can understand this specific model performance and another question you need to ask yourself is how do I want to split the traffic because one option that is easier is just random so let's say I have a model and I want this model to uh I want to test different versions um so I want to randomly just split the traffic between two versions let's say 80 to my current version and 20 to a new version it can walk but there are some challenges with that for example if your model is something that users are not calling only once but users calling this model a few times then the challenge with that is that users can actually get different results in each time it can be fine it depends on your model but it can have an impact and the different the second option is to have persistent audiences basically make sure that the same audience the same user get the same uh prediction so uh before I go and show you examples of random of persistent uh there are a few questions so first question is does a b testing relevant for all verticals all models for example fintech So my answer is yes um I will say that for most of the models or especially models that actually have uh retraining processes models that you rebuild for uh different uh data sets or changes if data set AP testing is is relevant even for uh Industries like fintech for example a b task as I said before first it's a way to reduce the federal impact so for fintech for example let's say um uh risk models using a b testing you can make sure and gradually add more and more customers you can make sure that the new version doesn't have any uh um any impact that you don't expect it to have you can make sure that the model actually gives you better results so yeah I do believe that a b testing is something that is relevant for every vertical maybe not every model and it's fine not every model need to be deployed like this it does have some engineering efforts to actually Implement AP testing uh second question is there a way to add automation on top of this a b testing so uh for example based on a b testing based on the performance that you get automate this thing and rebuild a model redeploy model so first the answer is yes it is possible it is possible on quack um we'll talk about it um in in a few slides from here but basically you can automate based on the metrics it's not just a b testing once you have metrics once you evaluate models once you get the feedback from Models you can automate on that and rebuild the model if it's performance degrading and automate the deployment of model to 20 80 Etc of the traffic so let's continue a bit with the presentation and then answer other questions that we have uh as well so uh one thing that I want to show is an example of random traffic split so this is from the Quark platform I just uh took a screenshot from one of our demo again examples one of our models so this is the same real-time sharing model that I showed before you can see that in here for example I configured that um 50 of my traffic will be sent to a new version V3 but as Shadow deployment so basically what it means it means that I replicate randomly 50 of the traffic and send it to one version the new version the newest version in this case no customer actually getting this result but I can measure this model on actual traffic second version is V2 it gets 20 of the traffic the actual traffic and eighty percent is getting to default version so basically I have here three different versions deployed eighty percent still goes to my default version my previous version twenty percent to the new version I'm also testing a new version so in this case it's a random traffic split I don't really know which specific inference which specific input will get to which specific model another example is this so in this case it's a persistent traffic split what basically I did is I configured audiences I configured basically uh different users in this case users from New York and users from California and all other invocations and I configured that all the California users will go to the V3 versus all the New York users that will go to the V2 model so in this case I have persistency I know that every user from New York will always get the same model things won't be changing for them they will get the same results and based on that persistency I can actually eventually add more and more audiences to this V2 model or the free model so um before I go to the advancing limitation implementation let's answer a few more questions so uh first of all can I use different hardware for different variants so so yes actually this is a quite a common solution of changing between different models types let's say I had um a simple logistic model OKAY logistic regression model and I want we now want to develop a new deep learning uh neural network based model basically what I can do is I can add a new variation that's running on different visual cells even GPU versus CPU maybe even more CPU more memory so I can actually change them both and in addition to the performance of the models themself I can also measure the infrastructure performance the CPU utilization memory GPU the amount of traffic that goes to each one of the models so this is those are things that I can also test once I use different algories even if not different routers but once I use different variations different versions of the same or other models a few more questions um so another question is regarding um best practices to do a b tests for unsupervised models so unsupervised models is is another question because foreign models basically you don't have a label but it really depends on the model and in some models um you do have an impact so first of all it can be a business impact and based on that business impact you can decide which what's the weight and which model actually acts better so this is one um example um but even if you don't have this business impact you can actually try to measure other other things so you can try to measure the uh um the distribution of the outputs and based on that for example understand if your new version works as you expect or not you can try to measure and basically manually or using a human in the loop label your unsupervised model and based on that understand the metric of the model and maybe even eventually train a semi supervised model or a supervised model so there are many things that you can do even in non-supervised models it's a bit more complicated of course challenge because you don't have labels but you do have the ability to create and find metric for that as well um so one of the questions is about multi-owned Bandits and for that I'll go to um the next slide and talk about some of the advanced implementations and so first of all and this is again a question that someone asked is regarding automation of version creation and evaluation so yes it's possible Quark support that and basically once you have a deployed model once you measure and monitor the performance of models you can find out if you have performance degradation you can detect if you have drift in your features and you can based on that create automations automations can also happen not just Based on data changes but even if you just need to retrain your model because you have a new code or you want to add features and you want to return your models based on new features and everything can be automated and this automation for example can based on a schedule or based on a metric retrain a model test evaluation test the metric of the model and if this model this metric is good enough then deploy that model for example as Shadow deployment and eventually graduate this model to be your production model another Advanced implementation that is can actually be implemented based on the same techniques that I talked about is tools like model per customer model per audience so because I can split my API to different audiences then eventually I can build different models I can split the traffic to different models and I have multiple models and only not only just one model that serve all my traffic and the third Advanced implementation is is multi-around Bandits so just in a quick exponential what is multi-on band it's basically the idea of multi-amband AIDS is the ability to dynamically change the weight that I give to each one of my models so let's say I have five different models I can dynamically change the amount of traffic the percentage of traffic that I send to each one of these models and because in Quark everything is an API so I can do that automatically I can measure my model that I can measure my metrics and based on those measurement I can change using the API the amount of traffic the the weight of each one of these models and I can do that automatically and make sure that eventually I Implement a multi-around bandit on top of this architecture so this is something that is quite easy to implement because in one side you have a matrix and in the other side you have deployed models and you can change the uh uh traffic percentage to each one of these deployed models so a few more questions that I have here in the uh q a is um first of all we've got enriched models and will it fall in batch processing or real-time processing so so it depends I will say that um the model deployment strategy doesn't really have to be connected to the type of model it's connected to the business implementation so for example if I want this risk to be part of its transaction and I want to make sure that for every and and that this is an example for fintech for example so I want to make sure that every transaction is calculated and based on the risk I approved or not then it's for sure a real-time model but if I want every let's say 24 hours to understand the risk of my customers and based on that decide what's the credit line that I give the customers then it's a bridge for a use case and it really depends and based on the model you need to understand how this model need to be implemented what's the integration between this model and my backend and my business and my application and based on that decide which deployment strategy actually makes more sense for this specific use case one of the things that we do in Quark is we make sure that every model that is built on quack can be deployed to each one of them and I can one day decide to the product as a batch execution but if I now have a real-time use case then with a click of a button I can move to be I can move this model to be a real-time deployment and basically an API uh so we are at the end of our time so uh last question um is regarding AV test and focus on short-term Impact versus for example measure the long-term impact uh that can can happen in in week and months Etc so the way quack Walks Behind the Scenes in terms of the modern metrics model measurement is that we basically save automatically all the the input features and predictions and models and based on this data that is saved automatically in a data Lake will allow you to create queries create a model monitoring and create uh basically understand the model metrics and based on this data you can measure different type of metrics you can measure long term metrics based on the last for example a few weeks or few months and then we'll send out this model behave even if you change versions during this time you can decide if you want to look at the versions or not and you can also look at Short Term and based on that for example decide things like for example the weights for multi-ambanded architecture so there are many things that you can do during this architecture during this using this uh this solution to implement both short-term and long-term architecture so as a quick summary of what we've been talking about today so uh we talked about a few of the techniques to measure the the performance of models we talk about deployment strategies how you can Implement them different type real time string batch we talk about a b testing different types of a b testing and how you can measure IV testing and eventually about some of the advanced implementations that are possible using the platform like multi-on Bandits like model per customer or model per audience and like automating all these processes and making sure that your models are actually as effective as they can do they can be so um thank you everyone for joining this session and we'll see you in the next sessions thank you thank you Val and thank you everyone for joining we will make this webinar available on"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML System Monitoring",
        "Topic":"A\/B Testing for ML Models",
        "Video Title":"Amazon re:MARS 2022 - A\/B testing with real-world data to evaluate ML model updates (MLR207)",
        "URL":"https:\/\/www.youtube.com\/watch?v=sk2DcwGBRO4",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/sk2DcwGBRO4\/hqdefault.jpg",
        "ID":"sk2DcwGBRO4",
        "Publish Time":"2022-06-27T16:53:27Z",
        "Channel":"AWS Events",
        "Channel ID":"UCdoadna9HFHsxXWhafhNvKw",
        "Transcript":"- Hey, everyone. Happy Friday. Today is the last day of the re:MARS Conference. Hope you all had some amazing learning sessions. Before we dive into the topic where we'll be talking about A\/B testing for ML systems, let's have a little pop quiz. Who here needs a cup of coffee to get their day started? Raise your hands. All right, a lot of coffee fans here (chuckling). So, today, we are going to be talking about A\/B testing for an ML system. The concept of A\/B testing itself is not new. You might have heard about it and even instrumented it for their application. But what is new is over the past few years, our systems have evolved from being monolith to incorporating more and more of machine learning features and services. So, in this talk, we'll be focusing on how we implemented A\/B testing for a real world ML system. I'm Riaz Liyakath, I'm a Product Manager at Amazon. Joining me as my colleague, Anu. - Hey, everyone. My name is Anu Jayanthi, I'm the Solutions Architect at AWS. For the first part of the session, Riaz is gonna walk us through a use case, which provides how the recommendation engine helped the startups. I'll come back for the second part, and then we will go over why ML models change, how those changes are tested in real-time, and then even cover about the best practices that we use in our project. And then do a little bit deep dive into A\/B testing. Back to Riaz for now. - Thank you, Anu. What is one of the main challenges that we had during the pandemic while trying to watch shows? It takes an enormous amount of time to discover content and one of the ways to solve this problem and to improve customer experience is to use machine learning and personalization. So, in today's example, we are not going to be focusing on the Amazon Prime Video use case, but we are going to be talking about how machine learning and personalization can help to address the challenges faced by startups when they're trying to build on AWS. So we spoke to hundreds of startups and first, try to understand that when they want to build on AWS, what are some of the challenges that they're facing? So, AWS has like more than 200 plus services. We have great length and depth to offer like many different services, but with that, comes also complexity. And that's why we want to understand, what exactly are the challenges that our startups are facing before we solve them? The first one, as I mentioned, is definitely around using what technical service? For example, like if startup customers are using EC2 or ECS, the question should be should we also use API Gateway? Should we use a service like Cognito, for example? So having that type of recommendation is certainly valuable to a startup. The second thing is AWS Billing. So, as I mentioned, AWS has 200 plus services. And with that, sometimes the challenge is also to understand the billing and cost of each of those services. So, having more granularity and understanding around the billing and how to keep the cost low, I think that is a second thing that we heard. And the third thing is, at AWS, we produce a high number of technical content, but if I am a founder or a CTO of an early stage startup, sometimes I want an easier way to discover this content. And discovery is certainly a challenge when it comes to technical content. So, that's the third one. So, let's dive a little bit deeper to understand what right guidance at the right time, because I think it's a very tough question to answer. So, here, first, is a startup itself is a pretty broad term. It could refer to a two person startup working out of a garage. It could also refer to a billion dollar unicorn as well. So, I would say, early on in the life cycle of a startup. So usually like what matters to startups is mostly funding and credits because they need to leverage this to build out their MVP. Once they have their MVP built out, then they can take this to investors to get additional round of funding. So when a startup gets seed funding or early stage funding, so then what becomes important to them is product market fit. So this is where AWS, we can provide not only technical guidance, we also have co-founders within our organization who can connect the startups with the right VCs and accelerators. As the startup continues to grow, scaling becomes important. So this is where making a lot of architectural decisions. Should they be using a data lake? Should they be using an analytics service, like QuickSight? So those kind of decisions and how it impacts your product roadmap, that becomes important. And the last stage is once a startup is generating millions of revenue and has a billion dollar evaluation, all they care about is deploying into other regions and then catering to different market segments. So to go back and answer this question: So what does right guidance at the right time mean? I would say, it really depends on the maturity level of the startup and how we can provide personalized recommendations based on their life cycle. So in order to solve the challenges faced by startups, AWS designed this program called AWS Activate. So what is AWS Activate? AWS Activate is a program launched to empower startups with funding as well as technical and business guidance so that they can successfully launch on AWS. There are several benefits to using Activate. Some of the key ones that I wanna highlight today are: So once startups can get access to up to $100,000 of credit, so if you are like a startup founder, you can definitely reach out to us for getting access to this funding. The second thing is, as I mentioned, getting technical guidance and support, understanding what architectural patterns to use. So, we can definitely provide that guidance. The third thing is if startups want to successfully build on AWS, it's important to have the cloud knowledge within their organization. So they can also leverage credits and funding to build cloud knowledge within their organization as well. And the fourth thing is we also work with a lot of third party partners who have special offers and discounts, like New Relic, Datadog. So we can also pass those discounts and savings to startups so that they can take advantage of it when they build a technical stack. And the last one is getting personalized guidance through the Activate Console, which is what we'll be talking about today. So, before we dive into the Activate Console and understand the personalized guidance, so I wanna highlight a couple of startups who have been part of Activate. And the first one I wanna talk about is Stripe. You might have heard about Stripe. Stripe provides API-based services for financial transaction automation. They enrolled as an early member of the Activate cohort. And they gradually grew to become a billion dollar unicorn. But what is fascinating about this growth is they not only grew, but they also launched products such as Stripe Atlas, which actually helps other startups build on AWS as well. So this essentially helped to launch additional growth to other startups within the community. The other startup who have been highly successful as part of Activate is Airbnb. I'm sure like all of us would've used Airbnb while we are on vacation. I'm a huge fan of Airbnb. So, Airbnb, again, started as a very small team. So initially they're just using some of very few services like Amazon EC2, but over the years, they grew around many of our managed services such as RDS, EMR and so on. But since there are a lot of managed services, they were able to reduce operational inefficiencies within their organization. And this helped them to innovate faster on behalf of the customer. So we saw a couple of highly successful startups. Now, the question is how do we help the next wave of startups? So, as I mentioned earlier, Activate was one of the programs designed to help startups. And in this slide, I wanna talk a little bit deeper on how Activate actually helps customers. The first thing that we wanna understand before helping startup customers is to understand their profile. So, we wanna understand what industries a startup is part of. Are they part of autonomous driving? Are they a health and life sciences startup? What is their level of funding? What is their tech interest? Like, for example, if they are interested in analytics, like something like S3, Amazon QuickSight, all these type of analytics services would definitely be helpful for them. So, as we collect more of this information, it becomes easier for us to be provide personalized guidance. So we take all this information and then we provide customers guidance through a personalized recommendation feed, which is what we are going to be focusing in our talk. In addition to that, we also provide them a cost and credit summary. So as I mentioned, billing is one of the important areas where we want to help customers understand better. We also provide third party exclusive offers. The Tech Hub is where we have CloudFormation templates and AWS samples, which is created by our incredible SA community, that we make it accessible for all the startups. So, to understand the Activate Console itself, so there are like two big pieces to this. The first one is the Tech Hub, so as I mentioned, this mainly contains technical content, which could be workshops, blogs, code snippets, GitHub repos, CloudFormation templates. All of these things are put in a single repository. And the second is the offers that we get from partners that our startups can leverage. So, now, we saw a couple of areas how we can help startups, so now to understand why customers want personalization. So in addition to just providing technical resources and third party offers, startups also want to understand what are the relevant trainings that they should attend? Because sometimes we have so many number of trainings, startups sometimes find it overwhelming to discover what might be relevant for them. So getting recommendations around which events to attend. So that, as I said, would be helpful. And the other is around networking events because when we think of building a successful startup, tech stack is just one portion, but how do we connect startups to different venture capitalists or different accelerators? So that is also something that becomes important. So, when we want to provide recommendations, and in this case, I wanna talk a little bit about service recommendations itself. So, I wanna talk about what is the complexity with providing this type of recommendation? And let's try to understand with an example. So let's say I am the CTO of an early stage startup, and I want to run my Docker container, right? Today, there are more than 10 different ways you can run Docker on AWS. You can run it on EC2, you can run it on Fargate, Lambda, there are so many different ways. But what startup usually look for is what is the best architectural pattern that I should use, which is not only cost effective now, but which benefits me in the long term? So that's the type of recommendation that they want us to provide. And what we wanna do is, since we get inputs from different customers and we wanna take these inputs and provide these service recommendations so that startups can start adopting different patterns early on. And then, that provides beneficial in the long term. So, based on why customers wanted personalization, so we definitely took that feedback and we built a recommendation system. So this is just the version one that I'm going to be talking about. One of the things about Amazon is we have this day one mentality where we build a minimum viable product which meets basic customer criteria. So, we built a version one of this recommendation system. So, in this version one, so the way it works is if we want to serve any type of recommendation, first, we need to aggregate the content. So the content along with the metadata is all captured to a homegrown content management system. And then the categorization of this content, what is the topic of this content? Or whether it is an event, whether it is a technical topic. So all those are done manually today. And this is exactly what we wanted to improve by using machine learning and in particular, personalization. So if you want to leverage machine learning and personalization, first, we need to understand what are the different attributes of a content, and how it affects personalization. So a couple of things. So one is the type of content itself. Whether it's an event, a training content, and things like that. So, that is a broad category. But in addition to that, there are several parameters that we look for. For example, what services this particular content might be relevant for? What broad tech category does it come under? For example, if this is a content around automating containers, maybe this is something that should come under containers. So these are the type of metadata that we capture. And let's actually look at a concrete example. For example, if we want to serve a recommendation based on localization, how do we go about it? So, let's say there is a startup in San Francisco, and they want to attend an event around storage, which is happening in AWS Loft. Most of the time discovery of these events tends to be a challenge. So, here in this scenario, we wanna understand that first, what is the startup? What is their area of interest? Where are they located at? So we capture metadata such as the city, their location. And we also capture the event date as well. So taking all these things into consideration. So we built an ML model, which is used to serve the recommendation. So what I'm showing you here is how the UI looks in the final stage. So you can see there is a recommendation feed. And the first event that shows up is the event that is happening in San Francisco. So far, I took you through the high level overview of what this recommendation system is, what problem that we are trying to solve. I'm gonna pass the ball to my colleague, Anu, who's going to walk us through the technical start. So one thing that I wanna call out here is, so we won't be focusing too much on the ML development itself, but we'll be focusing on the experimentation and how we tested multiple models before we launch into production. Over to you, Anu. - Thanks, Riaz. So, till now, we ask about how ML-based recommendation model helps startup customers. For the second part, we are going to talk about how these ML models need to be constantly updated and how can these changes be tested in real-time? As we all know, it is very unlikely to have a perfect ML model. 10 years from now, the technology will be a lot better, plus the user behavior and the traffic patterns keep changing from time to time. So we need to incorporate all these changes in our ML model and make it efficient. So one thing that stands out is when we talk about updating the model or making any changes, that means we are updating the system or making some deployments. And as soon as we talk about deployments, the first thing that comes to mind is risk. Just because there is risk associated to deployment, we should not shy away from it, but rather embrace and make our systems and pipelines efficient, and then handle the changes in the most effective manner. In this talk, we are going to discuss about some of the best practices that we have used in our system. But before that, let's see at a high level, the different other topics that are covered. At a high level, we'll be talking about the best practices even before starting A\/B test, the A\/B test itself, and share some of the cool future initiatives that are planned in the pipeline. We use A\/B test for determining the winning model. But before that, we have certain checks and balances in place to make sure that the platform is ready for performing the A\/B test. The first one is called the AA test. AA test is a precursor to A\/B test, and it is also called as platform testing. Here, we are making sure the platform is stable after the deployment, without invoking the new feature. So this is just to make sure that the deployment was stable and there are no variables that could affect the validity of the A\/B test. Let's see how we perform A\/B test in our project. So, first, the developers code their changes in development instance, then that is deployed to a stage instance, a non-production instance where extensive testing is performed, like the end-to-end health of the application is checked, integration tests are performed and also using the traffic simulation, stress test is extensively performed as well. Once all these tests have passed, the code is then deployed to the production instance. After deployment, that is where we would configure this AA test. In AA test, as I mentioned, we do not invoke the new feature. So this is configured behind a control and variant; because the new feature is not invoked, the control and variant would be set to the same value. And in this case, it would be to the existing value. Then the users are divided into groups or cohorts and directed either to the control or variant because both of them have the same value. No matter which group the user belong to, or whether they're directed to control or the variant, they should experience the same behavior as well as produce identical KPIs. This is then monitored for 24 hours. The next check, what we have in our project is our testing with the override. This is where the actual feature would be tested, but only for the testing users, this is exposed. For the general users or the production users, they're directed still to the old behavior. There are two steps to configure this override test. First, the variant is changed to invoke the new feature. Second, the traffic configuration is changed to ensure that only the testing users are allowed to have the new experience. So once this configuration is set up, the testers go verify it in real-time. And then after they're comfortable and feel it's working as expected, the live traffic is routed in stages to experience the new feature as well. So this AA test is actually a very neat way to spot check or smoke check new features. Once the AA tests and override tests have passed, that means the platform is ready for performing A\/B test in real-time. So till now we saw how the platform is ready for testing, and we are okay to make the changes, but this is all a happy path scenario. Say there are cases wherein everything passed in stage, and the spot check also passed, but then the new feature is not able to scale as per the traffic. So what do we do in this case? The best strategy is to mask the new feature, but should have the least impact to the business or to the user community. So this is where we need to understand that new model is not always better. Change has to prove itself in production in real-time analysis. And that is why we are setting this platform to ensure that the comparison between the old and the new behavior is tested before diverting all the traffic to the users. Let's take a look and see how we can safely mask the new feature in case it doesn't work as expected. There are several ways to do it. I'll just be covering two of them. The first one is using traffic shifting. So, all along, we built a mechanism wherein we control the feature to be solved on how the traffic is directed to run either control or the variant. So in this case, what we can do is we can direct 100% of the traffic to the control and nothing to the variant. This automatically masks the new variant. As you can see, it does not involve any complex deployments or any configuration changes. I think we all agree that deployments come with a risk and here we are mitigating just that. Let's look at the second way of doing it. It is to revert the variant to the old behavior. This is very much similar, or rather the same as the AA testing that we started with. So in this case, because variant and control are configured to have the same value. No matter from which group the users come into, they would be experiencing the same behavior. One thing to understand is that just because the experiment or the deployment was not successful, it doesn't mean we wasted our time. Rather, we were able to gather real-time data, insights onto why it was all successful all along, but not able to scale in production. So we can use these data points collected and fine tune for the iterations. So now that we saw some of the checks and balances before performing A\/B test, let's take a quick look at what A\/B test itself is. A\/B test is a mechanism of splitting traffic for drawing real-time inference. Here, the users are divided into different groups or cohorts, and funneled into different digital experiences. The different models that have to be tested will already be configured in the system. So, depending on the traffic configuration, the groups of users will be directed to one variant or the other. And the KPIs are generated at the end of the session, and they're analyzed to check which variant of the model performs better based on the success criteria mentioned in the conditions. So when we talk about having any component or any system, the first thing that comes to mind is how do we implement it? There are several ways to implement A\/B testing. Let's take a look at a few of them. The first one is the homegrown approach. So I think even in our project, we thought, \"Okay, we have to build this, so why not build it ourselves?\" And I think most of you also might have the same thoughts. It is very much possible, but let's take a look at advantages and disadvantages of homegrown system. So in some projects, we've seen that homegrown A\/B testing mechanism is configured using feature flags. As we all know that building a system from scratch is tedious, as well as time consuming. It's not just building, but there is constant maintenance and debugging associated to it as well. And plus, this system needs to scale as per the changes in the traffic. If it is unoptimized, then we might end up seeing timeout issues, SQL errors, so on and so forth, which again, impacts the business. And third is this A\/B testing component is not just a traffic routing mechanism, but it also has the KPI analysis and metrics for inferring the winning model. And that requires knowledge, not just in the engineering part of it, but also the teams need to have in depth knowledge in data science, as well as statistics to perform extensive analysis before declaring the result. So what is the other option other than homegrown? The one thing I wanna mention is the advantage of course, right? I mean, there is some advantage, or there are some cases where homegrown would be the only approach. And that is in systems where the requirement is extremely complicated, or it is one off and not catered by any other way. That is where the teams go with a homegrown approach because they can modify it or customize as per their use case. So the next option is to use any of the third party SaaS vendors. The third party vendors provide a consumer-grade UI, they offer seamless scaling, as well as they provide all the components related to the A\/B testing, starting right from the traffic shifting, all the way to inferring the winning model. But there are some of the disadvantages as well, which we have noticed in our project, as well as heard from other customers. As we all know, it is an extra step and it is tedious to integrate any of the third party component. And plus it is hard to configure, maintain, and debug additional component as well in a project. And one more disadvantage that we have heard from the other customers is that in case they are in very tight regulatory requirements, then it is not possible for them to use third party vendors, because most of them store the KPI data in their infrastructure and process it for inferring the results. What this means is that from the customer perspective, the data moves out of their customer trusted zone, as well as it resides outside. And plus, the other disadvantage is that in case the data has to be sliced and diced in a different way, then it is not possible because usually there is no mechanism to provide the data dump back to the customer. This bottleneck is avoided by actually saving the KPI information before sending it over for analysis. As you can see, it introduces additional duplication, as well as duplication of data, as well as effort. So let's look at what's the other approach. See, the other approach is to use AWS CloudWatch Evidently. Because of the drawbacks listed in the early slides, customers were looking for a native AWS service for A\/B testing. AWS has the culture of working backwards from the customer. So after collecting enough data points, we realize that there is a way to improve by providing a A\/B testing tool. And that is how CloudWatch Evidently was launched in the observability stack. CloudWatch Evidently offers a end-to-end A\/B testing component, starting all the way from traffic routing, to inferring the winning model. It has tight integration with other AWS services, and not to mention it is a managed service so it removes the undifferentiated heavy lifting, and lets the team work on business critical initiatives. Just like most of the AWS services, this also comes with our pay-as-you-go pricing and offers seamless scaling. Evidently has two different features. They're similar, but distinct. The first one is the Launch. Launch is where it is used to safely release new features, components, or experience to the users. Here, the developers can expose a small set of traffic to the new feature. And because this Evidently is tightly connected to other AWS services like CloudWatch, different alarms can be set up and automation can be put in place to roll the system back to a stable state in case of anomalies, or it starts producing KPIs which are way off what we expect it to be. But if everything is going well, then the traffic can be dialed up. But after each dial up, it's better to make sure that the system behaves as expected. And that is where all this integration come into place. And to safely dial up the traffic, there are options present as well. The safe dial up will be set up in a scheduled launch section. This is where you can say, \"Hey, I want 10% of the traffic to go to the new feature or the variant on day one. Monitor, make sure everything is fine, then increase it to 20%,\" so on and so forth. And then you can go all the way to a 100%. Evidently offers up to five launch steps. In the print screen, you can see how the traffic goes all the way from 50% to 100% over a period of time. So the next feature that we are going to talk about from Evidently is the Experimentation. Experimentation is very similar to Launch in terms of infrastructure, but it provides a completely different use case. Here, Experimentation is used to compare multiple versions of a single feature. The different features to be tested against each other are deployed in production. Then the user community is divided into groups or cohorts, and the traffic is located to each of the variant, and the users are directed to those variants. After the users are done with their session activity, the generated KPIs are posted. And those KPIs are analyzed using statistics to infer the winning model. One example that I can cite here is our eCommerce website. Here, say, we want to modify the Checkout button. So we would deploy the changes related to the Checkout button; Maybe it's the different size, shape, color. All these variants would be deployed. Users would be directed randomly to one variant or the other. Because in this case, it is a checkout scenario, the appropriate KPI would be to collect the dollar amount checked out. And then after a period of time, collect these data points and analyze. One way to measure the success in this case is to figure out which model had the highest dollar amount checked out and declared that as the winner. Evidently provides different ways to infer the results, it could be anytime p-value, confidence interval, and so on. And this experimentation also comes with the Schedule Launch feature that we just discussed. Now, let's take a look at how it looks on the UI, the comparison part of it. CloudWatch Evidently gives you recommendations when A\/B testing between variants and those are in a visual manner. On the slide, you can see the graph. And a graph which shows how each of the variant is performing against... How the variants are performing against each other. It also gives you the results at the bottom in a color coded manner. As you can see, red is worse compared to the different variations that it's tested against, and green is the best performing. One way to save cost and time while performing experiments is to use any time p-value. When we use that for inferring the results, we don't have to wait for the experiment to complete, rather as in when there is enough data point to infer the results, Evidently does it for us. And once the results are there, you can stop the experiment because you already know what the result is. By this, you will be saving not just cost, but also the time. Now that we saw a little bit of what Evidently is, let's take a look at how experimentation is set up in our project. So while conducting the experiment, we had three variants. The first is the control, second is the treatment or the variant one, and the third one actually is the treatment two or the variant two. So, C stands for control. The first one is treatment one, and two is the treatment two in the diagram. The control is the rules engine. This is the V1 of the product, and it is all written in OpenSearch custom code. There are no ML algorithms associated to it. The reason why we did this is because again, going back to the day one mentality, we wanted to get the product out to the customers, see how they use it, get their feedback, and then make for the changes based on the feedback that we have received. That is the reason we went with the rules engine approach. And that rules engine will be tested against the two different ML models. And they are, first one, is you'll be using the popularity score, which is nothing but a weighted session click count. And this has a mix of... This is again, combined not just the session count, it has a mix of session count as well as the rules defined in the rules engine. And treatment two is a completely popularity score. This is a very aggressive treatment. It's not viable in the long run. We'll talk a little bit more into it on why we even have it in the first place. So now that we know what these three different models that are compared against each other, let's quickly take a look at the architecture diagram. On the slide, you can see the complete Activate Console architecture diagram. We will not be covering all these pieces. We'll just be focusing on the A\/B testing component. And as you know, A\/B testing can be divided into two pieces, traffic splitting, and metric analysis. So for the traffic splitting part of it, we are using Evidently in our project. And in Evidently we have three models configured, that is the Control, Model One and Model Two. Whenever a user logs into Evidently, it makes a API call, and then it receives which variant it needs to be directed to. And for this experimentation part, we have the Control set to 50%, Model One set to 30%, and Model Two set to receive 20% of the traffic. So depending on the flag it receives from the Evidently on which variant it should be directed to, there is a switch case statement in the application, which actually does the routing of the model. And then, ultimately the users received the results of the recommendation engine based on which model it catered to. So this was the traffic splitting part. Now, coming to the metric analysis part. We do not use Evidently for now for metric analysis. The reason is, again, the same day one mentality. We had a custom data pipeline in place. We are just using that for now. So that we always make changes, small changes over a period of time. We don't have to touch all the application at once. So having this mentality, we are still using the custom data pipeline, but then there are plans to move this to Evidently as well in the near future. As you can see, Evidently has a lot of different components, but only the pieces that you want can be used. You don't have to use everything in one go. You just use what you want, and then you pay for that price. And in our case, we have a mix of Evidently and homegrown. So, similarly, you can have a mix of Evidently or third party, homegrown third party, any combination of those. And plus this A\/B testing, as you can see, it also, it's kind of like two separate things. It separates the data science as well as the platform. So it's easy to check one or the other end, make sure which piece is not working. So we ran this experiment for 30 days, and happy to say that we had a successful result. And this is the result that we achieved. As you can see, the uplift is very high. So usually when you look at it, you'll be like, \"Is that even true? Why is the uplift so high? We don't see that usually in our projects.\" The reason is because the very first version was all in custom code, OpenSearch, no ML. And the second version is ML-based. That is the reason between the version one and version two, you see a very high uplift. And usually we have seen in projects that the first change the uplift is high, but then subsequently it gets hard to achieve this kind of uplift. I would say that it's extremely difficult. And one thing to notice is that when you look at the graph, the red one is the control, the green is the mix of popularity score and the rules engine, and the third one is just a popularity score. I mentioned that the T2 version, the popularity score is very aggressive. We don't want to have it running in the long run. This is something like, there's a patient who's having some problem. You give a medicine and it treats the problem for a short term, but then ultimately the patient is going to die. We don't want all those things happening. And why we say this is aggressive is because just having the click count, it not really does not add value to the business. If the title is catchy or if there's any click bait, then it increases the count, but not add real value. That's the reason, even though T2 looks like the winner, we will not be going with T2, but with T1. So now you might be wondering why do they even have this T2 in the first place if they're not considering it? It is because as far as hypothesis, the T1 should lie between the Control and T2, and it is used as a roof to find the maximum value that can be accomplished. And as per the results, the hypothesis, what we had in mind is correct. So for our case, T1 would be the winning model. Now, I'll have a Riaz come up on stage and walk us through some of the future initiatives in place. - Thank you, Anu. That was great. So we definitely learned a lot from this project, not only about building ML models, but how to experiment and launch them in production. So we definitely want to use these learnings for our future initiatives. There are several projects that we are working on to improve the customer experience. The first one is improving startup onboarding experience. So, here, we wanna use machine learning to automate sign up process for our startups so that they can sign up and get credits instantly. So that's one area we are leveraging ML. The other we talked about is using machine learning to provide intelligent architecture recommendations. So whether a customer should use RDS versus DynamoDB versus Neptune, like all these types of recommendations. So we wanna provide using machine learning, so that's another project. And the last one is we also want to encourage startups and founders to come together, collaborate as a community and build on AWS. So that is also a startup community-based project that we are also working on. So with that, I wanna wrap up today's session. Thanks so much for listening to us. I hope the insights that we shared today was helpful. And I can wait to see how you apply it to your projects. If you have any questions, both me and Anu will be outside the room, and you can also reach us via the email as well. Thanks so much, have a nice day. (audience applauding)"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML System Monitoring",
        "Topic":"Model Explainability and Interpretability",
        "Video Title":"Interpretable vs Explainable Machine Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=VY7SCl_DFho",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/VY7SCl_DFho\/hqdefault.jpg",
        "ID":"VY7SCl_DFho",
        "Publish Time":"2023-05-08T06:00:09Z",
        "Channel":"A Data Odyssey",
        "Channel ID":"UChsoWqJbEjBwrn00Zvghi4w",
        "Transcript":"when you first dive into the field of interpretable machine learning you'll notice some very similar terms flying around we're going to discuss one definition and hopefully clarify some things that is the difference between an interpretable model and an explainable model although I should warn you there is no consensus part of the problem is that IML is a new field definitions are still being proposed and debated you can't even seem to decide on the name for the field is it interpretable machine learning IML or explainable AI X AI so we'll focus on one potential definition we will learn how to classify a model as either interpretable or explainable using this definition we'll also discuss the related concept of interpretability to end we'll discuss the problems with this definition and why it's actually probably not necessary to classify models using it if you miss some details or want the reference for the information in this video make sure to check out the article Link in the description also if you're interested in IML make sure to wait till the end of the video where I explain how you can get access to a python shap course so we say something is interpretable if it is capable of being understood with that in mind we say a model is interpretable if it is capable of being understood by humans on its own we can look at the model parameters or a model summary and understand exactly how a prediction was made another term you may have seen for these types of models is an intrinsically or an inherently interpretable model a decision tree is a good example to understand how a prediction was made we simply have to transverse down the nodes of the tree another example is linear regression the model you see here gives the predicted loan size for a customer based on their age and income we can immediately see why someone aged $29 with $3,000 monthly income is predicted to have a maximum loan of $ 33,100 it is also easy to see the general Trends captured by the model that is the loan size will increase by $100 for every year a person ages and $10 for every additional dollar of income so we can look directly at these models to understand how they make predictions this is because they are simple the decision tree only has a few nodes and the regression only has three parameters as our models get more complicated we can no longer understand them in this way a model is a function the model features are the input and the predictions are the output an explainable model is a function that is too complicated for a human to understand another name for this is a black box model we need an an additional method or technique to be able to peer into the black box and understand how it works an example is a random Forest which is made of many decision trees to understand how the random Forest works we need to simultaneously understand each of the individual decision trees even with a small number of trees this is not possible for a human things get even more complicated when we start to look at algorithms like neural networks to put it in perspective alexnet convolutional neural network used for image recognition has over 62 million parameters in comparison our regression had three parameters it's simply not possible to understand how a model like Alex net works by looking at the parameter weights alone so we need some additional techniques to understand how these models work this includes methods created for specific types of models such as deeplift which was created to explain neural networks they also include model agnostic approaches which can be used to explain any model these include methods like lime shap pdps and Ice plots we should always use these methods with a level of caution they all come with their own assumptions and limitations and really they only provide approximations for how the model makes predictions up to this point we have discussed models as either being inter interpretable or explainable yet it may not always make sense to apply this binary flag this is because interpretability is on a spectrum or in other words interpretability is the degree to which a model can be understood a convolutional neural network is less interpretable than a random Forest Which is less interpretable than a decision tree most models can generally be classified as either interpretable or explainable however there is a gray area where people would disagree on the classification this gray area is where we find our first issue with the definition we may agree that a random forest with two trees is interpretable but a random forest with 100 trees is not at what point does the model go from being interpretable to explainable even a decision tree with many nodes or a regression with many parameters can become too too complicated for a human to understand without additional techniques the issue is that we're trying to classify a model based on human comprehension and there's no formal way to measure this your ability to understand a model will depend on your technical skills and professional experience even amongst professionals there will be disagreement another issue is what we Define as additional techniques even with the simplest models we often seek help from from additional methods it is common to use a correlation Matrix when explaining the weights of linear regression does this mean regression is now an explainable model this leads to the question do we even need this definition the goal of IML is to understand and explain our models we do not need to classify them as interpretable or explainable to do this the methods we choose will ultimately depend on the type of model and the specific specific questions we seek to answer but like I said in the beginning there is no consensus so what do you think do you agree with this definition or do you think there's a better way to classify our models if you're interested in IML and want to get started then check out my course on shap it's the most powerful python package for understanding and debugging our models and from the theory to application my course will teach you everything you need to get started and also for a limited time if you sign up to the newsletter in the description you will get free access"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML System Monitoring",
        "Topic":"Model Explainability and Interpretability",
        "Video Title":"Accuracy versus Interpretability \/ Explainability in Machine Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=SYV2CNyJcjc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/SYV2CNyJcjc\/hqdefault.jpg",
        "ID":"SYV2CNyJcjc",
        "Publish Time":"2023-08-08T18:46:23Z",
        "Channel":"Rajistics - data science, AI, and machine learning",
        "Channel ID":"UCu9fxVjTz5AJO7FR1upY02w",
        "Transcript":"the best performing machine learning model says if you remove rooms from your house it's actually going to increase the sales price yeah that is confusing but understanding this requires understanding trade-offs between accuracy and interpretability in machine learning models once we have that we can start making smarter decisions about how to use machine learning we have two variables here bathrooms and square footage which control the sales price so if we increase the bathrooms sales price goes up here's a more accurate model for determining the sales price which takes into account a lot more factors but it's harder to explain now if you're a calculator you can understand how to get a prediction but for most people it's really hard to grasp and understand what's happening in this model like why does increasing the total rooms decrease the price this kind of behavior is not unusual in the machine learning model often machine learning models that have lots and lots of features are going to have some sort of multi-collinearity or interaction between features which may make models more accurate but makes them harder for us to understand understanding this trade-off is is crucial to figuring out which model you want to use"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML System Monitoring",
        "Topic":"Model Explainability and Interpretability",
        "Video Title":"Differentiating Between Explainability and Interpretability",
        "URL":"https:\/\/www.youtube.com\/watch?v=hTUKuf8uy9Y",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/hTUKuf8uy9Y\/hqdefault.jpg",
        "ID":"hTUKuf8uy9Y",
        "Publish Time":"2024-04-03T11:56:54Z",
        "Channel":"Scry AI",
        "Channel ID":"UC5up2eM91ErGnvBd_04vnmw",
        "Transcript":"today we'll dive into the differences between explainable Ai and interpretable AI let's start with explainable AI this concept focuses on making the decision-making process of AI models understandable and transparent for end users with explainable AI the system provides clear reasons why a particular decision was made and how that decision can be justified it could point to specific factors such as income credit score or employment history making it easy for users to grasp the logic behind the ai's choices now let's shift our Focus to interpretable AI unlike explainable AI which emphasizes Clarity in decision-making interpretable AI is concerned with understanding the decision-making process of the model itself think of it as looking inside the black box of the AI system interpretable AI allows us to comprehend how the model processes information the significance it assigns to different features and the overall structure of its decision-making architecture to put it briefly interpretability focuses on understanding the inner workings of the models while explainability focuses on explaining the decisions made Let's illustrate with an example consider a medical diagnosis AI an explainable AI would tell you why a specific patient was diagnosed with a certain condition on the other hand an interpretable AI would give you insights into how the AI reached that diagnosis showcasing the importance of each medical indicator in the decision process both explainable Ai and interpretable AI offer unique benefits explainable AI helps build trust among users and stakeholders while interpretable AI allows data scientists and developers to refine and improve models for better performance striking a balance between these two concepts is crucial for harnessing the full potential of artificial intelligence responsibly and ethically"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML System Monitoring",
        "Topic":"Model Explainability and Interpretability",
        "Video Title":"AWS re:Invent 2020: Interpretability and explainability in machine learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=EBQOaqhsnqM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/EBQOaqhsnqM\/hqdefault.jpg",
        "ID":"EBQOaqhsnqM",
        "Publish Time":"2021-03-01T18:49:30Z",
        "Channel":"AWS Events",
        "Channel ID":"UCdoadna9HFHsxXWhafhNvKw",
        "Transcript":"Hello, everyone. Welcome to AIM408, Interpretability and Explainability in Machine Learning. My name is Vikrant Kahlir, Senior Solutions Architect with AWS. Before we dig into the agenda, let me clarify a few terms I will be using in this session. eXplainable AI or XAI refers to methods and techniques used in machine learning to explain machine learning reasoning to humans. This is in contrast to black box machine learning where even the designer of the system doesn't know why a machine learning solution arrived at a specific outcome. Now, the technical problem or challenge of explaining machine learning reasoning to humans is sometimes called interpretability, and sometimes called explainability. There is no formal consensus in the community as to what is the fine line between interpretability and explainability. People use it interchangeably. So feel free to use whichever you like or whichever suits your purpose. There is no formal definition as to this interpretability should be used in the context when we are observing cause and effect or when we are using explainability when we are talking about deep learning systems. In this session, I will be using explainability to refer to the broader ecosystem of methods and techniques that are used to explain machine learning reasoning to humans. In terms of agenda, I will talk about why explainability is important. We will talk about different ways in which we can categorize explainability methods and we will list some of the methods that are popular in explainability. But most of the session would be focused on one specific method called SHAP. And we will also go into the science and maths behind SHAP method and towards the end, I will have a short demo of Amazon SageMaker to clarify a machine learning capability for explainability that we have added to Amazon SageMaker. So, why explainability? There are two dimensions to machine learning models. As you can see on the graph here, one is accuracy. And the other one is explainability. We use loss function to as an indicator for model accuracy. And that works very well. But loss function or model accuracy doesn't explain how fair or responsible our machine learning model is. You cannot trust the machine learning model prediction just based on the accuracy of the model. We need a second dimension and that is where explainability comes into picture. Explainability is important to establish trust between humans and machine learning systems. It is also important or becoming important because of increasing regulatory compliance around right to explanation. In 2018, GDPR Act was enacted and we started talking about the privacy of the data, but there is a less popular article in GDPR that talks about right to explanation. Now, right to explanation is only defined in principle for GDPR because it is precautionary, but there are countries like Singapore that are not only defining right to explanation in principle, but are also using or have developed a framework for enterprise companies to adopt explainability into machine learning systems. The interesting part is accuracy and explainability as you can see on the graph here are negatively correlated. What it means is that on the top left you will see machine learning models like rule based and logistic regression, which have low to moderate accuracy, but have high intrinsic explainability built into it and on the other hand, on the bottom right, you will see complex models like support vector machines and neural networks which have very high accuracy which is desired but are almost black box and are not explainable. So with that understanding, let us try to see what machine learning methods or explainability can do to a specific customer scenario. We understand explainability is important and let's try to develop an intuition around a specific business scenario where it can prove to be very helpful. Take for example you are traveling from city A to city B; in city B, you want to buy a bottle of water. You try to make a payment with your credit card and it was declined. You try it again, it was declined again. And you were annoyed you call the Card Services customer care to see what's wrong with your card, why it was declined. The customer care rep can see that okay, your specific payment was tagged as fraudulent with a 60% probability but that is not a good enough explanation for you because it doesn't help you in any way. Maybe there was a method or a technique that can take that 60% probability of fraud and break it down by its individual components. And in this specific scenario that you see here, there are three inputs that factored into the payment being tagged as fraudulent. And the topmost, that is inconsistency with the past activity has the highest weightage. And that alone influenced the payment to be tagged as fraudulent. Now, this is a very helpful explanation. The customer care rep can share this information with the customer and maybe in the future, he can set up a Travel Alert and avoid this situation. This is the type of applications that we see for explainability that are very useful. In terms of what are different explainability methods and how they are categorized, before going into specific methods, let's talk about how the methods are categorized. Explainability methods in one way can be categorized based on the scope. We can have local scope or a global scope. Local scope means something that we were trying to do in the previous example, come up with an explanation for a specific credit card transaction. To identify why it was declined, why it was tagged as fraudulent, that is local scope. But maybe data scientists and machine learning engineers are sometimes interested in learning overall, across multiple examples, which particular features contribute the most to the model prediction score. And that is where we take into account the global scope, and we usually use the averages across the training or the validation examples. The other dimensions that can be used to categorize explainability methods, is methods could be intrinsic. What it means is that by the nature or the way the information is represented inside the models that can be used to explain or decode the cause and effect observed in the system. Now, this type of methods are model specific because they take into account the information representation. On the other hand, we have methods where we don't care about explainability until at the time of the model training, and after the model training, we try to come up with a point of view to explain the machine learning reasoning behind the predictions made by that model and these models are called post hoc. And we are going to focus on post hoc models only in this session. Let us double click on post hoc and try to see what are some of the popular methods that falls under different categories. So, we are just focusing on the post hoc and we can see that post hoc can do both local and global predictions and under individual category, we can have model specific and model agnostic. On the left you see methods like integrated gradients, tree feature contribution, Sabas, but we are not going to go through each of them. The focus of this session is just to see how one particular method that is becoming very popular, that gives consistent result and is backed by a good theory. We are going to talk about it, and it's called SHAP. It's important to understand theory behind SHAP because that is the foundation for the consistent results or the popularity of the SHAP. In next few slides, I will be talking about theory behind SHAP and the maths behind the SHAP. How actually the Shapley values are derived. So Shapley value is a concept in cooperative game theory. It was introduced by Lloyd Shapley in 1951. Lloyd Shapley was awarded a Nobel Prize for Economics in 2012. Let us try to understand in what situations we can apply Shapley values and try to extrapolate it to apply to machine learning system later on. Let us take an example of a ride share. Let us say Grace, Bob and Alex on Friday evening met at a bar, they had a good conversation and during the conversation they realized they all are headed in the same direction. And they were excited they can travel together and save one the fare but the problem is that they were traveling unequal distances. Now, what is the fair way to divide the fare if they are traveling unequal distance and that is a problem. Now, Shapley value is the only solution that gives you the fairest solution for this type of scenario. What do I have to do to calculate the fair using Shapley values, let us try to translate this particular problem statement into the constructs defined by Shapley values. So we will assume rideshare is a game. The riders are the player. The miles traveled by individual rider is the contribution of individual player and the total reward is the total taxi fare. The problem statement we are trying to solve is given the contribution of individual players, what is the total reward or the fare that they should contribute to the final reward. And these are the types of problems that can be solved with the Shapley values. Now, Shapley values is the only solution that gives you the fairest possible solution, because it is backed by a solid theory. Let's try to understand the axioms behind the Shapley values theory. In order to understand the properties of Shapley values, let us assume another fictitious company example. Let us say there is a company of 10 employees and that company makes a profit and all the profit is distributed back to the employees based on their contribution. Assuming this is the scenario, let us try to understand efficiency. When we talk about efficiency, what it means is that bonuses add up to the total profit and the bonuses are based on the contribution of individual employees. And this is one of the important properties that distinguish Shapley values from other methods, then there is the concept of null and dummy. What it means is that, if there is a new contribution, maybe the employee was on a sabbatical for a year, no contribution from employee, no bonus. Same for symmetry. Equal contribution, equal bonus. Two employees contributing equally should get equal bonus and the last one is the linearity property, which basically means bonus from different departments add up to the total company bonus. This is little bit important because sometimes what we observe especially in machine learning system, the finest possible granular grain or the feature or the item is not accounted and we are interested in more high level features. So we try to group them into high level features. The principle of linearity apply even in terms of high level features. Take for example pixels. You can combine the pixels into group of pixels, it becomes a super pixel and super pixel becomes the feature set for you. And you can sum up the contribution of all the super pixels to calculate the total contribution of that particular super pixel at the machine learning model level and you can aggregate contribution of all the pixels and they will all sum up to the total final score provided by the machine learning model. Linearity and efficiency are very important properties and makes Shapley values unique. So, so far, we have spoken about general situations about the theory to understand and develop more intuition about Shapley values. How about how do we calculate Shapley values for an individual player? What we do is that assume Grace, Bob and Alex are playing the game of the right share. In this example, we will try to come up with the subset. The subsets could be Bob and Alex traveling together, Alex and Grace traveling together or all three traveling together, and these subsets are called Coalitions. What we will do is that we will take one coalition, and if we want to derive the marginal contribution of Alex for a specific coalition, we will try to calculate the reward, we will try to calculate the contribution with and without Alex. And that difference will give me the marginal contribution of Alex in a specific coalition. And then I will aggregate the sum of Alex marginal contribution from all the coalitions and try to get an average across all the examples. This will give me the Shapley value of Alex in a rideshare game. This is how we do the maths for Shapley values. I know it was mouthful, you don't have to worry about actually implementing or programming these complex formulas. This is already done. And we will talk about specific implementations that are used in explainability that already takes care of it. Now the problem is how do we translate this Shapley value concept to machine learning? It's very simple. All you have to do is you have to take the same constraints of the game and apply them or translate them to the machine learning world. Your game becomes the model. Your players becomes the features. Your subset of players becomes the subset of features. Your specific player becomes specific feature. Your instance being explained, a specific example remains the same. And this is a simple translation. This mapping is all you need to translate Shapley values or to apply Shapley values to machine learning problem. How about the maths? Even the maths remains the same. All you have to do is just the representation becomes different, the games becomes model and player becomes feature. That's it. So where is the problem? The complexity or the problem is in the number of the players are in the in the game in machine learning world. In the right share example, I was only talking about three players. But in machine learning world, we talk about thousands of features, which are basically thousands of players and forming coalitions among them and calculating the marginal contribution and then aggregating and taking the average across will convert this problem into an exponential time problem. And that is the problem that makes implementation of Shapley values in machine learning complex. And then there is another problem. In order to calculate marginal contribution of a specific feature, we have to calculate the contribution within and without. We have to take the coalition and calculate the marginal contribution with and without the feature, how do we represent or simulate that missing feature? This is especially a challenging problem, it's not so much of a problem in NLP and computer vision, maybe you can use black image as the base image to simulate the missing values. But in tabular data, it's a big challenge. There is no specific prescriptive guidance on this one. We call it baseline, but there are few things that we will discuss that can help you with it. So how do we solve it? So open-source SHAP implementation, implemented a kernel and took into account the exponential time problem and the way [?]. It basically, instead of calculating the marginal contribution across all the coalitions, it calculates it across some limited number of subsets or coalitions. So it doesn't give you the exact Shapley value for a feature, but it gives you the approximate value. Now, the art and science here is that if you take two less subsets or coalitions into your calculation, your values may not have a practical use, they may deviate and may not be close to the actual one. And if you try to take into account more coalitions, then you are adding the computational complexity. This is the fine balance that open-source SHAP implementation made and came up with this default set of 2[M] + 2048 subsets that are used to derive the approximate value. Now, this is step one of solving this problem, but there is further opportunity to improve on this particular solution. And we will talk about the improved version when we get to the AWS services for explainability. Now, there is another problem. How do we simulate the missing values in the tabular data? What we call these missing values as the baseline. We need to define the baseline to represent the missing values. Sometimes we can use training sets, sometimes we use median of the data set, K means representatives. I don't have a prescriptive example for you. But let's assume that we will use training set as the baseline in the examples, and I will demonstrate it in the demo how to specify the baselines when we are trying to kick off explainability job. So I'm switching the gears here and so far I've spoken about the theoretical aspect of SHAP and the open-source SHAP implementation and how SHAP solves the computational problem by taking into account only the approximation and not the real value. Amazon SageMaker clarify is extending and making it further simple for machine learning engineers and data scientists to build explainability into machine learning development life cycles, and Clarify is a capability that we have added into the Amazon SageMaker. It does six things. The top two things are not relevant for this session because they're more focused on the bias. It can help you detect the bias in data, it can help you detect bias in the training model. The bottom four are what we will focus on. The bottom four basically means this service can give you local explanation, it can give you the global explanation. It can also help you detect drift in the feature importance over time. And in case you are one you are operating in one of the industries that requires right to explanation, you can also generate reports that can be shared with auditor to comply with right to explanation guidelines. Now that is the high level capabilities of Amazon SageMaker Clarify that addresses both buyers and explainability. Let's try to dig in how under the hood like it works, what are the aspects of technology that it tries to simplify. The first thing that we did was we took the SHAP algorithm and we containerized it. We packaged it into a container and we optimized it further. And then we made it programmable through SageMaker SDK. So if machine learning engineers and data scientists are already using SageMaker SDK, it will seem very familiar to them the way they launch the training job, it will be the same experience that they can launch the explainability analysis job as well. And we will demonstrate it in the demo. Now another thing that we have optimized is data scientist and machine learning engineer time needs to be optimized. So there is an opportunity to parallelize the explainability jobs to make it run faster. And the way we did is you can specify more than one SageMaker instances to run your explainability jobs. And we have parallelized it using Apache Spark. This lets you complete the job faster and to improve the productivity of data scientists. They don't have to wait long for their explainability results to come back. And the bottom two points that you see here for Clarify, they're more about we have integrated-- Clarify is not a service that lives in just step one of machine learning, or during the model debugging. It lives throughout the model development lifecycle in different stages. You can use it from within the Amazon SageMaker Studio, or you can also use it to see how the feature importance or attributes are changing for a deployed model. And we provide charts for both model monitor and from within the studio that can be accessed by data scientists to understand their model better. Now, how do we take all this and apply it to the real world problem? One of the AWS customers, Zopa, which is a UK-based digital bank and peer-to-peer lender wanted to apply explainability capability to fraud detection, to earn trust of their internal and external stakeholders. And they used Amazon SageMaker Clarify and can now produce model explanations more quickly and seamlessly. I hope after listening through this session, you would try Amazon SageMaker Clarify and would apply it to more exciting use cases. So with that, we will jump into the demo. In the demo, I'm using the German Credit Data set. This data set had attributes, these attributes are like age, the length of the credit history, the gender, the foreign origin of the requester. And the label here is all the prediction we want to make is whether this particular requester falls into the high risk category or low risk category. So it's either zero or one, a binary classification based on the model attributes. I've already trained the model, an XGBoost model that I'm using in this particular example. And the problem that I'm trying to solve is I want to come up with an explanation of what is driving which particular inputs or factors are contributing the most to the classification. I would be using Amazon SageMaker Studio to demonstrate this example. And there are two parts to the demo. In part one, I would be using Jupyter Notebook and SageMaker SDK to start Amazon SageMaker Clarify job. And then I will pull the results from a destination, bring it into the Notebook and try to project some results. And then the second part of the demo is more focused on model monitoring. Amazon SageMaker Clarify integrates with model monitor. It leverages and integrates well with model monitor and you can add charts to your model monitored that shows change in feature attribution over time. So let's start with demo one. I'm in Amazon SageMaker Studio, I'm using the German Credit Data set. I already have a trained model, I'm using the analyzer processing job Jupyter Notebook. In this notebook, first thing I'm going to do is I will specify three different inputs that are required to start the processing job. The first is the analyzer, the analysis config, which basically says I want a SHAP explanation for my model. I will provide a path to the data set and then I will specify a destination where my results will be published. And I hit run and behind the scene what Clarify is doing, it will pull in the optimized SHAP algorithm container. And we'll use the baseline set, will deploy a shadow model in the same VPC to do the inference, and then will push back the results into the destination path. Now, once the results are available, you can see these results. This is how they look. I can pull it into the Jupyter Notebook and see how they're structured as a JSON. But that is not very intuitive. But on top, what you would like to do is you would like to draw charts using this data. And you can do it for bias and explainability. I'm going to skip the bias part. So we will try to analyze the feature importance. And the last chart is what showed me the feature importance. The next part of the demo that you see here is about how to track model monitoring. So in this case, I will go to components and registries, go to endpoints, and I will see my model being deployed here that says German XGB. I will pull that model. And again, on the top you see there are five entries, I will click on model explainability. And then it will take me to the place where I can add charts. The first chart that I'm adding here is changing the top 10 features. I'll try to read in all the predictions made in a specified time. And from there, I will try to see what are the top 10 features that contributed to the predictions in that duration. And let's try to do another chart this time. Maybe I'm interested in feature attribution changes across a specific timeline, maybe a week and see how it is wearing. So in this example, I took the duration as one week and I'm trying to see maturity months and how the feature attribution is fluctuating within that one week window. And you can do it for other features as well such as loan amount. So I showed you two places where you can use Amazon SageMaker Clarify. And you can see how simple it is for data scientist and machine learning to use SageMaker Clarify from within Amazon SageMaker and also from within SageMaker Studio to monitor the feature importance or feature attribution of a deployed model. With that, it brings us to the end of the demo and to the end of this session. Thank you all for listening in. And please complete the session survey. Hope you have a good rest of the day. Thank you."
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"ML System Design Patterns",
        "Video Title":"Top 7 Most-Used Distributed System Patterns",
        "URL":"https:\/\/www.youtube.com\/watch?v=nH4qjmP2KEE",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/nH4qjmP2KEE\/hqdefault.jpg",
        "ID":"nH4qjmP2KEE",
        "Publish Time":"2023-05-09T15:30:11Z",
        "Channel":"ByteByteGo",
        "Channel ID":"UCZgt6AzoyjslHTC9dz0UoTw",
        "Transcript":""
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"ML System Design Patterns",
        "Video Title":"20 System Design Concepts Explained in 10 Minutes",
        "URL":"https:\/\/www.youtube.com\/watch?v=i53Gi_K3o7I",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/i53Gi_K3o7I\/hqdefault.jpg",
        "ID":"i53Gi_K3o7I",
        "Publish Time":"2023-03-11T15:31:00Z",
        "Channel":"NeetCode",
        "Channel ID":"UC_mYaQAE6-71rjSN6CeCA-g",
        "Transcript":"do you want to level up from Junior Dev so that you can build scalable apps or maybe get a 100K pay Bump by passing your system design interview round well you're gonna need a box of neat codes 20 essential system Design Concepts which include networking API patterns databases and more all of which are covered more extensively on neetcode.io to start let's say you have a single server which is accepting and fulfilling requests for your application but now we're getting more users so we need to scale it the easiest thing to do would be to add more resources like Ram or maybe upgrade your CPU this is known as vertical scaling it's pretty easy but it's very limited a better approach would be to add replicas so that each server can handle a subset of requests this is known as horizontal scaling it's more powerful because we can almost scale infinitely and we don't even need good machines it also adds redundancy and fault tolerance because if one of our servers goes down all of the other servers can continue to fulfill requests this eliminates our previous single point of failure but the downside is that this approach is much more complicated first of all how do we ensure that one server won't get overloaded while the others sit idle well for that we'll need a load balancer which is just a server known as a reverse proxy it directs incoming requests to the appropriate server so we can use an algorithm like round robin which will balance traffic by cycling through our pool of servers or we could go with another approach like hashing the incoming request ID in our case the goal is to even the amount of traffic each server is getting but if our servers are located all around the world we could even use a load balancer to route a request to the nearest location which brings us to content delivery networks if you're just serving static files like images videos and sometimes even HTML CSS and JavaScript files you can just use a CDN it's a network of servers located all around the world cdns don't really run any application logic they work by taking files hosted on your server aka the origin server and copying them onto the CDN servers this can be done either on a push or pull basis but cdns are just one technique for caching which is all about creating copies of data so that it can be refetched faster in the future making Network requests can be expensive so our browsers will sometimes cache data onto our disk but reading disk can be expensive so our computers will copy it into memory but reading memory can be expensive so our operating systems will copy a subset of it into our L1 L2 or L3 CPU cache but how do computers communicate with each other well well every computer is assigned an IP address which uniquely identifies a device on a network and to round it all out we have the poorly named TCP slash IP or the Internet Protocol Suite since it actually includes UDP as well what but focusing on TCP for a second there has to be some set of rules AKA protocols that decide how we send data over the Internet just like how in real life we have a system that decides how we send mail to each other usually when we send data like files they're broken down into individual packets and they're sent over the internet and when they arrive at the destination the packets are numbered so that they can be reassembled in the same order if some packets are missing TCP ensures that they'll be resent this is what makes it a reliable protocol and why many other protocols like HTTP and websockets are built on top of TCP but when you type neetcode.io into your search bar how does your computer know which IP address it belongs to well for that we have the domain name system which is a largely decentralized service that works to translate a domain to its IP address when you buy a domain from a DNS registrar you can create a dnsa record which stands for address and then you can enter the IP address of your server so then when you search and your computer makes a DNS query to get the IP address it'll use the a record mapping to get the address and then your operating system will cache it so that it doesn't need to make a DNS query every single time but wait a minute why do we usually use HTTP to view websites well TCP is too low level we don't want to have to worry about individual data packets so we have an application Level protocol like HTTP which is what developers like like you and I actually use on a day-to-day basis it follows the client server model where a client will initiate a request which includes two parts one the request header think of that as the shipping label you put on a package it tells you where the package is going who it's from and maybe some other metadata about the package itself the second part is the request body which is basically the package contents to test it out just open your Dev tools Network Tab and click subscribe go on always taking a closer look at the request we can see that the response also includes a header as well as a body but even with HTTP there are multiple API patterns we could follow the most popular one being rest which is a standardization around HTTP apis making them stateless and following consistent guidelines like a successful request should include a 200 code in its response header whereas a bad request from the client would return a 400 code and an issue with the server would result in a 500 level code graphql is another API pattern introduced by Facebook in 2015. the idea is that instead of making another request for every single resource on your server like you would do with rest with graphql you can just make a single request AKA a query and you can choose exactly which resources you want to fetch this means you can fetch multiple resources with a single request and you also don't end up over fetching any data that's not actually needed another API pattern is grpc though it's really considered a framework released by Google in 2016 it was also meant to be an improvement over rest apis it's an RPC framework mainly used for server-to-server communication but there's also grpc web which allows using grpc from a browser which has also been growing quickly over the last few years the performance boost comes from protocol buffers comparing them to Json which is what rest apis use protocol buffers are an improvement in that data is serialized into a binary format which is usually more storage efficient and of course sending less data over a network will usually be faster the downside is that Json is a lot more human readable since it's just plain text another app layer protocol is websockets to understand the main problem that it solves let's take chat apps for example usually when someone sends you a message you receive it immediately if we were to implement this with HTTP we would have to use polling where we would periodically make a request to check if there was a new message available for us but unlike HTTP 1 websockets support bi-directional communication so when you get a new message it's immediately pushed to your device and whenever you send a message it's immediately pushed to the receiver's device and for actually storing data we have SQL or relational database Management Systems like MySQL and postgres but why should we use a database when we can just store everything in text files stored on disk well with databases we can more efficiently store data using data structures like B trees and we have fast retrieval of data using SQL queries since data is organized into rows and tables relational database Management systems are usually acid compliant which means that they offer durability because data is stored on disk so even if a machine restarts the data will still be there isolation means that different concurrent transactions won't interfere with each other atomicity means that every transaction is All or Nothing lastly we have consistency which means that foreign key and other constraints will always be enforced this one is really important because it led to the creation of nosql or non-relational databases consistency makes databases harder to scale so no SQL databases drop this constraint and the whole idea of relations all together there are many types of nosql databases but popular ones include key value stores document stores and graph databases but going back to consistency for a second if we don't have to enforce any foreign key constraints that means we can break up our database and scale it horizontally with different machines this technique is called sharding but how do we decide which portion of the data to put on which machine well for that we usually have a Shard key so given a table of people our Shard key could be the ID of the person but sharding can get complicated a simpler approach is replication if we want to scale our database reads we can make read-only copies of our database this is called leader follower replication wherever every right will get sent to the leader who sends those to the followers but every read could go to a leader or a follower there's also leader leader replication where every replica can be used to read or write but this can result in inconsistent data so it would be best to use it where you can have a replica for every region in the world for example it can be complex to keep replicas in sync so the cap theorem was created to weigh trade-offs with replicated design it states that given a network partition in a database as database designers we can only choose to favor either data consistency or data availability what makes it confusing is that consistency here means something different than with acid it's a somewhat controversial theorem which is why the more complete pack else theorem was created lastly we have message cues they're kind of like databases because they have durable storage and they can be replicated for redundancy see or sharded for scalability but they have many use cases if our system was receiving more data than it can process it would be good to introduce a message queue so that our data can be persisted before we're able to process it in doing so we also get the added benefit that different parts of our app can become decoupled and if you've learned anything by now it's that software engineering is all about finding new and complicated ways to store and move data around if you want to learn more you can check out my system design for beginners course on neetcode.io as well as my system design interview course thank you for supporting my work and I'll see you soon"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"ML System Design Patterns",
        "Video Title":"Machine Learning Course - 23. ML Design Pattern - Ranking",
        "URL":"https:\/\/www.youtube.com\/watch?v=kBYHOum_iUA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/kBYHOum_iUA\/hqdefault.jpg",
        "ID":"kBYHOum_iUA",
        "Publish Time":"2021-06-12T01:00:18Z",
        "Channel":"Geoff Hulten",
        "Channel ID":"UCgB8ZRxfGYzIf8fadYJMQZA",
        "Transcript":"hit record all right everyone very sad very happy very sad final lecture that i'm recording for this class let's go this lecture has really three components one component is to introduce a sketch of what ranking is in machine learning it's a type of supervised learning the other portion is to introduce a design pattern for how to use ranking and connect machine learning of a bunch of different types together to produce working systems like search and ad targeting and a bunch of other things and the third component is to talk a little bit about machine learning architecture in terms of where you would put your machine learning and why you might put it there so three topics to cover they're all going to be a little bit of sketch but the point of this talk is how those three come together into a whole how they relate with each other's how the how decisions in one place affect decisions in another place and also ranking all right let's go okay as you know by now you're well aware of goal of classification labels goal of regression predict a number we haven't done that as much in the course so maybe that's a little bit less familiar the goal of ranking is to sort samples objects entities information into the correct order so it's not like the score is seven it's this sample should be above that sample it's a little bit of a different way to look at a problem and why would you want to do this well obviously search results that's a ranking system and there you issue a query and you say i'm looking for information about ranking algorithms the system goes to the interwebs pulls everything back and then orders them from one to n in terms of how relevant that document from the web is to that query about ranking algorithms and there's a sense of you know even two documents that are relevant to that query uh one of them could be better or worse than the other and that's where ranking can come in you see this a lot of different places ad targeting you have a collection of ads that people have said hey i want a bid to have my ad shown when it's relevant and so then the question is for this user in this context whatever the user is trying to do or think about now which of these thousands tens of thousands millions of potential ads should i allow to compete in an auction to be shown to the user recommendations is another place you could do this there are other approaches to recommendation but you could imagine you want to show users hey here are other movies that people like you enjoyed so in that sense the user and their history is the query to the database of movies and the system has to say well of all the movies we have which of these would be most relevant to you or you know least relevant to you so we should suppress them that sort of stuff a couple more examples here um skills for assistance so when you say something to one of your digital things like what i have here or over you know i don't want to say their names because then they start uh but if you say one of those digital assistants names and then utter some text that text becomes a query now we're gonna go through a rough sketch of how a system like this sort of works at a super high level i mean i'm sure it you know there's a lot of detail in building big systems like this but sort of at a high level but the point is that that utterance of text needs to somehow turn into all the skills that are running on the back end to say hey which of you is most relevant to this series of syllables that my user just uttered which of you wants to give an answer which of you is better at giving an answer worse at giving an answer would the user prefer the most that's a ranking task and another thing is like imagine templates you're starting a new document you want a resume template or you've got a chart and you say well let's make this chart look good maybe there's a lot of different design experts that might have information about how to make that powerpoint slide that chart that whatever a little bit better looking well there's a ranking problem too like i want to show the user three alternate designs for that particular entity and here they are and of course there's a digital marketplaces where which product should i show for the query that the user issued which product should i show to the user just as like hey you might like these which product should i show off of a product details page to say here are other similar products or people who bought this went on and bought these other things those are all queries where you could have a set of all possible products maybe millions or billions of products well which should i rank highest in response to that query of here's the product that the user's looking at now i've launched a query to say well what are the related products i should show the user right now now a little bit more technically if you were going to think well how would i set up this ranking problem in terms of the machine learning things that we already know how to do one way is you could take for each entity whether it's a search ad an ad a movie a blah blah blah and for each query and say well this is how relevant this entity is for this query and then you could set it up as a regression problem or you could say the relevance is 17. so the next time you input that query into the system it should and that object and it should say 17. so you could potentially set it up like that another way you could set it up is pairwise so that you give your learning algorithm two possible responses and the algorithm has to return the probability that one of them you know the first one is more relevant than the second one so you probably get query entity one entity two probability entity one should be higher in a sort order then you could take that pairwise model and sweep it over everything and figure out you know do a little search that way another thing you could do is set this up as a list where you give your learning algorithm all the entities and it needs to output the scores on them directly simultaneously give them the one to end ranking so if we were going to look at this the point wise you would say you have a function of the query and a particular item in the catalog in the the search rank case you know i said i do a query and my query is tell me about ranking like it's just a single word so somehow you represent that as a feature vector and there's lots of work to do to do that well um and then you would take the web page that you found and put it in like you know the um the paper i'm gonna link you to on the next slide and you put it in there and you'd have somebody come along and say hey the relevance of that is gigantic ten thousand that's a beautiful relevance return a very high number something along those lines the pairwise it would be just like like this so the same the same features potentially could go there the same featurized web page or add or whatever and then and you give it another one and then it outputs a probability and then the final one would be like this where it's a list of items and you get the scores for each item so maybe maybe this is sort of a degenerate case of that but you could imagine setting it up so that you learn all those things simultaneously instead of just doing them point wise now we're gonna go through the architecture you might imagine for one of these ranking systems you know let's say it's a digital assistant of some sort and here's our user right there got these little cartoons from a friend of mine with permission from jonathan thank you jonathan for letting us use these in the course the user issues a query to the service to the device and the query can take lots of forms like we showed it could be some typed text it could be some audio it could be an entity like a movie or a product it could be a content like the like a slide could be a query the whole slide gets sent up to the service and says tell me something to do to make this slide better something along those lines now the device needs to have query triggering and in the case of some of these that are natural interactions that triggering in itself is a model there's lots of audio going on in the world but the device is only going to trigger a query when it hears it's like wake up word something along those lines and some of these other cases it's much easier to see when the trigger should occur like when you press the search button that's the trigger so sometimes that's an easier problem but sometimes it's quite a difficult problem the next thing you have to do is to interpret the query and that could be a whole bunch of machine learning models that looks at the words that i've typed you could imagine one part of interpretation would be to correct common spelling mistakes something along those lines but another thing might be to identify the topic to identify is this person asking for information a sports score an image like what type of things is this person asking for another thing is to figure out the parameters or the objects of interest that are contained in the query for example if there was a proper name or something along those lines you might extract that you might identify a particular structure of a sentence that has two names like did this team beat that team and so you may have the two teams as parameters and some information about how they relate in the query and then there's this notion of query augmentation if i as a user input kind of a query fragment you might say like other people that queried for that fragment actually were looking for this larger piece of information like if there's an incomplete slide or something like that you might fill it in with a little bit of information before you pass it along deeper into the system so that the downstream query engines have a simpler task to do so it's kind of like how do i interpret what the user is asking for and inject that knowledge the next step is you'll generally have a whole bunch of query engines or experts or you know different legacy systems that have been built up over time that can answer the types of queries that you might have and looking at this you'd be like nah i'd probably just use one query engine but sure on day one you'd use one query engine but a lot of these systems have been running for a decade for more than a decade for a very very long time and then there are really specific things like in the uh design case you might say hey here's an expert who knows how to deal with pie charts and make pie charts look really good or when there's a log there's a different way that you'd want to do things or clearly in web search there may be query engines that are very good at dealing with image searches or news searches or something along those lines and having these different experts and decoupling like this is a nice way to set up your team so that different people can focus on different areas but the system overall needs to interpret the query and then decide which of these engines should i hit with this particular query to possibly get back answers that i might want to rank and return to the user sometimes that's called query planning so once you've interpreted the query you plan how you're going to execute it on your machine learning systems in the back end and these things many of them will be large machine learning systems i mean but they could be handcrafted or you'd say here's specific information about a sensitive topic and these are the only things that i want to return anyway then you get the best answers from all the experts the query engines that you've decided to hit with the particular query and that's when you have a ranking step now there could be dozens hundreds thousands of hey here's a potential answer that we might want to show to the user but then for the user experience you don't want to return thousands of things so you want to return a nice set of a few things that you're going to ask the user hey here's what we're thinking look at these consider pick the one you want something along those lines and the goal of the ranker is to maximize some objective in terms of the click-through rate of this user or the engagement i mean all that stuff we talked about on goals so some user outcome for this user why did they come to this system and in addition to the properties here you're going to have all of this other stuff you're going to have the history of what the users liked in the past context you're going to have some version of the query saying hey this is what we were looking for and then you're going to have to output a few things in this case if it was a web search you probably have what like four slots six slots ten slots that a user would probably look at so in this case if you search for machine learning course you're gonna find a bor some boring course you're gonna find some spam ad another boring course and then you're gonna find this course number four so clearly there's more training to do on this algorithm it's not getting things right then you have the closed loop of course where the user interacts and that interaction creates training data that you could use to either improve the ranker to say like hey when you ranked these thousand things the user clicked number nine so there's a training sample let's let's go update the ranking algorithm or you could potentially use it to even train these query engines in the sense of like hey you keep returning this answer and nobody ever uses it are you sure are you sure that that answer is relevant to that query or maybe you'd want to stop this is a quick sketch of a general architecture that's used in a lot of places you if you work on one of these teams you may not recognize my sketches being in your system but many of this type of system have this general structure now let's talk a little bit about how do you evaluate a ranking algorithm to know if it's doing a good job each training sample each training and it's like kind of the the training domain the training conglomeration the things that you're going to give to the learning algorithm so that it can learn what to do or in this case it can evaluate how well it's doing is a combination of things it's the query but it's also a whole large set of possible answers that have the score on them in this case the score is this a a a and that means that these two elements are relevant to this query is what that means and then b would be two elements that are not relevant to that particular query then your x train would be a collection of these combinations query set of things with the scores and the set can be different sized for different elements of the training data and we'll see how to put this together in a sketch of an algorithm in a second but right now we're going to say if we ran a particular model that we learned through ranking on this training example testing example probably is more accurate to say because we're evaluating then you run the learning algorithm on these sets of items and it outputs scores remember this is like the point wise version of what we saw in the previous slide in this case it says for this query a this particular item which is also on topic a got a score of 0.9 and so you would say well that's pretty good is it really good well it depends on what it said about all these other ones because it says this b got a score of 0.8 oh that's not great but maybe as long as all the a's are above 0.8 it's doing pretty well because it's at least putting them in the right order that's what the algorithm is trying to do so here's the sense where if you were doing regression you might say b is relevance of 0 to this query so that 0.8 score is like terrible because it should be a lot closer to zero but in the ranking case maybe it doesn't matter so much as long as it gets them in the right order but you can see that it doesn't get them all in the right order because these two a's in some sense this a is a little bit wrong how do we evaluate how well this algorithm did on this particular learning sample and we're gonna you know mean average precision so what does that mean the first thing you do is that you sort the answers by their ranking then you would say that the model that you ran would take this set of elements and order them in this order in terms of their relevance to the query that you issued so correct incorrect incorrect correct incorrect incorrect now how do we convert this into a useful score the first thing that we define is this notion of the average precision of a single query and that is this training sample we're talking about so the average precision on this query we sum over each possible position and say the precision at i times whether or not it the ith element element is relevant so we're summing in this direction from i equals one all the way through the total number of possible answers then we'll say the precision at i and we'll go through an example of what that means times is relevant so is relevant means a a that's relevant b a that's not relevant so that top that's what that means and then divide at the end after you've got that summation divided by the total number that are relevant so in this case that's two all right let's walk through how this gets applied in the first spot so the first step of this summation you take the precision at one the precision at one we've returned one element and it is relevant so the precision is a hundred percent and it is relevant so you get that one the next one we have the precision at two so 1 2. the precision at 2 is 50 because half of these are relevant but this item is not relevant you don't get any credit for that one at all next one is c the precision at three is going down and it's not relevant so you don't get any credit now we come to the next relevant item at this point the precision at four is fifty percent and it's relevant so then you just add up a couple more zeros and that means that the average precision for this query is 1.5 because you have this plus this divided by 2 which is the number relevant so 0.75 is the average precision for this training example that's how well i did on this example if the rankers put both of the a's at the top the average precision would be one because then you would have had a precision at one of one precision at two of one divided by two this would have been 1.0 if on the other hand the ranker had put both a's at the bottom the average precision wouldn't have been zero because there's some precision when you start hitting these things but it would have been about 0.18 so a much much lower number that is the average precision of a single element in your evaluation set when you're building ranking algorithms you'd have a whole set of test queries with their own associated documents with their own associated labels for how relevant they are this a thing or the relevance may be more than just a binary concept it may be but there's lots and lots of details and remember we're just giving you a sketch so that you have some sense of how the techniques that you've learned so far apply to this domain anyway you take your entire set of test queries and you get the average precision on each of those queries sum it up divide it by the number of queries and that's the mean average precision and that's one useful way to evaluate ranking algorithms of course there are a lot more there are an awful lot more mean reciprocal rank precision at k you just pick a k that's relevant to you and say i just want the precision at that threshold that might be the most familiar it's kind of like an operating point click through rate is a little bit more like a user outcome where you say hey i don't really care all i care is that i'm going to show three things to a user and i want to know which percent of the time when i show them the top three things does the user click a button which makes me money or you know click the ad click click the product whatever it is so that's a more of a user outcome focused way to evaluate a ranking algorithm there you go simple setup for a ranking problem how to do the evaluation it's a little different than what we're used to but it's a little similar that's a little bit about evaluation now i'm going to give a pretty quick sketch just to give you conceptually how you might approach creating a ranking machine learning algorithm i'm not even going to be completely accurate on what i say here but the point is to give the concept and how it's different from what we know not to set you up to go and implement your own ranking algorithm now remember from the last slide that the training data is a set of query to end different items where the relevance of that item is associated with it and like you know this is a little bit odd because if in a regression sense you would say well here's the item here's the x for that item and here's the why of what you want to predict and if you set this up as a regression problem maybe you could sort of do that but instead of doing regression what we want to do is make sure the ordering is correct so in so instead of taking that setup we're going to take a different setup where we encourage the model to learn the weights that get these items in the right order how do we do that while not converged iterate over the training data apply the current model to all the items in the current query that you're considering so you take one query you get all the items you apply just like on the last slide so each one has its score for every pair of items i and j adjust the model weights to make them more correctly ordered so if i should be above j and i is not above j in fact j is above i you find the gradient that would tend to push that up and tend to push that down and take a step in that direction and then since you're doing each pair of these you kind of just like go through blah blah blah you're done with this query after you've taken all those steps and you move on to the next query and you can keep going until the model converges you might say the mean average precision converges like further steps of iteration aren't improving that or the improvement is a little bit too small all right so what does that mean this i is better than j take a step blah blah blah how do we convert this concept here into the gradient the loss that we're going to take a derivative of to find the gradient just walk through one simple way to do it and maybe you could see how it might apply another situation so if we're talking about items i and j this is from the paper and i'm probably slightly abusing the terminology but what this says is that item i should be ranked above item j according to the relevance that's in the training sample now we need to take that notion that it should be above and that it got a particular score from the learning algorithm on this iteration of the training data and somehow convert that into a loss the way the rank lab paper does it is that it uses this little bit of math if you don't instantly know what that is learn it before the final but it's a sigmoid and it says that the score that it puts in is the relevance which is the not this relevance that might be a little confusing but the score that the the current version of the ranker gave to item i and the score that the current version of the ranker gave to item j and then that turns into you know in some sense in the past we've been calling that kind of y hat so the y hat for this example that we're going to want to substitute into our loss calculation to figure out which direction the gradient's pointing to figure out which direction to take a step is a number between 1 and 0 that is larger than 0.5 when i is greater than j because that difference will be greater than 0 so larger than 0.5 and as i gets much much much much much larger than j that number will go towards 1.0 if i which should be above j according to the training data is actually below j then the number will tend towards zero and that's kind of the mapping from the ranking problem into something we're more familiar with which is like okay here's that then the next step is to just use cross entropy or log loss where remember this is the y hat equivalent this is the other y hat equivalent why people you know like this notation everybody uses their own notation i should and then this is more like the y equivalent and the y equivalent so it comes back exactly to what we know and love and recognize and know how to take the derivative of from our very first learning algorithm this is a one if i should rank above and otherwise it's a zero and then you go for each pair of items here you look at the scores that the current version of your model put on them you sort them in the right order according to what the training data says so i is the one that should be higher j is the one that should be lower you convert the score and the desired ordering into a number from zero to one which we kind of push into the y hat notion back all the way when we were working on logistic regression then you calculate this thing which is the cross entropy for getting to a place where you know how to take the gradient and update the weights in the parameters of your model all right really quick sketch i understand in a different class you would implement this but we've already implemented enough learning algorithms so just wanted to give you the flavor of that give you the sense of that related to concepts you know let's move on now an important part of a design pattern is where to get data from how to get labels corpus centric remember we pay people to produce our training data and then a closed loop you let the system produce data you can do both of these for this this ranking type architecture uh let's go through a few of the thoughts about it you gotta in the corpus centric approach you got to get the queries that you want to pay somebody to find relevant items for now if you have a working system you can sample the queries that your users are doing and that's what i've written here if you don't yet have a working system you have to go and create queries that you think your users are going to want to query for and you might be right you might be wrong so that could be you know the bootstrapping problem can be a little bit difficult but once you have the queries that you're looking for you can pay people to find relevant answers to the query and so you take the query then you go and get the set of items that are going to be part of that training sample for that query you might tell the humans hey search the web until you find 15 things that are relevant to each query and then you could use all the different combinations of the things that they've found to create some irrelevant things for each query and construct the training samples that you need that's one way to do it another is to actually if you have a system that's running pay the labelers to look at particular answers to particular queries and say hey the system is currently returning this that ain't so great um you should have swapped these four things and here's some other relevant documents that you're not returning that you really should be returning and add those into the training set for it and often in these big systems you end up with something called active learning which we could have a whole lecture on it's a deep topic just give a real quick sketch of what it is you would take the current system the way it's running have it go and find a document in a perfect setup it would go and find the potential item that would give it the most information about how to properly answer a query imagine there's a hundred thousand items in a database and you can't pay users to hand label all hundred thousand items for every query that you wanna have in the training set for your system so what you do is you run some version of the system across the hundred thousand and be like ah here's the 50 that if a human could just tell me something about those 50 items i could learn a pretty darn good model for that query because those 50 are somehow the most important for what we're trying to do you could down sample and randomly pick 50 but if you include the model and the human together in the process of figuring out what to label you can often do a lot better than either of these approaches up here i hear people talking all the time i just was talking to someone last week who's doing active learning for this type of problem that's a corpus centric and you probably will always have some version of that in a large system you can also of course do a closed loop for example somebody asked a question one of the discussion groups about this imagine that you're doing a music recommending system and the query to the system is here's the last song the user listened to and the response is what's the next song the user should listen to now if the user listens to that whole song that might be an indication that that song was relevant to the previous song or to the mood the user was in or something along those lines if the user gives a thumbs up that might be a sign that it was really relevant and you should include it in your training set with a high relevance score if the user gave a thumbs down or press skip this song or turned off the system or something along those lines then you might be able to give that song a lower relevance score now there's a lot of noise in that of course because users do all sorts of weird things for all sorts of weird reasons but in general if you're a little bit careful about how you set that up and don't try to read too much into every user interaction that can be a great way to improve a system now that works well for items that the system is already returning if there's like 15 items and they're out of order in terms of their ranking that first version of closing the loop might over time learn oh actually i should be ranking number seven is number four and number two is number nine or whatever it is but if there are a whole bunch of items that the system never returns then directly closing the loop won't ever get feedback on the items that the system never returns so it's often desirable in this type of a setup to do some exploration and what that means is that you have the answer that you think you want to return and you return something else instead precisely to see how the user is going to respond to it it's a little bit maybe like active learning with the end user as the person providing the label instead of as a with somebody that you've paid to provide the label but you don't have to be super sophisticated depending on how many different items you need to get feedback on just doing this epsilon greedy approach uh which is to show a random answer epsilon percent of the time like let's say 0.001 percent of the time if you're supposed to return let's say four elements 0.001 percent of the time or 0.1 percent of time or however many users you have you you take the second item and replace it by something random that you don't have a lot of data on in your training set yet or something something new or something that you would learn a lot from that can allow you to both have a system that's functional for users but also solve some degree of the bootstrap problem for like a new movie comes out what should i do with this a new web page goes up what should i do with this a new design template comes up what should i do with this well do a little bit of exploration and see how your users respond to it you can do this remember when we drew that system there were the experts in the upper left and then the ranking algorithm down there you can do this just to train the ranker to say hey all these experts have given me information and i want to figure out how to rank this new one that i've never seen before or i want to figure out how a new expert that just got stood up last week how users will respond to it so i might say well i'll take the best answer from this new expert 0.01 percent of the time and put it in slot number two for example to get training data for how to start incorporating something new into a large working system the other thing i just made this up epsilon squared greedy epsilon 2 greedy is to say hey this guy these experts need to train themselves as well and they need to figure out how to process the images that they're trying to learn how to rank whatever it is you could say hey x percent of the time epsilon something percent of the time one of these guys can take a random answer and route it through to the user in a mode where we're gonna log everything along the way so that when the user interaction comes back into the log you're able to say ah that interaction was related to when this particular expert was doing its experiment and now we can tell that expert here's a new training sample for you because of how the user interacted with it corpus centric closed loop you probably got to do both of these to build a large ranking system and there's lots of details in sophistication okay so now we know what ranking is why you might do it we've seen this as a really architecture sketch for the components that need to go together and kind of what they're responsible for we've gone through a ranking algorithm some evaluation how to get data now we're going to talk about another important element that you need to think about when building your system is where do i put the machine learning models what are some of the options and why looking at the different models that are in this system what are the properties that might make you want to put the model one place versus another place that is in the client that the user is holding versus in a server in the back end versus you know all the other different places you can do so we're just going to spend a few minutes going through that let's just walk through this and see all the different models we bump into and and think about them just a touch the initial query is something now if you have an obvious model for triggering it like the user pressed search or press return on a search box query triggering is easy it's a very simple model did they press return and that would certainly live on the client but if the query triggering is more complicated like something with speech or looking at other versions of how the user is interacting with the system you might want to put query triggering in the client or you might want to put it somewhere else key properties are the latency how long does it take from when the user utters the speech to when the system starts responding if you have to do a round trip to the cloud that latency might be noticeable another thing might be privacy do you actually want to send that user data to a server you own so you never have to see it do some other form of learning to build this query triggering model maybe corpus based approach another is just the cost of running the service you might be perfectly fine with latency and privacy to send stuff up to the cloud but you need to cache stuff on the client push the models out to save money if you're dealing with a billion user interactions a day 10 billion i think is the service that i work on now you can't always move everything into the cloud even though the cloud is awesome all right so that's one place another place that we'll talk about for a minute is these query engines each of these could be large machine learning systems they could have gazillions of like data like things about the web pages and indexes and all sorts of information back there they also might want to spend their time and really think and look at a web page or look at an image and you could imagine saying that it's worth it for one of these experts to spend 60 minutes evaluating a new movie in the catalog to figure out what do i need to know about that movie and that's 60 minutes i mean that's impossible to put in a live query loop but it might be the right thing for your application so where do those models live in order to give you the time to build up these gigantic structures then the ranker is another important component the query engine say hey here's the top ten here's the top hundred here's the top thousand whatever the things that we think are relevant this ranker needs to decide well what am i gonna actually show to the user what am i to play to them and where should that ranker live should it be in the cloud or should it also live in the client might depend on what features you need to bring to bear and where you want to store those features where you're able to store those features so access to the context that you need to make a decision is another reason you might consider making a model live somewhere else we have latency privacy data costs all the computation and analysis access to important features as criteria and here's just i wrote it out a little bit more detail here are some criteria you might want to look at when trying to decide well where should this model if should it live in the client should live in my service should it live like hybrid what what are the choices latency and updating the model you've trained a new model how do you get it to your users if the model's in the client you need to wait for an app update before you're able to push that new model or do you have other facilities to that if the problem is evolving quickly if it's a time changing problem this latency could really matter if there's a risk that you're going to make a mistake that costs your users a lot of money you might want the model to live somewhere where you could change it very very quickly if you find out you need to latency and execution that is at run time when the user is interacting with the model do they notice 100 milliseconds a second two seconds three seconds to get the answer back if you have to go to the service and the user maybe has good net or bad net or something along those lines that might show up in the experience as a negative i know search spends a lot of time trying to get those queries to come back super super fast because users don't like to wait for search answers but it's also important if you're using the model in a live control system like let's say flying a drone using a model you don't want latency because the drone needs to make a lot of decisions very quickly cost of operation is another big one if you're running a service it takes a lot of money to send you know you have 100 million users and you have to send a model update every day that could actually cost you money it could cost them money but then also whose cpu is going to run that model that's another important criteria and then do you need to work offline you might have a really awesome spell checker that does all sorts of advanced neural network stuff using gpus in the cloud but when somebody's on an airplane and they do a typo they still want some kind of spell checking so you might want to think like do i need to have maybe two versions one version of the model that can run offline and one that can't something along those lines and then there's this keeping secrets out of abusers hands you know i talked about the design pattern for abuse with a large public service but if you're dealing with a more private service where maybe abusers can't create accounts it may be much harder for them to attack it you don't want to push the models to the client where somebody could steal one client reverse engineer the model and launch other attacks so you might have criteria like that and of course privacy privacy privacy privacy privacy privacy privacy and this isn't like anything super deep on this slide but it's the type of thought process that you probably will need to do at some point if you get more and more into machine learning running services this may be just too too standard but you have the option to put the logic the model in the service or on the client so either the it lives there or it lives something like there then you have to say well which one's better it depends on what it's going to cost and here are relevant things you may have slightly different versions of relevant things but how much data do you need to distribute to update the model how often do you need to update the model how many users do you have how much information is in the raw elements that you're gonna then run the featurizer on to create the features for executing the model how many calls interactions whatever does each user do per day so if it's a one megabyte model you just have to transfer it in your back end maybe you have a little server farm or something like that so it's you've got to transfer 10 megabytes or 100 megabytes or something along those lines then the users do their 100 kilobytes call times 10 times the user base that you happen to have then you add that all up and it's you know something along those lines on the other side you instead have to distribute the models to the clients so right there there's a gigantic hit on the data costs but then you have to think about well the compute and the telemetry if you want to close the loop and make sure that your model is working in this case it probably isn't clear which of these is better and you'd want to look at some of the other criteria but i've certainly been in situations where we wanted to build a model and we wanted to put it in the cloud because that makes everything easier closing the loop monitoring things updating things that was the pattern that we used on the team and then we did a little bit of math and found out well there's a billion like 10 billion calls per day is going to have to happen or something crazy like that so we'll just simplify what we were thinking of as our model to something much much smaller than one megabyte and we'll simplify how often we were thinking of updating it and we're pushing that thing down to the client and that was the only way we could insert intelligence into that part of the process that we're trying to work with is just doing this type of calculation and being like uh okay fine fine now there are a bunch of places you could put intelligence some of them are pretty obvious static in the product where you compile it into the code and just a quick sketch of hand wavy like what is it good for what is it bad for compile into the product relying on users to update the app to get new versions of your model very poor in latency to update but execution at runtime it's fast it's generically very cheap for you to run because the user is downloading the app anyway so it's not a big incremental cost and it works perfectly well offline another place you could look at is client-side and client-side means it's still running in the product but the model itself is distributed asynchronously from the application you have a choice of how frequently to push the models based on all the costs and the calculations we talked about on the previous slide latency of execution is exactly the same the cost is based on what you do and it can run offline the next place it could live is in a server so the model itself actually lives in the server the client makes a call to the server passing the features probably not the feature vector probably the thing that the server will convert into the feature vector so that you can update that code and you don't have to distribute it to the client server centric latency updating is good because you just push it to your servers i don't know like good maybe that's hard for some places latency and execution you have you know a round trip on the internet so not great but you can often hide that cost of operation can be quite high because every time your user interacts with your model they have to send that data to you that can get out of hand very quickly and you can be surprised by that and of course it wouldn't work offline another place you could put it which is maybe the most surprising on this chart like the rest of this stuff you guys are probably like jeff come on why why are we talking about this another place you could put the model is in your back end and what that means is you don't even run the model live when the user asks for something you've run the model before and this might happen when like we're talking about on a previous slide you get a new movie in your catalog and you go off you analyze it for like a day you spend them like hours and hours and hours with very complicated fancy models analyzing what that is and essentially extract features or cache the answer for what context that movie makes sense in where you're going to return it how you're going to relate it to the other things you have latency and updating means that now i have a new version of the model and i want to go back and recompute that cache re-analyze everything in the catalog it could be quite expensive but maybe you you just kind of like round robin it or something along those lines um latency and execution right like blah blah blah but the offline thing another you may not be totally obvious is you could take part of this cache and distribute that to clients you could kind of do something along those lines to say it's not exactly the model but i'm distributing cached answers from a model that i ran in the cloud to the client and updating that periodically and then you know right there boop you might realize that of course you could have multiple layers of these do all sorts of like simple models on the client to know if i should trigger to call the server centric model which will run unless it has a cached answer already waiting obviously a big systems get really complicated hybrid and you can knock yourself out so now that we've seen all that you might say query triggering should be client-side possibly in your application to reduce server traffic and preserve user privacy whereas the query engines should live in the back end and in the server a hybrid between those two where you'd say common queries get cached and tail queries get executed live something along those lines probably common queries you'd say 50 of your traffic comes from five percent of the entities in your system or five percent of the queries that users are delivering to your system so you can you can cache that and then for the other fifty percent you you hit your live models something along those lines for the ranker it could really depend it could be heavily server centric if that's where your challenge is or you might push it to the client if most things are based on the user the user's history what the users interacted with recently and dealing with keeping track of all that for all your users in the service and privacy and blah blah just not worth it here we go ranking sorts possible responses into the correct order based on a query so you give a query and it takes items and lists them one to n in terms of how appropriate they are for that query and that can be used across like kind of odd definitions or broad definitions of what a query is where query doesn't have to be some text that someone types it could be something they spoke it could be a movie it could be some thing on a product page there are a lot of ways to think about loss or evaluating ranking algorithms we talked about mean average precision but you might see all of these types of things and remember these are sort of model properties and then you definitely we've talked about a lot of different types of outcomes that rankings feed into choosing where your model lives can have a large impact on whether or not machine learning is even viable and these are the places that we discussed and i think you know these are pretty obvious these might be a little bit less obvious that you end up having to put things on the back end and cache results and then do hybrids between those and ranking can be used with a closed loop and also a corpus centric and of course hybrids along those lines there you go hail i just want to thank you all for everything i had a great time hopefully you had a great time it was a lot to go through i want to wish you good luck with the rest of the course finishing up the assignments in the final but also good luck in everything you do thanks again"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"ML System Design Patterns",
        "Video Title":"Top Architectural Patterns #javascript #python #web #coding #programming",
        "URL":"https:\/\/www.youtube.com\/watch?v=nZuWCo52wTg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/nZuWCo52wTg\/hqdefault.jpg",
        "ID":"nZuWCo52wTg",
        "Publish Time":"2024-03-20T15:30:14Z",
        "Channel":"ByteByteGo",
        "Channel ID":"UCZgt6AzoyjslHTC9dz0UoTw",
        "Transcript":"every wonder about the architectural patterns powering the apps and platforms we use every day let's unpack five key designs layer architecture separates components into presentation business logic and data access layers a classic example is model view presenter then we have event dri architecture where components communicate by events cqrs is a good example separating right from read operations micronel architectures keep core functionality in a small kernel and extensive through plugins the Eclipse IDE show cases this with its plug-in based architecture microservices architecture breaks applications into small Loosely coupo Services Netflix uses this for everything from recommendations to building monolithic architecture bundles all components into a single unit the modular monolith emphasizes clear boundaries within the codebase for easier maintenance the Master System design get 158 page system design ebook for free"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"Scalable ML System Architecture",
        "Video Title":"This ML Design Interview strategy got me into Meta",
        "URL":"https:\/\/www.youtube.com\/watch?v=XN2ymraj27g",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/XN2ymraj27g\/hqdefault.jpg",
        "ID":"XN2ymraj27g",
        "Publish Time":"2024-08-19T11:00:34Z",
        "Channel":"MLEpath",
        "Channel ID":"UCRzZg46IpL2wjEAN8U4splg",
        "Transcript":"70% of machine learning engineering candidates I interviewed failed in the same round and the thing is it's actually really easy to pass all you need to know is what you're doing having worked at companies like Adobe Twitter and meta I spent the last 10 years on both sides of this interview and realized it all comes down to acing these six things so in this video I'll reveal what kinds of questions I had to answer to land a job at meta share my system for answering any machine learning design questions your interviewer might throw at you and I'll even give you one Stellar strategy to prepare in less than a day but before I can share my system with you you first need to realize that you're preparing for your interview all wrong if you're spending the hours leading up to your interview reading up on Lost functions then you're wasting your time the thing that trips up most candidates has nothing to do with their ml knowledge I played a lot of Microsoft flight simulator when I was in college I know where every button and instrument gauges on ssna 150 recently I had a chance to take a yoke of an actual plane and uh well my friend had to take over to keep us both safe the things that keep you from Landing your dream job are not about the buttons but the feel of the interview specifically insufficient understanding of the question confusing your interviewer and poor time manage and that's where my system comes in using the system allowed me to prepare for my interview at meta in half a Daye and the interviewer was so impressed that I got an up level from the senior position I was interviewing for to staff interviewing at Fang companies especially at senior levels is like shooting for the moon say you were actually to be tasked with that I give you unlimited funding and say fly to the moon it's up there and vaguely point up in the sky this is sort of what the ml design interviewers do they give you a generic question and then sit back watching your space fairing dreams Sail by being off by even half a degree on your course to the Moon already assures you will never get there if you don't fully understand the problem you might over complicate the solution or fall short of addressing it fully make sure you take 5 minutes or so to really understand the question you sound a lot more competent when you make assumptions than when you pester your interviewer with questions so limit the amount of clarifying questions you ask and instead of panicking about details like exactly how many you users do Facebook have just say Facebook has about 3 billion users this next stage should take you about 6 minutes but if you neglect it you will spend much more time in back and forth explanations with your interviewer sadly too many ml fail because they use this stage as the opportunity to dive deep into rabbit holes showing complete unability to do the high level design and wasting precious time making it impossible to recover instead you need a rocket blueprint you need to be very deliberate about building a very high level system diagram with barely enough detail to address main requirements of the problem so instead of XG boost you need to think model and leave the specifics to a later stage doing this R avoids most miscommunications between you and your interviewer be familiar with whiteboarding software you will use in the interview you do not want three out of six minutes in this stage to go to figuring out how to draw a square while having a good plan goes a long way toward clearing this interview this next part is the second largest compon of your score everything you have done in the first 11 minutes is a great setup for a successful interview but this stage is where ignition happens the rocket lifts off the Launchpad the objective of the next stage is to pick up speed and not experience a rapid unscheduled disassembly I know I have experienced a few of those in my interviews over the years how about you let me know in the comments if you have done the pre-work this is the fun part of course I'm talking about data considerations and there are a lot of data questions to consider to get this space you must at least understand what your labels are and where they will be coming from understand what your features are and how to translate them into numbers understand how to normalize those numbers understand how to split your data set are your data imbalance they almost always are how will you address that this part should take you about 8 to 9 Minutes note that this is a lot of ground to cover in a short amount of time you should not discuss every feature just a few representative ones congratulations you in low earth orbit but this next boost will get you all the way to the moon this section covers the most important thing your interviewer is looking for the next 15 to 16 minutes are critical this is no longer time for clarification you need to make the most out of your time your task in this stage is to talk about modeling metrics and training as well as overfitting and other issues that may come up like cold start and time travel problems and propos Sol solutions to them this is where you demonstrate your machine learning knowledge targeted to the scenario your interviewer actually cares about while this is a lot of material and you may be asked a wide range of questions don't panic at its score almost all of machine learning is either about classification or regression and in fact many problems can be solved with either once you define your problem as one of those there standard paths that will keep you safe congratulations you're now as successful as Apollo 13 Wait no that's a bad one Apollo 8 both were piloted by Jim L anyway Apollo 8 is a pretty good comparison here though it was a test mission that went around the Moon without Landing earlier in the Apollo program if you're early in your career you may already have it in the bag but just to be sure or if you're looking at more senior jobs let's land on the moon stage five you have 5 minutes to nug the interviewer socks off your one small step interviewers will sometimes focus on how to launch and operate a machine Learning System in production but if you are an expert in training paralyzation or online evaluation techniques this is your time to shine there often multiple ways to solve the problems you will find in the section at least once you should come up with trade-offs between two or more reasonable Solutions and make strong recommendations for one of the wait I thought you told me there were six stages indeed while the rest of the world was rooting for Neil Armstrong to step on the moon I am sure his family would not have liked the mission as much if he didn't land safely stage six is just for you it would be a big bummer to break into the company that breaks your will to be in the industry so use the remaining time to ask the interviewer questions about working at the company you only have time for one or two questions here so make them count if you follow these steps with more or less suggested timing you will Ace your ml design interview no matter what question is asked but you want to know what questions are usually asked don't care well I have some good news for you the great majority of the questions for machine learning design interview come from just a couple of areas of machine learning and even if these are not your area you can get up to speed on these by reading just a few papers and blog posts Linked In the description most big tech companies focus on only two types of questions for their generalist positions these two question types are popular because they test a broad swath of machine learning knowledge and the interviewer can dive into a lot of places to understand what you know the second most common type of question for this interview is harmful content or inappropriate content something like detective firearm in Amazon listing or nudity in a social media post this is the type of question I got asked that sealed the E6 meta offer for me but even more common is the question I asked as an interviewer in big Tech design a recommender system something like Twitter timeline or Amazon product recommendation for specialized roles and in smaller companies the questions tend to be very targeted to what your role will be if you are a vision specialist expect to design a vision system while machine learning is a very deep field a handful of areas are over represented nml design questions however you should never assume that you will get one of those problems instead let's talk about how you would study so it doesn't matter what questions you get asked machine Learning System Design Round is one of the many interviews you're preparing for on top of all the other things you're doing how do you prepare for it fast and without sacrificing sleep luckily in addition to my experience applying to meta I have since helped dozens of people get into Fame companies at levels from new grad to principal engineer with this advice and you already know the first step from earlier in this video there's an apocryphal story of Charles Proteus Steinman a renowned electrical engineer who was called to fix a generator for Henry Ford after marking a spot with chalk and instructing the engineers to make specific adjustments he charged $10,000 $1 as he said for the chalk Mark and $9,999 for knowing where to place it understanding the six stages of the interview is the absolute prerequisite to doing this efficiently pick one of the questions I mentioned in this video and go through how you would answer each stage in brief if you know offline metrics really well just check them off your list this is your $9,999 task figure out where to place your efforts then you read up on topics you identified as weak spots utilize resources in the description when you need them skip the ones you already know at this point you have learned to swim by reading a book well remember when you tried to learn how to swim using the internet I did learn how to swim on the floor the skills are transferable I just have no interest in going in the water Kudos however a real pool full of water may present some different challenges whatever else you do plan to do two full mock interviews you can do a paid mock just be very careful to look through reviews or just buddy up with a friend who is at a similar level I will walk you through how to conduct a mock interview in a future video on this channel if you apply what you learned in this video do a few hours of additional research to fill the gaps in your understanding and do two mock interviews you will be perfectly positioned to Ace your ml system design interview getting through the ml Design Round is only part of what it takes to have a stellar career in big Tech to learn more about what it takes to be an mle hit subscribe so you won't miss my future videos"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"Scalable ML System Architecture",
        "Video Title":"Modular ML Pipeline Architecture: Building Scalable Machine Learning Systems in Three Phases",
        "URL":"https:\/\/www.youtube.com\/watch?v=1jbq0cvprIM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/1jbq0cvprIM\/hqdefault.jpg",
        "ID":"1jbq0cvprIM",
        "Publish Time":"2024-12-11T17:23:08Z",
        "Channel":"Giuseppe Canale",
        "Channel ID":"UCrBZKEFFdXiRjm2yuvyEjkw",
        "Transcript":""
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"Scalable ML System Architecture",
        "Video Title":"Design an ML Recommendation Engine | System Design",
        "URL":"https:\/\/www.youtube.com\/watch?v=FoSCaue3lcg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/FoSCaue3lcg\/hqdefault.jpg",
        "ID":"FoSCaue3lcg",
        "Publish Time":"2024-04-11T19:40:57Z",
        "Channel":"Interview Pen",
        "Channel ID":"UC-4nsAH5j9AIhv5tHoQSP9g",
        "Transcript":"so in this video we're going to be talking about how to design an ml or AI based recommendation system for a large scale social media platform so essentially what a recommendation engine is supposed to do is take in actions that users have already performed and then train a machine learning model and that model will output recommendations that our user is going to like then as our user uses the platform they'll interact with those recommendations and that'll feed back into these previous user actions and the cycle will repeat itself generating better and better recommendations for the user as time goes on so it's pretty easy for me to just write machine learning here but to really think about how to design this system we need to think about what this machine learning model is actually going to do so the first step in figuring that out is going to be to decide what data we have and what data we want to get out of this machine learning model so for a social media Platform One Piece of data that we have is what posts a user liked and disliked another piece of information we have is metadata about recent posts and then what we want to get out of this is posts that a user is going to like so now let's take a look at how we can use a machine learning model to get from here to here so what we'll see is is that our likes and dislikes will become our training data so our machine learning model is going to be trained on what users have liked and disliked in the past and that information will be used by the model when it's generating our outputs as an input to the model every time we want to make a recommendation we're going to want to include recent posts and our machine learning model will then be responsible for transforming that list of recent posts into the probability that each user will like that post so for example this could be a number between 0 and one representing the probability that a user will like a specific post then all you have to do is sort that list and we can generate the top posts that user is most likely to click like on so just to recap our likes and dislikes are our training data our recent posts are the inputs to our model and the probability that a user will like the posts is going to be the output of our model so now that we understand what our model is actually supposed to do let's dive into actually how to deploy this model train it and run it on our recent data so let's tackle the training part first if you want to learn more about designing systems end to endend including how our users will interact with them how data will be stored and how it'll be propagated throughout our system I'd highly recommend you check out interview pen.com for our full course on system design so if we want to train our model we have to have some server that can go out and do that and remember our training data is going to be historical likes and dislikes so we probably have that data stored in a database somewhere and our training server can be responsible for going out to that database and fetching that data that it can use to train on once it has this data we can run our machine learning algorithm and that'll output just a file that represents a trained model and then we can load up that file later and use it to generate recommendations now our data on likes and dislikes is going to be changing over time so we're going to have to retrain the model periodically this can be as simple as a Chron job that runs on the training server so that every hour or day or whatever interval we see fit we can simply retrain the model and output a new model file so for a basic system this is a perfectly good workflow for training a model however we do need some place to put this trained model file and we want some visibility into how that model file is performing over time as the data updates there's a lot of tools out there that exist to do just that such as ml flow ml flow will store our model files and it'll allow us to store a bunch of metadata and track experiments over time and that'll help us improve the accuracy of our model and make sure that our data is actually helping us over time so as our training server generates our trained model file we can have it push this model file out to our mlflow tracking server and we can then use it from there now if this is a very small scale system this process of retraining the model every time should work however if this is a larger scale system will likely incur a lot of overhead in downloading all of our data on all likes and dislikes within our system and retraining the model on it every single time thankfully most machine learning algorithms have the ability to incrementally train a model so we can have an initial model that's trained on historical data and then we can import new data and simply update the model to include that new data in its training set if we're implementing this our workflow doesn't really change that much other than that we're going to have to download our old model file from the tracking server first and then we're going to have to download only the new data from our database and incrementally train our model on that and push the result back to our tracking server now if we dealing with just a simple database downloading only the new data could actually be a bit Troublesome we'd have to have some way to represent in that database whether our model has been trained on that data or not and we have to make sure that this process isn't error prone and that it can handle multiple concurrent processes writing and reading from that field this can certainly be accomplished with asset transactions but a database might not be the right tool for the job in this case to make our lives a bit easier and just to decouple our recommendations engine from our database we can introduce a q for this so wherever we're getting our data from for likes and dislikes that data will be pushed to the database of course but it'll also be pushed into a queue that we can use for training our training server will still run on a Cron job and every time it runs it'll pull a batch of data from the queue and use that data to incrementally train our model once we read the data from the queue it's then removed from the queue so this reduces a lot of the overhead of having to keep track of what we've read and what we haven't read now there's one more optimization that we can make to this in terms of observability and that's introducing a workflow orchestration system to give us some insight into when our process is running what failed what succeeded and how long everything is taking we can use a platform such as airflow which will allow us to create workflows to Define what exactly is done on our training server and then airflow will handle the process of actually sending that as a task out to our training server and monitoring the results using airflow as opposed to a simple Cron job will both increase scalability in terms of adding multiple machines later on and it'll give us a lot of observability into when things fail so the next step here is going to be inference but before we dive into that let's do a quick recap of what we've gone over so far so airflow will be responsible for scheduling our training jobs and whenever those training jobs are going to be run they'll be dispatch to our training server and our training server will be responsible for getting a batch of data from a queue and that batch of data will include any new likes and dislikes that our model hasn't yet been trained on our training server will then get the old model from mlflow incrementally train it on the new data and then push that model back to the tracking server so everything is done so far except for actually using that model to generate our recommendations so let's dive into how that would work the simplest option here is simply to have an inference API that will go out to mlflow load up our model and then use data in the database for our recent posts to actually provide recommendations to our users in real time there's actually plenty of ml model serving services that exist that are designed specifically for hosting servers for generating recommendations off of an already trained ml model the problem with this approach however is that our inference API has to actually run our machine learning model every single time we want to Prov recommendations to our users this could introduce a lot of latency for our users so we might want some way to actually generate these recommendations beforehand and have them ready for the user as soon as they need them so to do this we're going to introduce an inference server and just like with our training we can have this read from a que and when any new posts are added we can run inference on those posts to decide which users to serve them to so our inference server will be constantly pulling from this queue it'll use our model that's served on our tracking server and it'll generate our recommendations and push them into some sort of inmemory cache that we can then use to serve our results to our user when our user is actually accessing our system they'll have to go through some sort of API and that API will be responsible for reading the data from the cache and getting their recommendations because this is just a cache any older recommendations that aren't really relevant to the user anymore will automatically get removed as we start to run out of storage space so at this point we fully designed the inference and training for our machine learning recommendation system let's take a look at some next steps one really important consideration to make is going to be load balancing and distributed training so if we look back at our diagram we can see a lot of places where we have single points of failure and choke points where system could be too slow our training server for example we could easily distribute onto multiple machines by using Frameworks for machine learning that can handle Distributing that training job onto multiple servers our inference server could also be scaled because we can simply have multiple machines that are reading from the queue and then each machine would run inference on whatever data it gets from the queue our API could also be scaled by introducing multiple copies of the API and then a load balancer in front to manage the load another important thing to think about would be pre-processing the data and we'd likely want some sort of distributed data analytics framework that could handle processing all of our data that we're getting before it's actually sent to our machine learning model and finally we could certainly experiment with different data sources and also using that data in creative ways and that'll allow us to create more tailored results for our users if you enjoyed this video you can find more content like this on interview pen.com we have tons of more in-depth system design and data structures and algorithms content for any skill level along with a full coding environment and an AI teaching assistant you can also join our Discord where we're always available to answer any questions you might have if you are a friend wants to master the fundamentals of software engineering check us out at interview pen.com"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"Scalable ML System Architecture",
        "Video Title":"Building A Scalable ML Platform On Snowflake At Samsung Ads",
        "URL":"https:\/\/www.youtube.com\/watch?v=0ZpgZ11dIsk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/0ZpgZ11dIsk\/hqdefault.jpg",
        "ID":"0ZpgZ11dIsk",
        "Publish Time":"2023-08-29T13:12:11Z",
        "Channel":"Snowflake Inc.",
        "Channel ID":"UCs10x-muRrTQMJ4Ya-fmIlw",
        "Transcript":"foreign [Music] I hope you guys are enjoying the snowflake Summit and you'll enjoy this session as well so today we are going to share our journey of building a scalable machine learning platform on snowflake at Samsung ads this is I have been leading the data engineering and business intelligence efforts for more than a decade I have joined Samsung ads one year back there I am responsible to architect design and build our data infrastructure and data platform for our organization hi everyone I'm Ashwin a machine learning engineer at Samsung ads focused on designing building and maintaining our machine learning platform that caters to all and our entire machine learning life cycle I enjoy anything data and I am happy to be here to collaborate on the same so this is the agenda for today we'll start where it all begin there will discuss our Legacy system what we had before the snowflake implementation then we'll move on to what we have built to replace our Legacy system then we'll cover what we have accomplished and the next step that we are going to use with snowflake in our organization This Is Us Samsung ads and our purpose is simple we deliver unparalleled results for our customers as we are number one smart TV brand in the world from last 17 years straight we have number one TV weaving data this data is fueling our machine learning and analytics capabilities with that Samsung ads is uniquely positioned to transform the advertising landscape if you want to know more about us you can visit us at www.samsungettes.com or samsung.com so before going into detail like how we have built a scalable machine learning platform I would like to recall a machine learning learning flow this flow consists of five different stages which includes data collection data engineering data query model training and model serving your machine learning models are powerful because of the data they build upon so in our organization we are collecting the data from different sources it is it could be structure unstructure or semi structure and then we are bringing the data to our raw data stage and on top of that we are performing all our data engineering operations and then we are writing the cleansed data to the curation Zone this is snowflake and from there all our analytics team engineers and analyst querying this data along with that are machine learning platform team is reading this data they are training their models and then they are serving their models to the business these models are capable to handle billions of bidding requests within few seconds this is our Legacy system this is a conceptual diagram what we had before the snowflake implementation so at the left hand side you see we have devices we have billions of devices all across the world including Smart TV panels smartphones and these devices are interacting with the backend applications these applications are responsible to render the ads on the screen along with that they used to publish billions of events or hundreds of terabytes of data every day to these storage systems like sdfs S3 columnar and we had many more at the right side we have application teams this team used to manage their own data capabilities most of the teams so if you look at the overall diagram the landscape itself look likes little bit fragmented let me tell you why because most of the teams are managing their own infrastructure and then their own data capabilities data discoverability and reusability was a problem let me give you one example machine learning team is using sdfs and operation team is using S3 now operation team is looking for a data set what options they have either operation team can bring that data set from Source itself or they can search in hdfs or columnar if they found that data set in sdfs they will bring that data to S3 and they will use it but in that case it would be duplicate data at two places right or they have another option they can shift to SDA particle right so data discoverability and reusability was a challenge in this landscape cost management of course if you are managing multiple systems it is gonna be difficult to manage the cost scalability if you are operating within on-premises systems and during the future month like Black Friday there you feel sudden inflow in that case if you want to scale your system you need time because you are not operating in the cloud right compliance this is very important if you are operating within the advertising landscape and in order to run very heavy compliance job in multiple systems those jobs are responsible to scan creta bytes of data every day and if you are running this job in multiple system then this system is not able to scale so we had these multiple challenges in order to address all of these challenges we have built a platform using sunoflac that we'll see next this is our data Cloud platform using snowflake if you see we have devices then backend applications and they are publishing hundreds of terabytes of data every day to our data Lake and this data is growing continuously because our organization is growing very fast in our data Lake we have all our ETL jobs all are spark jobs these shops are responsible to translate that structure unstructure or semi-structured data they are cleaning the data and finally we are writing that data to our snowflake data cloud at the right side we have multiple application teams they are reading the data from Snowflake Cloud itself now how they are reading so we have multiple choice of languages that snowflake supports and for example if one of the team using Spark in the Legacy system we have a replacement we have snow park if Engineers are coming from the database background there they used SQL PL SQL we have snowflake scripting for them we have python connector as well and we have multiple bi tools if you are a data analyst business analyst you can use Pi tools you can connect to the Snowflake and then you could build your dashboard so this is the overall flow now if you see we have a centralized Warehouse that is in Snowflake and this Warehouse is single source of Truth to our organization right we are managing compliance at one place data discoverability and reusability is very simple because of the one system right now this is the flow how we are bringing the data in the snowflake now how we are using that data what features of snowflake we are using to build this kind of infrastructure that we'll see next so now we will deep dive into the snowflake data cloud this is the conceptual diagram of our original data platform since we are a global company we have data all around the world and we have multiple snowflake accounts which are in AMR emia and a pack these all accounts are sitting under the one Samsung ads organization there we have Consolidated Billing at one place for all of these snowflake accounts on top of that you see we have compliance layer we have Regional compliance layer for each and every snowflake instance these compliance layer are responsible to manage and apply the original compliance rules and on top of that we have our data consumers we have multiple teams like machine learning product Finance marketing operation audience Builder and we have many more teams they are reading this data from these Regional accounts and they are building their reports they are training their models and their building their dashboards in this ecosystem every team has their own dedicated warehouses and their own dedicated schema so they are operating within their own consumer space now let's take an example machine learning team they have 10 team members and they have different kind of roles so we are managing this by using the role based access control that comes with snowflake itself we have dedicated role for each Engineers if they are performing different kind of duties right so if machine learning team has 10 warehouse and we have 10 teams so it will become 100 Warehouse right and if we have three sin of like instance then it will become 300 Warehouse if you are running 300 Warehouse together in one organization how you are going to manage the cost snowflake has a very good answer for this object tagging we are tagging each and every Warehouse there we are monitoring the cost and if the warehouse cross the threshold we trigger the alert and be notified to the consumer so they act accordingly then we are using storage integration to read the data from the object storage it could be S3 gcp or azure you can create the external table and then you can read the data if you do not want to bring the data in the snowflake itself this is like five or ten percent in our Organization for the adoc use cases then we have Network policies in place in order to maintain the security and of course as I said we have Regional compliance management layer if you look at the overall picture now if machine learning team want to consume the data they can connect to the AMR emea in a pack but they have to go through three different accounts and we have three different account because of the original compliance rules now they need all the global data at a single place so that they can build a single report there they can show the global data global data platform so the difference here is instead of regional compliance layer we have a global compliance layer this compliance layer is responsible to manage the compliance rule that is common globally and then we are moving this data to a separate snowflake account that is globals account and there we have global data and on top of that you see we have all our data consumers they are operating within their own space now in order to build this kind of infrastructure there you have to move the data within the same region or across the region you need good capabilities if you build this kind of infrastructure with the Legacy databases it is going to take months or years but with snowflake it is very simple it's data sharing and replication so when we are moving the data within the region we are using snowflake data sharing and if the we are moving data across the region then we are using database level application now if you have beta bytes of data in your system and this is the global environment and in your one table you have let's say hundreds of countries of data in a table and based on the roles based on the compliance rules you have multiple consumers so let's say you have a consumer John and you want to use U.S data only but in your table you have hundreds of countries of data what option you have you can create a view there you can have a filter and that view would be very specific for John or you can create a separate table for John but that is not going to scale we have a very good answer within the snow flag row level policies you can configure the rollable policies for John and based on that you can just share one view to all of your users when they query the data they can only read the data based on the policies now the next feature that we are using is auto clustering and this is very important if you are operating within a system there you have hundreds of beta bytes of data your table size is continuously growing every day you are putting 10 terabytes of data in a table and it is keep continuously it is growing right at some point of time it is gonna be slow so you need table maintenance right so if you do the table maintenance you have to go either you have to gather the stats or you need to rebuild the table right but with snowflake it is very simple just go and enable the auto clustering it will do all the things for you and you can disable this Auto clustering at any point of time next we are using snowflake alerts let's take an example you have a team the team is basically reading hundreds of tables they are writing their semantic logic they are aggregating the data and finally writing the data into a table and it is a scheduled task which is running let's say in every one hour let's say it is failed and you need notification if you are using any other orchestration like airflow or some other tools then you can configure those kind of highlights but you have the solution within the snowflake itself you do not have to bring up another environment and run your orchestration right just go and create the alert in the exception handling you can raise the alert whenever it is failed and it will notify you then as I told we have using snowflake task and snowflake has a very good UI there you can go and see all the your task dependencies you can create group of tasks you can put dependency so these are all the features we are using within our platform now if you look at the overall picture and if I tie back with the challenges where we started now we are managing all our data at a single place data discoverability and reusability is not at all a problem we are managing compliance at single place right for cost management we have object tagging in place and of course we are operating in the cloud we can scale our warehouses at any point of time and we can disable them at any point of time if they are not being used right so this is the architecture of our global data platform now we are going to hear a successful story from our machine learning team how they are using this platform how they are doing their analytics and machine learnings with that I would like to invite Ashwin and you will take it from here thank you so um now that we have a data Cloud platform over to cater to our regional and Global needs let's see how we can print a good use for machine learning use cases so Samsung adds uses machine learning to make our demand side ad platform intelligent so our demand side platform or DSP for short receives a million requests per second so it becomes extremely inefficient to respond to every one of these build requests so we need a efficient way to correlate each bit request to our advertisers kpis these kpis could be Impressions installs and etc etc so here is where machine learning comes into play we use machine learning to correlate these bid requests and see how valuable they are to our advertisers kpi so let's go over a typical machine learning lifecycle to see how this works in order for good machine learning we need good machine learning models and good machine learning features as well right so let's go over the life cycle and I know you probably have listed like five different sessions today which talk about the same machine learning life cycle but I'm just going to go over this real quick so that it brings everyone else up to speed right so let's consider Mr Tony not Stark a brilliant engineer with a state of the art idea to solve a machine learning problem not Jarvis so machine learning being data driven he goes on a journey and his first step is to collect the data from different sources he then does expiratory data analysis to extract meaningful insights from this data he notices noise in this data which could be very harmful for machine learning modeling and so he goes ahead and clears the data he needs to transform it and then he needs to aggregate the data to create meaningful features he then builds a machine learning model which ingests this features and trains it to solve his business problems now in order to see how his model performs he needs a test set of data and he also needs an offline performance metric which allows him to see how his model is performing so he can then validate his model's performance and once he's happy with the offline performance he goes ahead and deploys these Mo this model into production and monitors how this model performs with his business metrics right now this is an iterative process and he needs to do this over and over again until a stable state is reached but this makes Tony the bottleneck in most organizations so in most organizations we have domain level experts who take care of every step of this life cycle for example we have data Engineers who take care of ETL Pipelines we have data scientists who take care of machine learning modeling and training and we have machine learning Ops who take care of monitoring how these models perform and also deploying these models into production so we quickly see the need for multiple teams to collaborate and also work together to build successful machine learning products this opens up quite a few challenges for machine learning and skill so in an organization like Samsung ads we have hundreds of machine learning Engineers working on thousands of machine learning models which cater to tens of thousands of concurrent ad campaigns which deals with a trillion data points now with the number of variables at play here it becomes evident how difficult it becomes to handle all of these parameters in a scalable reliable and cost effective way hence the need for a machine learning platform which helps find the determinism in this non-deterministic space so now let's take a closer look at our machine Learning System and see how we have built that over whatever actually is talked about So recollecting what at least said we have raw data ingested into our data platform in batch this raw data is exposed to us as snowflake news now this is really important because previously we had data scattered over multiple systems as actually said right so when aggregating all of this data within a single platform it helped us access this data very easily and being views we didn't have like right access to it so it's easy to just have multiple teams query it especially multiple members of our team query all of these raw data sources within the same source but just access to data and processing data at scale are two different ball games altogether so this is where we use snowflakes Warehouse to process terabytes of data at scale in a reliable way so we also have the role-based access control which is enabled by the data platform team so that the right people have the access to the right data and the right warehouses so our Engineers use a combination of the raw data we use they can use that to analyze the data and then they use the virtual warehouses to create these features all within snowflake but being only adopters of snowflake we quickly saw that it also became a problem for us to migrate all our previous part-based pipelines to SQL this is mainly because we're forcing our Engineers to use SQL to use our platform and it also made it difficult to unit test SQL workloads when we're moving from spark to SQL so this went against our design goals to make it easy for engineers to iterate end to end from machine learning lifecycle so enter snow park so snowpark helped our developers use a language of their choice by pushing down this all the heavy lifting to snowflake this enables them to work with data of any size wherever it is in Snowflake and still do the same processing with the Frameworks that they're most familiar with so if they use snowpart to create the data they still need an efficient way to train these machine learning models so we use machine learning with tensorflow and we use distributed training to consume this data in parallel and train these models on terabytes of data that are generated every day to create these models and store them in our model registry so our Engineers use these training instances through our machine learning training apis that we've built in-house so this makes it easy for them to deploy the training jobs rather than having to maintain it or like use the instances directly but there's also one more challenge just with snowpark and the distributed training way we have right we previously we have we wanted Engineers to actually access both of these things separately and that made it difficult for engineers to context switch as much so we provided them access to both of these through a single medium Jupiter so using jupyter notebooks Engineers can directly use our machine learning training apis to deploy their trading jobs and they can also use snowpark to do their exploratory data analysis and feature engineering as well they deploy these features into our data cache for online serving they also can deploy these machine learning models to their model registry where they keep track of their versioning and experiment tracking but again we see the need for model Engineers to deploy these models and the features to production easily so this brings us to the third and final phase of our machine Learning System the interface between our batch system and a rails time system so our in-house real-time bidder helps load the features from the data cache and also helps load the model registry based on the versioning required and it helps serve these real-time requests um I mean quite easily in order to serve the request we use tensorflow serving and we also use open source tools like Victoria metrics and Prometheus and also grafana to monitor how these models perform in production the Synergy between our batch system and real-time system show how it is easy for an engineer from model Inception to deployment so basing off this platform let's quickly go over how a data engineer would use this use the machine learning platform so a data engineer would use a jupyter notebook powered by snowpark he would ingest the data from raw data from Snowflake and then he writes the data back into snowflake or S3 stf records They also use snowflake warehouses so that we can they can scale this process based on varying workloads and it's this is very reliable to do this at scale once they're comfortable with how they do their machine learning feature engineering they then automate this process and have scheduled pipelines with the airflow which runs every day to do the same process and those and and pushes these features back into the data cache so from this workflow we can see four steps of the life cycle being automated within the same workflow the data collection the data analysis the data processing and the feature engineering as well now that we have processed features that a data scientist can use the the data scientist still uses the same workflow as very similar to the data engineer he then uses the Jupiter notebook accesses our training apis which can still which can use spark ml or tensorflow to train the models with distributed training and they ingest the mod ingest the data from Snowflake or S3 and then save the model metrics back to Snowflake and also the model registry so we use our model registry as previously said so that we can do some tracking with I mean each data scientist can track how their models work and then they can also store the models in the model registry so that they can roll back if they need appropriate if they need to roll back at all we also use tensorboard for monitoring which so that we can see how our machine learning models converge over time during model training once he's comfortable with the hyper parameters that he's tuned he's then going back and going to schedule the same Azure pipelines to do with it to schedule the training jobs which then deploys the models to our model registry on a daily basis so we've discussed about like how an engineer would train the model and also we even discuss how the features have generated every day as well but during iterative machine learning we do see some issues with deployment as well so we see that we have new training data that is generated every day by our data engineers and so new features created which means there is going to be new data distribution we also see our data scientists creating new models with different hyper parameters and so this creates a lot of variation and we have multiple experimental models by the combination of the new data and the new parameters so now a problem arises like how do we know when we have multiple experimental models that if we deploy these one of these models to production that they're going to work better than production so one way to do this is to create a testing data set based on the most recent data and then we use like an offline performance metric that can directly correlate with our business metrics and then we see like if we try to do inference with our experimental models and a production model and if everything works well and our experimental model performs better than a production model we should ideally deploy to production right but this more often than not has stumped us we've seen that when an experimental model worked better in our local or like offline settings we deploy a different direction we see that our business metrics actually performed worse this was because of two reasons one the data distribution with offline and online were completely different and the second one is that the offline performance metrics that we thought would correlate with our business metrics were completely off so our answer to this was Shadow deployment so using Shadow deployment we run production traffic through our machine learning models in prod we run duplicated traffic through our experimental models as well so now we have both of these models serving with requests simultaneously but we only return or respond to the live bid request from production models but we still collect the responses from both these models and we store them offline now in real time we can simulate based on the responses how our experimental models would power our campaign metrics or business metrics so this solves both the challenges that we discussed in the previous slide we're working on the same live bit traffic that our experimental model and production models serve we don't need to create an offline metric that correlates to our business metrics because we can see how the campaigns perform in life so we've seen or taken some successful strides by adopting snowflake for a machine learning platform we've seen 64 percent improvement in our performance by adopting snowpark which allowed the pre-processing to reduce from three hours to 1R we also see by centralizing our data within a single data platform we simply access to data and it allowed us to make unit testing simple it also helped us to like identify and fix data issues locally which helped improve QA iteration time from four weeks to two weeks and this improved our productivity by 50 percent moreover during Peak data load using during feature month we use snowflake warehouses which could scale very easily and quickly which stayed approximately 5 hours per day so this improved our overall time to Market by 30 percent by using snowpark we were able to push down our query down Snowflake and snowflakes warehouses and enable usage of light lightweight instances rather than on Legacy clusters so this also brought down our costs by 20 percent so in summary we've created a machine learning platform with highly scalable infrastructure which is limited only by the cloud and of course your imagination we've enhanced the data access and discoverability by by aggregating all our access to data in a single data source snowflake we've improved the usability by keeping the platform as self-service possible and we've reduced the Errors By abstracting The compute we have stringent security and privacy in place by enabling rule-based Access Control and we have backward compatibility by using snowpark which helps Engineers to work with snowflake with the language of their choice finally using our platform we've managed to keep the deterministic aspects of machine learning as deterministic as possible which means like any input that you have like any data that you give as input you get the same output except for the machine learning models which are non-deterministic in nature but it doesn't stop here for us though so in the future we see a seamless integration between Snowflake and tensorflow and in order to do this we seek to consume data directly from Snowflake using TF data or we plan to move our training the machine learning models within snowflake so we don't have any data transfer out of snowflake I mean with no data transfer out of snowflake this would help us have all the compliance handled by the data platform team with improved compliance comes the savings of cost as well we also see the possibility to do real-time data ingestion using snow pipe streaming and move our architecture to more of a Kappa architecture we also see the possibility of doing Federated learning in which case we can train our machine learning models within snowflake in all of our regional accounts and still share the models across each other but not the data alone so this brings us to the end of our presentation I take this time an opportunity to thank the organizers for giving us this platform to collaborate and learn together I'd like to conclude by saying features make it a model and that's a snow-brainer foreign"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"Cost Optimization for ML Systems",
        "Video Title":"Accelerating ML development with optimized performance and cost",
        "URL":"https:\/\/www.youtube.com\/watch?v=8Dph6ki1TJc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/8Dph6ki1TJc\/hqdefault.jpg",
        "ID":"8Dph6ki1TJc",
        "Publish Time":"2022-10-11T17:01:38Z",
        "Channel":"Google Cloud Tech",
        "Channel ID":"UCJS9pqu9BzkAMNTmzNMNhvg",
        "Transcript":"[Music] thank you everyone for joining our Google Cloud next 2022 section on how AI infrastructure on gcp helps teams accelerate ml development with optimized performance and cost my name is Mikhail kreska an outbound product manager on vertex AI Google Cloud's AI platform I'm excited to have three other guests with me here today we have Kai from Uber who's a senior product manager on Uber's ml platform Michelangelo and Joanna and Sid from coheer ml Engineers building tools to make natural language AI capabilities accessible to all Developers we'll start with a quick intro on why AI infrastructure is important walk through AI infrastructure services and solutions available on gcp here customer stories about leveraging AI infrastructure on gcp and then wrap up with a quick summary so let's start with why is AI infrastructure imported and the best way to start really answering this question is to give you a sneak peek into our customer stories about how they're using AI infra to accelerate development and deployment cohere is leveraging tpus on gcp to innovate faster and iterate faster on large language models Uber has integrated vertex ai's tabnet and automl services into their platform to allow different teams to leverage these new algorithms Credit Karma is leveraging vertex ai's feature store to manage embeddings that feed into their content engagement and offer conversions engine and finally Arbor biotech is leveraging Alpha fold running on vertex AI to help their scientists predict structures of protein sequences in hours instead of months these examples and many more cannot be done on general purpose infrastructure this requires robust ai purpose-built ai infrastructure in addition to these specific examples there is a broader Trend across data and AI data is getting bigger it's getting multi-format AI models are getting larger and more developers are adopting Frameworks like Pi torch tensorflow the Transformers library and Jax making AI research and development more accessible so infrastructure really needs to keep up to make AI models performant and cost effective and having access to strong AI infrastructure is becoming a competitive advantage to getting the most value from AI and this is also exemplified by these two quotes from IDC and Forester IDC shares that the lack of purpose-built AI infrastructure is now becoming one of the leading causes of AI infrastructure failing and Forester shares that AI infrastructure is really crucial to keep AI teams productive to keep them experimenting developing and deploying rather than just waiting around for large AI tasks to complete so now let's move into what does AI infrastructure on Google cloud look like and we have three pillars when thinking about this one flexible and scalable Hardware to support diverse ml workloads two manage infrastructure to allow practitioners to focus on experimenting and deploying models and this is offered through vertex AI a fully managed data science ml Ops platform and number three really easily access state-of-the-art AI with optimized infrastructure and software paired together we're really looking to bridge the gap between research and applied AI what we call research to ready so with a focus of AI infrastructure as part of an AI strategy the performance and cost benefits can be quite staggering along the bottom we see some great results where some workloads get up to 6X cost reduction by optimizing on the right hardware and we've seen some teams go from ideation to experimentation to production in weeks rather than months or quarters with many teams sharing stories of 80 Improvement and overall velocity across the ml life cycle so now let's dig into some of the specific services on the hardware side we're excited to welcome two new AI accelerators onto gcp first the Nvidia A2 Ultra GPU VM instance this machine has twice the GPU Ram of the previous A2 Mega machine providing 80 gigabytes of GPU memory this provides big performance gains in throughput as well as latency for use cases like deep learning recommendations and computer vision models as well as large language models second we're excited to welcome the newest generation of the tensor Processing Unit or the TPU this is infrastructure designed by Google specifically for machine learning with support for pi torch Jacks and tensorflow the fourth generation of tpus scale to a super pod made up of over 4 000 chips and each chip has big improvements in flops and flops per dollar versus the previous Third Generation in addition to Pure horsepower the data center hosting the tpus is operating at 90 carbon free energy with a big focus on sustainability so now let's move on to some of the more machine learning specific tasks and large-scale training is really one of the first ml tasks that requires purpose-built AI infrastructure vertex AI provides a fully managed training service that eliminates the need to provision and manage clusters users can submit and forget jobs with the python SDK get out of the box queue management access to on-demand accelerators and built-in hyper parameter tuning moving on to the middle vertex also provides a new distributed training capability that works with existing Frameworks like horavod or tensorflow's mirror strategy or Pi towards distributed business reduction server architecture provides a new way that minimizes latency and data transfer benchmarks for language models have shown up to 30 to 40 percent reduction in training time and cost and finally we know many times data feeding into these accelerators can often be the bottleneck so we're launching first class support for cloud storage and NFS file store as part of vertex to be easily able to mount these into your on-demand training jobs for cloud storage specifically this allows you to use cloud storage's new auto class capability that automatically moves data to lower and colder storage classes based on access time resulting in automated cost savings next we move into the serving layer with vertex's managed prediction Service First this is a fully managed API endpoint service this allows users to deploy their models into Auto scaling endpoints with a wide selection of CPUs and gpus with minimal infrastructure knowledge required within this we're excited to announce two new capabilities model co-hosting that allows you to share underlying resources across models for cost optimization and custom prediction routines which allows users to include pre and post processing python code alongside your model binary to simplify your architecture and then in the middle we're also excited to introduce an optimized tensorflow runtime only available on vertex this leverages model pre-compilation and smart op placement across gpus and CPUs to drive big improvements in throughput and latency early benchmarks show huge improvements up to 8 and 6X for throughput and latency for tabular data and material Improvement for based natural language models and finally deploying models for serving is only half the battle vertex provides model monitoring and explainable capabilities to ensure your models stay healthy or are retrained and redeployed as they erode next AI infrastructure isn't just about training and serving machine learning models managing features and embeddings to feed into your ml models is non-trivial as organizations scale both the number of models and the usage of complex unstructured data as part of these models feature stores and Vector databases can become a challenge to implement to support this newer AI infrastructure need vertex AI offers a fully managed feature store and matching engine and today we're actually excited to announce support for streaming ingestion across both these Services where you can now have new features and embeddings synchronized and updated in real time to improve the accuracy and timeliness of predictions and both these Services support a wide variety of use cases ranging from recommendations engines search engines image classification add targeting and much more and finally we don't want to stop at just providing hardware and management infrastructure we want to provide a path to adopting Leading Edge Solutions coming from research and partners that can drive true business value through AI first we have tabnet now available through vertex ai's tabular workflows this makes it easier to build accurate and explainable models on billion scale data sets with built-in interpretability on vertex the tabnet pipeline automatically selects the appropriate feature Transformations search space based on the input data data size prediction type and your training budget we'll hear a little bit more from Kai about Uber's early experience with tabnet second we have an alpha fold batch entrance Pipeline and vertex AI this allows biotech companies to embed Alpha fold into their own workflows to predict protein structures at scale with optimal architecture and third we have a solution from our partner Nvidia called Merlin Merlin is an end-to-end framework to design and deploy custom large-scale recommender systems we've partnered with Nvidia to build a reference solution Deployable on vertex Ai and gpus and gcp which includes data processing model training and model inference next we'll jump into some great examples of Enterprise teams leveraging some of these Technologies and let's go ahead and start with Kai from Uber thank you so much Michael hi my name is Kai I'm the product manager for Wilbur's internal entry and virtual learning platform called Michelangelo Michelangelo Powers 100 of Uber's most business security commissioner use cases such as rice ETA eats ETD driver Rider matching and is home field recommendations allowing our machine learning developers that we were to focus on what they are good at building and deploying models without worrying about the underlying infra complexities our strategy of building Mark Angel is based on actively evaluating and integrating industry-leading third-party components while selectively investing in key platform areas to building house so we architecture Mark angel in a modularized way so that we can easily integrate third-party tools and components near plug and play fashion when we explore the solutions for large-scale model architecture search and training we found the vertex Ai automl and tablet in the past year we'll be working closely with the vertex AI team to evaluate the performance of of automl and tablets with Uber's real-life use cases and data the evaluation results met the POC success criteria in terms of model accuracy training efficiency compatibility with Michelangelo Tech stack so we've decided to integrate both tools with Michelangelo and made them available to All Machine learning teams at Uber on the other hand along the way to meet Uber's extensive and complex machine learning needs we've made multiple feature requests and product Improvement suggestions to vertex AI team which in turn improves automl and tablets one example use case is Uber is prep time model which is used to estimate how long it takes a restaurant to prepare the food after water is received this is one of the most critical models at Uber with the highest QPS query per second we compare the tablet results with the Baseline model and the tablet demonstrated a big lift in terms of the model performance overall the POC results are promising and we are excited to continue closely working with the gcp team to drive production adoption within Uber with that I passed down to Joanna to talk about their use cases at kohir thank you Kai hey everyone I'm Joanna Franco here I'm very excited to present how cohere is accelerating large language model development with Google Cloud queer's mission is to unlock NLP capabilities enabled by large language models and make them accessible to all developers with just an API call to achieve this cohere abstracts the way the heavy lifting from the end user including collecting and creating a large Corpus of high quality data training a pre-trained learning models and post fine tuning optimizations for low latency inference in a highly reliable environment here we outline coherence achievements in building a scalable training evaluation and inference stack taken as a whole this technical stack as a key competitive Advantage which has enabled cohere to rapidly scale our models while ensuring quality responsiveness and safety of our end products all of which is implemented in Google Cloud platform with that I'll pass it on to set hey I'm Seth I'm a machine learning engineer at cohere I'll give a high level overview of oh heels training framework so go here has a proprietary training framework called fax it uses Jax and Cloud TPU v-force to train large language models TPU V4 pods are some of the most powerful AI supercomputers in the world a full V4 pod has 4096 chips and the diagram on the right you see a few topologies of slices from the full V4 pod that are available on the Google Cloud platform and that can be used by facts to train these large language models again kohio uses facts to train our Baseline models as well as custom models that are trained on customer data sets using the fine tuning feature on the cohere platform Fox's job is to consume billions of tokens and train models as small as hundreds of millions to as large as hundreds of billions of parameters coming to some concrete benchmarks in the middle we see pipeline parallelism which was the old way of training large language models that go here with facts the scaling and the plot is represented by tensor parallelism so for a fixed number of tokens the tensor parallelism method of training large language model scales much better and on the right we see that with using 512 V4 cores the maximum model size we can schedule is 340 billion parameters and at a minimum batch size the step time is 6.21 seconds this enables us to train large language models very fast and bring those improvements to customers right away thanks for having us uh Mikhail ambassador of the year great yeah great to hear those stories from both Uber and Co here really pushing the boundaries of AI and really leveraging the AI infrastructure services on gcp I also wanted to end this with actually two quick stories from two other customers first is with credit karma for those not familiar Credit Karma is a multinational personal finance company that continues to innovate to provide more personalized products and services they have a mission to make Financial progress possible to everyone one of their use cases is to better recommend Financial content and financial offers for their members their approach was to generate embeddings from Financial content and mix those with features from offers based on user cohorts to improve their input features for their recommendation system the diagram on the bottom left gives you a quick system overview of how this architecture works we have Financial content on the left where they used a pre-trained sentence encoder model to extract embeddings on the right they also bring in features from Financial offers they bring both these different domains into vertex's feature store that are then leveraged for training and serving their recommendation engine and this really had two benefits highlighted on the right side with a few of their quotes one is really for the internal Credit Karma machine learning platform team shout out to debashy's das who's been working with us um his team can Now launch richer and faster features up to 3x faster and second is of course the user experience the consumer customer experience and using a b testing they've seen significant increase in content engagement and offer conversion next we have a customer Arbor bio leveraging deepmind's Alpha fold Solution on vertex AI Arbor bio is a biotech company based in Massachusetts discovering and developing the next generation of genetic medicine our bio is really developing extensive toolbox of proprietary Gene editors using both High throughput data screening as well as Ai and why do they really look at Google cloud and gcp number one they really look to augment their toolkit and processes with deepmind's Alpha fold model and they really wanted to inject the capability to predict structures of proprietary protein sequences in a scalable robust and cost-effective way now Arbor bios data driven and AI based approach really has a lot of high variability of Hardware needs across CPUs and gpus and this was a great use case for gcp and vertex ai's managed infrastructure and really the impact you can see with the quote in the bottom right is that autobio scientists are able to increase their productivity and generate actionable insights in hours instead of months so let's go ahead and and end with a summary um you know to summarize we truly believe that to drive the most value from ai ai infrastructure now needs to be at the core of your AI strategy we cover three pillars of AI infra on gcp flexible and scalable Hardware manage infrastructure on vertex AI an accessible state-of-the-art AI Solutions and you can see we can continue to invest heavily in each of these with lots of new capabilities and launches rolling out and finally again we're super thankful for our customers and partners leveraging these Services providing us feedback to continue to improve these and sharing their stories with the broader industry so we can all learn from each other so thank you so much for joining us today please enjoy the rest of Google Cloud next and have a great day foreign"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"Cost Optimization for ML Systems",
        "Video Title":"Leveraging Machine Learning for Predictive Maintenance: Optimization &amp; Cost Saving",
        "URL":"https:\/\/www.youtube.com\/watch?v=ftrkXmHE0vs",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ftrkXmHE0vs\/hqdefault.jpg",
        "ID":"ftrkXmHE0vs",
        "Publish Time":"2024-02-22T00:19:37Z",
        "Channel":"UpKeep ",
        "Channel ID":"UC4ewS5K1zt6TOpZBfXx8Yfw",
        "Transcript":"hello there I'm Ryan Chan CEO and founder of upkeep in this video we're going to discuss how The Cutting Edge technology of failure prediction machine learning can revolutionize your maintenance and operations teams you'll learn how the combination of artificial intelligence and maintenance can help you monitor your critical assets around the clock and take Swift action when anomalies occur we'll also delve into the advantages of machine learning over human monitoring and how it can help you save costs and increase efficiency so let's dive into this fascinating world where Tech meets maintenance welcome to the exciting world of failure prediction machine learning this is where artificial intelligence meets the maintenance Arena giving you the power to monitor your critical assets Around the Clock the moment something seems out of the ordinary your monitoring equipment can automatically communicate with a centralized computer system allowing you to take action before or immediately after failures occur let's talk about maintenance most companies operate somewhere between 100% reactive maintenance and 100% predictive maintenance usually facilities have elements of both reactive and predictive maintenance as well as some preventive maintenance strategy but here's the key point the closer a business can move toward predictive maintenance the better and guess what using machine learning effectively will help move your organization in that direction leading to better performance and lower costs now in this age of artificial intelligence it's crucial to understand what machines can do better than humans and what humans can do better than machines both are vital to maintaining a successful company if strengths are understood and employed machine monitoring can be done constantly while human technicians can only monitor assets periodically let's dive into the advantages of machine learning over human monitoring first up real-time alerts equipment like sensors can monitor constantly providing real time alerts as soon as a potential problem arises for instance technicians can specify acceptable ranges and machines can send an alert as soon as an asset falls out of that range if a person manually checks ranges even daily you could have a 24hour lag time before the problem is detected next predictive repairs at a more advanced level machine learning can use past data to generate conclusions it may be able to calculate the likelihood that errors will result in a full malfunction or failure management can then use this analysis to perform predictive maintenance tasks before the failure occurs then we have malfunction sensing in some cases it may be safer and less damaging if a process or machine stops before a malfunction occurs many machines will cut the power before an engine overheats causing much more damage you can program specific assets to pull the plug as soon as a malfunction happens and finally less data sifting machines can sift through thousands of data points every second making conclusions based on human Direction and programming humans simply cannot process data at the same speed now let's explore six ways machine learning improves systems firstly lower costs employing sensors and artificial intelligence can be much less expensive than relying on Engineers or technicians to perform the same tasks secondly quicker repairs because machine learning provides Around the Clock monitoring you'll be alerted to needed repairs immediately train maintenance technicians can be sent to address the potential problem or malfunction right away keeping things running thirdly safer systems sensors and machine learning can spot problems early and be programmed to shut down before something catastrophic happens this increases safety for all your employees especially when working around potentially dangerous processes or equipment fourthly less downtime the fact that sensors and machine learning can let you know when potential problems might occur leads to less downtime significant failures can easily halt a production line possibly for hours at a time if you discover your line is at risk due to a machine learning alert you can order needed parts and schedule predictive maintenance tasks at convenient times fifthly easier scalability machine learning leads to everything running more smoothly as your company grows you'll be able to scale production and expansion more easily and effectively if you approach a maximum capacity your machine learning will be able to warn you before you overload your system and finally higher level work for employees although there is a fear of machines taking human jobs remember that machines are best at tedious tasks this should free up technicians and Engineers to do more challenging and rewarding tasks be sure to invest some of your AI cost Savings in helping your employees grow in other Knowledge and Skills so how does machine learning work in predictive maintenance initially data is collected from sensors and when performance falls out of range alerts are sent longer term however machine learning can process ongoing performance and provide data about frequency of threats and potential consequences first you'll collect data from sensors use a wide variety of sensors on your critical equipment to collect data sensors can monitor temperature vibration water leakage or levels mileage or usage data pressure and many other performance items next you'll extract standout anomalies specify within your system the acceptable ranges of the factors you're monitoring if anomalies arise program your machine to send necessary alerts particular anomalies may include pressure Peaks temperature Peaks or vibration Peaks then you'll analyze anomalies to detect threats the machine learning tools can be programmed to analyze potential threats when a certain level is reached or exceeded an immediate alert can be sent however machine learning can also look at the history of anomalies for a particular asset to determine larger failure threats following this you'll report threats in your cmms your computerized maintenance management system once threats are detected sensors and machine learning equipment can send the data to your cmms at that point your cmms can initiate purchase orders if parts are needed or work orders to address the threat and finally your machine system learns after you've generated some historic data your machine Learning System can consider these data points to draw some conclusions for example it can suggest potential threats or report on the frequency of anomalies on a particular piece of equipment using this data you can make more methodical business decisions to wrap up machine learning is an excellent way to better monitor the performance of your critical assets effective affordable sensor technology provides 247 monitoring and alerts can be sent to your cmms for immediate action over time you can accumulate data that can help you your AI learn more about your assets and Report potential threats these reports can be the foundation for more effective smarter business decisions thank you for watching today we've explored how failure prediction machine learning can revolutionize your maintenance and operations teams and how it can save costs and increase efficiency don't forget to subscribe to our channel for more informative content for more details visit our website at upkeep tocom remember the future of maintenance is is here and it's powered by artificial intelligence"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"Cost Optimization for ML Systems",
        "Video Title":"Saving cost on your machine learning training and inference on AWS",
        "URL":"https:\/\/www.youtube.com\/watch?v=keowy9Yfxlc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/keowy9Yfxlc\/hqdefault.jpg",
        "ID":"keowy9Yfxlc",
        "Publish Time":"2022-09-19T14:30:45Z",
        "Channel":"AWS Developers",
        "Channel ID":"UCT-nPlVzJI-ccQXlxjSvJmw",
        "Transcript":"okay good morning uh good morning julian good morning jonathan thank you for joining me today for the ml cost optimization session good morning nice to be here today good morning happy to be here great so let's start so startups founders that i meet these days tell me that the current economic climate really forces many of them to reevaluate their monthly burn rate and find ways to better control costs so in today's session what we would like to discuss is which phases of the ml projects cost the most mean money people time and how to save money and make the most of the budget you already have but before diving deeper let's start with a short introduction i'll start quickly my name is marav ash i lead the machine learning business development for startups in india i've been helping startups with the animal research and projects in the past three years and i'm happy to be here and share the experience of working with startups on machine learning julian you can go next yeah hi i'm julian from berlin and at amazon web services i work as a machine learning solutions architect focusing on startups across europe middle east and africa and it's startups of all sizes and all industries and i really like to help them finding a technical solution and the right approach using aws for their model building experimentation training and deploying of the machine learning models over to you jonathan hi i'm ionatan the ceo and co-founder of desi this is a deep learning development platform focusing on building optimizing and deploying deploying deep learning uh solutions while focusing on the the inference performance which also highly correlated with the cost associated with running our production workloads of deep learning perfect so um again thank you for joining um you two get to meet a lot of data scientists in your daily work uh what do you hear from them about cost when it comes to ml yeah that's true in my role i have the pleasure not only to work with data scientists but also with ctos machine learning engineers or the heads of data from machine learning startups and especially in this more earlier machine learning startups typically the ones that are maybe pre-product launch or in the early phases where they still invest a lot of energy into building their pocs getting their models right we can see that didn't in this phase a lot of the cost comes from training especially if large compute clusters or gpu instances aren't worked so typically from these kind of startups we hear questions about how can we optimize our training costs why not limiting our data scientists in their productivity or in the experiments that they can run what do you think jonathan so you're right early on people focus on building the model and then they get to a mature product that they start to run inference workloads and then at some point they see the inflection point where inference workload uh grow and grow and then the the cost is starting to accumulate around inference workloads they're running multiple models in the cloud and they start to think about uh optimizations and how they can reduce uh that cloud cost which is comes later after having a product that's starting to see some uh scale so those are the two aspects one is the training the second one is the inference and we are seeing a company a relatively early stage startup which has a product that is running with 24 7 clusters of about 10 to 20 gpu instances are running all the time and the cost of this workload is very significant for that size of startup and they are starting to think about um how to reduce those costs and this is what we're going to talk about today uh i agree and before going deeper um you mentioned potential cost saving when it comes to in training and inference and we're going to touch that in details but you also mentioned data scientists ml engineers maybe devops team i would like to ask you when it comes to machine learning who's responsible for cost optimization yeah it's a good question i think especially in early startups or small startups we see this pattern that everyone wears multiple hats and everyone has to fulfill multiple functions so it's not uncommon that in these early days the the first data scientist is also the machine learning engineer and responsible for the infrastructure or that even the cto is still directly involved in such tasks so there's clear but even as startups grow i think cost optimization can best tackle it as a as a team effort and every role can contribute so for example as a data scientist it's in your hand to make efficient training code and smart experimentation design as sml or devops engineer you can really think about the architecture and the right choice of infrastructure to make it cost efficient and so on and so on yeah it's a really team effort as you mentioned so let's start with training um julian based on your experience what are the top three factors driving training costs yeah so for machine learning uh training really the biggest cost driver that we see in in startups is often the cost of compute or instance costs and of course there's secondary costs like cost of storage for data sets or the costs that go into data pre-processing and data cleaning but usually the compute costs are an order of magnitude larger so let's focus on compute costs today compute costs i think makes that we can break these down into two aspects where cost optimization can happen time training setup and instance type so let's dive quickly into each of these three pillars so for number one for time i really mean reducing the time that uh instances are running especially if they're not utilized that's a huge cost optimization opportunity so to make it more concrete let's look at this picture on the top row we see a setup how i often see it in early startups or startups that migrate from on-premise workstations into the cloud and they just manually set up for example a gpu instance or a gpu powered jupiter notebook and then a data scientist connects to it to develop and build the model and the whole experiment setup and then manually starts the training and when the training finishes the instance keeps running until the next time a data scientist sets up a new experiment as you can see there's a lot of idle time where the gpu instance is running but no experiments are happening so as a first step in the middle row i would always recommend to start with some automation on the shutdown functionality so for example if you're using a jupiter notebook or an amazon sagemaker studio notebook you can use a lifecycle configuration to automatically shut down idle resources after it done as we can see now we've already massively reduced the time the gpu instance is running but we can go even one step further an even better approach that i see startups adopting is switching the building and the development from a powerful instance to a more affordable smaller instance or even running it on local machines so for example running your jupiter notebook on a cpu instance for development on your local machine and then using a managed api to create your training as a job on managed infrastructure using technologies like aws batch or amazon sagemaker training jobs where the service handles for you creating the infrastructure starting the training job and terminating the infrastructure once the job has finished or if an error occurs and as we can see with such and setup we really reduce the time and instances running to the actual training time and get the most cost optimal setup as a nice plus with this approach is also much more streamlined to run many jobs in parallel from the same development environment so that was about instance training time let's come back to the middle pillar as i said the middle pillar is all about tray is all about the training setup itself and by that i mean all the things that are in the hand when we decide how to how to create the training so what we see sometimes with early startups that they have quite a bit of sub optimized setups here so recommendations that i give them is start small use a small data set to first make a proof of concept create a baseline and use things like early stopping to not train longer than needed also there's a lot of other techniques in this bucket for example using transfer learning where you use an already pre-trained model for example from a hugging face hub and just fine tune it on your data set instead of training from scratch other aspects in here are for example making hyper parameter optimization not unnecessarily exhaustive and there's a couple of tricks that i'm going to link to in the descriptions for example on on amazon sagemaker you can use warm starts or reusing of the instances in your hyper parameter jobs or use tricks like bayesian optimization to reduce the number of hyper parameter jobs that are running the last one in the training setup category that i think is a really nice low hanging fruit is making sure you use an optimized and up-to-date machine learning framework so for example in this example just the difference between using the vanilla tensorflow version and using one that using the one that you can get when you use the amazon machine learning images and container that is optimized for the aws infrastructure you can get a 7.4 improvement of performance just by switching the machine learning framework without needing to make any code changes in your training setup so that was all about the training setup let's come back to number three which i said is the instance types or the purchase options whatever i mean by purchase options i mean instead of using your instances on demand right when you need it you can use spot training and for spot training you really use ec2 access capacity that is available the problem here is the spot instances can be reclaimed at any time so you should only use this for workloads that can be interrupted and start from checkpoints again and for training workloads that are not time critical as it can take a little bit longer for the training job to finish if spot interruptions occur but spot interruptions are a great way to run the same training job on on a much cheaper purchasing option the next one that falls into this category is really using the right instance so instance and hardware right sizing for your problem this means if really making sure that you use the smallest gpu in the smallest instance that is needed for your training job or using hardware that is optimized for your job so for example aws recently launched intel habana gaudi instance that have a specialized training chip that can provide 40 better price performance and i'm really excited to see it soon in the next weeks coming into general availability aws's own training jib the first chip from aws specifically built for training that's going to see probably amazing price performance super interesting um so it seems like you know we can control try to control time training setup uh instant start what are the potential saving costs that we can expect by performing some or all of these yeah that's a great question so for the time pillar the numbers that i typically see in startups is if they go from manually created instance to manage training jobs an improvement of 20 to even 60 because often these instances were running not very well utilized for in the training setup bucket it's really hard to put a number on that because there were so many different techniques that i mentioned but as we said this low hanging fruit of just switching to your machine learning optimized framework was a easy win with a 7.4 percent performance improvement on the instance type and purchase options for spot training you can get up to 90 percent cheaper costs over on demand so that's a big gain for right sizing your instance i would say something in the order of 30 is what i typically see and for these new hardware types they uh like the aws trainium or the intel habana gaudi training chip we are in the order of magnitude of 40 percent better price performance over traditional gpu instances so let me try to just make this a little bit clearer within uh with a with an example customer that i recently had so i recently got the chance to speak to the head of a male of an exciting young startup that uses machine learning for drug and compound discuss discovery so they are crunching huge amounts of this data on spark and on amazon emr and then they're training a traffic transformer model from scratch um on this data and they were using a powerful gpu instance on uh running jupyter notebook on it to run their training workload this allowed them to get started very quickly but created a lot of unnecessary costs as this instance was running around the clock so we decided to take an immediate step and enabling auto shutdown via lifecycle configuration on the jupyter notebook instance and created a plan for the midterm to switch from a manually managed created instance to having amazon sagemaker manage training jobs or using aws batch super interesting thanks for sharing the last example seems like there's a lot of options for cost saving when it comes to training uh some of them are really uh choosing the the optimized hardware or even just choosing to run on spot instances uh and much more i want to move to inference and and ask you jonathan if you could share which are the top three cost driver when it comes to inference so when it comes to inference performance or cost optimization we see multiple layers that we can touch that each one of them is independent on the other and they are all together contribute to the uh overall performance or the overall cost of the the inference workload we'll start from a cluster level optimization where we set the setting of how we do auto scaling how we work with the largest batches possible either doing automatic batching or setting the batch in advance and also considering options of running serverless inference instead of running standard vms so all those settings on the cluster level can reduce the amount of machines or the amount of compute resources that we are running at a time the next level the next layer that we're considering is the instance and runtime level optimization here we consider for example what is the type of instance that we are selecting what is the type of underlying hardware we can when where we can run gpus cpus or other inference hardware that is available in the cloud like the aws however and also considering what is the runtime optimization that we are using and how we leverage methods like uh graph compilation and quantization in order to get better performance for the model running on the given album this the quantization technique also reduces the model size that is running and can enable us to use multiple models on the same gpu for example and use multi model runtime inference workloads where we use one gpu for running two three or four models in parallel and share the resources the computer resources off and the gpu over those models and running all of them on the same node reducing the amount of hardware that we need to provision in order to enable that workload to run the last layer is the model level optimization where we want to improve the fun time performance or the memory footprint of the model in order to be able to get more queries per second or get more throughput from the available hardware that we provision for that we can use distillation in order to use smaller model with the same accuracy we can do uh model pruning especially channel pruning we can use dassi as a solution to model optimization and improving the performance the runtime performance and the model size of the model by automatic automatically designing better neural architectures and all of these can enable us to run for the same hardware more inference queries per second improve the throughput of the machine and improve the cost associated with inference so when we combine all those three layers starting from the cluster running to the instance and the runtime and ending up with the model level optimization we come to an end-to-end optimization uh that can improve significantly the cost and the performance of our inference workload that that's uh that's nice and and it seems like dassey had a lot of experience helping uh customers you know uh in inference efficiency so based on your experience what is the expected cost saving uh if you want to focus on inference efficiency one thing that we need to remember that each one of those layers are independent by the other at some sense so every saving that we can bring in one layer is multiplied by the saving that we can bring in another lid if we'll start from the cluster level optimization it's mostly a trade-off between the availability of computer run inference in low latency and the over provisioning or idle time of the computed reprovision and and spending cost on that and by setting better settings for the auto scaling for example and running in batches we can get um an improvement of about two weeks in the uh cost of this cluster without losing anything in terms of latency or availability of compute for for running inference on the instance runtime we can get two to four x uh based on the techniques and the methods that we leveraged from graph compilation to model quantization and also selecting the right hardware that will be best utilized by the model type that we're using and on the model level optimization we see something between two to four x uh by improving the model selecting or designing better newer architecture for one that modelling at inference time so if we multiply all of this together we can get more than 10x of uh inference performance improvement which sums up to more than 90 percent reduction in cost but obviously we can't do everything at day one and we need to decide where to start from and this is something that we'll later discuss sure i want to ask you julian for another example in our last call you mentioned some great customer examples that really illustrate cost saving uh at inference uh if you can share one of them yeah i would be happy to so this is an example that i'm also gonna link to full story for uh in the description below that is a customer called info jobs and two of my colleagues work with them info jobs um they build a platform to make the perfect match between uh candidates and employers and they're using natural language processing for things like automatically processing an applicant cv so they had a problem that they wanted to reduce inference costs but at the same time they didn't want to sacrifice on performance especially latency so they sat down with the team from aws with the machining specialist and tried a variety of approaches and as you can see in the chart below from starting on cpu instances with unoptimized code over going to optimizing this going to gpu instances going to gpu optimization going all the way to the inf1 instance which is aws custom inferential chip and compiling for this target hardware they were able to not only achieve 75 percent lower inference costs but at the same time reducing the prediction latency so really getting the best both of like more or less gpu power and cpu cost structure i think that's quite an astonishing yeah really impressive um so we covered the top three cost drivers in training as well as inference and you said there's some low-hanging fruits but in some cases you really need to decide where to to start um so i want to ask you what are your is your tips and and maybe others that you didn't mention when it comes to cost savings yeah as we already quickly mentioned it's always important to focus on the low hanging fruits especially as a startup if you're short on time so we mentioned things like switching to the right machine learning framework to the latest version and optimized version enabling auto shutdown these things are really quick to do so start with stuff like that and then prioritize the midterm cost savings based on where you really see your spending we focus this session today really on the the core driver for compute and instance costs but of course there can also be other cost optimizations in the machine learning space if you see that for example data storage is exploding so we will link in the description below to a blog that nicely compares various storage options and makes should make it easier for you to choose the right one for example between s3 efs or lustre fsx um yeah over to you unit on so yes i think that the first thing to assess is to understand where the money goes in terms of what are the layers that we need to optimize either it's the model level optimization or the runtime level optimization uh and where to start from because we can start doing everything at the startup and choosing one uh one layer to start and see how it impacts the the overall cost and i think that one of the questions is to check is what is the average utilization of the hardware that you we are using if the utilization is relatively low then we can think how we can provision more efficiently and improve the utilization of the hardware that we are already paying for but if we are in high utilization for that we need to think about other techniques um in improving what we can get uh from that hardware that we we use to get better throughput or better utilization of that in terms of queries per seconds uh of deep learning inference workloads so those will be the guidance for startups to see whether utilization is high or low and select which are the layers to start to optimize from so seems like that's that the first thing is really to identify where to invest time and where to start optimizing cost and i'm guessing if you have a better estimation of your cloud resources usage you can you might want to consider ml compute saving plans so these saving plans um you can then commit to one up to three years uh so 12 to 36 years yes sorry 12 to 36 months um and these ml uh uh compute saving plans will help you reduce cost by 64 percent up to 64 which is a huge uh cost saving um but again if it's all on top of first decide where to start then choose the the options to optimize then of course uh saving plans as another layer that you can utilize and to save cost i want to thank you all it was super interesting hearing your experience working with customers working with startups on cost saving and optimizing uh saving um and and performance so thank you jonathan thank you julian it was pleasure hosting you um on this ml cost optimization session thanks a lot for having us thank you very much and thank you all for listening uh please see the link uh the links below we are sharing more technical content um in the video description so a lot of pointers to to more uh technical content that will help you you know make a better decisions about uh storage about compute the right hardware and things like that also some benchmark use cases and keep an eye out for an upcoming ask me anything session that we will host where you can ask us questions and we will do our best to help you with a specific and concrete answers to your challenges when it comes to machine learning cost saving thank you bye"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"Cost Optimization for ML Systems",
        "Video Title":"Akamas: Using ML to automatically optimize Kubernetes for cost efficiency &amp; reliability",
        "URL":"https:\/\/www.youtube.com\/watch?v=AUAEeyu2-ec",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/AUAEeyu2-ec\/hqdefault.jpg",
        "ID":"AUAEeyu2-ec",
        "Publish Time":"2022-03-22T11:51:27Z",
        "Channel":"Akamas",
        "Channel ID":"UCYnKz2RfYkEto9Fk7fI6XxQ",
        "Transcript":"[Music] hi everybody and it's great to be here thanks a lot for having me today so that's the agenda for today session basically we will start with a quick introduction to kubernetes and what are the main factors that we need to consider when we and we need to ensure that kubernetes applications are reliable cost efficient and performant and then we will look at the actual the resource management mechanisms that connect is put in place to ensure that that application are correctly managed and deliver the current level of fully service and reliability we'll also look at the challenges that uh arise from making sure that grenada's applications are highly performant cost efficient and reliable and then we will see a new approach that we i cannot have created using machine learning to actually get rid of all those kind of challenges and automate the entire optimization process for kubernetes applications we'll then have a look at a real-world example of using this approach in optimizing a real-life kubernetes service based application and then we will conclude with scientific way just a few words about myself so i'm the co-founder and cto at akama so i have close to 15 years of experience in performance engineering capacity planning capacity management and basically that that's it so performance engineering has always been my passion and area of interest so let's start with a quick introduction to kubernetes so i wanted to actually start with a a funny story about how actually kubernetes was born so connected actually came out from google which always said immense challenges to actually optimize their own and their own application high traffic application like you know smart google maps and gmail so since the beginning actually google and to invent a way to actually ensure those highly scalable applications were currently in a way manager who fleets of hundreds and hundreds of servers so the idea it was to create actually a kind of platform a highly distributed platform that was able to guarantee the scalability that those kind of eye traffic application required so the internal name for this new kind of platform that were scheduling this kind of container based application was bart berg and it was very very interesting to notice that it was kind of secret sauce you know within google and um it was considered one of the key reasons for for google actually being able to deliver this kind of high traffic application in a cost effective manner and it's very interesting that in 2013 actually a couple of google engineers actually wanted to um in a way open source this kind of secret salt system and at the beginning actually this from the story that we can hear here in in this blog post from google itself actually the idea was not considered very good actually from the internal you know leadership team at google but eventually the the team had a green light so the kubernetes uh project was born and they and they started on actually creating the new version uh in a kind of open source fashion so that's that's the way actually kubernetes was born i think it was very interesting by the way they recently also shared a kubernetes documentary in a two part which actually says kind of tell the story in a deeper detail which i really definitely recommend watching so that's that's how kubernetes was born of course kubernetes now is being actually widely adopted across pretty much across every enterprise for quite a variety of reasons very very good reasons of course kubernetes gives automated anyway deployment it gives actually abstract infrastructure so it gives workflow portability across cloud providers and on-prem data centers one of the things that i wanted to focus on is on of course on the performance side of it so one of the main values is that of course kubernetes takes care of scheduling all our application containerized application into potentially hundreds or thousands of worker nodes to make the best use of available infrastructure resources so to increase to densely pack in a way all those conve containers into the machines in the underlying machines but at the same time it has been thing capabilities actually to ensure that those applications do not uh in a way um interfere among each other so they could write the provides cavalry to actually guarantee performance isolation among applications so that's that's the kind of capability that the platform provides the kind of a problem of promise that kubernetes is making on application developers which is actually very interesting and very compelling so what is very interesting is that if you look at the adoption of kubernetes the one report that came out last year from the phenom organization uh is that kubernetes costs are actually on the rise so the this report actually did a survey in among very also very mature organizations that are actually adopting kubernetes and what they found out is that of course the pretty much the vast majority of the respondents actually reported that kubernetes costs are on the rise pretty significantly also close to 20 percent and here over here and what is very interesting is that what are the reasons for this cost increases so the report of course um comment about the fact that of course kubernetes is on the rise the adoption so it's pretty natural that costs are increasing but also they are actually you know considering the fact that there may be quite a bit of quite a bit of inefficiency in how people is managing kubernetes today so part of the reason why those costs are so highly rising is that of course it's not very easy actually despite all those capabilities that the platform provides to make sure that applications are run you know in the cost efficient way possible and that's that's one of the in a way interesting uh data point that we are starting to see on the market oh and of course we will discuss a little bit more in the when we talk about in the resource management uh how that is why that is the case but it's not only cost actually so there if you look at the actual the early adopters of kubernetes again very quite a bit of very mature and highly skilled organizations in the silicon valley area most of them actually we see that they have they experienced quite a bit also of performance and reliability issues in the kubernetes in the applications running on kubernetes so they even created this very interesting website which is called kubernetes failure stories it was created by a zamando engineer actually which actually was meant to in a way kind of share all the incidents and failures that people had while running on kubernetes with the hope of actually learning from each of them and improving the status of of the platform itself and what is very interesting is that again we will see the reason why you know in a bit but due to the way that kubernetes works in a way lots of engineers and lots of application developers found some pretty counter-intuitive situation where the application performance actually suffered like for for example high latency or cpu throttling so lots of access to this underlying platform resources and it's not just performance issues but it's also stability issues that may arise like for example out of memory keys it's one of the reasons where we see what it means you know in a couple of flights actually so the fact that application may get killed while they are running if they you know consume too much resources and that is another kind of potential uh in a way outcomes of not properly configuring our application kubernetes so that's kind of the uh the issue so we see that people are facing both costs on one side cost issues and on the other side performance and stability issues when running application on on kubernetes so the key question at this moment is actually what is the reason why it is so so why kubernetes that provides such a great set of capabilities for running high skills containerizing a native application white people is you know we find it difficult to actually make sure the applications are actually cost efficient and reliable and highly performant that's the main the main question so in order to answer this question we need to actually uh go a little bit deeper into how kubernetes actually is managing the resources of the containerized applications so let's have a look at some key facts here so the first key fact which is uh very interesting to understand is the concept of resource request so basically resource requests are the resources that a container is guaranteed to get so when an application developer needs to run a kubernetes application it has to define this yaml file which is kind of represented on the left here where it has the ability to say okay for for cpu memory how much resources should this container always get so as you can see in this example this is an nginx container within a given part which is the unit of scheduling within kubernetes and for this nginx container which is our application in this example basically we are requesting two cpus and two gig of memory so when we we speak we ask kubernetes to schedule this container kubernetes will actually consider this amount of resources for in order to understand where to run this part when to run this container across the infrastructure so the as you can see in this example accredited schedule we'll find a node let's say that we have a node with four cpu and eight gig of memory which initially is empty so we'll find a node that actually uh is able to host this this amount of resources so i know that has two cpus two free cpus and two giveaway of memory still available so in this case in this example let's say that for they get scheduled on this node so let's now assume that another pod comes and needs to be scheduled uh again let's let's assume that these are the brother again require two cpus and this time it requires four gig of memory so this spot b you can see in the picture it's the blue one is again it may be scheduled on the same note as the node still has some capacity available from the cpu and memory perspective so in this situation actually if another part should come let's say podc with pretty much the same amount of resources it won't be scalar on this node because actually as you can see there are no more cpu resources left so kubernetes we find a way to find other nodes other infrastructure that has this kind of level of capacity required that are still free and if it is not able to ma to identify a new node basically won't be scheduled on the cluster so what is the uh anyway the main takeaway here uh is that of course resource requests are very very important from a cluster capacity management perspective because actually it's a key concept that the kubernetes itself is considering in order to understand when and where actually it should actually locate upon so the other consequence of that is there is no really over commitment on this on on the resource request so with respect to other technologies like we might have in mind like virtualization where actually we might schedule many more virtual machines on the same with many more viewport cpus for example with respect to the uh the number of physical cpu that were available in the host that's that's not really how kubernetes was designed to work so actually there is no other commitment as we got the request so once we have you know we allocated the number of available capacity sets of requests there is really no more uh you know a normal application that can be scheduled on on the cluster the other consequence that we of this fact is that the whole topic of kubernetes capacity management is basically done via resource request so it's basically done via a kind of allocation mechanism so resource requests are the amount of resources that the pod actually allocates actually require and it's totally unrelated to the amount of resources that the application actually you run time so what does that mean is that for example we might very very easily end up in a situation where our classes is actually fully allocated in terms of requests so there is really no more space for to allocate more application but at the same time the cluster may be highly under utilized so it may be you know even a one percent utilization of cpu while still from a community management scheduling part of you the the cluster could be will be considered a pretty much capacity for so that's one of the uh you know a key concept because actually based on the or how we allocate the resources of the pod especially related to requests of course the that's one of the main way that we can drive uh and we can manage also the cost of running our application which we have as we have seen is one of the issues that people using kubernetes is experiencing today so the other uh concept which is very interesting is that it's a resource limit so as we have seen resource requests uh are you know a kind of allocated amount of resources that we require to be available in the cluster but in actually the container can consume more resources that they request so because the resource limit is actually the way that kubernetes provides application developers to specify the maximum resource that a container can use so when a container actually when we say for example cpu limits equal to this means that the container can already consume more than two cpus and if it tries to do so actually um if if a container try to actually go above the resource limits bad things can happen and what does it happen is actually a function of the kind of resources that is being utilized so if we are talking about cpu so if a container actually tries to use more cpus than the the limits basically kubernetes starts to throttle the container cpu so this means that actually the container will see a kind of a cpu slow down so the access to the cpu will be actually you know kind of paused for for for a moment let's say we will see how that what does that mean in the next slide and the result of this uh this effect will it will be most likely an application performance go down the other thing that is very interesting which is kind of uh more important in our way more as we got the application stability at least is what is happening what the application starts with limits in terms of related to the memory so why memory usage actually start to to approach and hit the limit kubernetes will actually uh adopt a pretty drastic way kind of measure which basically would mean that the kubernetes will kill the container so basically uh again that's pretty in a way counter or unexpected if you think about the virtualization world where uh or traditional operating system memory management where this kind of situation is being treated with a kind of swapping mechanism so basically again the application will slow down but it will still be in a way alive depending on the severity of the situation of course but koreans actually policy is kind of more strict here so as soon as the amount of resources is that is being used the amount of memory being used hits the limit the application will be um will be killed so again this can have pretty significant stability issues for the for the application itself the fact number three that i wanted to i like here is the kind of uh in a way effects of the cpu limits that we had talked about before which actually it's a kind of very interesting and somewhat unexpected uh effect that can uh that can result with by with especially with kubernetes i would say it's kind of not that well documented there are some way information on on the web about this effect but i wanted to stress it out here because it's very very interesting so we have mentioned the fact that when cpu user in a way approaches the limit the kubernetes starts slowing down the to throttle the cpu the cpu access basically but what is very interesting is that of course when this kind of phenomenon is starting to happen so it's happening like i don't know why when the cpu is uh i don't know 90 percent close to their limits or is it happening before so we have started this this um this approach with this process in our labs and we have done lots of benchmarking with different applications to understand when the cpu throughout things start to happen uh also studying different kind of applications with respect to the cpu limits so we measure the cpu usage we did lots of benchmarking with increasing level of cpu usage with respect to the limit and and then at the same time we measure cpu throttling and what we we discovered was pretty pretty interesting and unexpected is that tpu surrounding can happen pretty early so when cpu usage is up to 30 percent of cpu of the cpu limit so actually that was not what we expected we're setting much more something like 80 percent so when we are getting close we start to being struggling but instead we we have measured that cyber throttling can happen pretty early in this process and that can of course be a source of kind of unexpected and sudden latency spikes for example in kubernetes application which is uh actually something that we see a lot also in our customers and it's something that that might be kind of hard to explain by looking also at the observability tools but that's that's one of the main kind of reasons why is that happening is due to the the way that people throttling is managed at the linux kernel level we won't go deep here but um feel free to reach out if you if you want to know more and that's that's on the right you can see an example where actually people is have tried to uh actually remove the limits and see what was happening to the service latency that's an example that was published by bucker inc which is an internet company basically that is running their services on kubernetes and what you can see here is that the latency of the service over time and then you can see that basically the latency starts to drop near zero when they actually remove the cpu limit so the the thing is that cpu limit seems to have this kind of pretty important effect on performance uh especially high percentage internal latency so the main question that would that arise in our mind would be actually should we get rid of cpu images so why don't we simply run we why don't we simply use cpu request which is important to let the cpu the kubernetes schedule understand the amount of resources that we need and we won't actually use cpu limits we get rid of limits actually that's actually not the best practice and here you can see that also google recommends always setting both requests and limits because actually limits if you remember about the key point of kubernetes is ensuring on one side efficiency but also performance isolation around among the application so if we run without limits if a single runaway container may in fact they may basically bring down our most business critical application uh without anyone noticing so it's it's a very it's very dangerous so for the stability of the entire quick message platform it's it's actually very important to both to set both requests and limits and that's again a best practice that also google recommends so basically we are left with this kind of challenge of actually properly setting resource requests and limits on one side to uh of course to not not to waste money because we might over allocate the resources on the cluster and on the other side not actually have performance and stability issues um they may reside if those configurations are not set properly well another interesting fact that i wanted to bring up here is that of course those we as we have seen we have in a way we have seen that those kind of settings if you request the limits are very very important from basically for the success of running kubernetes applications but on the other side on the other side uh basically it's very very hard for for people to understand how to set those parameters so the the answer from the kubernetes project and that we see on again as a key recommendation is to to help users set those kind of parameters is to leverage kubernetes auto scaler so in particular the vertica kubernetes provides true i out actually more than one more than two auto scalers as we as regard the setting of the pod requests and limits the auto scada is that kubernetes provides out of the box is called the vpa which stands for vertical pod autoscaler vertical means the amount of resources that a single uh a single container is allocated they you may remember also that kubernetes also provide the horizontal palatal scaler which has more the uh in a way the goal of ensuring auto scanning which is replicating the the part uh based on the traffic which is not noticeable for for the moment in our in in our story so let's focus on the vpa so the vpa basically provides a way to uh in a way suggest the recommended cpu memory requests for a pod based on the actual usage that depart actually showed the uh measure that the gpa can measure while the product is running in production so the thing is that uh we we have kind of mixed experience with the vpa so and that that is mirroring also what the the industry is finding out and i wanted actually to highlight here um a kind of experience that we did where we we basically took a microservice a container running a microservice application in kubernetes which were you know it was subject to um a typical production traffic so daily the journal pattern uh as you can see on the service drupal chart here on the top left on top right so that that's the traffic that we actually injected to these uh we sent we generated to this pod and you can see here on the top left the rest of time of this mod so you can see basically 90 percentile latency on the uh the other chart here shows the actual cpu request and memory requests that were actually assigned to this board and here on the on on the bottom uh right you can see the resulting cpu utilization and memory utilization which is actually the actual usage for example of the cpus with respect to the limits which were set in this case equal to the cpu request so the key thing to notice is that we in the first period we actually run this we this load test where the vpa was turned off so it's basically the the container had the 200 millicourse assigned um for you know for for the cpu and close to uh five uh 450 meg for the memory so that was the original in a location of request and limit and uh cpu memory requests for the container by the way the limits were set basically equal and you can see actually the kind of the yes the load that was kind of being breached a little bit when the traffic was uh was uh was higher and then what is very interesting is that we turn it on in this in this moment we turn it on the vpa so the vpa actually looked at the past utilization of the of this container and then actually based on the utilization vpa decided that the container was kind of overlocated so pretty much cutting off the the amount of cpu request that was assigned to the to this point and pretty much did pretty similarly to the pre similar to the cpu to the memory request it was actually reduced close to half what is very interesting is that as a result of this of this decision which was pretty purely done looking at cpu utilization of the container over time as you can see the latency of the of the container pretty much skyrocket so that's by the way i look at it mix axis so the container the latency response time p90 now is now close to one second while it was i don't know 128 milliseconds before so close to 10x uh worst performance as a result of the the suggestion that the vpa did so the vpa did actually it's a kind of initial answer from the out of the box that is available but we are finding out that it's kind of not working very well to ensure performance of the application cost efficiency and reliability so that that actually concludes pretty much the challenges uh that we see on kubernetes uh so we're setting properly settings requests properly setting the source limits also the replicas in auto scaling we didn't touch on this point but also that's another area where you need to actually pay a lot of attention of the configuration which kind of matches you use for other scaling which kind of threshold and that for each container you typically not only have the kubernetes setting but you also have the application settings like the jvm uh if you if you are running a java application uh for example which kind of gambes collection are you using uh which kind of hip settings do you have there but also maybe golanga settings go gc for example it's another similar tunable that can provide interesting benefits for for golang application but you may also have application specific configuration like the application server etc so the key thing to realize that of course moving from a kind of monolithic architecture to a microservice application application architecture you know you will have dozens of microservice all interacting in order to provide a kind of business service with its own performance and availability slos so the real life optimization challenges become actually even anyway even harder with respect to what we have seen till now where we have looked just at one kubernetes container but we need to re-multiply that uh according to a number of the i don't know tens of micro services that may be running our business critical application so that's that's that's the challenges that series and performance engineers and developers have today and to this today these kind of challenges are pretty much solved manually with a lot of try and other approaches perhaps doing load testing and understanding what are the belt necks or other other times just observing with observability tools how the applications are running in production and trying to to optimize and to improve their resilience etc the the process that we use today is highly highly manual of course it's time consuming is complex because actually the problem is very you know very very complex so the idea that we have at rcms is actually to try to bring these new uh these capabilities these processes of optimization to kind of new levels where actually um we kind of move away from a world where we as experts as performance engineers as slv and developers try to fiddle with the stack and try to tune all those configurations manually and try to see what is the effect with a new world where actually we have autonomous systems that are actually much smarter than us and actually are able to optimize the application you know kind of more much more effective and time efficient and cost efficient way so this approach uh is based on a kind of four new requirements that we that we have identified so on one side it's full stack meaning that it's not just about tuning the resource request of i don't know one layer of kubernetes just one layer in the stack we also have the runtime we also we may even have databases configuration we may have big data configuration we may have i don't know instance types on on the cloud that we might choose from so it's the the whole stack is actually composed by main ladies and if you really wanted to solve the problem we don't just need to in a way to to solve the um to properly optimize just one layer but it is important to actually optimize and to end the entire stack because it's it's what our uh actually at the end of the day our enterprise application runs on top of the entire stack so we need to make sure that the all the layers are optimized in a kind of holistic way smart exploration is important because if you think about a number of knobs and configuration options that we have in the stack it's huge as we are talking about and they are all interacting among each other so it's very complex for the human brain to reason about this space so the idea is that we need kind of help you know in a way to explore this huge space on configuration and that's where we use the ai to actually being able to identify the best configuration in the in the most cost effective and time efficient way possible the third pillar which is very important is that our optimization is goal oriented meaning that the goal for this optimization can be defined by the user so it's not just about i don't know making kubernetes more efficient but sometimes our goal is let's improve our reliability of the application let's make sure that we we have the minimal amount of cost on the cloud while still matching performance requirements and that's the loss that we have on throughput and whatever rate and response time something like that so the the goal of the of the optimization is more complex than just simply let's reduce cpu the allocated cpus because all those parameters at the end of the day impact those application performance so we must have a way actually to define what is our end goal which is realistic for for for the needs of modern performance engineers and the series the first pillar is pretty uh important because also we wanted to automate as much as possible the entire optimization process as we are going to see um it's just about a platform saying okay you are gonna uh and you are getting close to the amount of memory assigned to your container you need to find out to do something but our platform aims at actually suggesting and also autonomously fixing the configuration problems and apply the configuration parameters to the to the live system so we are going to talk a little bit more how this approach works in a in a minute so that the entire process gets automated as much as possible with the proper level of control of course for for the humans for the sres that need to for example approve the configurations etc so that's the high level approach so at sms we have actually designed an optimization platform that we call a autonomous optimization platform that can actually support uh that implements this kind of new approach uh um with a kind of autonomous optimization process like we say so it's not uh in a way let's move away from a world where engineers tune this process these applications manually and see the results kind of time-consuming error prone and lengthy process to a world where we have an autonomous system that is based on that is capable of providing these optimization services that can run automatically based on the objectives that the user can define what a typical important question is do we work on reproduction or do we work in production system the answer is that we support both scenarios so you can see here the practice optimization is what the the kind of optimization process that we typically put in place before the application gets released in production so that might include scenarios different scenarios where for example i have a change in the architecture i want to upgrade my change my cloud provider i want to upgrade my technology layers or even i want to evaluate a new design of the application let's try to make sure that my application has the proper configuration optimized for this new change maybe for example a i don't know a jvm change an upgrade on open jk or change in the in a library pretty critical library etc i wanted to make sure that all my configurations of the application are optimized before it is into production other scenarios maybe i am expecting a peak in the workload the typical example is the bright friday how do i make sure that my application is correctly configured to handle this kind of peak actually before the event occurs so i may stress i may leverage load testing in in pre-production environments and leverage load testing and approach like acomas to make sure that our configuration for the auto scaling for the containers for the jvm for the replicas whatever you need are properly configured to make sure that this this will uh happens mostly other typical scenarios maybe also the calcium engineering where that's typical that's a big trend today because engineering we might for example highlight an area where we might focus in order to to improve the resiliency of my application for example we need to set up more replicas or we need to set up the auto scanning system to make sure that application can sustain load in in the face of failures of course this comes out and this implies being able to select properly configure those elements you know of the other scaling of the runtime etc and that's where atoms can help automatic all those configurations the live optimization is something uh very also very very interesting because we can also in a way optimize the application when they have been released in production so when they run when they are subjected to the real-life user traffic and that that's very interesting very exciting uh cabbage that that we um we are offering so in this case we for kubernetes application for example we may adjust kubernetes resource limits and also the runtime like the jvm based on the traffic that is running on on on the application so it becomes much more in a way dynamic and runtime in the that can be also based on a human approval process so the user can be can evaluate a recommendation that can that are in a way provided by the platform they can review it before applying to the production environment or even the next stage for us is actually platform it will be fully autonomous and that once the user in a way gets accustomed to the to the recommendation he can actually switch to a fully autonomous mode where akamas is constantly optimizing the the application live in production as they're being subjected in to the different traffic that is happening so what that's the general idea of the platform so let's now see uh how actually ml uh comes in because actually we this this our optimization cycle is based on five phases so the first phase is we need to apply it a configuration so facebook number one of course is not only in a way to the suggested configuration in a dashboard but it's it's it's aiming at automating the entire process so the first step is really we need to apply a configuration that gets suggested to the system so again we can do that by leveraging whatever configuration management tools that you may already have like terraform and cyborg uh kubernetes apis etc so that's that's the idea we don't have any agent but we rely on what configuration management tools typically customers already have second step is what is called here applied workload so based on the scenarios if you have move if you are working in pre-prod environment that may want that maybe in triggering a load test that replicates a realistic workload so that application is stressed properly and we can see and we can actually measure if a new configuration is in a way is improving our goals or we it is actually making worse and how they it needs to be adjusted but while we are running in production basically we don't need any application and any work of them to be applied because actually we simply observe the application as it is running in production while it is being it is processing the real-life user workload step number three we collect the key performance indicator which means the performance matrix and all the methods that we we are interested in again no we don't provide any agents but the idea is that we collect uh metrics from the typical observability tools and monitoring tools that customers already have like for example diana trace or prometheus and the first step is that once we collect all those metrics remember arguments is a good reason to a solution meaning that of course you can define your goal which may be even a combination of metrics like for example uh efficiency metrics like throughput divided by cpu usage so the first phase is where those goals and those metrics are those the experiment gets curved and the fifth phase is where really machine learning kicks in so we won't deep dive into the machine learning piece in this session but i wanna just to comment a little bit on the kind of approach that we use because it's a typical question that we receive we use reinforcement learning so meaning that of course our yeah and its machine learning needs to actually interact with the environment so it actually um applies configuration so it changes the environment it applies the configuration to the environment then we measure the result which is called the reward in the machine learning the refresh and learning language and then actually we decide what is the best configuration the next best configuration to apply so the process is kind of it's iterative up to a kind of budget that can be defined and the machine learning actually continuously learn how the the system needs to be optimized even with creating the presence of dozens of parameters the machine learning can understand what are the best parameters to tune what are the best regions of this use space which which we might visit in order to improve the performance or residency whatever so in this new process it's important to actually understand that of course the the performance engineers the sre and the developer is still in the picture so it's not something like a magic system that automates everything but the role of performance engineer develop against a is still key because of course they need to define the optimization goal and to define the constraints like i have my slos response timer rates that i don't wanna in a way to to make worse etc and then the system can can actually automatically tune the the target system as as we we have described and the output is really actually an optimized configuration that in the case of reproduction environment we can later on promote to to production or in the case of live optimization it's basically automatically applied to the to the um to the system so the result that we get is that our systems are constantly being optimized in in order to ensure that they are cost efficient early performance reliable uh according to the goals so that's the approach at the high level so i want to also to provide you a real life example to let you better understand how this process actually uh plays out in a kind of concrete example so the example that that i wanted to show you here is based on google online boutique which is a typical micro service application by google so it's basically composed by 10 micro services that feature a modern software stack the application is open source and it also includes within the reaper you can also find a load generator which is very handy based on locus which basically is able to generate really kind of realistic workloads and that are basically users that are visiting and doing purchases on this store so the architecture is basically uh shown here so you have in the center part of this picture you have the online boutique application which is again time microservices running on kubernetes as we got the optimization you can see that we are optimizing in this example with our command 22 parameters which are basically the cpu and memory limits for each microservice so they are 22 and on try and not 20 because actually we have 11 containers because there is also a database in there in one of the microservices so basically we have 22 parameters that is the scope of the tuning and within the application has it is being subjected by real-life workloads by lotus as we have mentioned as we got the way that we change the parameters we leverage basically kubernetes api so basically we change the deployments of those microservices where you have those uh those values gridded in and then we also measure the service the microservice performance and the actual resource usage leveraging basically permissions and issue that's that's the idea and what is very important is that when we do optimization with this kind of a new approaches the capabilities you know the clarity really defined the goal that we wanted to achieve as we have said so the nature of arkham as being all driven you can see it here in a way in practice so in this case we wanted to maximize the cost efficiency of the application meaning that we wanted to maximize the throughput as you can see the matrix is that we're measuring the front end which is where all requests are are being processed so you have the success transaction processed by the front end as measured by by east which is a service mesh running in kubernetes and we divide that by the application cost the cost is basically what is the cost of running this application on the cloud so it's related of course to how big are the containers which is the the parameters that acomas is tuning what is the interesting is that we also set constraints because of course we don't want to make the application has lower or we don't want to make the user see failures so again you can see the power of the gold rate and nature because we we say okay we want our application cost less but we don't want to have error so we don't want to have higher risk of time so that's that's the way the our goal driven approach works and you can see here basically the result once you have defined those kind of high level within acomas you can press play and not gonna start tuning the the platform and you can see in these tracks basically the the progress of this optimization over time so each each dot in this chart is a different experiment as we call it so it's basically a different configuration of the parameters so remember the 22 parameters cpu memory for each container each dot is a single in a way a location of cpu memory for all the containers at the same time so machine learning is able to change them at the same time as you can see that the first dot is the zero percent is basically the baseline configuration so the initial configuration of those container as per the open source project and you can see that over time basically the score starts to improve so the score again remember is throughput divided by the cloud cost so we wanted this number to be higher so we want to maximize and you can see that we reached that seven pretty significant 77 percent improvement in the cost efficiency of the application just by touching the configuration options and by the way without any performance degradation because each dot actually is respecting our slos in terms of response time and errors and you can see basically that uh 24 the whole optimization process was also very fast so it took uh the platform about 24 hours to completely autonomously or between the 22 basically containers uh sorry 11 containers running in this application so this this is pretty pretty interesting we refer to the manual process that we typically use if you look at the best configuration sure um so about the previous slide so did you measure the cost for actually um doing this machine learning based approach yeah that's a good great question yeah we we can do that actually you can decide the the kind of time the budget that you leverage that you assign to the optimization so actually you can actually for example say i want to just run this weekend i want to run for this amount of iteration so you you have these ways to actually in a way limit the time and the the badges that you use for the optimization process of course it's typically very in a way a good roi exercise because once we have found out the configuration that that's what the application runs in production so it's where the bigger biggest savings actually may be realized okay so that's the optimal resources so how can actually i can achieve this kind of result you can see here for each container actually basically uh i have reconfigured the number of cpu memory uh requesting limits you can see basically here the true color of the baseline the orange one and the green one it's the best so what is very interesting is that uh machine learning and atomizer autonomously learned that in order to improve the performance and the cost efficiency it had to pretty much decrease the cpu limit set for almost all containers and that is very kind of a smart move because cpu of course is the biggest cost in a way ratio magnesium driver of running the application on the cloud so actually machine learning automatically learned learning that during experiments but as you can see also machine learning didn't simply increase decrease the cpu because that would lead to a violation of the slos in fact actually for true specific microservice machine learning actually figured out that it had to increase the amount of cpus because for example they may become bottlenecks and violate our our slos but it's important to realize that that with this process is basically that's the best configuration that we can get and we can analyze for example before applying to production without having spent pretty much any any time doing this kind of process ourselves so pretty much across them maybe um have been around for 24 hours and at the end of the optimization process the performance engineer or application developers get this kind of picture that is the result of this uh obvious optimization that is still matching the slos and improving the cost so what is also interesting is to see what is about the performance of the time of the application when we apply this new configuration so you can see here two charts that are coming from directly from acomas where we compare the overtime the baseline and the best configuration in terms of service output on the left and the rest of time on the right so basically those are two experiments you can see it's about half an hour performance test and you can see the transaction per second at the baseline which which is the original configuration and you can see that despite this configuration improving the cost it also increased the the success basically by 29 by 19 you as you can see the experiment under 31 which is actually the best experiment is improving the performance over time but it's also very interesting to see the improvement on the latency so the original configuration actually had a pretty erratic behavior in terms of latency i must find out that actually by reconfiguring all those options the the the performance of the entire application end-to-end application was uh was made much better uh by adjusting those parameters as well so that that's that's about it so that i want to just to conclude a few takeaways and then we'll leave space for for your question which i'm very happy to answer so the key takeaway is that of course it is getting more and more complex that that's what we all know and in particular kubernetes application are actually not the exception quite quite the opposite meaning that lots of teams are earning hard time to actually make sure the application increments are actually cost efficient reliable and performant with respect to the traditional monoliths so that's that's an important challenge that that we are working on and we are seeing on the market our new approach is basically a way to actually automate uh the process that is today in largely manual uh towards a kind of new approach that we call autonomous performance optimization which is leveraging machine learning actually as a way to not substitute but actually as a additional tools in the performance engineer or actually sre developers toolbag and actually to to improve the situation and and and the uh the actual benefits that actually with these kind of processes we we can really make sure our services are cost um in a way optimized for cost efficiency for throughput so they can consider of course we can actually make sure that we deliver those services with the best uh possible reliability by incorporating also our slos and the beauty of that is that the entire process becomes largely automated so we don't spend a huge effort and uh in a way lots of resources of the typically application teams for for doing these kind of exercises this is it just really actually i forgot that we have one last slide uh you if people is interested in um akamascan you can try out archives we have a free trial in our website with kind of a lot of exercises that and the service guys that you can use with a kind of step-by-step approach to try out the platform in in your own environment so that is it and thanks all for for your time and i'm i'm happy to answer any question you may have thank you very much stefano and thank you for everyone for joining last chance to ask questions okay so let us thank the speaker and thank you thank you stefano so very interesting thanks a lot again thanks for having me really really enjoy the session talk to you soon guys bye thank you bye"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"ML Technical Debt Management",
        "Video Title":"Machine Learning Technical Debt",
        "URL":"https:\/\/www.youtube.com\/watch?v=YVZGPUM6Kcs",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/YVZGPUM6Kcs\/hqdefault.jpg",
        "ID":"YVZGPUM6Kcs",
        "Publish Time":"2020-11-25T12:00:24Z",
        "Channel":"The Tesseract Academy",
        "Channel ID":"UCAJJVgH9xZCzGw6SwAWDCiw",
        "Transcript":""
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"ML Technical Debt Management",
        "Video Title":"Laszlo Sragner - Clean Architecture: How to Structure Your ML Projects to Reduce Technical Debt",
        "URL":"https:\/\/www.youtube.com\/watch?v=QXfsS-ZOeyA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/QXfsS-ZOeyA\/hqdefault.jpg",
        "ID":"QXfsS-ZOeyA",
        "Publish Time":"2022-07-09T05:00:13Z",
        "Channel":"PyData",
        "Channel ID":"UCOjD18EJYcsBog4IozkF_7w",
        "Transcript":"very much for turning up my talk it's almost full house uh it's pretty amazing thank you very much again i would like to start my talk uh with a quote from 18th century uh prussian general and military theorists from klaushevitz machine learning is the continuation of software engineering by other means we are going to see in this presentation that we can borrow a lot from software engineering applied to machine learning to make ourselves more productive the title of my talk is clean architecture how to structure your ml projects to reduce technical debt first of all we are going to see how technical debt ruins your productivity uh we will look at why machine learning products particularly prone to collect technical debt and a couple of relevant software engineering concepts um like decoupling refactoring and design patterns to help yourself avoid this about me about me but my past i have a finance background mobile gaming and i was head of data science at a london fintech startup now i teach data scientists to write better code also i am a startup consultant please check out my blog at laszlo.substance.com um what do we mean by ml products it's important to sorry for uh why is it um should i give it a try okay cool sorry uh i was just mixed up and so let's look at what do we mean by ml products it's important to understand that mlops is not a ml product mlops is an infrastructure that you are using to facilitate ml products and it's also not data analyzes data analyzers have a fixed starting point when you are gathering the data and then it has a relatively short period of lifetime when you are handing back your results to the stakeholders even that applies if you're using machine learning techniques in advanced analytics situation that still not create an ml product um ml products are complex business processes that are usually described by input output relationships end to end and they work usually autonomously they don't require particular amount of manual work to to facilitate the operation and they usually have a long lifespan and one of the reasons why dealing with technical debt refactoring investing into quality code is a highled activity because you can yield this back over a long period of time over the entire lifecycle of this ml product what does technical that why does technical that matter um ml products usually solve complicated business processes that are very fuzzy in nature they're not described by rules they are described by edge cases and ad hoc rule of thumbs and this is why usually rule-based systems are not a good baseline for machine learning products and they are also inherently unspecifiable it means that no matter how careful you are specifying your ml product there is going to be aspects of the work and features that you are going to find out about while your product is in production why because of the fuzzy nature of every business process you know if it would be easy to specify you can write rules and these rules would solve your problem and not only that but also um they need continuous updates you know the the context uh they are operating changes the environment the business needs are changing uh you know this is particularly important in data drift uh so you need to by definition prepare for change and changing creates a technical debt let's take a look at the ml lifecycle and how that impacts data take that in machine learning products um classically there is five steps of an ml lifecycle you have eda like explorer data analyzes when you are concerned about the data proof of concept when you are concerned about the models in the mvp stage you are doing productionization and fine tuning in a b testing and classically you think of business as usual as um as an important place for monitoring so why this is a not a particularly good structure and often the first two steps are not even done by the same people than the rest you know the first two steps are done by data scientists and then from the productionization is done by machine learning engineers the translation between them creates a friction that is slowing down the iteration speed when you are analyzing a b tests you know these are necessarily fed back to the early stages uh because only the data scientists are able to collate this knowledge into one place they need to take this knowledge and update the machine learning engineers to update the project this is a high friction uh technique the and which unable to run at a high speed iteration which means that changes can accumulate before they are translated into production which means that two steps of the same version we're gonna go further away from each other um and that means that uh sooner or later uh each version update they're gonna be a big deal and it can create a business continuity problem so it means that you need to plan for it and it's even longer and often what you see in machine learning products is instead of setting up a continuous integration or a continuous update pipeline rather they just scratch the previous version and then write a new one from scratch you know this is clearly not an agile way of solving problem which we know that it is fuzzy in nature and changing as we go along uh what's a different difference between technical uh debt and technical mass so uh it's a it's a specific term for for what you sometimes experience it's important that lack of specification documentation and infrastructure is not technical that you are just haven't finished particular aspects of your work it's also not missing features you know if you are missing something you just need to add it you don't um address that as technical that and it's also not broken models if your model is is drifting you know you just need to update it and that that's the right term for that it is uh drifted and most importantly it is not bad practice uh technical debt and referring to certain practices as creating technical debt is not a license to create bad practice and it's a very important term to use technical mess for these expressions uh because it otherwise the technical debt would give you a license to get away for doing technical mess so what is technical that technical debt is an attempt to gain knowledge with a plan to correct it later um just like when you are uh getting into that and here i would refer to corporate that not credit card that credit card that is off is always a a bad idea but corporate debt is a natural way of facilitating a business with if you have a plan to pay it back you know if you don't have a plan to pay ben then it's just recklessness you know you're just not a prudent a way to run a business evart cunningham who coined the phrase technical that said that painting back that depends on writing code clean enough to be refactored and let's take a look at what refactoring means refactoring means changing the code without changing its behavior why would you do that why would you change the code without changing its behavior we know that everything can be solved multiple ways in computer science why do you think that the first solution is the best solution that you can come up with so often the way you solve problems in computer sciences that first you solve this problem and then you transfer this solution in a way that can enable the solution of future problems and the maintainability and this is exactly the purpose of refactoring is turning the an existing code an existing solution into something which is nicer it requires two steps it needs fast testing it means that you need to have an easy way to verify that the change what you did just to the code didn't break that code and it also requires what's called experimental operational symmetry eos for short which we are going to see why is it important so what is eos is google's fancy name for running the same code everywhere and this is you can see that in the traditional machine learning life cycle when the data scientist and the machine learning engine is not using the same code this is this is the principle that is broken um this enables refactoring um so the same code where do you run the same code you run it on your laptop uh you're running in testing training evaluation and production but it's always the the same code and it requires uh two things so it enables refactoring um but it requires something called decoupling what is decoupling um clearly on your laptop you don't have the same resources as in a cloud infrastructure or a distributed architecture so somehow needs to be able to run this code without certain features existing and decoupling is the paradigm to remove um the reduce the dependency between components of your architecture um it allows you to with interfaces to hide implementation details of the peripherals so these peripherals can change without impacting the the the core of your problem and this also helps you to break a cake which is changing anything changes everything which is a particular pro problem for machine learning how do you achieve this decoupling you can imagine your machine learning product is defined as a class this is what you see in the the blue box and it requires two decoupling steps one first of is inversion of control the second one is dependency injection what is inversion of control in traditionally uh when you write a code you think of your code as running um and it calls infrastructure services this is a direct control inversion of control tells you that the infrastructure calls instantiates your product as a class and then calls the main entry point and this is what you see in the in the the image um why is it good because it means that the running code doesn't is not aware of uh whether it runs in an infrastructure or in a test infrastructure and therefore you achieve decoupling from wherever you run the code you also decouple with dependency injection from external interfaces for databases or external services this happens through adapter classes and we are going to see a couple of examples later in the talk so in the test environment you are using the same code you are instantiated in the test hardness and call the main entry point by the test setup and replace the adapters with connections that connect to temporary services that allows you to verify whether your code did the same thing as it would have done in production and passes the the test setups uh inside your code how do you structure um and every kind of machine learning product has um some kind of business problem at its center this is why i draw into the buzzer the business problem and this is surrounded by a business-specific terminology domain model so for example if you deal with customers you know you have a customer class if you deal with transactions then you have a transaction class or anything that is using the same language as your business uses it means that um at this point you can communicate with this without translation you know if you have data frames and column names and sql queries it's much harder to talk to a subject matter experts if you um don't use the same language and obvious this is true for the business languages this is surrounded by the machine learning specific techniques like data sets or training or evaluation and only this outer layer is connected to the infrastructure which is like databases or computing infrastructure this have the additional benefit of outsourcing these to other teams and cooperating with other technical teams at the company because obviously these are decoupled from your main problem so it means that you can just outsource them and they can work with you much more efficiently while you can focus all your attention to the to the center uh to the actual machine learning product that is where you are adding most of the value how does it happen when you see the previous uh image images you see moving from um the infrastructure to the test structure here you can see the same thing in clean architecture how do you swap the out test layer to temporary structures that enables to run the same code in a much smaller place and still do it efficiently and how do we actually implement that so we are going to look at three simple design patterns which is essentially doing the same thing how do you handle dependency injection for three particular states the adapter pattern which you have already seen allows you to decouple from external services and the factory pattern allows you to decouple data sources and the strategy pattern allows you to decouple yourself from algorithmic decisions which is particularly important in machine learning because usually you try to [Music] experiment with different [Music] algorithmic decisions and you still need to maintain some kind of coherence in your solutions and don't need to rewrite your entire code base for every problem you do so let's take a look at the code is it visible can you read it okay so you can see that the code has already been refactored into a class and the infrastructure which is now a shell script uh instantiates this class and calls the main entry point so you can see that inversion of control is uh is already implemented so what does this code do it creates a database connection and then goes to that database run an sql query um and get the data uh into a data frame what's the problem with this obviously if you're running it locally on your machine and you don't have access to this database that this is code will not gonna run if you change this code um you know you have no idea whether you didn't break it you know so what you can do clearly the problem is that the main function is aware of the existence of a database that is may or may not be present so essentially you move all of the code into a separate class which is named sql source and the main class only knows the existence of a source which has an interface called get data and this get data we're going to give you that data frame but the main program doesn't know where is this data coming from and you can see that now the infrastructure instantiates the main program with this source and if you want to run it in a test structure you can replace only this sql source with a let's say file based class and do the same thing and run it again the next one is a factory pattern so we see in the previous query that the query is concerned about customers but instead of a customer you are getting back a data frame which is not a domain specific class so what do would you do let's do another refactoring step when instead of a nondescript technical implementation like a pandas data frame you are getting back domain specific classes and the rest of your code can depend on something which is the same as you talk to uh the subject matter expert and you can see that in the the last line you are using um the same um instantiation instead of the the sql database you are instantiating a domain specific data source let's take a look at the last pattern what we i'm going to talk about is a strategy pattern which is classic uh problem that happens in a lot of machine learning products um this is the same situation a customer class is transformed into features and that's fed to a logistic regression which creates a prediction what's the problem with this is that obviously if you would like to replace and use a different model and do an experiment you need to rewrite your main code you know this can be entangled several places which means that anytime you change the code you are risking that you are going to break something you would like to have this version running and another version with a different model running at the same time as well and make sure that you change the less amount of what you can do is you can move all of this code into a specific function strategy pattern is just an externalization of an algorithmic dependency and you can see that the main code now only deals with a model predict interface it doesn't know what kind of model it was calling it means that you can replace with whatever model you want if you want to have random fresh you can instantiate that here down you need to write another class and your main program will going to run in the same way if they have the same interface right this is a one of the most used design patterns in machine learning applications the strategy pattern also have another important driving force which is called coherence um essentially let's say on the left-hand side you can see that here is a deep model which has layers and obviously in instantiation i specify that i need five layers for this model this information that it has five layers is put into the constructor and then passed on to in the main function to instantiate a model with with five layers what's the problem here if you would like to use a different type of model that doesn't have layers what do you need to do you need to rewrite every place where i was was specifying these layers instead if you use a strategy pattern you externalize this information in the construction you are only passing this information to the model itself and the main code is not aware of um of what kind of model it is running um and the code is more compact and the it is at the place where it is supposed to be if you replace this model to let's say you use random forest which has a number of trees as a parameter um you don't need to change your main code again um while at the same time you are maintaining the old version as well because obviously this is just one line to change this so how does a general workflow look when you start from scratch usually you define your data classes based on the specification this is usually coming out when you're talking to subject matter experts of what kind of problems your machine learning product needs to solve and then you wrap your data sources with the factories that are creating these data classes and you only deal with these data classes in the rest of your machine learning product then you set up a test environment one of the fastest way to do that is to create functional tests with like a small amount of the data what you need to use you run your entire pipeline with 0.1 percent of the data sets or whatever you think that is enough to ensure that the changes of the code is um saved then you define your interfaces these are usually the verbs so the data classes are usually the nouns the verbs are you know do or get or create or all of this and then collect these interfaces into service classes if you are thinking about your product let's say a micro services architecture then these service classes is like the proto abstraction of a service that is going to be a standalone program in the future or not you know you don't know whether refactoring will going to eliminate some of these services and once you have done that then you can start dependency injection uh with uh like you structure your code you move the right places to the uh usually you collect quite a considerable amount of technical debt uh on this uh because this is a the first part is a very evolutionary step so the structuring and the dependence injection we're going to help you to increase the coherence and create optionality for you to go to production then you wrap your external services with adapters um doing the same interface that you were using previously with the in the test environment and then you can set up the production environment namely instantiating your product with the adapters that connects you to the um the real world and then you can create experiments with a strategy pattern uh because that's what it is purpose that without destroying the production environment you can create other contexts where you can run these experiments and solve these problems obviously most of you work in notebooks like us so how does this work with in a notebook environment we usually load because the factories are classes themselves you can instantiate them and get the data to get the data classes in notebooks and then we turn these data classes into data frames and do any kind of analysis on that and then the other thing what you can do is you can create custom experiments by instantiating main program the main program itself is a class which you can create inside your notebook and then you can create experiments in your notebook with a strategy pattern to a certain extent obviously dependent on where your notebook server is running what are the benefits we found that there is good layers of abstraction in in this structure you have a one place where you do analyzes in notebooks you have one place where you are writing your production code which is in vs code and then you can outsource a lot of the infrastructure problems or allocate you know time box time for dealing with how do you write adapters and what do you do with them but you can externalize these problems it gives you a high level of flexibility you are able to change your code and back out of it so that's a very important um you know a productivity tool um in in machine learning where you usually do high-risk experiments um and you don't want to accidentally ruin your code i'm sure everyone did it before i'm plenty of time when i regretted changing my code and it also have managed complexity we found that machine learning problems are inherently complex you cannot have simple solutions it's that they don't exist you don't simply you don't try simple solutions you are managing complex solutions uh and this allowables you through the decoupling and the externalization of infrastructure problems and it enables high speed iteration because you have the same code running in every places you have only one version of it it means that when you change it and you know that it didn't break this change and you have a cascade of test suits that we are going to validate that this isn't going to go into production and it's not going to break anything then you can reach you know high-speed iteration so let's go through this again so we see that machine learning is inherently unspecifiable that makes technical that inevitable and you need refactoring to remove this technical that you can't have refactoring without testing and eos to have eos you need decoupling to decouple you need you can do it through clean architecture and inversion of control and dependence injection and you only you can achieve this by three small design patterns called adapters factory and strategy um and thank you very much for your attention this concludes my talk uh please subscribe to my blog this was a very short amount of introduction to this concept and i write regularly about it thank you very much thank you very much leslie we've got some time for questions um given the high amount of data frame usage within more machine learning and i guess python and data analysis in general uh given these patterns as well how do you feel about my pie and typing to kind of conform to interfaces and make sure that i don't know say a technical person who's implementing say the adapters that the person is actually using them actually conforms to them and they actually uses them because i didn't see any of that uh yeah i didn't use typing yeah yeah i would think um okay so data scientists are a bit reluctant to care about their software engineering skills i'm sure you experienced that um or philosophy like this is obviously an advocacy and evangelism of this to data scientists and we try to use the least amount of moving components to teach in one go i would consider typing to a optional with the caveat of obviously it's a corporate policy or an engineering policy whether you demand it or not and i i do i do see usefulness in that but i do think it is optional that makes sense and i do think that you should minimize your data frame usage outside of analytics thank you for the talk uh one consequence of dependency injection is now you need to manage the bootstrapping just to provision all this i do the actual injection for your system and i feel that you might be pushed the complexity to that part and python being a dynamic language it can be really hard to to to achieve this because you might break something without you realize uh you might break the specification the interface without realize you have also broken the um the bootstrappings how do you manage that yes very good question um i think you need to have testing you know it managing complex systems is not going to be simpler just because we use one technique or another um you still need to write enough tests um and usually deal with enough edge cases to resolve this problem do you answer did i answer the question so it's more about the bootstrapping is the because if you look at the slide of the the example of the uh dependence injection yeah uh you there's a part in uh in in the main in the main logic to actually inject the object into it yeah so uh potentially because you dependence can be can go really deep it can be a long chain yeah so how do you manage the complexity of that with efficiency interesting i i don't know let's take this offline right hi thank you great talk i wanted to understand what you think about the usage of jupiter notebooks and if you think that it's an anti-pattern in machine learning projects i would think that um we found that it is usually very like there was just a talk before this who showed relatively a lot of infrastructure to manage um [Music] notebooks in in like a verified environment uh and this i think it's a sign that um you know if you have all your relevant code in you know in pure code and then you just use notebooks for analysis then then it's a better pattern and we only use um data frames in notebooks for analytical reasons but everything else is in domain specific classes thank you how do you deal with unit testing a machine learning algorithm which is inherently stochastic and for example i've worked with data scientists who have written a unit test where they basically ran the algorithm dumped the output of the resulting data frame and then their test compares it the the next run with that to 10 decimal places yeah and that seems worse than no test at all that's what we do as well yeah um obviously within reason the goal here um we spent actually quite a lot of time on figuring out and kent beck said that when he worked for facebook facebook was not doing testing and ken beck was like the original test-driven development guy and then he had this kind of existential crisis of how is it possible that one of the most successful company in the world doesn't follow his principles and still you know thrives and it means that if you can verify your code that it does what it did before after changing then you are good so find something to test and and this is what we do you know running the the anti pipeline on a small amount of data so that's one problem the second is machine learning we found that the engineering style testing for statistical problems is just not feasible and you always end up in some kind of analytical state where essentially you're running the entire pipeline and data scientists just stare at these numbers for days to trying to figure out whether the change latest change in a in a model was having the intended effect or not and these are enough to verify that major break points not didn't happen so that's that's our workflow in terms of testing so this kind of small amount of data testing is literally just testing whether your code is not broken you know because it will going to fail relatively easy okay so so do you distinguish between not broken and not giving the right results exactly exactly because it's so difficult to specify i mean that's exactly why you're using a machine learning algorithm because you you have no unit test would require relatively simple rules for defining your system with through the specification and that that's the very nature why you are not why you're not the software engineers haven't solved this problem with software engineering technique that's why you ended up on your table okay thank you hi thanks for the talk so my original question was regarding the factory pattern and how you replace the data frame with a customer class or whatever is domain relevant and my question was yes but if we want to have a performant operation we need to vectorize it so effectively whoever is going to operate on that customer whatever abstraction is has to have a vectorized operation which is panda specific so we haven't really decoupled anything yeah and i guess your answer now would be don't use pandas [Laughter] so i would like one thing only if your answer is don't use pandas my follow-up question is how can you justify losing that performance because like i did exactly what you did in your code sometime in the past and then it became so slow that i had to go back and use pandas again yeah okay very good question i do think that um first of all um like obviously this is a generic advice and um performance optimization is very targeted so you need to identify the place which is exactly the slowest one and i would actually use strategy pattern to wrap this and then somehow isolate this kind of complicatedness um of figuring out how to do that so that's one one technique what we use we use a lot of number if you need to perform do performant coding um and any kind of hackery hide it away behind this kind of strategy there is there is a quote called um wrap your difficult decisions in a class you know and this is exactly when you end up that i must break readability or maintainability in terms of for achieving some kind of feasible performance improvement um that's a difficult decision and you need to isolate it and make sure that it you achieve the goal what you want but you know these are not you know iron laws you know please feel free to break them at any point you want any other questions no okay so thank you for the talk there are a lot of tools already in the open source community what do you think about cookie cutter the data science cookie cutter in term of clean architecture and how can you combine it with the whole thing with head on system i would i would see that these are parallel things cookie cutter is mostly for structuring your your project profile if i remember correctly um this is more like like structuring your the abstract structuring your code you know the product itself and not the physical location of the files but these are are parallel to each other great uh well if no one has any more questions uh yeah we can finish the launch thanks again lastly"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"ML Technical Debt Management",
        "Video Title":"ML Technical Debt : Using Kubeflow to Pay It Off Quickly",
        "URL":"https:\/\/www.youtube.com\/watch?v=gVbDyvuHEu8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/gVbDyvuHEu8\/hqdefault.jpg",
        "ID":"gVbDyvuHEu8",
        "Publish Time":"2021-09-08T17:34:48Z",
        "Channel":"Arrikto",
        "Channel ID":"UCqaivxQcvHYpgFsyPyhc-CA",
        "Transcript":"you're good now hello yeah uh thank you jimmy and thank you josh uh i think you flow 1.4 would uh help us you know overcome a lot of the logistic issues while using kefla and especially on installation right there starting with this uh i am joining i work as a data scientist hello doc here today what i'll cover is you know what are the various issues with you know pushing a model into production and also the overall uh ml life cycle and also uh like there are some my personal views that i have seen and experienced and also uh how we can then solve that using or adopting uh keyflow uh yeah so engineering uh like efforts into uh enabling ml is a lot right i'm developing the model is is one bit of part and you know the the issues that are there along or in the in the ecosystem is is not actually on on the machine learning but the tooling that we around it you know it can be it can be the infrastructure it can be data engineering related issues it can be monitoring dashboarding and a lot of those kind of issues and and these uh work that that goes together while deploying a more uh machine learning solution into production is actually what taking a lot of time uh you know maybe maybe what i have seen is you know the data science team can you know develop a solution within months but then but then it takes a double the amount just to make sure that it gets deployed because it needs to get integrated with the overall product we need to take care of how do we deploy this how do we make sure that you know it it auto scales and all of those issues and that is where uh you know the whole um ml technical depth uh concept comes in um here i'm just showing uh you know a typical view of how a machine learning project uh life cycle starts starting from sourcing the data to you know the next step we do is exploratory data analysis data transformation uh then once we and and there are certain scheduling automation job required to maybe load the data retrain the models uh train the models and everything then uh once we have a model train then the next part of things comes into how do we manage and deploy the models uh serving and monitoring and at the last right since uh model can go for still very fast how do we retrain and make sure that you know our models are fresh at production you know and the predictions are always relevant uh yeah so uh you know typically uh you know how a data sensitive performs right uh uh our data centers develop the model in in siloed notebooks maybe uh running on the local machine maybe some cloud uh jupiter hub uh installation and where they basically develop the models and also the data scientists train these models on on files databases uh uh which are also very staggered and you know it can be inefficient that could be in on premise storage and you know all over it once the models are there uh then you know a back-end engineer comes into the picture where you know they then work with the data scientists to put the modeling deployment as an infinite service uh along with that uh while while these two guys are trying to you know put the model into production there are there comes and devops engineer that comes a security engineer who takes care of the other parts of the work and you know when uh and why when while they are working together this takes a lot of time even to understand you know and and push the mod into production and this also has uh you know repeated uh work that needs to be done every time uh they do together uh yeah so there are several problems one is you know no standard uh ml life cycle uh uh and and this leads to hidden innovation and duplicated efforts right uh as i said right maybe maybe you know for one project we are writing an airflow tag that that migrates the data from from one social destination then for every project we need to do that we need to write the dag we need to provision the infrastructure and everything uh same with uh same with how do we you know same with when we uh push a model into production right at the same pla same kind of things needs to be repeated uh and and also the infrastructure related issues that need to be solved every time uh there is an uh project that needs to be pushed forward and which leads to a lot of duplicated effort uh second is yes uh challenging the data science team into production is uh the engineering effort that is required because as a data scientist uh these are not you know uh primary uh skills of a data scientist where you know they they take care of how the how the how the project or how the model needs to be integrated how it is need to be auto scale and all of these issues right and and that is where you know the second problems comes in uh third thing in again uh monitoring these models into into production is an another issue uh how do you make sure that you know the models are always fresh and the predictions are the models are making always make sense to the user and you know uh because user behavior change there are a lot of implicit explicit feedback loops that can influence the model's performance over time then how do you solve these issues also a lot of time you know these data science models are developed as a poc or just you know uh can we do this right can we can we solve uh certain issues for our example uh at hello dog we figured out okay can we predict uh the context of the of the consultation that happened and if if we can do we can then all solve a lot of downstream healthcare issues uh yeah so you know talking about these problems uh now let's look a bit forward and how and what we can do to solve it uh so here comes ml ops you know amalops is basically when you merge machine learning development or data scientist to you know uh operations or a ml sorry yeah uh data scientist with an operations flow rate uh so this mlops platform uh i i think you know this should be adopted in the industry very widely and all of the companies should have it what did make sure that you know all the uh all the you know monitoring automation on the infrastructure related work integration testing releasing is already taken care of so right so the people the data scientists like they can do uh or focus on at what they are good at they can just focus on building the models building the solutions and the infrastructure part of it can automatically be taken care of yes and and also with uh with queue flow uh it it makes you know deploying this uh models uh very easy with you know docker and and kubernetes into into the picture that you know that make sure that you know these these solutions can be are portable and can can be run anywhere which with a very less effort uh and with the atricos uh sorry arikto's um amalops offering that we have you know kl rock it makes even even simpler right uh instead of me writing the whole uh flow pipeline i can just you know tag the tag the my notebook with appropriate text using kale and then and that and the pipeline can be built automatically and then it can be deployed and also the training process can be automated uh yes so an enterprise envelopes platform uh what it should offer right or what the goal should be of an ml of platform it should create you know it should empower the data scientists to drive you know direct business effects and building the models uh without actually worrying uh on the infrastructure uh this should you know provide a unified set of tools you know all the all the tools that are required right from uh uh from a phase where we are just thinking about building a model uh the data engineering how do we get the data to a point where how do we deploy them on it the whole lifecycle should be taken care of and and you know the the folks like the data scientists should not worry about it and you know the whole uh give flow uh is orchestra is designed around dockering and containerization right and it runs on kubernetes so it makes sure that you know it can be uh it's portable enough to run it everywhere uh yeah so a simple you know how to then you know how it looks like uh using q flow for an um end to end a project right uh starting from and also you know these are the best practices that you know we should always containerize our our sample or our base images so that you know every time we need to do an experiment we can just pull out an image and you know start working on that instead of you know every time creating a fresh uh environment installing on the packages and then and then you know running it if we do not go on top of that and then once we have once we have our baseline uh environment continuous then we can run a jupyter notebook on top of that and and write our codes into a notebook and then and then we can convert them into a into a pipeline where for training the jobs and once we have you know a decent enough pipeline ready then we can go ahead and do a hybrid parameter optimization on that and then once we have a have a very good model that we we are very confident on that it can perform good we can then go forward and deploy and once uh and in deployment we can then look at you know how how different models are performing uh with the same set of users or maybe you know we can look at the explanative part of it you know the predictions that the model is making why it is making that we can look at uh the decision points of them of the modular maybe using shaft values may be using the other metadata it generates and figure out okay why a certain decision has been made uh yeah so the first is containerizing the ml workflow uh this is you know should be again always adopted uh because uh uh the ml workflow makes make sure that you know it can be run anywhere and and the distribution and let's say i i made a job today and uh and let's say a couple of months later i need to rerun it and make sure and and you know then then it it helps me right i don't need to you know make make the environment again i do not need to install everything again the container is just there i can just pull it up uh i can just pull the docker image and start running it uh yeah it also helps you know if if i need to run run a job video declare it then it also helps that direct we can we can make the overall ml workflow faster using customization second is the jupiter notebooks uh yes you know uh you obviously that all the data scientists tend to write uh all their coding into jupiter notebooks and and using jupiter notebooks right we can uh and we can basically you know also make it very presentable to the other folks who who are not very much into like showing them an id would be could be very you know uh difficult for them to understand the whole part of the code but in engineering notebooks we can divide all the segments into different cells and then from there we can we can explain to them right and and with uh with keyflow uh notebooks uh which comes inside it and also with you know the authentication access control infrastructure sorted out right it also enables the teams to collaborate together uh you know the like people from the same team can can access each other's notebooks and see how they are doing their work for or you know collaborate to solve issues uh uh yeah uh yeah so once uh once these notebooks are ready and and you know we have tested out our solution then we can go ahead and write a q flow pipeline uh keep flow pipeline using using all these um all these you know sections of the notebook then we can use scale to tag all this together and and and what kills them does is basically uh dockerize everything uh and builds the pipeline for us so that you know we don't need to write using a keyflow dsm and and it also um helps an end-to-end orchestration of of the machine learning pipelines uh yeah so another uh uh you know helpful uh tool that comes with the queue flow uh is is kathy right cutting helps you know into uh optimizing the models and running hyper parameter tuning on top of that you know we can select okay i i need to run a machine optimization on this uh so basically on on the on the top of same uh pipeline we can then cut even you know and we can log all the all the methods that are generated and and then decide okay uh the which model works the best for us and go ahead and deploy it uh deployment is again very easy we can just simply you know use kf serving on top of that keep serving again takes care of uh like josh mentioned that server configuration networking um auto scaling uh hardware issues and all of these are already taken care of so we don't need to figure out every time okay every time we need to deploy model we don't did not need not need to go to the go to this issue we can just decide okay our model is this is need to be deployed we can just expose it using uh kev serving and it also takes care of all the different uh frameworks that data scientists use the tensorflow pytorch exhibits psychic learning all of them there are also some of additional features that comes with it one is yeah uh one of this is you know uh auto scaling the inference service so in the uh deployment jamal we can just uh enable this flag the auto scaling uh on on community and once you auto when so enable this flag it it uh the kubernetes cluster takes care of water or the scaling automatically and the next is canary uh deployment strategy so there could be times where you know we are we are having multiple models so let's say one version of the model is in production and i want to release the new version of it instead of you know releasing or transferring all the traffic to the new version we can just decide uh on on a canary uh deployment strategy where you know we are sending let's only ten percent of traffic to the new model and and there is no of typically the old model light and and and we monitor the performance and as a and gradually we can then transfer the whole traffic uh through the new model and then and retire the old model and this uh feature uh also helps us to run a b test on the platform because here i can just have you know multiple modules having multiple um segments or various segments of your traffic going to various models and then we can log along how these different models are performing uh and then and then and then decide which versions of the model are we know which predictions makes more sense to the user and and which are more uh efficient uh one more thing comes into uh you know while uh into model deployment and ml governance is ml explainability right why the model made uh certain predictions and it is also very uh uh necessary in in certain domains let's say in uh in banking in healthcare right uh uh sometimes we need to explain you know why um why uh the bank decided not to give a loan to the person right so we can then look at uh all the all the decision points the model make uh to explain okay why a certain decision has been made right in our case that healthcare where we are extracting the disease information from the whole doctor notes we we we can see okay which part of the whole uh sentence the model gave more importance right using using some visualization on top of it or using this raw downside and then and then this can be used to you know explain to other stakeholders that why the model is performing in a certain way why a certain decision has been made uh yeah so you know instead of uh using different tools um into uh building the ml of the platform here uh keyflow uh comes very handy right because keyflow offers a suit of tools inside it i write from right from uh for like kevlar notebooks or keyflow pipelines kf serving um and all of this right so that that covers and also now the team doesn't needs to uh have multiple tools uh to saw to you know solve various issues like specific issues and and if you have multiple schools then you know that also makes uh you know the person who is managing them uh uh life's difficult right so here uh there's only one tool there are you know a suit of tools that comes inside it and using that the whole uh ml of journey can be uh can be uh done uh yeah so while um while uh you know keyflow has these two there are certain more offerings uh from uh that helps um uh in in the whole ml of journey one is scale uh so scale uh using you know it automates a lot of uh things like uh from right from notebook and using just you know simple tagging these the pipelines can be made uh like hyperventilated jobs can be uh can be started uh we can also look at the matrix we can solve or we can expose the models um using apis uh like like josh uh showed right we can we can uh deploy uh our tensorflow dashboard on top of that which then helps us to explain or look at how the models is performing how uh and and every step of it"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"ML Technical Debt Management",
        "Video Title":"Ravi Singh - Unravelling Hidden Technical Debt in ML: A Pythonic Approach to Robust Systems",
        "URL":"https:\/\/www.youtube.com\/watch?v=YBnCPr4wmUo",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/YBnCPr4wmUo\/hqdefault.jpg",
        "ID":"YBnCPr4wmUo",
        "Publish Time":"2024-01-31T18:58:27Z",
        "Channel":"PyData",
        "Channel ID":"UCOjD18EJYcsBog4IozkF_7w",
        "Transcript":"uh my name is Ravi and I'm going to give my introduction a bit later if hopefully that's okay with everyone uh because it would make more sense I guess towards stand that's what I wanted to say uh today I wanted to talk about a topic which I think sometime gets overlooked in machine learning is the hidden technical depth in machine learning and I will try to Showcase how me and my team has tried to kind of handle it uh so yeah let's get into it I would say so let's start with the story right like stories are always nice at least for me so let's start with a story basically story of me uh a little early in my career I would say where I joined as a data scientist at a subscription based company and uh I was very excited to be honest I was a bit overconfident and I didn't Knew Too Much so but I I was very confident so and I was tasked with this uh churn model which is very classical thing so I was very confident I was like yeah I know all the algorithms I have worked with them so much it should be a easy thing and I mean I could produce result very fast get onto it with quickly and yeah maybe impress my boss at that point of time and uh yeah and it should be easy for me to just like create something get it running and maybe as I move forward I can streamline it make it better and I was so excited just to start with it right when you're new in career you want to do those things uh so I started where everybody would start like my trusty Jupiter notebook that's where I started started with my data analysis preparation did all the mugging with the data and everything data preparation uh I I was like I know there is imbalance in the class I was taking care of those so the basic thing things which youc counted in the machine learning I was like yeah I can handle those I did the model training I did the validation and I was like yeah this looks good and I was like getting happier and happier uh and then the question was like can you put it to production I didn't know much but I looked around I'm like yeah sure I can do that so what I did at that point of time was something like this where I was and this were like manual steps in a Jupiter notebook where I was extracting my data from our database doing all the preparation training evaluation all of those in Jupiter notebook now I was like I have to put into the production so basically I used my train model the model I was very happy with which had all the good good score and then I looked up online and I was like maybe I can because we were on AWS at that point of time so I was like maybe I can use uh lambra which was like new and exciting thing at that point of time uh and I was so what I did basically was create a Lambda function uh using the runtime um which is at that part I think it was psyit learn so I used that runtime to support it and wrote everything the Handler function for it in Python and it was at that time I was very proud of it because uh it was pretty good for me at that point of time where it could uh react to an event um get the model file deserialize it make it into a model object used the data which was sent by the event object and produce the prediction and serve it and I was like damn I'm good like you can see where I'm going right I was so clueless at that point so I was like yeah my code is in production maybe I can move to a different project I already put it into production so it should be nice and fine and yeah everything is automated so I don't need to do anything I can maybe start with the segmentation project I wanted to do for some time and that's where the the things started not looking good right uh I spent a lot of days drinking a lot of coffees I don't know how much caffeine I had in my system at that point of time but the very first thing which came like to me is like I got some feed Frack from the business I went and I tried to change some feature because we had a plan uh which changed so we had two tier plan which changed into three tier PL I wanted to change the feature and that changed everything because as you know in machine learning you change anything you basically change everything and if you look at the core of machine learning model it's a it's it's basically a machine for mixing different signal to predict something and when you have so much signal mixing each signal is kind of dependent upon each other so there is no inter they are so interdependent there is nothing which is totally independent and that's commonly known as intag liit so that was the first time I saw that uh I was getting over that and then I was like oh I have so many chunk feature because i' like I tried so many model and I guess some of you might have seen where you create like 10 different version of your notebook where you try different things and sometimes some of the old junk feature which might not be that valuable gets into your uh model and now you're like not sure where you did it why you did it and I like and the whole data depends why I added which table it's coming from so that was another thing I was trying to figure these things out and then I realized okay I did change something in my development pipeline which I forgot to push to production and now you're kind of feeling Ravi is like a really bad data scientist I was not or I was maybe but then I realized I didn't do it so like the good result I was getting in Dev was not getting replicated in production and that is very commonly known and you might have heard of it as the training service skew right so I'm like still figuring it out this data and everything and because I have automated everything I had lot of configuration issues because I did something looking it up on the internet trying to fix it and now a lot of those configuration were not making sense for me and I had multiple configuration issues because of that and then I use a lot of glue code because you're like okay I want to have my data from this place but I want to push it to S3 but also I want to use Lambda so you end using a lot of glue code because that's the fastest way for you to move ahead with this and that's what I did uh at that point of time thinking that okay that might be a good thing to do right but as you know in machine learning it's not always good to move so fast because like the technical depth it has been there for some time and it has been used very often in software engineering but in software engineering when we are working with technical depth what we do is like we go back we refactor our code we change some settings and it's usually easier because you have like all your code in a GitHub and you have proper not that much interactions among the code but in machine learning it's more difficult because it it has so many moving parts and sometime we forget that machine learning is not just about your algorithm or the code around it it's much more complex than that and when we try to move fast we keep on building this technical depth and then you have to spend a lot of time trying to fix these things and this is not actually like adding a new feature you're just trying to optimize your current state because when you want to scale you want it to be cost efficient because at the end of the day business wants everything to be cost efficient even though how great it is if it's not cost efficient most probably that project would get killed uh this I didn't understand start of my career right uh slowly this was a tough learning but but slowly I understood and I want to share this picture uh which makes a lot of sense now so when we think of a ml system your ml code is one of the smallest part of it right uh the bigger part which I think we all have heard thousand time is like a data scientist spends 80% of their time working with like data so you take your data data verification feature engineering and I kind of disagree I don't think it's 80% there are much more places where we put more time to it uh that would be like testing our model doing all the checking all the metadata of our model doing the different Process Management doing analysis between different version of the model or different sets of the model and then when you finally are ready to put into production then a big heavy load of configuration comes right you have so many configuration for your serving phase for your training phase and all those thing if you're doing hyperparameter tuning then you want to automate this uh you also have lot of resource management because you don't have all the resp resources at your disposal do you want to spin up more CPUs do you want to spin up more gpus or do you want to scale up you want to scale down what would be your serving infrastructure at that point of time so these things right and you also want to monitor because like if anything is in production you want to have a pulse of how things are moving you just don't want to leave it there uh and these adds a lot of things on top of it and one thing which I think has now becoming more and more prevalent as we are moving ahead are basically these three things right security is obviously makes sense but also the legal compliance it really doesn't matter which part of the world you're living in I I'm based out of Europe so we have lot of compliances we have to follow when we are working with machine learning so you have to make sure that whatever you're building is compliant otherwise you can get into lot of trouble and it's not just because there is is a law you have to do it as a person or as a company we also has this ethical thing where we should be transparent about how our model is deciding what it is deciding because sometime the output of a model could decide a fate of a human being whether it's a insurance company or anything else so you should be very clear you should have things set up in a place which could showcase this right and you can very clearly explain and if you see biases you can go and improve it great so I talked a lot about what are the problems right let let let me talk you through how hopefully we can manage it or how I have been and my team has been trying to manage these things uh so let's let me quickly start my leisure pointer so that easier for me to explain awesome so I think this this whole topic like having a good data platform I can can speak about it for like an hour because that's the base of everything if you don't have a good data platform any machine learning you do would be garbage this has been like the case since the Inception of machine learning right so first make sure you have a good data platform where your you are sure of the quality of data and everything and then once you have this data platform this will take care of the stuff which I talked about in the previous the problem I faced right the data dependency so if I go to DBT I can see the different references I can see the lineage I can see how each uh feature in my model where is it coming from has it chain in the past so these kind of having a good data platform can help you a lot with the data dependencies and it's and it's a very important component of a machine learning infrastructure which I feel sometime gets overlooked uh then I mean we we're still doing our initial data analysis in Jupiter which is good tool to be honest like if you are at a very early stage trying to figure out things trying to play with it notebook is a great environment where you can still go and run a small block see how it's outputting but once you are ready with your notebook what we are trying to do is like a good way to do would be to automate your workflow and one way to do this would be using flight so I'm not here trying to push everybody to use flight but I'm just sharing my experience this has worked best for us uh given our scenario maybe something else for you because there are lot of good tools out there so you have to evaluate for yourself so what we have tried to do here is uh basically create Docker image for each of the flight tasks so uh this would be [Music] my one Docker image running one flight task in DBT which would do my data prep and because it's in DBT I can see the lineage and everything once the data prep is done I can have my python which I'm very comfortable with running in a separate flight task again a Docker image and then you can R have another image running your evaluation and validation right and this whole workflow you can have a workflow config uh where you can Define this and flight has this great feature where you can par parameterize your workflow so even if you have to change your workflow here and there are small changes uh it can be done using parameter which is a great way to do uh and this makes the whole taking tracking your config ation much easier uh flight also provides us with a great uh UI where I can see the whole workflow and I can do some input and output validation so that works good for us and yeah you can have like multiple workflow running and one of the good uh Advantage following this approach is that even if you have worked with python you know that setting on the environment is a night me right you get a code from your colleague and you try to run it and then you figure out oh this version of python does doesn't support this version of this package for and those kind of things but if you have your registry created for Docker where you have already uploaded that image they can download image and can get started working it with quite fast and you can take care of those dependency there once you're ready with your workflow once you feel like okay this is the best uh output of a model uh you can save it at a model registry again it's up to you which model registry suits best for my case it's a simple s three where we are putting all our models the train models and once you know this is like okay this is great and obviously because it's like everything is in Python code everything is parameterized and it's it's easier for me to work with kit so I can have the Version Control on top of it which makes it easy for me to look at my code get peer reviews because like when you're working alone you might Overlook some stuff but because it's a code it's on git you can ask for PRS and reviews that makes the chances of error less obviously not foolproof but that makes it easier to understand and once you have that ready you can have a very separate uh workflow for your production so you not hampering your production workflow which would again be something like a different flight workflow and same way the containers right so you will have the feature engineering part of it done one of your flight tasks could load your artifacts and once you're done you can use any serving architecture you want like here I have given example of Selden code but it's up to you which one you want and basically from there you can serve your model and here like some of the things I talked about earlier about uh like the whole configuration issue and this it makes it a bit manageable because you can set the templates you can set the uh standards for this uh workflows and that you can share across the organization and you can work with it um and then I think one of the important part which is like the whole communication if you can set up event bus among them so that they have a standard way of communicating between task that would be a great help here in any case and I think one of the most important which like as a data scientist you would appreciate a lot is setting up those uh tracking so having a matric dashboard where you can see the different Matrix related to your algorithm U it could be depending upon your algorithm Roc curve or like me error or all those things and using something like homeo for logs and resource monitoring so you can see okay how much resource we have using what are the different logs so if a configuration is failing now it's easier because you have the locks so you can go to that locks and check and fix those issue and then you can set some set up some monitoring uh with a tool like a data dog where you can see has my pipeline been working or was there a delay in the data pipeline or was some error and these all tools the they provide great graphical user interface so it's easier for you to daily go and check and when you want to go into details you can go into the logs and get the more detail about it so this like this gives you quite a good place to start with it and I mean you can never get rid of the technical depth but it keep makes you more prepared to handle those as they come and be like you can minimize it to the most and the way we have we are trying to also do it is like all of this containers I talked about doer they are orchestrate using kubernetes so again it's in code so you can have those code in GitHub you can review this code and basically it's much easier to maintain than everybody running their own system and for kubernetes and we are using terraform as our basically orchestration of our infrastructure right so it's a infrastructure as a code so it's easier for one team to share this code with other team where they can replicate and the code is again so having these kind of approach where you're not kind of working inside a tool but at a code level uh provides much more control over things uh and when you think you are ready with all of this and you are ready to move to multiple ml project one great thing for anybody to look would be to have a feature store uh feature store basically will help you a lot with the initial things we talked about like the training serving skills or in tag liament because once a data science team has created a feature for the model and they know that it's good they can put it into feature store and there are multiple feature store in the market or you can use your own where it provides like the same copy for your analysis and also for development so the chances of Sor not development sorry production so the chances of training and servy queue reduces and most of the feature store nowadays also have this thin layer of high velocity database so when you are working at a real time it all always stores the latest feature value for that feature and that could be accessed within millisecond so you can have real-time prediction which could be a challenging if you want to fetch that data from a database and then it on top of it obviously had more feature and it it kind of provides this a common place where as a data scientist when you're starting with things you know these are the feature created by some of the data scientist in your team and you can reuse them so you don't have to worry about like making those pipeline again and working towards it so you have something already there and it makes you move faster uh one more thing which I want to say here is like because you have this nice monitoring and you can see all the loggings you can Implement different things for like heuristic explanation and fairness these are the things I talked about during the ethical part right you want to have some kind of heuristic measures you don't just want to say this is my mean error or this you also want to talk to business and say like like my model has this percent error is it good enough and just checking with the business you should have some kind of uh framework like sapply or anything where you can explain your model and given that this is quite automated infrastructure you can run it put those graphs into one of your dashboard so people can see how the model is performing and then you can put a human in the loop where a domain expert could sit and see those graphs and maybe say that okay this is not making sense and not and blah blah blah basically and that's how you control the ethic parts of it right or uh and then one thing I'm was really not sure how to put into it but I give it wanted to give a shout out is to use a tool like a carpenter in one of your like that would come here which could provide you like Auto scaling so when you are doing hyperparameter tuning you can Auto scale or upscale your workflow to use more GPU and then when you had find out those nice parameter you can downscale to like so suppose you upscale to 20 GPU because you're hyper parameter you know you can down scale to again like 2 GPU and then you're saving cost but you're also saving time so these kind of thing if you keep on mind when you're designing about your infrastructure could actually really help you I would say so uh coming to the moral of the story drink a lot of water yeah I I was so dehydrated with all the coffee so one thing would obviously drink a lot of water another thing like making shortcuts because sometime you want to just start with things taking shortcut would actually impact your project worse than taking the time initially to think about everything so think about those thing at the start uh and it's very difficult for anybody to say how you would quantify this technical depth and just moving fast doesn't mean you're doing it right uh you have to carefully see what are the areas where you can fail and think continuously over it if you are working in different like Sprint or if you're working in quarters you have to put some time into refining your process or looking into technical depth you can't put all the time just into adding new feature you also have to improve currently improve on new feature one of the paper by Google I have linked to it uh at the end of the presentation has very nicely mentioned that some of the guidelines you can see to do this is like how easy it is for you to test a new algorithm on a large scale right so if you have the kind of workflow you can most probably run it very easily U again data like make sure you have a pretty good data platform set up uh and basically if you are going to change one part of your system can you measure how it's going to impact the other because as I mentioned earlier everything is quite linked and finally is like improving one model should not negatively affect the other because sometime one model could be input to your another model so you try to improve one model but it kind of so you should be able to see that and those kind of Matrix dashboard and the threshold that provides you a very powerful tool to do those things so yeah and how easy for you to get a new team members get up and running and it's easier to explain your code where is the code and all those things yes I think that was what I wanted to talk about I have added the links to the papers which I would recommend and some books uh you should read because I think this problem is very specific to each organization and each problem so choose the solution which works best for you uh and yeah this is me I'm working as head of data analytics at sead it's a big company in NAIC if you outside NAIC I'm not sure you know about it I worked as a data scientist at HBO uh before that tell2 and I've been working with data for last 13 years and yeah now the stage is all yours to ask me any question I'm really looking forward to hearing from you guys if you have fa similar challenges or what are your things I would hang around in the conference even after the talk if you have anything please reach out or if you have a question let me know Robie that was awesome and perfect timing you left uh a mere five minutes for any questions I see a few right here in Q&A it looks like there might be some in the general chat um we can kind of start there and obviously anyone who's listening if you have additional questions or as you get answered and um again you think of anything feel free to post them in the Q&A we can and we can pull them from there [Music] I see there are two questions on the chat at least yeah we can dive into them within next four minutes we got yeah uh so I think the first one is in managing technical depth slid some of the tools logo aren't captioned can you please tell which tools there are yeah sure so like the idea for me is not to tell you which tool to use uh but to give you an idea there are tools so I can most probably let me go back to that slide and uh basically explain what those were uh wait this is I have to click through it quickly so that yeah so in terms of logging you have grafana which you can use as a metrix dashboard then homeo we use homeo for our resource management and logs and everything and then you have data dog which we also use quite a lot uh for monitoring our services how they are running and everything is working fine but I also want to stress again that I'm not saying these are the only tools you can use there are multiple tools the idea is that you should have metric dashboard you should have those logs you should have monitoring in this place doesn't matter which tool you use if that makes sense good should I move to the next one why flight instead of airflow yes let's do that yes so I think like if you go to flight website they have a pretty good uh blog post about it but I will try to go over briefly in this so flight is made for your data pipelines and orchestrating that uh sorry airf flow but flight is made for orchestrating your ml workflow so it is I would say more polish to handle those things and it provides some of those feature where it is easier for you to build those pipelines and work with Dockers images and make it work better because flight is uh much more focused on machine learning I would say and airflow I would say for your data flow so I think both tools are great and they actually work great together so don't think of one as a replacement of other they are complimenting tools yep awesome um with the next two minutes again if anyone has any further questions um or if there's anything again they can we can always continue this conversation in Discord they're also via link available in the reception area um and just a heads up that we are recording all of these talks so they will be published soon and you should get notified um when they do go live if you want to rewatch but Robie again appreciate you greatly for the presentation I found it super helpful I'm sure everyone else did um but yeah with the next 60 seconds if there's any closing remarks we can go from there no I mean like I want to learn from what you guys have been fishing so I would hang around in the lounge area and like the I would be here in the conference event tomorrow so I'm happy to hear your thoughts and like what you are facing so and happy to present thanks a lot for giving me this opportunity"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"ML System Security and Compliance",
        "Video Title":"IBM Research AI Privacy and Compliance Toolkit",
        "URL":"https:\/\/www.youtube.com\/watch?v=JpxkC6-bIaM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/JpxkC6-bIaM\/hqdefault.jpg",
        "ID":"JpxkC6-bIaM",
        "Publish Time":"2020-10-21T12:34:13Z",
        "Channel":"IBM Research",
        "Channel ID":"UCwx7Y3W30N8aS_tiCy2x-2g",
        "Transcript":"the ipm research ai privacy and compliance toolkit can help organizations and researchers develop and deploy machine learning models that are privacy preserving and comply with relevant data protection regulations many machine learning models used to tackle the world's most difficult and complex problems need to be trained using personal sensitive data the data protection regulations such as the european general data protection regulation the california consumer protection act and others impose strict obligations and restrictions on the processing of personal data some examples of these constraints are the need to ask for consent collect the minimal amount of data required and erase one's data upon request how is this relevant to machine learning recent studies show that a malicious third party with access to a trained ml model even without access to the training data itself can still reveal sensitive personal information about the people whose data was used to train the model in fact a gardner survey from 2019 cited governance issues security and privacy concerns and risk or liabilities as some of the top challenges to ai adoption in the enterprise now that we understand why ml model should be considered personal data and that data privacy and regulations are relevant to ml what are the specific obligations that apply to them first let's look at the training phase anonymizing models before or during the training process ensures that personal information is no longer present in the trained model and therefore not subject to restrictions for personal data processing privacy risk assessment of the trained model may also be required to determine the level of risk incurred while using or releasing the model but just protecting the training data is not enough the data minimization principle must also be applied to new data that is collected for analysis this principle states that you can collect only that personal data needed for the specific purpose and finally once the model is being used in production environments you still need the ability to remove individual records from the trained model in response to right to be forgotten requests hundreds of fines for the violation of data subjects rights under gdpr have been imposed by european data protection authorities in the last few years these fines range from a few thousand euros to tens and even hundreds of millions of euros this includes a 14 million euro fine to a german real estate company for storing the personal data of its customers indefinitely and without giving them the option to have it removed but most data scientists are not experts in privacy and do not know how to comply with these complex regulations to solve all these issues we propose a comprehensive suite of innovative tools that can be easily applied during or after the training process to help ensure the privacy and compliance of the resulting models this lifts the burden from the model developers and lets them concentrate on what really matters to them a functioning and accurate ml model meet cynthia a machine learning expert she's trying to create a machine learning model to predict people's purchases in an online retail shop the retail website is asking for people's permission to collect data about their browsing and purchase habits to improve their online shopping experience and provide more personalized recommendations however the retailer does not want to collect and store more sensitive personal information than is really needed for this purpose in addition some of the shops customers may later change their mind and ask to have their personal data removed from the website ryan the company's cto understands that cynthia needs help to protect the company from potentially devastating financial ramifications and reputational damage due to non-compliance using the ai privacy and compliance toolkit cynthia can determine the level of detail required when collecting data from customers as well as remove the data of people who make such a request including their contribution to the company's ai models in summary the ai privacy and compliance toolkit from ibm research helps create machine learning models that preserve the privacy of their training data and comply with relevant data protection regulations leaving the data scientists to focus on what they do best using data to solve real problems stay tuned for more videos describing the details of our exciting tools for more information contact us at ibm research haifa"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"ML System Security and Compliance",
        "Video Title":"AI in Cybersecurity",
        "URL":"https:\/\/www.youtube.com\/watch?v=4QzBdeUQ0Dc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/4QzBdeUQ0Dc\/hqdefault.jpg",
        "ID":"4QzBdeUQ0Dc",
        "Publish Time":"2023-05-19T11:00:04Z",
        "Channel":"IBM Technology",
        "Channel ID":"UCKWaEZ-_VweaEx1j62do_vQ",
        "Transcript":"Right now there are hundreds of thousands of jobs open in the cybersecurity space. And we can't fill those positions fast enough and we can't make experts fast enough to fill them either. So what are we going to do? With the people we have, we're going to have to use force multipliers in order to be more effective and meet the need. And two of the things that we can do for force multipliers is we can use automation. That allows us to work more efficiently, or we can use artificial intelligence--that allows us to work more intelligently. I'm going to specifically focus on this one in the video today--to talk about how we can use AI to investigate a problem, to identify an issue, to report on a particular problem, and ultimately to research and find out more about a particular problem. So let's start with this first one: investigate. How could we use AI to investigate a particular issue, if we become aware that there might be an issue? Well, we can use a construct called a knowledge graph, which is a way of representing information about the physical or logical world, but representing it as a data structure. And the way this works is--to give you an example. Let's say we have a domain. And this would be like the name of a web domain. And that domain then resolves to a particular IP address. Also we--so this is what we normally have with a website. Now, what else do we have? Well, we might also have a URL. That's the actual link that you're going to type into your browser. And that is going to link to a particular file on the file system. Now, let's take, for instance, if that file on the file system ends up pointing-- because we know through an AV signature, an antivirus signature --what if this points to malware? Then this is some information that we can now connect together. Then, if we say that this URL is in fact contained by that domain, and then I add a user out here unsuspecting--who connects then to this IP address. Then, all of a sudden I have a path that goes all the way through from this user to this malware. And now I have this data structure that has represented, in fact, the connection that occurred. I now know this user has been infected by this malware, and here's the path it took to get there. And in fact, if this knowledge graph is good enough, I'll be able to look and see what other users might also be affected and what other malware and what other sites. So this is a way of representing information and then we can do some reasoning over that in order to do inference. Now, this is how an AI system might do this internally. Now, so that's one way we could do investigation. How about to identify in more detail a particular problem? So systems will typically write out lots of log records. Once an event occurs on a system, then we cut a log record. We put out information about--here's the time, the date, here's who did it. Here's what they did, here's the system they did it to. Here's where they did it from. Those kinds of bits of information would be contained in these log records. And we have loads and loads of these. So it's very difficult to sort through all of that and find where are the anomalous activities. Where are the outliers? Well, in particular, what we'll find is, in this case, let's go with an example and say here is a record where a privileged user logged into the system and created a new account. Then, almost immediately afterward, in almost no time, they copied all the contents of a database. And then, almost directly immediately, they deleted the account. Now, each one of these activities independently wouldn't represent necessarily a problem, but if you do all of these within a very short period of time, then we could use a time decay function and something like machine learning, which is essentially pattern matching on steroids, to look at all of these things and look at multiple factors across multiple records and realize we have an outlier, we have an anomaly. We have what may be an attack scenario where an insider has taken advantage of the system. So that's another use of AI and machine learning, in particular, in order to diagnose a problem. What else could we do? Well, we could report. There's a requirement in security circles that you report against: Are you complying with regulatory requirements or not? And some of the things that we might do in those cases is gather the log records and process those. We might also use information that we've gained here to enrich our reporting data. So that's another example where enriching the report with the information we have from the AI system, and that's also allowing us to report, we're spending less time. And then finally, to do research. Imagine I'm investigating, I'm identifying, I'm doing all these kinds of things. And what I'd like to be able to do is find out, what is this bit of malware? And I'd like to know more about it. I want to know more about any of these systems. So it would be nice if I had a natural language processing system--a chatbot that I could go and talk to and ask it questions and it has a knowledge base that it draws on. So, in fact, we're going to see more and more of this kind of capability going forward where a chatbot becomes essentially another member of the staff to answer questions as we're trying to do investigations. So you can see now, AI can help us a lot in the cybersecurity space. And that's in fact why IBM, 100% of our security software products include AI. Thanks for watching. If you found this video interesting and would like to learn more about cybersecurity, please remember to hit like and subscribe to this channel."
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"ML System Security and Compliance",
        "Video Title":"Global Standards for AI Security and Compliance | Exclusive Lesson",
        "URL":"https:\/\/www.youtube.com\/watch?v=7FAJZ-KsL1Q",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/7FAJZ-KsL1Q\/hqdefault.jpg",
        "ID":"7FAJZ-KsL1Q",
        "Publish Time":"2025-01-18T14:31:18Z",
        "Channel":"YouAccel",
        "Channel ID":"UC169VXpkalDQGSkK4xhw3hA",
        "Transcript":"welcome to uxl where you can learn new skills anytime and anywhere visit ux l.com to explore our full course selection this lesson is part of a larger course check the video description below for a link to the full course where you can enroll if you enjoy our content please like And subscribe for your convenience the course cover displayed on your screen provides the complete course title and lesson title let's proceed with the lesson lesson Global standards for AI security and compliance Global standards for AI security and compliance Encompass a set of guidelines principles and regulations designed to ensure that artificial intelligence systems are developed and deployed in a manner that is safe ethical and compliant with legal and regulatory requirements the Advent of AI has brought about significant advancements in various Fields but it has also introduced new challenges and risks ensuring the security and compliance of AI systems is critical to maintaining public trust protecting sensitive data and preventing misuse or harm AI security involves protecting AI systems from threats that could compromise their functionality Integrity or availability this includes safeguarding against cyber attacks data breaches and other malicious activities compliance on the other hand refers to adhering to laws regulations and standards that govern the development and use of AI these may include data protection laws ethical guidelines and Industry specific regulations one of the key Frameworks for AI security and compliance is the European Union's general data protection regulation the gdpr which came into effect in May 2018 sets strict rules for data protection and privacy for individuals within the EU it has significant implications for AI particularly in terms of consent transparency and data minimization for example AI systems that process personal data must ensure that individuals have given explicit consent for their data to be used additionally AI developers must Implement measures to ensure that data is processed in a transparent Manner and that only the minimum amount of data necessary for the intended purpose is collected and used another important standard is the IEC 2701 an international standard for Information Security Management Systems this standard provides a systematic approach to managing sensitive company information ensuring its confidentiality integrity and availability implementing ISO 2701 can help organizations mitigate the risks associated with AI by establishing a robust security framework this includes conducting regular risk assessments implementing security controls and continuously monitoring and improving the isms the i e Global initiative on ethics of autonomous and intelligent systems has also developed a comprehensive set of guidelines aimed at promoting ethical principles in AI development these guidelines emphasize the importance of accountability transparency and fairness for instance AI systems should be designed to be accountable meaning that there should be mechanisms in place to ensure that the actions and decisions of AI can be traced and audited transparency is also crucial as it allows stakeholders to understand AI systems operate and make decisions this includes providing clear explanations of AI algorithms and ensuring that AI systems are interpretable by humans fairness on the other hand involves ensuring that AI systems do not perpetuate or exacerbate biases in discrimination this can be achieved by implementing measures to detect and mitigate bias in AI algorithms and data sets in addition to these International standards and guidelines there are also industry specific regulations that govern the use of AI for example in the healthare sector the health insurance portability and accountability act in the United States sets strict rules for the protection of patient data AI systems used in healthcare must comply with HIPPA requirements which include implementing safeguards to ensure the confidentiality integrity and availability of electronic protected health information this includes conducting regular risk assessments implementing access controls and encrypting eii furthermore the financial services industry is subject to regulations such as the payment card industry data security standard which sets requirements for protecting card holder data AI systems used in financial services must comply with PCI DSS requirements which include implementing security controls such as fir walls encryption and access controls compliance with these regulations helps to ensure that AI systems are secure and that sensitive financial data is protected a critical aspect of AI Security in compliance is the need for continuous monitoring and Improvement AI systems are Dynamic and constantly evolving which means that security and compliance measures must also be continuously updated and improved this involves regularly reviewing and updating security controls conducting regular Audits and assessments and staying informed about the latest threats and vulnerabilities adopting a proactive approach to AI security and compliance can help organizations stay ahead of emerging risks and ensure that their AI systems remain secure and compliant implementing Global standards for AI security and compliance can also provide a competitive Advantage organizations that adhere to these standards are more likely to gain the trust of customers partners and Regulators this can lead to increased business opportunities Improv improved reputation and reduced risk of legal and Regulatory penalties additionally organizations that prioritize AI security and compliance are better positioned to innovate and leverage AI Technologies in a responsible and ethical manner statistics highlight the growing importance of AI security and compliance according to a report by Gartner 85% of AI projects will deliver erroneous outcomes due to bias in data algorithms or the teams responsible for managing them this underscores the need for robust standards and guidelines to ensure that AI systems are fair and unbiased furthermore a study by kep Gemini found that 62% of consumers would Place higher trust in AI if it adhered to ethical guidelines and standards this demonstrates the critical role that security and compliance play in building public trust in AI in conclusion Global standards for AI security and compliance are are essential for ensuring that AI systems are developed and deployed in a manner that is safe ethical and compliant with legal and regulatory requirements Frameworks such as the gdpr iso 270001 itle e guidelines Hippa and PCI DSS provide valuable guidance for organizations seeking to implement robust security and compliance measures by adhering to these standards organizations can mitigate risks gain a competitive advantage and build public trust in AI continuous monitoring and Improvement are also critical to staying ahead of emerging threats and vulnerabilities as AI continues to evolve so too must the standards and guidelines that govern its use ensuring that AI remains a Force for good in society this concludes the lesson we trust you found this video informative and engaging to enroll in the complete course please use the link provided in the description below at uxl we are committed to your Learning Journey each course offers a certificate of completion online support and downloadable resource files to enhance your experience if you found our content valuable we invite you to like this video And subscribe to our Channel thank you for joining us and we look forward to supporting your continued pursuit of knowledge"
    },
    {
        "Domain":"MLOps",
        "Sub Domain":"ML Engineering Best Practices",
        "Topic":"ML System Security and Compliance",
        "Video Title":"Introduction to AI Security and Compliance | Exclusive Lesson",
        "URL":"https:\/\/www.youtube.com\/watch?v=DI72FjdYLbs",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/DI72FjdYLbs\/hqdefault.jpg",
        "ID":"DI72FjdYLbs",
        "Publish Time":"2025-01-18T14:20:50Z",
        "Channel":"YouAccel",
        "Channel ID":"UC169VXpkalDQGSkK4xhw3hA",
        "Transcript":"welcome to uxl where you can learn new skills anytime and anywhere visit ux l.com to explore our full course selection this lesson is part of a larger course check the video description below for a link to the full course where you can enroll if you enjoy our content please like And subscribe for your convenience the course cover displayed on your screen provides the complete course title and lesson title let's proceed with the lesson lesson introduction to AI security and compliance artificial intelligence security and compliance are critical components of the development and deployment of AI systems particularly in a cloud environment like AWS as AI Technologies continue to evolve so do the security threats and regulatory requirements that organizations must address ensuring AI security involves protecting AI systems from malicious attacks unauthorized access and data breaches while compliance involves adhering to Legal ethical and Industry specific standards AI security encompasses several aspects including data security model security and infrastructure security data security is crucial because AI systems often rely on large data sets which may contain sensitive or personally identifiable information unauthorized access to these data sets can lead to significant privacy violations and financial losses to mitigate these risks organizations must Implement robust encryption techniques for data at rest and in transit ensuring that only authorized users have access to the data model security focuses on protecting AI models from adversarial attacks where malicious actors attempt to manipulate the input data to cause the model to make incorrect predictions these attacks can lead to severe consequences especially in critical applications such as healthare and autonomous driving techniques such as adversarial training where models are trained on adversarial examples and the use of robust optimization algorithms can enhance model resilience against such attacks infrastructure security involves safeguarding the hardware and software components that support AI systems this includes securing Cloud environments like AWS where AI models are often deployed AWS offers a range of security features such as identity and access management which allows organizations to control who can access access their resources and virtual private Cloud which provides isolated Network environments by leveraging these features organizations can create secure and compliant AI deployments compliance in AI involves adhering to various legal and ethical standards to ensure that AI systems are used responsibly and transparently regulatory requirements can vary significantly across different regions and industries for instance the general data protection regulation in the European Union imposes strict data protection and privacy requirements on organizations that process personal data compliance with gdpr requires organizations to implement measures such as data minimization where only the necessary data is collected and processed and data subject rights which allow individuals to access correct or delete their personal data in the healthcare industry compliance with regulations like the health insurance portability and accountability act in the United States is essential Hippa sets standards for protecting sensitive patient information and AI systems used in healthcare must ensure that they comply with these standards this includes implementing access controls audit logs and encryption to protect patient data from unauthorized access and breaches ethical considerations are also Paramount in AI compliance AI systems can perpetuate biases present in the training data leading to unfair or discriminatory outcomes to address this issue organizations must Implement fairness and bias mitigation techniques such as reweighting training data to ensure balanced representation and using fairness aware algorithms that account for potential biases transparency is another critical ethical consideration as AI systems should be explainable allowing users to understand the rationale behind their decisions techniques like model interpretability an explainable AI can help achieve this transparency moreover organizations must establish governance Frameworks to oversee AI development and deployment ensuring that security and compliance are integrated into every stage of the AI life cycle this includes conducting regular Audits and assessments to identify potential security vulnerabilities and compliance gaps establishing clear policies and procedures for data handling model training and employment can help organizations maintain a secure and compliant AI environment statistics underscore the importance of AI security and compliance according to a report by The panamon Institute the average cost of a data breach in 2020 was $3.8 million with Healthcare being the most affected industry this highlights the financial impact of inadequate data security measures additionally a survey conducted by O'Reilly found that 54% of of organizations cited security and compliance as their top concerns when deploying AI systems these figures emphasize the need for robust security and compliance strategies in AI examples of AI security and compliance in practice can be seen in various Industries in the financial sector AI is used for fraud detection and risk management organizations must ensure that their AI systems comply with regulations such as the payment card industry data security standard and the sarbanes Oxley act this involves implementing measures like data encryption access controls and regular security audits to protect financial data and maintain Regulatory Compliance in the automotive industry AI is used in autonomous vehicles to enhance safety and efficiency ensuring the security and compliance of these AI systems involves adhering to standards such as ISO 26262 which provides guidelines for the function safety of road vehicles this includes implementing rigorous testing and validation procedures to ensure that AI systems operate safely and reliably under various conditions in conclusion AI security and compliance are essential components of responsible AI development and deployment organizations must Implement robust security measures to protect data models and infrastructure from threats while ensuring compliance with legal ethical and Industry specific standards by doing so they can mitigate risks avoid costly breaches and build trust in their AI systems leveraging Cloud platforms like AWS which offer a range of security features can further enhance the security and compliance of AI deployments as AI Technologies continue to advance organizations must remain Vigilant and proactive in addressing the evolving security and compliance challenges to ensure the safe and ethical use of AI this concludes clud the lesson we trust you found this video informative and engaging to enroll in the complete course please use the link provided in the description below at uxl we are committed to your Learning Journey each course offers a certificate of completion online support and downloadable resource files to enhance your experience if you found our content valuable we invite you to like this video And subscribe to our Channel thank you for joining us and we look forward to support ing your continued pursuit of knowledge"
    }
]