[
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Data Preprocessing for Machine Learning",
        "Video Title":"\ud83d\ude80 Data Cleaning\/Data Preprocessing Before Building a Model - A Comprehensive Guide",
        "URL":"https:\/\/www.youtube.com\/watch?v=GP-2634exqA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/GP-2634exqA\/hqdefault.jpg",
        "ID":"GP-2634exqA",
        "Publish Time":"2023-11-15T17:47:13Z",
        "Channel":"Learn with Ankith",
        "Channel ID":"UCCNTHSoPugHTUqhg4HL6C_g",
        "Transcript":"welcome to the new video in this video we are going to see how to do the pre-processing of data before building machine learning model so before we fit the data to the machine learning model we need to pre-process the data in order to see whether there is any missing value whether there's any outliers whether there is any duplicates value present in the data or either there is any garbage values are present in the data so we need to understand the data first before building any machine learning model we need to check for the relationship between the data how the data is related to each other then only we can build a best machine learning model so for that purpose we need to do the pre-processing of data so in this video we're going to see step by step how to pre-process the data before fitting the data into machine learning model so in this video we are going to cover the topics like the steps of pre-processing of data so we start from importing necessary libraries and then read the data set then we do the sanity check of the data and after that we do the exploratory data analysis to understand the data better after that we will do the missing value treatments of data and after that we'll do outli treatments then check for the duplicates and garbage value then do the treatment of the duplicates and garbage value after that let's see what's the normalization how to do normalization of data and why needed then after that at last encoding of categorical data to fit in the model so then let's see how to do pre-processing of data step by step first I will import all the necessary Library so I will import pandas as PD then import numpy Library as in P after that visualization libraries like cbor and math plot Li [Music] input math plot CP plts sorry math plotly P plot and as PLT so first I will import all the libraries yeah my libraries have been imported then after that in step two I will read the data to the data frame so so PD do read so my data is in CSV so read CSV after that I will copy the name of the data so this this is data set is about life expectancy data so I will read that and I will store it in a variable called DF so so I have read the data to DF as a data frames so if I check for the head of the data so DF do head so DF do head it will show the top five so it have the information like country year status life expectancy adult mod ity infant alcohol percentage of expenditure htis pness portfolio like that many columns are there in this data set so this is basically data set of the life expectancy by having all those column we have to predict the life expectancy of the data so in this video we are covering the topic only the pre-processing of data before building the model what the step we need to be taken so after that we will fit this data to the model so I have read the head then if I check for the tail that shows the to bottom five so DF dot so DF sorry so DF do tail it will show the bottom five record so this is bottom five record so this is how after that I will do as a step three we have to do the s check in step three we will do the sanity check of the data so sanity check means we will do the sanity check like identifying the missing value identify the outliers is there any garbage values in the data is there any duplicates values in the data set so we will start the S check by checking the shape of the data shape means how many rows and columns we have so it's very simple DF do shape if I do DF do shape it will give the shape of data so this is my shape it means 2009 38 rows and 22 Columns of data we have in my data data frame I have 2938 rows and 22 columns I have so if I check for the information so DF do info if I give DF do info it will give the information like data types the column information it will give the overall information like the entries so here we can see that it has 2938 whatever You' seen in the shape same information get by the information it has a 2938 entries indexing from 0 to 2937 so in total it has 22 columns so all the column information is there in respect of the not null values and D types so here we can see that it has the columns in total 20 1 22 columns each this represent the not null counts of the data so here we can see that in total we have 2938 row so all here from here to here it have the full values but in this part we can observe some missing values are there so it also show the data types of each column so the countryes in the data type of object and here is in the int form so that like that all the column information is present so it will Al also show here that in my data frame I have the 16 column with the float data type and four column with the int data type and two columns is the object data type that is object of country and the second one is the status so this is how I can do some sanity check then after that in sanity check I will find the missing value of my data frame I will find is there any missing value so for finding the missing value I need to check for DF do eal so DF do eal do sum so before fitting into model I should mention I should make sure that my data is clean it should not have any missing value so I need to pre-process the data in that pre-processing I need to check for is there any missing value so if I do isal or sum it will give the counts of missing value of each column so here I can find that in country year and status I don't have any missing value but in life expectancy and adult modality we can find that the there is 10 10 like that in the alcohol it have 194 missing value and hepatitis B it have 550 three missing value like that we can find the count of missing value of each column if I want the percentage of missing value so while doing the missing value treatment we need to decide whether to do the missing value treatment or to delete the column or to delete the rows with missing value so we need to decide based on many criteria based on the number of missing value if the number of missing value is very huge in any column we decided to delete that column unless the Imp in or the filling with the other value so first I need to find the uh the percentage of missing value so by the percentage I can get easy inferences so that I can decide whether to do missing value treatment or not so let's I will do the percentage in the different cell so I will first copy this so finding percentage is very easy crl C if I do this then crl V I will divide it by the the DF of shape I want one number DF dot shape in that the first so this will give the counts of the overall value so I will multiply this to 100 so it will give the percentage of missing value so so here I can get that in this part it have zero this part it have 3% of overall data compared to overall data it have the 3% missing value in this uh hyptis P it have 18% missing value compared to overall data in the GDP the 15% so it's in the minority side don't have any 50% missing value If the percentage of the missing value is above 50% we decide to delete that column so here is no problem there is no any missing value with the more than 50% of the overall count so this is how we can do the understand the missing value so after checking for missing value so we will do the treatment of missing value after in the treatment of missing Val in this sanity Che we will just understand whether we have the missing value or not so I will do step by step don't do the missing value treatment here because it will create some confusion so after that after checking for missing value I will check for the duplicates is there any duplicates in my data set so while checking for duplicate we need to have the unique value so if we have the unique value in my data set then only I can check for the duplicate otherwise the duplicate checking is useless because some besides unique value if I have the duplicates it may be possible if I had the unique value and then also I have the duplicate that's the problem so first I will check for the your purpose so DF do duplicated if I do this DF do duplicated do sum it will give the counts of duplicate so here we can see that is give that zero that means I have zero duplicates Valu so this is the done with we are done with the missing value number of missing value and also done with the number of duplicates after that we will check for the garbage value we will find is there any garbage value in our data set so the one thing about garbage value is if if the garbage value in the present in the any of the column it is always be in the form of object data type so if uh for example if if it is the ear if it is in the form of int so is there any garbage Val if the garbage Val is present is there any special character within this column the data type will show it as object is there any single garbage value it will show it as object so garbage value is always in the form of object so we will check the garbage value in the column where there is the data type of object so we will do one for Loop for count the unique values in the object column then we will find whether there is a garbage value or not so I will use for Loop for I in DF dot select D types I include only the object include is equal to objects because the garbage value will be in the object object column so object do columns so I want this column after that I will do the value count of that so DF do I the value count value counts of this so I will print this so it will give the count of unique value so if there is any garbage value it will find that garbage value as a unique and it will print so I will def defer the result so I will print like the separator I want it like 10 times so if I print this let's see what will we get so we get the garbage the values like for the country it have the we will find here the Afghanistan Peru it don't have any garbage value so after the status we have developing and so we will find it over here that there is no garbage value in this data set if there is any garbage value it will find it will show it over here whether it's a star or any special character it will show it to here and it will give the count so here I can found no garbage values so I need not to worry about the garbage values so this is how we can check the garbage value we can check it by the value count as well as the unique value anyhow if to find find any garbage value we have to do impute or the we have to change that garbage value to the any median mode anything you want so make sure that our data set is free of garbage value so after s it check we will do data exploration data analysis to understand the data set the data in my data set so to understand the data we will do exploratory data analysis so how to do that so first start with the descriptive statistics so in order to get the descriptive statistics of the numerical column I just use DF to describe so if I use this code it will give the descriptive statistics if I want to transpose I have to give T it will give the information like this it give the counts mean standard devation minimum 25 percentile 50 percentile 75 percentile and maximum by this we understand about the number how the data is distributed what is the standard deviation of the data like that we will have the many information if you read all this we will get to know about how the data is presented how that is distributed how the skilless of data every information you will get you will get the basic understanding of the data how it is so this is about descriptive of the numerical column if I want the descriptive statistics of the the object column so I will use DF do describe within bracket I have to use include object so if I use include object it will give the descriptive statistics of the sorry describe dis DH DF sorry DF do describe and it will give the descriptive statistic but in the different not as the numerical column information it will give the counts the number of values and the unique values so in country column I have the 193 unique value means in total I have 193 countries information in status I have only two two values like developing and developer so this is how we can inference from the descriptive statistics of the object column so after that I will do some explor data analysis by understanding the distribution of data to understand the distribution of data visually we will use histogram so I can use a his histogram so for drawing histogram for the each numerical column I will use obviously the for Loop for each column in DF dot select I will select only so this is a simple code to select only data types with the sorry D types include is equal to sorry include number so I just want the column of the numerical data type so include the number so if I use column over here so column and after that I will I want to print SNS that is cboard Library I use the flot the histogram CBO history flot CBO history flot within that data is my data frame and I will give the xaxis as each I means each column of numerical data type and I will use PLT do show to show every chart so if I run this code it will give the result like this so if I don't want this red color warning I just use import warnings import warnings after that warnings warnings. filter warnings within that just use ignore so by this I don't get that red color warning so this is how I can get the histogram for each numerical column for the year I can see to that that is constantly distributed here I can see the left skilless of the data in life expectancy of adult modity right skewness after that in the infant de there is very minimal there is zero most of the values are zero after that in the alcohol like that for each numerical column I have the distribution chart so this is how I can get the distribution by this I can understand how the data is distributed in my data set so this is how I can get the histogram so this is visually explorating the data so in exploratory data analysis we will do histogram to understand the distribution of data and we will also do the Box FL to identify the outlier in the datas to identify whether there is any outlier in the datas we will do the histogram the the Box blot so I will copy the same code and change a little bit so for the filter so D types and here I use the Box flot just use the box flot for this and I want to show that so just use box flot so I get the box flot for each numerical column so in the year column I don't have any box plot so in the life expectancy I have the box uh outliers in the lower side and in the adult mortality I have the outliers in the upper side of the data so in the infant I have the major outliers and in the alcohol also I have the outliers then the percentage expenditure and many column I have outlier I can see it by the boxlot information so this is the use of box blot by this way we can understand about the data I can understand about the distribution and I can understand about the outlier of the data so after that in order to I will do the scatter flot so scatter flot is basically for by analysis is it is used to get whether there is any relationship between the data or not is there any relationship between the target variable and the independent variable so by fitting the model so we will cover it in the next video and I will explain just simply or here so in order to build the model there should be relationship between the target variable and independent variable so here we in the explor data anal we will just check whether there is any positive relationship or negative relationship between the data so in this data set for example in this data set here the life expectancy is the dependent factor all the other columns are the independent Factor by using all the other Factor we will fit the model to predict the life expectance so here we have to check the relationship between that so I have to use a scatter flot to check the relationship so first I will get the column for the columns so columns so so I want to get the numerical column first so D types DF do select D types include is equal to number so this will give this will give the column of the numerical column only so I want to get The Columns of that so I will use this The Columns so I will get the columns of that so by using this I will flot a scatter plot so for scatter plot I have to use this like SNS so before that I have to use a for Loop for getting the scatter flot for every column so for each element in so I have to use a column so I will first select all the column then paste it over here then like life expectancy is my target variable so I will remove this okay I will remove this and after that I put it over here then for each column in my data I want SNS means scatter flot so from cbor library scatter flot scatter Flo data is my data frame then the X I will choose the each column then as y I will use a fixed column that is my life expectancy that is my target variable so it will create a scatter flot for this combination like year and the life expectancy adult mortality and life expectancy infant deaths and life expect like that it will create a scatter flot for each combination so PLT do show so let's see we will get the result or not yeah we get a scatter flot for the each combination like life expectancy and the year then we have the life expectancy and the adult mortality it have the negative relationship then this also showing some relationship over here then it have the relationship like this by this way we can find the relationship we have to check the relationship before before and after outl treatment so if we find the relationship is better after the outl treatment we will do the outl treatment otherwise we don't do the outl treatment we will do fit the model first before the outl treatment then we will check for the result of the model then we do the outl treatment then again we build the model like that the process will go on so in this just see about the relationship to understand the data so showing some relationship so this is how the interpret the scatter flot after that correlation hit maps to understand the correlation between the dat we will use the HTE M so DF dot if I use DF do core that will give the correlation Matrix so so here I have good string yeah I have the that so I will select only the numeric column do c so if I use this that here control V and Dot core so let's see yeah we got the correlation Matrix of the numerical column so it will give the correlation here we can see the correlation between the data by it ranges from 0 to 1 the above 50 show the strong correlation of less than left than 50 show the some correlation between data so if I want to flot this in a chart I will use the heat map to flot that so I will use SNS do heat map data so I will select this I will first store it in any variable so I will store it in yes then I will give the data as my yes then if I flot this let's see what will we get yeah we got a heat map so if I want the values in the hit map I have to use a not is equal to true if I use that I will get the values in the hit map so if I want to increase the size of that I have to use the PIP plot. figure fig size figure size is equal to let's give 15 comma 15 so what the [Music] wrong lt. figure fix size sorry we have to give it in a bracket so if I give it in a bracket let's see yeah we got a heat map with the proper size in this I can understand the correlation between in this data set I have the many column so it have the many information so by this I can get the correlation means here this color represent the high correlation and here we can find the correlation between each data so here in this part it have the 8% correlation then the minus 24% correlation minus 12% correlation between alcohol and the infant death so this is how I can get the inferences of the correlation means the relationship between the data so we can go through the each column information like this color I can find the correlation between the data so if the correlation is good I can assume that I can predict that my model will be the will give the better result because there is a relationship between the data so this is the use of the correlation matrix by using hit Maps so this is about exploratory data analysis so till now we have covered like the S Check in the sity check we will do the shape we will check the shape of the data we will get the information of the data to understand the columns information the data types and all we will get we will also check for the missing value is there any missing value by using DF do e null of sum then we will we will find the percentage of missing value then we'll find the duplicates then we we saw about the is there in garbage value by using the value count after that we will do we did exploratory data analysis by describe data by descriptive statistics of the numerical and the object columns then we flot the data to understand the distribution we use histogram then we understand the outlier in the data we will use box flot then to get the Rel relationship between that we use scatter flot and the heat M heat map with the correlation Matrix so this is about the sanity check and explorat data anal then we will come to the step five that is missing value treatment in the sanity check we find there is a missing value in the data so in this step we will treat the missing value we will trat the missing value in the sense we will fill it with the median mode or the we will use some algorithm to fit it like a imputer to fill the missing value so by this way we will get the data set without missing value so that is the necessity before fitting the data into model we need to give the model without any missing value so that's why we will do the missing values the decision of the missing values depend on various Factor so let's figure out what it is then let's see how to do the missing value treatment so for missing value treatment we have the option like we can fill it with the median mean or the mode of the data column also we have another option that is on module we have that is K imputer to fill the missing value so it work only for the numerical column so for the categorical column we used to fill it with the mode so we have we can use both we can use this median mode or mean method to inut the data or we can use the K and inputter for the fill the missing value of the numerical column so I will explain both in this video so first if I want to impute the missing so first I want to check for the missing value column so DF do Isn Su we already check in the sanity check the missing value the count of missing value so I will get it here itself so have some I have the missing value in the life expectancy and adult mortality so life expectancy and adult mortality have the missing value so life expectance and mortality these column are like uh numerical but a discrete column like life expectance is the full value if it is in the continuous data we can fill it with the median or the mean if it is in the discrete variable we have to put fill it with the mode if it is a categorical column with a missing value we have to fill it with the mode so let's see how to fill the missing value so for that I need to get the column so I will get the life expectancy adult mortality so in this for life expectancy we don't do the missing value treatment because it's a Target variable so while fitting the model while building the machine learning model model for Target variable we don't do any treatment like missing value treatment or the outl treatment we don't do that because it's a Target variable so it will become the artificial data if you do the any treatment on the target variable so we will skip this life expectancy for remaining all the columns we we will do the outl treatment so while doing outl treatment we have to decide whether to fill it with the median or the more so for the categorical or the discrete values we need to fill it with the mode and for the numerical column we can fill it with the median or the mode that is we need to decide based on the data the type of data that's depends on defers to data to data so that's a individual choice so here I have to use first I will show it in median mode mq2 then I will use this can imputer to fill the missing value so for some column I will do the medium on mode so so I will need I need to select the column first so I have to select the adult mortality first so I will write the for Loop so for each column for each element in this so I have to create the list of column so first I will select this adult morality so first I will select the numerical column which which I have to fill it with the Medan so I will use the the not infant the percentage ofte B the L the BMI so I can use the BMI body mass index control C then I will use it over here then I can use the PO the number of Po let's check for the let's check for it the type so for polio we have discrete or numeric so for polio I can see here it's a yeah it's numerical so I can fill it with the me or mode Medan Med or mean so then I will choose the income composition so this column contrl C and and here I fill with so I have these three columns so I will impute this with the median so resources I will import UT this with the median so DF dot that column DF of I means that column and I will use the fill end to fill the value I have we have to will use use the fill na so I will use the fill NA means is there any to fill it with the we have to give the parameter as the which value we want to fill so I want to fill it with the median of that so I will use DF of I median so this will give the median of this column so median and I want to make this change in the actual data so I will use in place is equal to true so if I run this code let's let let's see what will we get so if if now I check for the missing value in the life in the which column we did BMI and Vol like that if I do now we can see it over here in the income composition we have Zer null value we just imputed and in BMI also we have Zer null value we just imported imputed then in the polio like over here the polio we have zero Val we just imputed so so by this we impute the data with the median for the numerical so if I want to impute the categorical data so here I have the category only to country and the status I have zero null value no need to impute if I want to impute if there is any missing value in this category letter I have to includ with the mode just in the place of the median first select the column in the place of the median we have to use a mode and the first mode because the column can have the more than one mode value so we have to use a mode of the column then use the zero index so it will impute the missing value with the mode of that column so that's how I can impute with the missing Val so I have another one this module I have to imput the missing value that is from SK scalar if I use that I can do the missing value treatment very easily without any confusion and so first I need to import that from Escalon from Escalon do from escalar do impute from escaler do impute import k nni i means imputer so after that I have to initialize to inut so inut is equal to K n n i sorry iuter so if I initialize this so after that I got one error imputer sorry yeah so after that I will use this to fill the missing value so after that just need to use the column so I will use all the column so for I in so I will use all the columns like for I in DF of do select D types include all the numerical column so I will use include number so include number and I want the column of this so I will use column so so after that inut I have just imported inut so DF of I means that column is equal to inut dot we have to use fit transform fit transform that column DF of I so so let's see what will we get for R this I got one error the impute fit transform DF of I I need to give it in a tble braet so if I give this fit transform so fit transform that column so here we need to give it in a double square bracket so DF of I so if I impute this let's see yeah that is imputed all the numerical column with the so now I check for the null value I don't have any null value because I just imputed that by using K NN imputer so how this work is it will take the average of the nearest neighbor so K NN means the nearest neighbor it it will impute it will take the average of the nearest the value and it will fill it that missing value with the nearest average of that nearest values so this is how the K imputer works so this is Al algorithm for the filling the missing value so it's the best in the industry to fill the missing value of the numerical column so we can use this to fill the missing value or if if the number of missing value is less we can use also use the median or the mode method we can also use the K imputer from the escalar so anything is okay so so this is how we can deal with the missing value so now I check for the missing value of my data it will have no missing value we imputed all the missing value of the categorical and numerical variable so the missing value treatment is done after the missing value treatment we have the step six of the outlier treatment so what is outlier so outlier is the any extreme value whether it's an upper side or lower side of the r that's called as outl so if it is if outlier is present in the data that's that's not useful for the building a model so with outlier the model will will won't give the best result so we will do the outlier treatment we will cap the outliers with the whiskers or any values based on our decision so we will fit the model after doing the outl treatment so in the pre-processing of data we need to do the outlier treatment so for outlier treatment we need need to decide based on the our data we need to decide whether to do outl treatment or not to do outl treatment it's ented up to us to while building the model so here in this video I will show how to do the outl treatment so for outl treatment we first need to need to First need to Define one function to get the wiers so after that so in the Box slot we analyzed that there is a outlier in the data so here we can see that like in the here we can see that there is a outl in the population there is the outliers in the thinness many column we have the outliers in the above side and the Lower Side so we can cap this outl to the upper whisker so this is our upper whisker so we can cap this all this value to this upper whisker and if there is any values below this lower whisker we can cap that to the lower whisker means we we replace these values with the upper whisker value so if it is 20 is more than upper whisker we fill it with the like 70 like that we will do the outlier treatment so let's see how to do the outlier treatment for the data so and while doing the outlier treatment we need to keep it in mind that the outl treatment is done only for the continuous numerical data it won't we don't the outl treatment for the Target variable we don't do the outl treatment for the categorical and discrete variable so we need to First figure it out which are the columns are the numerical continuous numerical data for that column we we will do the outl treatment so for in this in this data for example for the GDP it's a continuous data we can do the outl treatment for the GDP then here we have the more outl treatment so we can decide to not to do the outl treatment like that we have to decide whether to do the outl treatment or not so so for this video I will show how to do the outl treatment so for total expenditure I will do the outl treatment for the polio I don't do the outlier treatment as it is discrete show the discrete so for the BMI you don't have any outliers for the isless it have the many outlier so I don't do the outlier treatment for for this like that I have to decide whether to do the outl treatment or not to do the outl treatment but the outl treatment is done only for the continuous numerical column so I will do the outl treatment for the GDP and the schooling I don't do the outlet treatment then for the income composition resource it have only one Outlet so no need to do the outl treatment for that so for thinness 5 to 9 years so first need to check for the how the data is present in that column so for thinness it have the yeah fractional data so I can do the outl treatment for thinness of 5 to 9 and like that so I will select the three column for do outl treatment so just to show how to do the outl treatment so for outline treatment I have to first get the upper RAR and lower rare for that I have to Define one function that is whisker function I can give the name as whisker so in that I will will give the input input as column so first I need to get the quartile so in order to find the upper scare lower whare I need to First need to get the quel so I will use the q1 and Q3 that is 25 percentile and the 75 percentile of the data so is equal to NP dot NP do percentile so this is a function percen function of that Nary so np. percentile that column then I want to get the 25 percentile and the 75 percentile of this column so whatever the column I give to this function it will give the it will calculate first the 25 percentile and 75% and asign it to this after that I will found the inter qual range so inter qual range means Q3 means 75 percentile that is my Q3 minus q1 Q3 minus q1 after that by after getting the IQR I can find the lower whare so the lower Vare is equal to Q q1 minus 1.5 into IQR 1.5 into IQR then the upper Vare the upper V sare is equal to Q3 plus 1.5 into IQR so after calculating this I want to return this so return return so return lower risker first then upper whisker so let's check whether this execute oh sorry here I need to give it that so return so if I check for the whisker whether I get the upper whisker lowerer whisker so whisker all first I need to get the column information DF off so I will get the columns of each columns so yeah I have the column so I will give the GDP where is my GDP yeah this column if I give this column so DF of this column let's see what will we get we got an error course model number have the new percentage so sorry percent okay okay okay percent n is missing here so I need to yeah it it will give the lower whisker and the upper whisker of this GDP column so this is how I can get the lower whisker and the upper whisker of this so after the lower RAR upper RAR I need to fill it with that so this is I just use it to check that whether it is working or not so after after that I will fill the lower RAR upper RAR of this three column whichever I selected so for I in that column so first I will select that GDP first one is GDP so control V then the total expenditure contrl C and contrl V then I will do the outlet treatment for this thinness and this this column so contrl C and control V so let's take it over here so I have this 1 2 3 four columns to do the outlier treatment so after that here I first need to get the lower whisker that lower whisker comma upper whisker of that column so for getting lower whare upper I already defined that visker function so I will use that visker function over here that and I will use the DF of I over here means that column then after that I will use the fill that so I will use T of of I is equal to NP dot means nump dot I will use where to fill it so NP dot sorry NP do where NP do where DF of I means that column check DF of I is lesser than low lower whisker fill it with lower whisker otherwise keep it as it is so this code tells like that so if DF do DF do I is equal to NP dot where DF of I is greater than upper whisker fill it with upper whisker otherwise keep it as it is so this is about this code so if I run this code yeah the lower whisker and upper whisker is kep the outl is capped with the lower and upper whisker so if I now check for that box flot again so let's check for the box flot so I will use this columns to check the box slot so control V so then SNS sorry SNS do boxplot I will directly use that DF of I then PLT dot show pl. show let's see what will we get yeah we got a box slot with no outlier before we had the outliers now we don't have outl because we did just did the outl treatment for this four column so this is how we can do the outl treatment so the outl treatment decision is based on the data it defers to data to data we need to decide whether to do the outl treatment or not so if I want to do the outl treatment we need to decide whether to cap to the upper whisker lower whisker or we need to select in specific value to forfill that so it's based on the data it depends to data to data so this is how we can do the ideally do the outlier treatment by using that upper RAR and lower RAR so this is about outlier treatment so then we have the duplicates and garbage value so we already find find in the S check we don't have any duplicates value so we don't have any garbage value also so we do we skip this if I have any duplicate value we need to delete that column by using drop duplicates to drop the duplicates value we have to use DF dot so here DF dot drop duplicates DF do drop duplicates if any duplicates is there that will be deleted so in this data struct data frame we don't have any duplicates so no need to worry about duplicates if you have the any garbage value we have to change it to the Medan or mode of that column so that's how we will do the duplicat treatment and the garbage treatment garbage value treatment so so at last let's see how to do encoding of data so encoding means for fitting the model fitting the data into model we need all the columns in the form of numerical so in my data in my data we have some columns like like here country and the status it is in the object data form so we need to convert that object data into numerical that is called as encoding so converting the object into numerical that is called as encoding so we can do encoding of data in two ways that is one hot encoding and label encoding one hard means we will create a dummies for the each category and we will do tamies and we will also use the label encoding for some categorical data which can be the categorical variable which can be ordinal we can use the label encoding otherwise we will use the tamies one H encoding so for creating D is we have the easy technique using the pandas pd. get Dumis we can use the use that module to create a Dumis so I have to use the pd. get to create a tmy variable so I will use like PD dot get damis so pd. get damis within the parenthesis I have to give the data the data is my data frame then I have to give the column for which I need to create adamy the column is I have two column like the country the first one is country then the second column is the status so status status then I have to use drop first is equal to true it will delete or drop the first level so it will it won't give the any multicolinearity problem if I drop the first level of the data so by using pd. get D of this data and I have to give the column for which I need to create dumy in my present data I need to create a dummies for the country and the status because that only two columns with the categorical features so if I run this I have one problem that is Callum sorry I'm missing columns so if I give it has has created a dummy for each country like country ugu country like that it will create a dummies for all the country so this is how I can create a dummy in a one single code that is pd. get tamies and I have to give the tamies so if I use the categorical ordinal column I have to use the replace function to replace that the true or false we can use the label encoding by using replace function I can give the label like 1 2 3 4 like that so in this data I don't have any columns which can I do label encoding so I just did a one hot encoding of this data by using pd. G so this is how and I have to store it in a dumy another data so this will give the the new data that is dummy so if I now check check for this dumy it will give all the information in the form of numerical so I don't have any categorical feature here so now my data is ready to fit in the model after this stage I can fit this data into model whether it's regression model by using a scalar or stats model I can use this data to fil the model so this is about this video in this video we learned about many things like pre-processing of data we learned about how to import necessary libraries how to read a data set and how to do a sanity check to find missing value outlier garbage value and how to do explorat data analysis to understand the data better like uh descriptive statistics and we will also visualize the data for histogram box flot scatter flot to understand the relationship and after that we did the missing value treatment by mean media mode method and K nni imputer method then we did the outlier treatment by upper risk and lower risk impute then we did the duplicates and garbage value treatment then we did the encoding of data so this is about this video in this video we learned about the pre-processing of data so thank you for watching stay tuned for the next video"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Data Preprocessing for Machine Learning",
        "Video Title":"How is data prepared for machine learning?",
        "URL":"https:\/\/www.youtube.com\/watch?v=P8ERBy91Y90",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/P8ERBy91Y90\/hqdefault.jpg",
        "ID":"P8ERBy91Y90",
        "Publish Time":"2021-08-31T13:57:30Z",
        "Channel":"AltexSoft",
        "Channel ID":"UCEKI_F16hUtBHcw9tBtr24Q",
        "Transcript":"in 2014 amazon started working on its experimental ml driven recruitment tool similar to the amazon rating system the hiring tool was supposed to give job applicants scores ranging from one to five stars when screening resumes for the best candidates yeah the idea was great but it seemed that the machine learning model only liked men it penalized all resumes containing the word women's as in women's softball team captain in 2018 reuters broke the news that amazon eventually had shut down the project now the million-dollar question how come amazon's machine learning model turned out to be sexist a ai goes rogue b inexperienced data scientists c faulty data set d alexa gets jealous the correct answer is c faulty dataset not exclusively of course getdata is one of the main factors determining whether ml projects will succeed or fail in the case of amazon models were trained on 10 years worth of resumes submitted to the company for the most part by men so here's another million dollar question how is data prepared for machine learning all the magic begins with planning and formulating the problem that needs to be solved with the help of machine learning pretty much the same as with any other business decision then you start constructing a training data set and stumble on the first rock how much data is enough to train a good model just a couple samples thousands of them or even more the thing is there's no one-size-fits-all formula to help you calculate the right size of data set for a machine learning model here many factors play their role from the problem you want to address to the learning algorithm you apply within the model the simple rule of thumb is to collect as much data as possible because it's difficult to predict which and how many data samples will bring the most value in simple words there should be a lot of training data well a lot sounds a bit too vague right here are a couple of real-life examples for a better understanding you know gmail from google right it's smart reply suggestions save time for users generating short email responses right away to make that happen the google team collected and pre-processed the training set that consisted of 238 million sample messages with and without responses as far as google translate it took trillions of examples for the whole project but it doesn't mean you also need to strive for these huge numbers ai chang yay tam kang university professor used the data set consisting of only 630 data samples with them he successfully trained the model of a neural network to accurately predict the compressive strength of high performance concrete as you can see the size of training data depends on the complexity of the project in the first place at the same time it is not only the size of the data set that matters but also its quality what can be considered as quality data the good old principle garbage in garbage out states a machine learns exactly what it's taught feed your model inaccurate or poor quality data and no matter how great the model is how experienced your data scientists are or how much money you spend on the project you won't get any decent results remember amazon that's what we're talking about okay it seems that the solution to the problem is kind of obvious avoid the garbage in part and you're golden but it's not that easy say you need to forecast turkey sales during the thanksgiving holidays in the u.s but the historical data you're about to train your model on encompasses only canada you may think thanksgiving here thanksgiving there what's the difference to start with canadians don't make that big of a fuss about turkey the bird suffers an embarrassing loss in the battle to pumpkin pies also the holiday isn't observed nationwide not to mention that canada celebrates thanksgiving in october not november chances are such data is just inadequate for the u.s market this example shows how important it is to ensure not only the high quality of data but also its adequacy to the set task then the selected data has to be transformed into the most digestible form for a model so you need data preparation for instance in supervised machine learning you inevitably go through a process called labeling this means you show a model the correct answers to the given problem by leaving corresponding labels within a data set labeling can be compared to how you teach a kid what apples look like first you show pictures and you see that these are well apples then you repeat the procedure when the kid has seen enough pictures of different apples the kid will be able to distinguish apples from other kinds of fruit okay what if it's not a kid that needs to detect apples and pictures but a machine the model needs some measurable characteristics that will describe data to it such characteristics are called features in the case of apples the features that differentiate apples from other fruit on images are their shape color and texture to name a few just like the kid when the model has seen enough examples of the features it needs to predict it can apply learned patterns and decide on new data inputs on its own when it comes to images humans must label them manually for the machine to learn from of course there are some tricks like what google does with their recaptcha yeah just so you know you've been helping google build its database for years every time you proved you weren't a robot but labels can be already available in data for instance if you're building a model to predict whether a person is going to repay a loan you'd have the loan repayments and bankruptcy's history anyway it's so cool and easy in an ideal world in practice there may be issues like mislabeled data samples getting back to our apple recognition example well you see that the third part of training images shows peaches marked as apples if you leave it like that the model will think that pages are apples too and that's not the result you're looking for so it makes sense to have several people double check or cross-label the data set of course labeling isn't the only procedure needed when preparing data for machine learning one of the most crucial data preparation processes is data reduction and cleansing wait what reduce data clean it shouldn't we collect all the data possible well you do need to collect all possible data but it doesn't mean that every piece of it carries value for your machine learning project so you do the reduction to put only relevant data in your model picture this you work for a hotel and want to build an ml model to forecast customer demand for twin and single rooms this year you have a huge data set with different variables like customer demographics and information on how many times each customer booked a particular hotel room last year what you see here is just a tiny piece of a spreadsheet in reality there may be thousands of columns and rows let's imagine that the columns are dimensions on the 100 dimensional space with rows of data as points within that space it will be difficult to do since we are used to three space dimensions but each column is really a separate dimension here and it's also a feature fed as input to a model the thing is when the number of dimensions is too big and some of those aren't very useful the performance of the machine learning algorithms can decrease logically you need to reduce the number right that's what dimensionality reduction is about for example you can completely remove features that have zero or close to zero variance like in the case of the country feature in our table since all customers come from the us the presence of this feature won't make much impact on the prediction accuracy there's also redundant data like the year of birth feature as it presents the same info as the age variable why use both if it's basically a duplicate another common pre-processing practice is sampling often you need to prototype solutions before actual production if collected data sets are just too big they can slow down the training process as they require larger computational and memory resources and take more time for algorithms to run on with sampling you single out just a subset of examples for training instead of using the whole data set right away speeding the exploration and prototyping of solutions sampling methods can also be applied to solve the imbalanced data issue involving data sets where the class representation is not equal that's the problem amazon had when building their tool the training data was imbalanced with the prevailing part of resumes submitted by men making female resumes a minority class the model would have provided less biased results if it had been trained on a sampled training data set with a more equal class distribution made prior what about cleaning them data sets are often incomplete containing empty cells meaningless records or question marks instead of necessary values not to mention that some data can be corrupted or just inaccurate that needs to be fixed it's better to feed a model with imputed data than leave blank spaces for it to speculate as an example you fill in missing values with selected constants or some predicted values based on other observations in the data set as far as corrupted or inaccurate data you simply delete it from a set okay data is reduced and cleansed here comes another fun part data wrangling this means transforming raw data into a form that best describes the underlying problem to a model the step may include such techniques as formatting and normalization well these words sound too techy but they aren't that scary combining data from multiple sources may not be in a format that fits your machine learning system best for example collected data comes in xls file format but you need it to be in plain text formats like dot csv so you perform formatting in addition to that you should make all data instances consistent throughout the data sets say a state in one system could be florida in another it could be fl pick one and make it a standard you may have different data attributes with numbers of different scales presenting quantities like pounds dollars or sales volumes for example you need to predict how much turkey people will buy during this year's thanksgiving holiday consider that your historical data contains two features the number of turkeys sold and the amount of money received from the sales but here's the thing the turkey quantity ranges from 100 to 900 per day while the amount of money ranges from 1500 to 13 000 if you leave it like this some models may consider that money values have higher importance to the prediction because they are simply bigger numbers to ensure each feature has equal importance to model performance normalization is applied it helps unify the scale of figures from say 0.0 to 1.0 for the smallest and largest value of a given feature one of the classical ways to do that is the min max normalization approach for example if we were to normalize the amount of money the minimum value 1500 is transformed into a zero the maximum value 13 000 is transformed into one values in between become decimals say 2700 will be 0.1 and 7 000 will become 0.5 you get the idea up until now we've been talking about working with only those features already present in data sometimes you deal with tasks that require the creation of new features this is called feature engineering for instance we can split complex variables into parts that can be more useful for the model say you want to predict customer demand for hotel rooms in your data set you have date time information in its native form that looks like this you know that demand changes depending on days and months you have more bookings during holidays and peak seasons on top of that your demand fluctuates depending on specific time say you have more bookings at night and much fewer in the morning if that's the case both time and date information have their own predictive powers to make the model more efficient you can decompose the date from the time by creating two new numerical features one for the date and the other for the time a machine learning model can only get as smart and accurate as the training data you're feeding it it can't get biased on its own it can't get sexist on its own it can't get anything on its own and while the unfitting data set wasn't the only reason for the amazon ai project failure it still owned a lion's share of the result the truth is there are no flawless data sets but striving to make them flawless is the key to success that's why data preparation is such a crucial step in the machine learning process and that's why it takes up to 80 percent of every data science project's time speaking of projects more information can be found in our videos about data science teams and data engineering thank you for watching"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Data Preprocessing for Machine Learning",
        "Video Title":"Data Preprocessing for Machine Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=20oq7hEoV2A",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/20oq7hEoV2A\/hqdefault.jpg",
        "ID":"20oq7hEoV2A",
        "Publish Time":"2024-10-27T02:32:00Z",
        "Channel":"Responsible AI",
        "Channel ID":"UCQYqGrtjZirXSMlJ5kgrk2w",
        "Transcript":"[Music] welcome to module 2 of machine learning fundamentals a beginner's guide in this segment you'll immerse yourself in the essential process of data pre-processing which is crucial before any machine learning model can effectively learn from data here you'll gain skills in preparing your data to ensure the success of your machine learning projects introduction to Data pre-processing before data can be used for machine learning it must be clean and well structured this process is known as data pre-processing it involves various steps to convert raw data into a format that machines can understand in this module you'll learn why this process is crucial and how it directly impacts the performance of machine learning models in four main sections section one data collection and cleaning data collection understanding the source of your data and what you need to collect is your first step data can come from various sources databases online repositories or real-time systems the key is to gather highquality data that is relevant to the problem you are trying to solve data cleaning this step involves removing inaccuracies and inconsistencies from your data this might include removing duplicates ensuring no repeated entries exist handling incorrect data rectifying records that do not make sense or are incorrectly inputed deleting irrelevant entries removing data that does not contribute to your analysis section two feature selection and engineering feature selection not all data attributes contribute equally to the prediction process feature selection means choosing only those features that are most important for your model this not only speeds up the learning process but also improves model accuracy feature engineering this involves creating new features from the existing ones to improve model performance techniques include binning which is converting numerical data into categories decomposition breaking down features into simpler components and aggregation summarizing multiple data points into single features section three data normalization and standardization all right let's talk about data normalization normalization involves adjusting values measured on different scales to a common scale this prevents data skewness and helps the algorithm to converge more efficiently now moving on to data standardization standardization involves rescaling data to have a mean of zero and a standard deviation of one this is especially beneficial for algorithms that assume data is normally distributed and finally we're at section four handling missing data and outliers missing data can lead to biased or inaccurate models techniques to handle missing data include imputation which means replacing missing values with substituted ones and deletion which involves removing records with missing values when they constitute a small portion of the data set outliers can significantly distort the results of your data analysis and model accuracy techniques to deal with outliers include trimming which means removing outlier data points and capping which involves capping values at a certain [Music] range data pre-processing is a critical step in the machine learning pipeline this module covered essential techniques like data cleaning feature selection ction and Engineering as well as data normalization and standardization each of these steps ensures that the input data to your machine learning model is of the highest quality and structured in the most beneficial way for learning next in module 3 you'll be applying these pre-processing techniques as you dive into building and evaluating classifiers within supervised learning scenarios remember the better your data preparation the more reliable your machine learning outcomes will be Start experiment ING with these components in practice exercises and you'll see how vital they are to your success in machine learning thank you so much for taking the time to watch this online training module I hope you found it valuable and insightful to stay updated on upcoming content including new modules case studies and quizzes be sure to subscribe to my channel and share in your community your support helps me create more content that enhances your learning experience experience don't miss out on the next steps to watch other module videos to deepen your understanding and apply the concepts in real world scenarios thanks again and I look forward to seeing you in the next video"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Data Preprocessing for Machine Learning",
        "Video Title":"Preprocessing data for Machine Learning - Deep Dive",
        "URL":"https:\/\/www.youtube.com\/watch?v=CmVYGW5CgCc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/CmVYGW5CgCc\/hqdefault.jpg",
        "ID":"CmVYGW5CgCc",
        "Publish Time":"2021-04-19T14:00:03Z",
        "Channel":"CodeEmporium",
        "Channel ID":"UC5_6ZD6s8klmMu9TXEB_1IA",
        "Transcript":"hello everyone welcome to another episode and today we're going to be talking about pre-processing techniques in logistic regression before we move on just a quick favor can you destroy that like button for the youtube algorithm gods to pick up that would be lovely the more likes videos like this get the algorithm will be like hey this video is pretty sick and send this video out to people like yourself by just hitting that like button you're helping us all out as a community this is so much appreciated so thank you um all right so basically starting off with some libraries numpy standard math library pandas data frame manipulation library simple computer a good way to impute values in a data frame which is filling in missing values required for pre-processing logistic aggression to build our classifier the main object of this video a couple of performance metrics roc score as well as a precision recall score test terrain split to split your data set into tests and training sets pipeline to create machine learning pipelines so that we have a streamlined process from pre-processing to actual model implementation i have an entire video on just pipelines dedicated to this so do check that out after this video then uh some what is this yeah encoders like the order encoder standard scaler and one hot encoder data frame mapper is used to map these pre-processing techniques to a particular column in your data set or your training set and then last but not least we have our data set generator which is called snape so quick shout out to snape i always find it very difficult for me to just create data on the run and i think you've seen the last few videos that i spend a lot of time just describing how data how my data is because i'm not using like a normal kaggle dataset and snape just makes that job easy because they have well here's a repository they have just it's just a convenient artificial dataset generator so if you need to generate artificial data snape is kind of the way to go i'm going to link to this in the description too so that you can read through it and also install it now what i have done here though is i've built a wrapper function around snape because you know just to cater to the to the confines of this video so that we could just pass in okay do we want categorical features yes or no do we want our data set to be balanced do we want correlated features do we want missing values do we need um what what's the size of the data set that we need and yeah i'm returning the data frame the label column the categorical feature columns and the numerical feature columns so cool yeah that's our wrapper and then we have this evaluation piece right over here which is basically computing the rca uc score as well as the precision recall auc cool all right so that's kind of all the the helper functions and now we can actually get into the meat um how is logistic regression affected by the five pre-processing techniques that we mentioned so first is standardization uh so really quick standardization is the process of making sure that your column or this particular feature has a mean of zero and standard deviation of one and you can see that the mean is definitely not zero and the standard deviation is definitely not one for these columns so it's not standardized data now with this data we are going to split it into train and test set uh where the trains the test size will be 10 of the train set of the overall sorry and then uh yeah just assign it to x train y train x test and y test now what we have here is a little machine learning pipeline where we first create the logistic regression classifier we create a pipeline and the pipeline only has one thing in it right now it's just the classifier i'm applying verbose is equal to true so that we could see additional information about every single step in this pipeline and in this case you'll see well the main information that we get is the total time that it takes to execute that section of the pipeline so first we fit our pipeline uh yeah train you know just fit it with the train set and pass in labels and then we evaluate the pipeline looking at this the aucs seem to be yeah 81 which is yeah that's pretty good i guess yeah and it took a total of 0.3 seconds to train this classifier now let's say that we did standardize this entire process so if we did apply standardization in this case well there's a built-in function so i don't need to code it out it's called standard scalar um yeah what we're doing here is that for every single numerical variable i want to apply a standard scalar so this will ensure that the mean is zero standard deviation is one for every single one of our of our numerical variables looking at this now though the total time for pre-processing is 0.1 seconds and the total time for training is much now it's like 0.1 seconds only as opposed to 0.3 seconds that it was before although the the exact performance of both of them they're exactly the same so what's the result it's that standardizing your features in logistic regression will help speed up your convergence time for training right but it doesn't necessarily help improve the performance of your model and so it's a good practice to standardize your numerical features all right now let's do encoding so for encoding uh two there are two major ways to encode um there's many but like there's two well-known ways that we would encode categorical variables one of them is using one hot encoding and the other is ordinal encoding so let's just see both cases right now so i'm just creating my dummy data right now no no fancy parameters just standard data with like four categorical features four numerical features let it rip right now for one hot encoding let's say that i want to one hot encode um the categorical features which is about four features here we'll see that well it took almost a second for the entire process to complete and we have a precision recall which is around 80 sorry precision auc and um the normal rcaoc as 80 which looks pretty good now notice here though i put the number of iterations i wanted for my logistic regression to be a hundred sorry a thousand by default it's a hundred but i needed a little more for there to be convergence for this one hot encoding now this is mostly because there's just so much more data here one hot encoding really just expands your entire data set so you kind of need that training time especially for larger data sets or smaller data sets i should say now the other way is ordinal encoding so instead of you know for if there was like five different values that a categorical feature can take you have five columns or six columns that's not the case for order encoding uh we here though i mean all we need to do is just make sure that you know the first class is zero the second class is one the third class is two and so on and so your data if we were to just pre just the pre-processing phase of looking at you know five columns five rows i should say transposed here it's still just the same number of features as it was before instead of like there's eight features here as opposed to like 32 features or something that we saw before and because of this you can see that the training time is completely sped up there is it's much less much faster now it's harder to compare like here i guess you could say okay auc is 81 which is kind of similar to what we saw here more or less right the only thing is though that we have to be careful when using ordinal encoding is that the nature of categorical variables it won't make sense if you know the categorical variable is like gender where it's zero is male one is female that that doesn't make much sense because that would mean that like female is greater than male or the other way around if we had swapped these very values but it does make sense in the context of like i don't know whether where it's like sunny cloudy rainy you know define categories that you can kind of definitively say that that one is greater than the other so overall ordinal encoding it works well when the relationship exists between categorical variables like size and weather otherwise you prefer one hot encoding and one high encoding though it takes a lot of space takes more training time so you're gonna have to be careful of convergence because sometimes it just won't converge within the default set of configurations for logistic regression in which case you will need to modify it cool so two of them down all right next is data imbalance so what happens if your data is not balanced with logistic regression where you know the positive class is severely undervalued over the negative class now again using the same exact pipeline but this time i'm just using orderly encoder for the sake of simplicity right i imagine that all of our our value our variables can be ordinal encoded so that should be fine and overall we get an auc of like 78 but the p but the precision recall score is well it's at 40 so this is kind of also the main reason why i want to introduce this uh precision recall aoc as a metric because in the case of imbalanced data it is so much easier let's say that you know if 90 of our data is negative class i could just write a model i don't have to write a model i can write a function that just says return false or return hey the class is going to be zero and always predict zero and in that way my accuracy is just going to shoot through the roof right and so it's not very fair way to assess the intelligence or rather the performance of a model with just using normal auc or accuracy metrics and hence this is used too so you can see that performance takes a hit here you can see that you know i didn't actually put i'm just passing in a normal logistic regression here it's just hard for it to pick up but in order to deal with how do you actually deal with unbalanced data it's that you need to pass in kind of a class weight is equal to balanced since the the ratio is like what nine is to one i want to weigh the positive samples though nine times as important as the negative samples this is required because you want your logistic regression model to pick up on these specific um samples you want it to be able to detect them the results are that yeah having an unbalanced data set doesn't harm accuracy but it harms precision recall metrics of the positive class and this is mostly due to lower predicted well probability values so cool all right moving on to correlated features um so what happens if your data is correlated right now i just pass in at correlated features is true so the performance is pretty high it's like almost 90 that's cool now i'm using stats models here just to see like the explanation of these variables right so we know that our model is like a 90 performance but which variables are contributing to that exactly one of these values it seems to be a little redundant already so x6 we can kind of see that well it might be explained by some of these other variables and also there might be a chance of very strong multicollinearity as suggested here so you can kind of get all of these insights just with a simple you know training like well a simple ols this is ols is just basically an um a least squares regression model so let's now uh try to compute something called the variance inflation factor so this is the function that is you know the variance inflation factor and what it's actually doing is it takes like let's say that there's eight columns right and you're doubting like right now x6 which is like the sixth column i guess right what it will do is it will train a logistic regret or linear regression for taking the features as the first seven columns and then the the output of this would be s x6 which would be the that other column that we're trying to trying to debunk or trying to see if there is collinearity here now since we're training a logistic regression which is exactly what's happening in this step what the output will be is that well if if indeed that x6 is redundant then that means the r squared value of this model it should well it should be pretty high it should be explaining everything that is in x6 if it is explaining everything that means high means that it should be close to one and if it's close to one that means this denominator is close to zero and hence the variance inflation factor is close to infinity right and that's exactly what's happening r squared by the way is percentage of explainability so how much of x6 in this case would be explained and we do that for every single column so for the first we do it for x0 then taking everything else x1 taking everything else's features x2 is a label taking everything else and what we get is well infinity for everything which means that almost every column can be somewhat explained by all the other columns put together now this is a little hard to this doesn't mean that you know everything is useless we just need to pick out you know which ones are actually useful here and so what i do is well i'll create a correlation matrix to just get us started out right first of all biggest thing that comes to mind x3 and x0 they are perfectly correlated so you can eliminate start by eliminating one of them right so start with removing perfect multi collinearity i'm removing the feature x3 and i'm just like passing all of this into the same exact pipeline no changes whatsoever so you can see that well the model performance is still at 90 and when we try to explain the variables it's still at the same 48.3 nothing has changed x3 is now removed and well first of all we still see that there is some sign of high collinearity that exists and we're still at the same spot perfect all right so well let's just keep going further then at least we removed the full multicollinearity but now let's try to remove just higher collinear features in one of these cases though we kind of saw well right here right x6 is not being it isn't significant already which means that a lot of it could already be explained by some of the features that already exist and so let's start by removing x6 as well which we do here and then well lo and behold the training the model it doesn't doesn't decrease performance which is great and um the r squared is still 48.3 explainability which is good hasn't changed even though we removed like two features right now and well all of our features seem significant um the coefficients are a little less for quite a bit of them but yeah the we're not getting that error of like high multicollinearity so so far we probably might be good let's take a look at the now let's take a look at the new variance inflation factors and look at that now we're getting some actual finite values so cool removing x6 we didn't lose explainability or performance which is great so now when you get to this stage um a typical rule of thumb is like hey you kind of want to you don't want to use features that are that have a variance inflation factor that's too high because it can be explained by other values like in this case x7 could be explained right so a remedy you would think is like removing that so let's see what happens if we remove that while we remove x7 you see now performance is really starting to decline right and even the explainability is now declining here too um although you know but removing it though like the variance inflation factor is now one which is very ideal which means that all of these features that are now included they're very decorated from each other which is kind of what you want but again we've lost some explainability here so something that you can do to remedy this is well first of all you can try to include like polynomial features of these variables into this model to see that you know if you can get some other interactions that are more complex because only linear interactions would be detected with this logistic regression and these linear regression in general too and these other linear models or two well you can just go for a more complex model than logistic regression since it might not be enough to capture all the patterns that were required in your data so just some remedies there but i hope this entire discussion of variance inflation factor made sense and how you would also come to you know get past that hurdle i also left those notes here so in case you're wondering and yeah all right and the last stage now is like what happens if logistic regression encounters missing values well you generate that data over here and i am literally just trying to train our model with missing values and what happens is well nothing because it's an error and we we can't we can't have nands in logistic regression like that and train it and so in in general like it's just not good practice to leave it as none we would want to impute this value though so data amputation which i mentioned before is now performed here for numerical values we would want to impute it with well typically we do the mean and that is the standard for simple computer but you can also you know look at if you look at like the docs you can also impute it with a very constant value again this really depends on the business study safe it's to go with mean but if you have like another case situation where you want it to be imputed with the zero for example you can do that too and so you train your model um you can see that missing values though you know compared to all of the cases that we had before of a balanced data set non-missing values it was definitely higher missing values does obviously hamper performance and yeah that's it i have a summary of exactly everything that we talked about right here so to summarize uh standardize so we looked at five pre-processing techniques how it affects logistic regression standardization which is the process of making sure that your every column every numerical column is normally distributed mean zero standard deviation one uh performance doesn't necessarily improve but convergence is faster now encoding of categorical variables well you can do order encoding wherever it's appropriate because it converges much faster than the alternative which is one hot encoding and that could explode your data set but one hot encoding may be required in some cases where you know you just have data categorical data that is not you can't say one is greater than the other like in gender now data imbalance well logistic regression obviously performs better with balanced data but in the case of unbalanced data we can over sample or overweight actually the positive class which is the the minority and you can also do some sub-sampling of the majority class too and a combination of both might actually work well fun fact though if you are trying to use the probability values of logistic regression as probabilities you would need to do some model calibration here because the outputs would not be representative of actual probabilities i have an entire video dedicated to model calibration so please do check that out and then we have data collinearity so if well if we we want to remove it like perfect linear data we want to uh we want to then try you know different modeling strategies to ensure that we are capturing non-linear interactions or you can also try to use you know polynomial features within the same logistic regression to see you know other more complex interactions and then missing values which you would want to impute with either a constant value the mean value or something else that is defined by well it depends on the business use case depends on the semantics of what that column actually means and all this code will be available on github so yep that's it i hope you all enjoyed this video and do stay tuned for some more awesome machine learning data science ai content and i will see you guys in the next video take care"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Encoding Categorical Variables: One-Hot Encoding and Label Encoding",
        "Video Title":"One-Hot, Label, Target and K-Fold Target Encoding, Clearly Explained!!!",
        "URL":"https:\/\/www.youtube.com\/watch?v=589nCGeWG1w",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/589nCGeWG1w\/hqdefault.jpg",
        "ID":"589nCGeWG1w",
        "Publish Time":"2023-02-13T05:00:03Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"one hot label Target encoding yeah yeah stack Quest hello I'm Josh starmer and welcome to statquest today we're going to talk about one hot label and Target encoding and they're going to be clearly explained you don't have to worry about the details of scaling your stuff up in the cloud cause lightning will take care of it for you bam this stat Quest is also brought to you by the letters a b and c a always b b c curious always be curious imagine we had this data and we wanted to use favorite color and height to predict if someone loves Troll 2 which is a movie that some people love and some people don't in this case favorite color has three discrete values blue red and green now in theory discrete features like favorite color are fine for most machine learning algorithms but in practice a lot of popular machine learning algorithms including neural networks do not work well with them as a result discrete data are often converted into numerical values before being used for machine learning one popular method for converting discrete variables or features into numbers is to use something called one hot encoding when we have three or more options for a discrete variable and in the case of favorite color we have three options we start by creating a new column for each option in this case that means creating three new columns blue red and green now in the blue column we set the value to 1 if we had blue in the original favorite color column and we set the remaining values to zero likewise for the red column we set the value to 1 the one time we had read in the original favorite color column and we set the remaining values to zero lastly for the green column we set the value to 1 if we had green in the original favorite color column and we set the remaining values to zero note the last column loves Troll 2 is also discrete but it only has two options yes and no so we simply replace yes with one and no with zero and now all of the columns in our new data set are numeric and can be used with algorithms that don't do well with discrete data like neural networks or XG boost bam using one hot encoding to convert discrete data into numeric data works fine when we don't have too many options in this case we only have three options for favored color so we replace favorite color with three new columns but when we have a lot of options for example if we had a column of postal codes and there are 41 683 postal codes in the United States then we would end up replacing the one postal code column with 41 683 new columns which might make the data difficult to work with so when we have tons of options for a discrete variable one alternative to one hot encoding is to Simply assign numbers from low to high to each option so in this case we might set blue to zero red to one and green to two and just like before we could convert Love's Troll 2 to be numeric by setting yes to one and no to zero simply converting the discrete values to random numbers like what we did here is called label encoding and again just like before all of the columns are now numeric and we can run the data through a neural network double bam note one thing that people don't like about using label encoding is that the numbers we use are just arbitrary and some machine learning algorithms will treat the order of the numbers as if they might mean something and that can cause problems for example a decision tree splitting on favored color would be forced to group red and green together or blue and red together simply because of the random numbers we assign to each color so instead of just picking random numbers to represent the options blue red and green we can calculate the mean value of the target the thing we want to predict which in this case is loves Troll 2 for each option for example of the three people that like the color blue only one of them loves troll 2. so the mean value for blue is 1 divided by 3 or 0.33 so we replace blue with 0.33 likewise because only one person likes red and they do not love Troll 2 the mean for red is zero so we replace red with zero lastly because two of three people who like green also love Troll 2 we replace green with 0.67 because we use the target the thing we want to predict to determine what values to replace the discrete options this method is called Target encoding that being said we've only talked about the simplest type of Target encoding a more commonly used version of Target encoding deals with the fact that we only had one person who liked the color red and that means we only used one person to determine the mean value for red and thus we don't have a lot of data supporting the use of zero to replace red in contrast both blue and green have more data three people each supporting the values we use to replace them because less data supports the value we replaced red with we have less confidence that we replaced red with the best value than we have for blue and green so in order to deal with this target encoding usually is done using a weighted mean that combines the mean for a specific option like red with the overall mean of the target which is Love's troll 2. for example in order to use the fancier Target encoding with our data we start by plugging in the mean of the target for blue 1 divided by three then because three people were used to calculate the mean for blue we plug in 3 for n then we plug in the overall main for the Target loves troll 2. 3 divided by 7 because overall three of the seven people love troll 2. now we just need to pick a value for M the weight for the overall mean m is a user-defined parameter or hyper parameter and in this example we set m equal to 2. setting m equal to 2 means we need at least three rows of data before the option mean the mean we calculated for blue becomes more important than the overall mean now we just do the math and get 0.37 so we plug in 0.37 for blue now we calculate the weighted mean for red beep beep boop and we get 0.29 so we plug in 0.29 for red lastly we calculate the weighted mean for green beep [Music] and we get 0.57 so we plug in 0.57 for green bam now let's compare the target encoding when we use the weighted mean to the Target encoding without the weighted mean the target encoding for blue and green are similar to what they were before and this makes sense because we had a relatively large amount of data for both blue and green in contrast with the weighted mean the value for red is much closer to the overall mean than before and this also makes sense because we have so little data for red only one row in a way we can think of the overall mean as our best guess given no data however as we get more data more rows for each option we use the data more rather than our best guess to determine the target encoding note if you're familiar with Bayesian methods this approach may look familiar because a lot of Bayesian methods boil down to calculating a weighted average between a guess and the data as a result some people call this Bayesian mean encoding triple bam note some of you may have noticed that we are using the target the thing we want to predict to modify the values in favored color and doing this sort of thing is a data science no no that we call data leakage data leakage results in models that work great with training data but not so well with testing data in other words data leakage results in models that are over fit the good news is that there are a bunch of relatively simple ways to avoid data leakage or at least reduce the amount of data leakage so that you can use Target encoding without overfitting your model one of the most popular methods to reduce leakage is called k-fold Target encoding so let's go back to the original data set that had blue red and green categories for favored color and talk about k-fold targeting coding note the word fold in k-fold Target encoding refers to splitting the data into equal sized subsets and the K refers to how many subsets we create for example if we did two-fold Target encoding then we would divide the data into two equal sized subsets note because we have an uneven number of rows we just made the subsets as similar in size as possible now to make it easier to keep track of things let's label the first subset a and the second subset B now to Target in code blue in subset a we ignore the target values in this subset in other words we ignore the values for Love's Troll 2 in this subset and instead plug the target values from subset B into the weighted mean equation we start by plugging in the subset b mean of the target for blue zero divided by one because the one person in subset B that likes blue does not love troll 2. then because there is only one person in subset B that likes blue we plug in 1 for n then we plug in the overall mean for the Target in subset b 1 divided by 3 because overall one of the three people in subset B loves troll 2. and just like we did before we'll set m equal to 2. now we just do the math and get 0.22 so we plug in 0.22 for the two rows and subset a with blue now we need to Target and code the one row with blue in subset B so we ignore the target values in this subset and instead plug the target values from subset a into the equation for the weighted mean [Music] then we do the math and get 0.5 so we plug in 0.5 for blue but only in subset B note you may have noticed that the different subsets have different values for blue this is okay because favorite color is becoming a continuous variable just like height and now let's encode the color red in subset a so we ignore the target values in subset a and instead plug the target values from subset B into the equation for the weighted mean now because subset B doesn't have anyone who likes the color red the mean for red is zero and n equals zero the other values are the same as before and we end up replacing red in subset a with 0.33 likewise Green in subset a uses the target values in subset B and turns into 0.42 and green in subset B uses the target values from subset a and turns into 0.67 now that each color and each subset has been encoded we merge the subsets back together and we're done note this process reduces data leakage because the rows do not use their own Target values to calculate their encoding bam now going back to the original data with blue red and green if we set k equal to 7 then we would divide the data into seven subsets now Target encoding the first subset which consists of a single row with favorite color equal to Blue means we ignore its Target value and use the target values from all of the other subsets to calculate the weighted mean likewise encoding the other subsets would use all of the other Target values except their own note when we use all of the target values except 1 to do the encoding it's called leave one out Target encoding a quick scan of the internet shows that some people are successful with leave one out Target encoding and other people are successful with setting k equal to five triple bam now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the statquest PDF study guides in my book The stackquest Illustrated guide to machine learning at stackwest.org there's something for everyone hooray we've made it to the end of another exciting stat Quest if you like this stat Quest and want to see more please subscribe and if you want to support stack Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time Quest on"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Encoding Categorical Variables: One-Hot Encoding and Label Encoding",
        "Video Title":"Quick explanation: One-hot encoding",
        "URL":"https:\/\/www.youtube.com\/watch?v=G2iVj7WKDFk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/G2iVj7WKDFk\/hqdefault.jpg",
        "ID":"G2iVj7WKDFk",
        "Publish Time":"2023-02-10T19:18:34Z",
        "Channel":"M\u0131sra Turp",
        "Channel ID":"UCpNUYWW0kiqyh0j5Qy3aU7w",
        "Transcript":"one hot encoding is a way of representing categorical values in a numerical way how does it work we create an array that has as many elements as the number of categories to represent the category we have an array that consists of zero everywhere except the element that corresponds to this category in that location the value is 1. and why not just show them in numbers because numbers have an ordinal relationship between them meaning 3 is greater than 2 and 5 is lower than eight but categories do not necessarily have this relationship if Category 3 represents sweater and Category 5 represents genes it does not mean that genes are greater than sweaters machine learning models make sense of the data that is given to them based on mathematical equations if we provide categories to a machine learning model in numbers the model will assume the relationship between these numbers represent the relationship between the categories and that will not be correct and any other way to represent one HUD encoding well you can also use dummy variables dummy variables use an array of length n minus 1 instead of n when representing the categories n being the number of categories it uses an array that consists completely of zeros to represent one of the categories for example if cat is 0 1 and dog is one zero and the only other category out there is Turtle it can be represented as zero zero once you prepare your data you will need to split it into train test and validation data sets watch this video to find out why and how we separate our data sets into sub data sets"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Encoding Categorical Variables: One-Hot Encoding and Label Encoding",
        "Video Title":"Data Preprocessing: Encoding Categorical Variables, Label Encoding, One-Hot Encoding.",
        "URL":"https:\/\/www.youtube.com\/watch?v=iY1hrai9u2I",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/iY1hrai9u2I\/hqdefault.jpg",
        "ID":"iY1hrai9u2I",
        "Publish Time":"2024-07-13T13:00:46Z",
        "Channel":"Program Lover",
        "Channel ID":"UCembuqYBtOvtVvaXIuQsNBw",
        "Transcript":"Introduction: Encoding Categorical Variables Label Encoding Label Encoding Data Convergence Example Label Encoding With Data Frame Example One-Hot Encoding One-Hot Encoding Data Convergence Example One-Hot Encoding With Data Frame Example Summary"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Encoding Categorical Variables: One-Hot Encoding and Label Encoding",
        "Video Title":"The A to Z of Feature Encoding | Label Encoding | One Hot Encoding | Data Preprocessing in Python",
        "URL":"https:\/\/www.youtube.com\/watch?v=Jm3KRsgOiHQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Jm3KRsgOiHQ\/hqdefault.jpg",
        "ID":"Jm3KRsgOiHQ",
        "Publish Time":"2023-11-01T12:24:46Z",
        "Channel":"Six Sigma Pro SMART",
        "Channel ID":"UCIPCsG_-FkS3r1AUTRhMTDA",
        "Transcript":"hello and welcome in our sequence of videos on data pre-processing today we going to cover feature and coding if you've been following the sequence you know that we've already covered feature scaling missing values outlier treatment and multicolinearity and finally we move to feature encoding which is an important pre-processing step in almost all the real world data sets why because most of the data that we deal with would have a mix of variables we'll have some variables which are numerical in nature and some variables which are categorical in nature and how to deal with these categorical features before we give an input to a model is what feature encoding is about so we'll talk about it at length as we proceed we're going to cover all about feature encoding starting with what is feature encoding why to perform feature en coding choosing appropriate feature en coding and what are the common mistakes to be avoided and if you know we typically emphasize on this because we tell you how to practically Implement these things before you apply feature encoding you need to be aware of the common mistakes that people make and what could be the consequences of those and finally we'll be covering the Hands-On piece in Python let's get started so to begin with what is feature encoding conversion of categorical data or non-numeric data into numerical format primarily for providing input to the machine learning algorithms is called feature encoding so let's say we have two tables here the table on the left represents the categorical data for example you may subscribe to an Ott app where you may have favorite shows that you watch now these shows could come from different genres for example you may be liking action your friend like lik comedy and someone else like sci-fi likewise when you go to an e- retail website to make purchases you may have favorite categories to browse through for example somebody might be interested in books someone else might be interested in furniture and they could be people who are very much interested in the electronics and the latest gadgets likewise there could be a column in the data which talks about ratings and these could be good average and poor so if you see there is a general format that these variables on the left follow they all have string entries which are not directly numbers so directly adding subtracting these categories of Performing arithmetic operations here won't be feasible so we'll have to bring them to some numerical format before we give them as input to our models and we'll discuss that however one more point that you want to be clear about there could be such columns where there is no order for example your preference might be that you like action movies and someone else likes comedy movies you can't say action is greater than comedy or comedy is better than action it's a personal choice similarly the categories of items that you want to browse through on a website can't have an order if you like books that's your preference someone else might be interested in electronics and that's that individual's preference you don't have an order here necessarily but when it comes to ratings you know this tends to give a sense of order when you're saying something is good versus an average versus a poor there is a sense of order here so the categorical variables could be nominal when there is no sense of order or ordinal where there is an order on the right we have a very interpretable table which is a collection of numbers so we could be talking about time we could be talking about a score obtained out of 100 we could be talking about distance from one place to another all the these places we have numbers and these numbers could be rounded off integers or could also be floating points which may contain decimals but end of the day these are numbers and we can meaningfully perform all arithmetic operations on these numbers let's now discuss why to perform feature encoding once again so feature encoding is essential to prepare categorical data for machine learning ensuring that it can be correctly interpreted by algorithms it allows the model to extract valuable information contained in categorical variables while performing both supervised and unsupervised tasks whether you're talking about predicting an outcome or you're talking about studying patterns in the data and coding becomes very very important just on the previous slide we discussed ratings as good fair and average now there is no way that a model will be able to figure out this sense of order unless we Supply it that way similarly if there's a column which does not have an order and we try to bring it that is again not going to be good so we'll discuss this at length as we proceed for now just keep in mind that this is essential for our model to be given the right input choosing the appropriate feature encoding now there are a couple of choices here for example we can perform something that's known as label encoding we can perform one heart encoding we can perform ordinal encoding custom encoding or Target encoding this is not an exhaustive list we can have at least 10 more different types of encodings but the bottom line is if you know these many you'll be sorted for more than 95% of the cases let's discuss each of these one by one coming to label encoding first of all label encoding is supposed to be used only for Target variable so if you're doing doing a predictive modeling task and your target column is categorical in nature that's only when the label encoding is applicable it converts the categories into numbers as per the alphabetical order that's again an important point it doesn't really rely on any other order but goes by the alphabetical order to look at an example let's say we have a column which is a target column so a bank is interested in profiling the type of loan that's most appropriate for them to pitch to their prospects so somebody who is an existing customer could could be eligible for a home loan someone else could be eligible for a personal loan Fe are deciding which is the most appropriate type of loan to be offered to a given Prospect now this kind of column that you're predicting could have these labels like home personal Auto some other customer has home as a preferred option and auto there could be more data but we're just showing you first five rows now if we apply label encoding on this what will happen is it'll assign an alphabetical order so alphabetically a comes first you'll get a zero for that then comes a which is for home so you get one wherever home was the Preferred Choice and finally p in the alphabetical order would be the last so that would be given a value of two it's kind of starting from the lowest to the highest alphabetical order that's how the encoding is done now this is totally acceptable for a Target column because in the Target column these all will be seen as different categories and we retain this information in one column keep this in mind label encoder is used only for the Target column in fact label is just another name for the dependent variable machine learning lingo that's how we normally call it moving on to the next one hard encoding now all these other encodings that I'm going to talk about are primarily applicable for the independent variables the features which are explaining an outcome or maybe just the features in case of an unsupervised learning where you are studying the patterns in the data so one out en coding is used when categories within a column cannot be assigned in order there is no order that exists each category is equally important to look at an example you may look at the same genre example so we can't say that somebody's preference that's action is less important compared to somebody's preference that's comedy or the other way or sci-fi is better than action and comedy we don't have a sense of order it's an individual's preference so in a data set this might be one of the columns which is talking about an individual's preference whether this person prefers action comedy or sci-fi these are the choices may be given and this person has chosen one another person has chosen comedy the next person has chosen sci-fi so on and so forth now in this case since we cannot give an giving equal importance to each category is important how do we do that we create three new columns for the three categories that we had in the genre column now in order to fill these columns we follow a simple rule so wherever in our data we had action these two places in the action column here we'll have to put a one and the remaining places will be filled as zero likewise wherever in our data we had comedy these two places we'll put this as one and the remaining places will be filled with zeros likewise wherever in our data we had sci-fi we'll put that as one and the remaining places will be put as zeros So eventually if you apply it on all the columns this is how your data would look like notice that the sum of each row in this case is one because in our case we just looked at one genre for a given viewer now if you realize there's some information that's a bit redundant here if we know any two features of the three can we derive the third one let's say for example we blindfold the action piece now can we guess where action would have been a one in the first case if it's not comedy it's not CCI and we only have three categories as possibilities it must have been action similarly in this case when it's not comedy not sci-fi it must have been action how about this case when it's already known to be comedy and it's not sci-fi it cannot be action because it's already known to be comedy likewise for the last row if you see it's already known to be comedy it's not sci-fi so it has to be a zero for Action as well similarly in this case it's not comedy but it's known to be sci-fi if that's the case then it cannot be action so action again will have to be zero so in a way in such cases when you do one heart encoding if you have n categories in a column and you create n new columns you don't need n columns you can figure out the nth column by using the N minus one columns generally we drop the first feature there so this is a practice this is also called dummy variable encoding so we are kind of converting a categorical column into these dummy variables and there's a concept of a dummy variable trap basically which indicates that we may not need to use all the features that we've derived we can manage with n minus one features if we had n categories n minus one columns would capture the necessary information and we are not losing that information because it's automatically implied this is called one hot encoding now there could be times when we actually have an order in the features like some kind of ratings and we want to retain that order for example let's say we have these kind of ratings in a column which capture ratings anywhere from poor to excellent everything else in between now in this case if we do one heart encoding it may not be a fair idea because it'll lose a sense of order so how to deal with such scenarios where we want to maintain a sense of order we may want to give an order as per the rating and there are ways we'll discuss this in the Hands-On piece in Python generally the indexing starts from zero so you can give it ratings from 0 to four because we have five levels so 0 to four would suffice this is called an ordinal encoding an extension of ordinal encoding could be a custom encoding custom encoding would be applicable when you don't want to go linearly for example you don't want to call it 0 1 and two maybe you know that high in case of let's say you're doing a credit risk assessment High has much greater impact compared to medium and low with your subject matter knowledge have relative weightages for these and how do you assign that that's something which could be done through a custom encoding ordinal encoding would always go in a linear way 0 1 and two custom encoding could be allowing me to say that high is three times more severe compared to medium and medium is three times more compared to low this is a very custom weightage that we have given here we use this encoding when a column follows an order but the order needs to be weighted in a custom way now moving to the last encoding that we're going to cover it is known as the Target encoding it is used to transform categorical variable based on the mean or some central tendency of the target variable what is a target variable the dependent variable or the label generally preferred when the categorical variable has very high cardinality which means there are too many categories in such cases we prefer to apply Target encoding and we'll discuss more on this as we go to the Hands-On piece so let's look at an example of Target encoding we are trying to predict sales and one of the features that we are trying to encode is State now state has 50 cat categories so would you get 50 different columns in your data 50 - 1 would be n minus 1 would be 49 in this case but 49 columns could be a lot of columns to deal with at once so in such cases compared to one hard encoding we would prefer performing a Target encoding in the Hands-On piece we'll also show you multiple ways to deal with such scenarios which is a common occurrence in most of the data sets let's take a better example of Target encoding let's say we have some data related to an individual's preferred genre the plan or the subscription the OT Services generally come with multiple options how many devices do you want access to and things like that and then there is a prediction that's being made on whether a customer would renew the subscription or not so one means the subscription would be renewed and zero means the subscription would not be renewed so in this case when we talk about encoding we don't have to worry about the plan feature why because this is already numeric we'll instead have to worry about the column called genre and when we come to a list of genres you can come up with a list of 20 OD genres as well how do you actually go ahead and encode let's look at this with the help of this simple example where you only have three categories right now how do you go about doing that so first of all it would require us to calculate some probabilities so if we are asked can we find out the probability of renewal given that the person follows action genre so which means we are focusing on only the records which belong to action genre in whatever data is visible to us there are three such instances of action genre being a preferred genre out of which two times the renewal has happened so can we say the probability of this is going to be 2x3 right and likewise can we look at the overall probability or Global probability of renewal assuming that we have only just this much data so we will only be looking at the renewal column now and you can see out of six instances three times the subscription has been renewed so the overall probability of renewal is 3x6 three times out of total six times now Target encoding is an elaborate concept but to tell you in a nutshell it uses a blended probability it gives a relative weightage to the genre or the category and it gives a weightage to the average belonging to the glob Global scenario and this is a weighted average kind of a concept so where you have W1 times the probability of renewal given a genre plus the global mean or global average with a certain weightage and you're dividing it by its total this is done to ensure that for those records which have minimal occurrences for example sci-fi in our data just occurs once we do not make wrong decisions as for this looks like that if somebody has sci-fi as the preferred genre this person will never renew but would that be the case be a very limited representation of sci-fi here so in those cases is also when we calculate the Blended probability we will be able to find out some weightage for sci-fi given the overall app will not be just overshadowed by the Limited occurrences of sci-fi plus there is a problem known as the data leakage to avoid that Target encoding has some built-in intelligence but we'll talk about it at the appropriate stage right now since we lack the necessary background we'll not go too deep into it as we progress through the course we'll also be talking about kfold Target encoding and leave out Target encoding these are essentially advanced concepts that build a little further on this but since as of now we don't have the necessary background we'll not delve too deep into these now let's move on to the common mistakes to be avoided in a way you already got a hint Rel to that so let's say if we use these encodings interchangeably will that be wrong for example let's say we use ordinal encoding in place of one H encoding we introduce an order when there is none same example genre if we start giving it an ordinal encoding which is something based on the alphabetical order maybe we'll say action will always be given zero because a comes first then comes c as in comedy so you'll give it one and sci-fi comes last so you'll give it two but are we not in a way suggesting the machine learning algorithms that sci-fi is carrying a lot of weightage compared to action and is there actually any such comparison that we can do not really so this would be wrong input to the model what if we do it the other way for example if you use one heart encoding in place of ordinal encoding so you had these kind of ratings good average and poor and you start giving them all equal importance are you not hiding information from the algorithm by doing this because right now the way you've given this input is suggesting the algorithm that good average and poor are all equally important so changing the encoding types would lead your model to suffer why because it would not get the right intelligence in the form of inputs our machine learning models will struggle to make sense out of these variables if the encoding is not right so it's very very important to give the right encoding in my experience I've seen examples where people have encoded months of a calendar year from January to December in an order from 1 to 12 that suggests that December is always going to be 12old more compared to January and if you bring it to the right order if you say that all the months are equally important the things are very different in a regression problem the R square value jumped from a 42% to an 84% by just choosing the right encoding not giving an order when there is none present with this we come to an end of the theory piece we'll do the Hands-On in the next video and that'll be a detailed Hands-On covering each type of encoding that we have theoretically introduced here you will get complete understanding of how to apply it thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Implementing Categorical Encoding with Scikit-Learn",
        "Video Title":"How do I encode categorical features using scikit-learn?",
        "URL":"https:\/\/www.youtube.com\/watch?v=irHhDMbw3xo",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/irHhDMbw3xo\/hqdefault.jpg",
        "ID":"irHhDMbw3xo",
        "Publish Time":"2019-11-12T15:36:09Z",
        "Channel":"Data School",
        "Channel ID":"UCnVzApLJE2ljPZSeQylSEyg",
        "Transcript":"next question from Vishwanatha I was wondering if you could help me to understand the process of building a pipeline using the scikit-learn pipeline module it would be great if you could use scaler and one hot encoder as part of the tutorial as well this is a great question so what is the point of pipeline the point of pipeline is to chain steps together sequentially normally you put pre-processing steps and model building steps in a pipeline now why should you build a pipeline there are two main reasons and these reasons will become more clear after I go through this lesson at least one of them will the two reasons you use pipeline number one it allows you to properly cross validate a process rather than just a model in other words when you're doing cross validation like cross valve normally you just pass a model object to it well there are cases when that is not going to give you good accurate results because you're doing the pre-processing outside of the cross validation so pipeline generally speaking is useful because you can cross validate a process that includes pre-processing as well as model building the other reason pipeline is useful is because you can do a grid search or a randomized search of a pipeline which allows you to do a grid search or randomize search of both tuning parameters for model and the pre-processing steps normally when you use grid search you probably think of it as I'm going to do a search of parameters for a model but sometimes you want to do a search of parameters for pre-processing steps in combination with the model what I mean by that is like let say I want to change check different values of the C value for logistic regression and I want to check different values of the strategy for missing for imputed missing values you can do a grid search that allows you to check both of those at once okay now I've got a notebook and I'm gonna try to code it in real time if I get really behind I'll maybe copy and paste some stuff but this is a humongous topic so I'm going to narrow down this topic I am gonna talk about pipeline of course I'm gonna talk about one hot encoder and I'm not gonna talk about standard scalar though I have a resource I can share on that but I am gonna talk about column transformer because if you're using one hot encoder in a pipeline you probably need to use column transformer so I'm going to teach you that the other thing that's really important is I'm going to be using scikit-learn point 20 oh it's probably point 20 point two if you if you're running scikit-learn previous two point twenty you're not gonna have column transformer and one hot encoder is gonna work slightly differently so you won't be able to reuse this code unless you're you're using at least 0.20 in scikit-learn okay so with all of that being said let me go over to my empty notebook I am gonna try to do this quickly and it is hard to type and talk at the same time but I will do my best so I'm gonna read in an actual dataset be Titanic dataset which I know it's overused but but it's a useful way to teach this topic okay what do I have I have a data frame and you can follow along if you like that is a URL you can read from this is a data frame of 891 rows in twelve columns okay so what are my columns well they're all these let's just take a quick look at null values so these are the columns that have null values most of there are no null values there are few columns with null values okay I've picked out a couple columns I am going to focus on like you you do with any machine learning problem you have to select features and I'm just gonna select a couple features for teaching purposes so here is what I'm going to select I'm going to use a Lok and use dot Lok and for the moment I'm gonna say all rows but I'll change that in a second I'm gonna select the survived column which is our target the p class column the sex column and the embarked column okay if you've never heard of the Titanic data set you're predicting whether passengers on the Titanic survived or did not survive so survived as the target and then we're going to use these three features P classes passenger class sex is male or female embarked is a port they embarked from now everything I'm demoing gets a lot more complicated if I leave in these two rows that have null values so I'm going to exclude any rows in which embarked is missing okay and you'll see in a second what I mean okay so that is my great afraid I'm going to overwrite my existing beta frame with this we're going to take DF dot shape you will see I've lost two rows 891 to 889 and I'm down to four columns here are my four columns okay and there are now no null values and they got some let's just take a look at the head of the Staver data frame and here we are and once again let me just say survived is our target passenger class it's technically a categorical variable but for reasons I'm not gonna explain we're treating it as a numeric variable and that's actually the best approach and then why I've got two categorical variables okay now I'm gonna start by cross validating a model that predicts survived using only P class alright and then I will show you how to use pipeline as well as one hot encoder as well as column transformer - well you'll see okay so I've got the first thing I always do is to find my ax so I'm going to select all rows and only B P class column okay so that's my X and my Y is the F dot survived okay and X dot shape is 889 by 1 y dot shape is 889 by nothing just remember even if you have only one feature in your X in your training matrix it needs to be two-dimensional okay it can't be one-dimensional for reasons that take a while to explain but that is on purpose okay alright let's say I'm building a model I'm we're gonna just start with logistic regression because I love it's a classification problem and I love logistic regression from cycle and linear model import logistic regression alright and then I tend to name my models like this log reg equals an order logistic regression and it's gonna throw me a warning if I don't specify a solver and I could explain why but it's not super important also you can read the documentation if you want to decide which solver to use but this is this one will work just fine they all have different limitations and strengths and weaknesses in the scikit-learn documentation talks about it so anyway let's evaluate our model from s Kalyn got model selection which is new which is how they reorganized in like 0.18 maybe because this used to be in from SK learned got cross Val I think or cross-validation so from SK learn model selection import cross Val score and I want to cross validate my logistic regression model using not using using cross belt or so I'm passing it sorry I have to stop talking for a second so I can remember what to type a scoring kills accuracy okay all right so let me run this and we'll just talk about it for one second I'm cross validating a logistic regression model with one feature which is passenger class five fold cross validation checking the accuracy the accuracy is 67.8% the mean accuracy so the mean of the five folds of cross validation okay and just I always like to check how that compares to the how that compares to the null accuracy and the null accuracy is 61% and the null accuracy is the accuracy you would get by predicting the most frequent class okay so you can your know like you want to generally be Tanel accuracy you don't actually have to in all cases and that's another complicated topic but anyway the point of all this so far is to quickly build my basic cross validated model okay now what if here is like the motivating got a motivating question you might say I want to add more features to my model and cross validated how do I do that the answer is pipeline but first we have to talk about encoding the sex column and the embarked column okay so let's go back and show the head of the data frame okay for encoding categorical features if they are unordered usually the best approach is is called dummy encoding which is also known as one hot encoding okay scikit-learn calls it one hot encoding pandas calls it dummy encoding it's the same thing okay now we're gonna do it in scikit-learn and there's a bunch of reasons for the that I will explain at the end of this kind of lesson alright so if you want to use dummy encoder here's how you do it from SK learn got pre-processing import one hot encoder okay this is a funny name but it has a reason for it but I'll save that for another time right then you instantiate a one hot encoder just like you instantiate a model you make an instance of it okay there's my one hot encoder and for teaching purposes I need to make it not sparse and you can make it dense don't worry about it you can basically you never have to write that in the real world so one hot encoder like any scikit-learn transformer has a fit and a transform method and a fit transform that allows you to do both at the same time so if I pass it a data frame column be one hot encoder is going to one hot encode fils X column now let's look at these first three rows what it is done is create a numpy array with two columns the first column represents and I'll you'll see the first column represents female the second column represents male in other words this is saying male female female and you can confirm that the first three rows of the data frame male female female so this is the dummy encoding of the sex column now what I'm writing right now is code you don't have to write this is teaching code this import and the instantiation is what you would need to do for real okay so this is just teaching code lines 20 and 21 okay now if i encode if i one-hot encode the embark'd column let's look at the categories the three categories of embarked RC queue ass as such it creates three columns the first column represents C the second column represents Q the third column represents s so we can tell this is s C s are the first three rows and you can see here s CS you can confirm that it did properly one-hot encode that data now generally speaking the way i've taught people to do dummy encoding or one hot encoding is in pandas i taught that because up until version point 2 0 of scikit-learn it was painful to do in scikit-learn in my opinion okay it had way too many steps now it's easier and it's better for a bunch of reasons that I will explain so in the old days what I would do is I would dummy encode in the data frame so I would add the two so my data frame right now is four columns I would add two more columns for sex I might drop one of them we don't have to get into that and I would add three columns for embark'd so my data frame would keep getting wider and wider but I would do that in pandas and then I would select out p class and then all of the dummied columns as they're called and I would select those to become my X and then pass them to cross-validation okay so that's how I would do things previously but we are instead going to do it with pipeline okay so here's what we're gonna do okay I'm gonna define my X as if I previously oh I guess previously I defined X as one feature okay but now I'm gonna define it as three features so I'm actually going to do DF drop survived axis equals columns okay and you can see that here's my so you'll notice that I've defined my feature matrix capital X it's just these three columns now the next thing I need to show you is column transformer okay and let me import it from SK learn got compose import make column transformer okay and then I will start writing this okay but before I do this here is the use case for column transformer you use column transformer anytime you have features in your data frame that need different pre-processing okay what do I mean well dummy encoding or one hot encoding is a pre-processing step I want to employ I want to apply it to embarked and sex but I do not want to apply it to P class because we're treating that as a numeric variable not a categorical variable so I am going to create a column transformer that accomplishes that objective so what I'm passing to it I'm gonna pass well let me just type it and then I will it will I will explain what I'm doing I'll get you have to get used to how it's done but I think it'll make sense in a second Bart and then wait comma no comment after and then remainder equals pass through okay okay here is my column transformer okay I make a column transformer which says I want to apply a one hot encoder to these two columns in my data frame and the remainder of the columns I want to pass through okay and let me show you what that means it's Co so with a column transformer you will do a fit transform and we'll pass it our training data and here's what comes out and you'll notice what we have is these two columns the first two columns are the one hot encoded sex the next three columns these three columns are the one hot encoded embark'd the final column is a pass-through of the p class column because I didn't want to encode it okay so to be very clear if I had a bunch more columns that I wanted to do other pre-processing steps to I would have added them to the column transformer so I could say I want to one hot encode sex and embark'd I want to do a simple impute er to some other column I want to do something else to some other column and then the remainder you can either pass through or I think ignore okay so I'm using column transformer to do my pre-processing on all of my columns at the same time without doing it in pandas okay we are finally at the pipeline step from SK learn got pipeline import make pipeline there's both pipeline and mate pipeline and for reasons I won't get into I strongly prefer make pipeline but it's functionally equivalent so here is my pipeline I'm gonna make a pipeline of my column transformer and my logistic regression model okay so remember pipeline is for chaining steps together so I'm creating a pipeline that does the following things it takes my my data that I pass it it transforms the columns which is my pre-processing steps and then it builds my model which is logistic regression okay so it builds my model on the result so what am i doing with all of this if you've gotten lost I am now going to past the entire pipeline to cross Val score X Y C B equals five scoring equals accuracy I guess you can't autocomplete that got mean okay all right there you go our accuracy went up to 0.77 meaning this adding these two features improved our model from previously which was 0.67 so it went up from 0.67 to 0.77 which is great so I'm I want to do a few things just so you know where this lesson is going number one I'm gonna explain what just happened number two I'm gonna show you how to make predictions on new data number three I'm gonna show you just a quick some a recap of all the code in case you got lost and then number four I'm gonna comment on at a high level on why we're doing what we're doing okay so that's those are my steps and so let's keep this moving for it what happens when I run this line of code this means I am cross validating my entire pipeline in other words I am NOT cross validating a model I am cross validating a pipeline of steps that include pre-processing of data and model building in other words cross Val score is going to do my split of data my fivefold split and then after it splits the data it will then run the pipeline the point of cross validation remember is to evaluate your model so that you can decide whether you're building a good model and then you can use it to make predictions on new data so let's go ahead and make up some new data to pass to the model so I'm gonna make something called X new and as a kind of lazy way of doing this I'm gonna sample five rows from X and I know technically I shouldn't pull from training data to make my out-of-sample data because it's not out-of-sample data but it's the fastest way to create a good data frame with five rows in the real world I would be pulling out out-of-sample data for this but I just need some data to make predictions on okay so there's my data how normally if you've built your model and evaluated it and you want to make predictions what do you do you do like model dot fit well I don't have a model I have a pipeline that includes a model so I do pipe dot fit and I say I'm training it on X&Y ok and then I do pipe not predict X new okay so pipe dot fit is like model dot fit except it runs the pre-processing as well as the model fitting okay pipe dot predict is just like model dot predict except it runs the pre-processing on X new think about that for a second this is X new it has strings in there so this only works because the pipeline is doing the dummy encoding of the new data of the out-of-sample data and then is making predictions so it's actually quite amazing what's what it's accomplishing in that one line of code ok moving on the next thing I want to show you is a recap of what we just did if you got lost and everything we just did it's actually very little code this is like all of the in quote important code that we just wrote meaning that I I've eliminated the exploratory code and the teaching code and this is going to be in the notebook that I will share with you after the webcast here and I'll summarize it briefly here my imports ok here's where I read in my data frame I selected my columns I defined my X&Y here I made my column transformer made up of a1 Haughton coder and pass through the remaining columns here's my model here's my pipeline that's a column transformer and a model here's cross validation of the pipeline here is building my ex new data frame and here is fitting and then here's making predictions on new data okay so this is everything we just did if you got lost this is like the work flow all right I'm gonna leave that up on screen while I kind of comment on this for the next couple minutes what we just did is we used one hot encoder column transformer and pipeline here's the question why would we not use get dummies instead because we could have used Pandya that pandas get dummies appended those to the data frame and then defined X based on the pandas dataframe that's what I used to teach as I was saying but why is the approach that I just showed you better it is better in four important ways okay number one you don't have to create a gigantic data frame you'll notice that one hot encoder does not affect our data frame so our data frame stays three or four columns and that's it and that's easier to explore and easier to manage number two when new data comes in you don't have to use get dummies on it right because if you are using get dummies on all of your training data then when out-of-sample data comes in you still have to use get dummies on it plus you're gonna have problems if your out-of-sample data has different categories than your in sample data like let's say our out our in sample data had C Q and s but our out-of-sample data only had C and Q well it's not going to produce the correctly shaped data frame this is going to cause problems all right number three as to why this process with cycle is better you can do a grid search as I was mentioning with both model parameters and pre-processing parameters and then finally reason number four in some cases pre-processing outside of scikit-learn can make cross-validation scores less reliable okay and this gets complicated but basically if you're using a standard scaler if you're doing missing value imputation if you're using text data and a variety of other circumstances if you do your pre-processing before scikit-learn your cross-validation scores are possibly going to be unreliable okay so those are four huge reasons why you should you know ultimately use the process I've laid out rather than get dummies and pandas hope this video was helpful to you if you'd like to join my monthly webcasts and ask your own question sign up for my membership program at the $5 level by going to patreon.com slash data school there's a link in the description below or you can click the Box on your screen thank you so much for watching and I'll see you again soon"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Implementing Categorical Encoding with Scikit-Learn",
        "Video Title":"One Hot Encoder with Python Machine Learning (Scikit-Learn)",
        "URL":"https:\/\/www.youtube.com\/watch?v=rsyrZnZ8J2o",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/rsyrZnZ8J2o\/hqdefault.jpg",
        "ID":"rsyrZnZ8J2o",
        "Publish Time":"2023-08-14T13:10:41Z",
        "Channel":"Ryan & Matt Data Science",
        "Channel ID":"UCKq-lHnyradGRmFClX_ACMw",
        "Transcript":"today we're going to be covering how you can use one hot encoder within python with the help of sklearn pre-processing now one hot encoder is really good when you're working with categorical data the reason why is you're gonna have to transform a category Oracle data into numbers if you're going to run any specific machine learning model now one hot encoder focuses on something called nominal data and if you're not too familiar with nominal data is it's categorical data with no numeric significance so think of something like colors right blue red white orange green there's no hierarchy associated with it unlike something like ordinal data where there's always going to be a hierarchy think of like small medium or large so we want to focus on nominal Theta for one hot encoder and I'm going to show you guys how we can code this out and also another example right now on my computer we start let's import pandas as PD I'm going to shift and enter and every time that runs a cell and creates a new one down below now I'm going to input our data so I'm going to put this down Down Below in the description so that way you guys can copy it and then I'll show you what this data exactly looks like in a second essentially I'm grabbing all this over here I'm going to be building this into a data frame DF and let's just run ADF dot head so that way you guys can see how what we're doing all right so here we go we have sales we also have a city and then we have the size of let's say for example a store so before we go on let's take a second and think what is one hot encoder going to be best to use are we going to take a look at a city or we're going to take a look at the size of the store think about it for a second and the reason why I'm going to be using city is again there's no like numeric difference between these right we have Tampa Orlando Jacksonville Miami uh but when you look at the size of a store we have a small medium and large and we can actually quantify them and we're going to be doing this for another video talking about how we can change these sizes into numbers uh but for this one we'll be taking a look specifically at City okay so next thing that we're going to want to do and we're going to take a look at all the different unique values on this and all you have to do is DF we're going to put City in here and then we're going to put dot unique like this and then we're going to run it which that over here and then you can see the different cities so we have Tampa Orlando Jacksonville and Miami uh the four most popular cities here in Florida so essentially what one hot encoder is going to do is it's going to expand out this table over here and instead of having a city right here we're gonna have like for example Tampa then Orlando then Miami and then Jacksonville and for the first example over here right which is Tampa so for all the other cities it's going to be a zero but for the Tampa new column it's going to be one if you look at this one over here same exact thing right so our Orlando column is going to be zero Tampa will be one in Jacksonville zero Miami zero now on this one we have Orlando so our Orlando column will have a one in it and then every other city is going to have a zero and you'll see how this is built out in a second a much easier to show you when this is coded rather than before okay so with that in mind let's start importing or one hot encoder so all you have to do is from sklearn dot pre-processing we're gonna import and then one hot encoder make sure you have capitals o h and E and now that is imported okay so now we have to call our one hot encoder and I'm just going to say ohe for one hot encoder uh so essentially what we're gonna do is ohg equals and we're going to just copy one hot encoder like this and there's a few things that yeah you're going to want to call inside over here so the first thing is we're going to be taking a look at unknowns like what happens with it so I'm just going to put handle unknown equals ignore and that means I'm just going to ignore now in my data which I'll just show you real quick I clearly defined everything so there's going to be no unknowns but I'm just going to put that in here for this instance next we're going to put a sparse output we're going to put that equal to false and the reason why is we're going to be setting our output instead to a pandas data frame which I think is pretty helpful and the way that you do that also as you do set output right alt puts and then inside over here we're gonna put transform equals and then pandas and it should be lowercase p and essentially what this is going to allow us to do is change the city over here within our pandas data frame really nice so I'm going to shift and enter on that and I do have an error over there and let's see what it is and I could not spell transform correctly and now that I did this is running and working correctly okay so now what we're gonna do is fit our data so I'm just gonna do ohe transform [Music] equals and then I'm going to say ohe dot fit transform which if you use sklearn you're going to know this command quite a lot right and then inside we're going to put our data frame so I'm going to do a DF and this time we're going to throw in our city right and it's a lowercase on this side of things so let's throw our city in here and just to show you guys what this looks like before we merge it back into this data frame I'm just going to put ohe transform and then you can see what I was talking about so earlier we said it's gonna be all zeros and ones so for our first instance right here with the sales right it's Tampa you can see it Tampa is a one our second one sample also we have one over here and then over here with the two we have Orlando right and Orlando over here Jacksonville Miami so just to show you Jacksonville and also Miami another thing to call out is these columns are now created ABC order right we have Jacksonville up first right J then we have M for Miami o for Orlando and then t for Tampa so that's built out automatically on that side of things so one last thing that we need to do on this data side of things is we have our data frame over here right and if I just go over here df.head great we still have sales City and size and then if we go over here ohe transform just to show you it is a data frame right I'll just put the head over here um these need to be merged because I would like to have this data populate in here so one way we can do this is through concat so I'll just show you how to do that right now so we're going to say our data frame is equal to and PD dot cats and now let's start throwing in some other stuff in here so what are we concatting right we're concetting original data frame so I'll put our data frame here like this and then we're going to put ohe transform which we just generated so let's put both of those over here we're going to put axis equals one and then let's think about this right now that we have this it's going to be added in over here we can drop the city so all we have to do is add into the very end dot drop and we're going to put columns equal and then inside over here we're going to say City like this all right shift and enter so now this has run we don't have any errors on it and just to show you this works first we're going to do a df.head inside this head we are going to put 10. and check this out so our city now has been dropped and we have our sales our size of the store and then we have zero zero zero one zero zero zero one and it is working properly so just again kind of to reiterate how this specific code works so first we have this over here sales City and then also size so we take a look only at City uh because this fits what we're gonna be using one hot encoder for our size would not make sense to use this next we take a look at some of the different unique values the reason is is each unique value is going to be a specific column right and we have four over here and just as a heads up each of those specific columns is going to be built out in ABC order where this is not an ABC order right now then we Import in one hot encoder just like that then we call our one hot encoder I throw in here for a handle unknown and then also for the sparse output false so that way we can use our set output over here and we're going to transform uh through pandas so it can turn into a pandas data frame and we fit our data so ohe dot fit transform we throw in here the city column then we have that that's been expanded out then essentially what I do is concat the two data frames together right DF and ohe transform on axis equals one and then we're dropping the city and boom we have our final output hope you guys found this video helpful if you did make sure to subscribe to the channel as it does help and these videos take quite a while to make by the way I have a video on train tests split over here it's another important thing that you need to do with your data before you run a machine learning model and I highly suggest you watch it right now"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Implementing Categorical Encoding with Scikit-Learn",
        "Video Title":"Encode categorical features using OneHotEncoder or OrdinalEncoder",
        "URL":"https:\/\/www.youtube.com\/watch?v=0w78CHM_ubM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/0w78CHM_ubM\/hqdefault.jpg",
        "ID":"0w78CHM_ubM",
        "Publish Time":"2020-10-29T15:15:46Z",
        "Channel":"Data School",
        "Channel ID":"UCnVzApLJE2ljPZSeQylSEyg",
        "Transcript":"two common ways to encode categorical features one hot encoder for unordered also known as nominal data an ordinal encoder for ordered or ordinal data and we'll come back to this statement in a bit but here's our example so we've got this data frame shape is an unordered feature meaning there's no logical ordering to um the categories because of that we're going to use one hot encoder and here we have one hot encoder we pass it shape and it generates three columns because there are three possible categories of shape one hot encoding is the same as dummy encoding this left to right order is alphabetical thus this represents circle this represents oval and this represents square now you could use git dummies in pandas to do this but one hot encoder integrates much better into the cycle learn workflow meaning pipeline meaning cross validation meaning grid search so one hot encoder is definitely the way to go rather than get dummies okay so that is our unordered data our ordered categories are class and size so by ordered i mean there is a logical ordering within each feature that may have a relationship with the target and i'll give an example in a bit but um with class we see first second third and what we do is we define the ordering of the categories when we create our ordinal encoder instance so first before second before third and then for this size feature we say small medium large and extra large then when we use ordinal encoder it encodes each feature as a single feature so you can see that first gets encoded as zero second gets encoded as one and third gets encoded as two and then same here small medium large extra-large becomes zero one two and three now we included m for medium in this data even though it doesn't appear as a size because we know it will occur in the data so it makes sense to include it now when we have ordered data like these this this ordinal encoder is the optimal scenario because the model can learn that relationship between it increasing or decreasing and the target okay so let me give an example that might help so let's pretend we're predicting airplane ticket price and class is one of the features okay the we want the model to be able to learn that increasing the class value decreases the price in other words first class tickets are the most expensive second class tickets are the second most expensive and third class tickets are the third most expensive by using ordinal encoder we can encode that as a single feature and the model can learn okay as the class number goes up the price goes down so it learns that inverse relationship if you were to use a one hot encoder for class meaning dummy encoding it could still learn that relationship in a way but it would only learn the relationship between first class and not first class so if you dummy encoded this it would be three columns and it could learn okay when you have first class and not first class here's the impact on the price but it's even it will result in a better model if you encode it as a single feature using ordinal encoder okay let's go back to label encoder and i said label encoder is for labels not features so label encoder is designed for class labels okay it works very similar to ordinal encoder so label encoder and ordinal encoder work similarly except with label encoder you can't define the ordering and you can only encode one thing at a time whereas with ordinal encoder you can encode multiple features at a time thus label encoder is not useful for features and scikit-learn docs recommend against using it for features they explicitly recommend against it so ordinal encoder is for features label encoder is for labels um and in fact just as a side note all scikit-learn classification models that i'm aware of can accept string values as the labels so you don't actually need label encoder usually so the use cases for label encoder these days are quite limited you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Implementing Categorical Encoding with Scikit-Learn",
        "Video Title":"Quick explanation: One-hot encoding",
        "URL":"https:\/\/www.youtube.com\/watch?v=G2iVj7WKDFk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/G2iVj7WKDFk\/hqdefault.jpg",
        "ID":"G2iVj7WKDFk",
        "Publish Time":"2023-02-10T19:18:34Z",
        "Channel":"M\u0131sra Turp",
        "Channel ID":"UCpNUYWW0kiqyh0j5Qy3aU7w",
        "Transcript":"one hot encoding is a way of representing categorical values in a numerical way how does it work we create an array that has as many elements as the number of categories to represent the category we have an array that consists of zero everywhere except the element that corresponds to this category in that location the value is 1. and why not just show them in numbers because numbers have an ordinal relationship between them meaning 3 is greater than 2 and 5 is lower than eight but categories do not necessarily have this relationship if Category 3 represents sweater and Category 5 represents genes it does not mean that genes are greater than sweaters machine learning models make sense of the data that is given to them based on mathematical equations if we provide categories to a machine learning model in numbers the model will assume the relationship between these numbers represent the relationship between the categories and that will not be correct and any other way to represent one HUD encoding well you can also use dummy variables dummy variables use an array of length n minus 1 instead of n when representing the categories n being the number of categories it uses an array that consists completely of zeros to represent one of the categories for example if cat is 0 1 and dog is one zero and the only other category out there is Turtle it can be represented as zero zero once you prepare your data you will need to split it into train test and validation data sets watch this video to find out why and how we separate our data sets into sub data sets"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Feature Scaling Techniques: Min-Max Scaling, Standardization, Log Transformations",
        "Video Title":"Standardization vs Normalization Clearly Explained!",
        "URL":"https:\/\/www.youtube.com\/watch?v=sxEqtjLC0aM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/sxEqtjLC0aM\/hqdefault.jpg",
        "ID":"sxEqtjLC0aM",
        "Publish Time":"2022-08-31T03:44:56Z",
        "Channel":"Normalized Nerd",
        "Channel ID":"UC7Fs-Fdpe0I8GYg3lboEuXw",
        "Transcript":"hello people from the future welcome to normalized nerd i know it's been a year since i uploaded my last video but as i mentioned in my community post a lot of things happen in my life in the past year and i was super busy with it but finally i am here with a new exciting video in this video i'm gonna talk about feature scaling and normalization and hopefully i'll be able to clear some misconceptions along the way so without any further ado let's get started first of all let's see what feature scaling really is for that we need a feature suppose x is a continuous feature from a raw data set and we haven't performed any preprocessing on it to visualize the distribution of x let's plot its histogram now i'm going to divide every value of x by 2 and this shrinks the histogram well this was an example of feature scaling where we literally scale the feature by half we can also subtract a number first and then divide it by another number here i am subtracting by 7 and then dividing by 2 this shifts the histogram to the left and shrinks it here i chose the subtracting and scaling factors randomly but if we choose some specific values for them then we achieve normalization for example if we subtract by the lowest value of x and divide it by the difference between the max and mean values of x then we call it min max normalization let's see how it changes the data oh it's too thin let's zoom in a bit you can see that now the data lies between 0 and 1 and this is not a coincidence min max normalization maps the data in the range 0 to 1. next comes the most popular one that is standardization here we subtract x by its mean and divide by its standard deviation after applying standardization our histogram looks like this now there's a huge misconception among the beginners that is if we apply standardization then it makes the distribution a normal distribution this is simply not true don't believe me let's take a very skew distribution if we apply standardization on this distribution you see the shape doesn't look like a normal distribution does it well the thing is standardization is a linear transformation all it does is it makes the mean 0 and standard deviation 1. if we want to change the general shape of our distribution we need to apply non-linear transformations like logarithms square roots etc but i don't want to get into those things today let's move on to the most important question why do we even care about feature scaling and normalization well the first reason is to achieve faster convergence take the good old gradient descent algorithm assume that we have two features x1 and x2 and theta 1 and theta 2 are the corresponding weights i'm ignoring the bias term here if we assume the least square cost function the weight update steps will look something like this now let's think of a scenario where we haven't done any feature scaling hence x1 and x2 have vastly different ranges let's say x1 lies between 0 and 1 and the range of x2 is 100 to 1000 so what do you think will happen during gradient descent due to the small value of x1 the step size for theta 1 will be small on the other hand as x2 is huge the step size for theta 2 will be large and this difference between the step sizes can cause unwanted oscillations during the optimization and delay the convergence the second point is related to computing distances we know that memory based algorithms like k n and k means depend on computing distances between the data points but what happens when the features are not scaled properly let's take an example here we have two different features along two axis you can clearly see that the range of the feature along the horizontal axis is much bigger than the other one so when we compute the distance between any two points the horizontal component dominates the vertical one but it is not necessary that the horizontal feature is more important it's just due to the bigger scale let's apply normalization along both axis now if we compute the distance you see both features contribute roughly equally to the distance and by using normalization we made sure that the algorithm you use won't be affected by the feature with a higher scale so that was it guys i hope you learned something new about feature scaling today and if you did please share this video and subscribe to my channel stay safe and thanks for watching [Music] you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Feature Scaling Techniques: Min-Max Scaling, Standardization, Log Transformations",
        "Video Title":"Standardization Vs Normalization- Feature Scaling",
        "URL":"https:\/\/www.youtube.com\/watch?v=mnKm3YP56PY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/mnKm3YP56PY\/hqdefault.jpg",
        "ID":"mnKm3YP56PY",
        "Publish Time":"2019-11-07T13:20:38Z",
        "Channel":"Krish Naik",
        "Channel ID":"UCNU_lfiiWBdtULKOw6X0Dig",
        "Transcript":"hello my name is Krishna and welcome to my youtube channel today in this particular video we'll be discussing the basic difference between normalization and standardization now guys I hope you have heard of this particular topic it is a very important topic in feature scaling which is an integral part of feature engineering so we should try to understand that why he should use feature scaling and what is the basic difference between normalization and standardization I'll also say that when to use this particular thing and when you should prefer for which algorithm you should prefer using this normalization or standardization technique you know and for which algorithm feature scaling suits you know it is not necessary that you need to apply the feature scaling for each and every algorithm okay so we will try to understand that I'll also show you the coding pattern coding how we can basically code how we can basically perform normalization and standardization with the help of a small data set that I have and I bounded it from tagging you know so I'll show you that particular example so make sure that you watch this particular video till the end now guys suppose if you have a use case and the most important thing for a use case is data so initially you'll be collecting the data and once you collect the data you will be having lot of features okay and those features will be including independent features and independent feature right with the help of the independent feature you will try to predict the dependent feature in supervised machine learning right so when you consider this features this features has two important properties one is unit the other one is magnitude I have collected some of the data with respect to a person and I have collected data like age weight height so all this particular information is collected now if I consider these two important properties that is unit and magnitude now if I consider the feature age the unit that is basically used to calculate age is number of years right from his date of birth like how many number of years has been happened right so apart from that the magnitude thing if I talk about magnitude that is basically the value suppose if I say 25 years the person age is 25 years so 25 over here is the magnitude and years right years is basically your unit so this is the basic thing and for each and every feature this will be there it will be either calculated with the use units it will be calculated with the help of units and magnitude now the main thing to understand is that if you are having many features it will definitely be getting computed by different different units and magnitudes right it need not be always same because if I take an example of height feature so in height feature it may be calculated using feed it may be calculated using hinges right so this unit and magnitude will always vary between all the features so it is very very necessary that for a machine learning algorithm the data that we provide reg you know we should try to scale down that particular data into some scale now what kind of scale I'll just discuss about that in just a while so the two skills the two most common techniques that is basically used is normalization and standardization now if I give you a simple definition of normalization normalization helps you to scale down your feature between 0 to 1 this is what is the definition yes definitely I will show you the formula when I will be actually showing your practical application but just understand that in simple terms normalization helps you to scale down your feature between 0 to 1 now what about standardization standardization will help you to scale down your feature based on standard normal distribution now if I talk about standard normal distribution over there the mean is usually 0 and the standard deviation is usually 1 right so this is the basic difference between standardization and normalization right and which one to use when I just discuss once I show you the practical application and then you will be able to understand more nicely now let us go ahead and try to see a practical application and I also show you the formula of a normalization which is also called as min max gala and show you how we can use a scale on library in order to perform that with the help of Python code and I'll also be discussing about standardization and the library name is basically called a standard scale ax so we'll discuss about this so let us go ahead and try to see the practical application now let us go ahead and try to see a practical application and we'll try to see the basic difference between normal and discuss I when we should be used normalization and we should be suspended ization nowadays if I just talk about normalization over here you can see that the mean definition is that we need to scale down the values of the feature between 0 to 1 and this is the formula that is basically mentioned so this is basically like X minus x mean divided by x max minus X me okay so this is basically the form of min\/max Killah which will actually scale down your values between 0 to 1 now oh hey I'm taking an example where I am basically written that p dot read underscore seriously which is the Andaz function and I'm reading this particular content and now all this particular code will be given in the github link so you can basically consider this from the be coupling I give in the description box you can download it from there okay so over here what I'm doing is that I'm just considering the three columns initially from this particular CSV file which is called as my heartis could you cut our CSV and then I'm renaming the column columns as class alcohol and Malak so these are the properties that are present inside that particular dataset so if you consider about wine it is basically combined with various various features you know various chemicals that is mixture of various chemicals and it is basically prepared so once I do DL dot head over here you can see that my output looks something like this right the door five record so I have class alcohol and Malak now in order to show you how to perform in max scalar which is also called as a normalization and again there are varieties of normalization but many people prefer mil max kala in some of the use cases and again I'll discuss about the use cases when you should use that so here I've written from a scale unload pre-processing import min max Killa then I am basically creating an object of min max kala which is inside the scaling object and then I'm just doing fit underscore transform and I'm passing which on features I need to basically scale down now understand guys over here this alcohol and value will be basically calculated or note it down here based on various units and magnitudes right so there is a huge difference over here right there is a huge difference between this but again still I hope you are not burned the idea when we should do it just wait for some time and just explain you men you should be sitting perform this okay so over here you can see that the values are completely different this this may be a bigger number than in some of the but it may be a smaller number right so the magnitude is huge away so we should try to scale down between the same scale and for that I'm using min max kala so as soon as I do this and I pass the flip transform and ask their attributes inside this you can see all the values present inside this and it is getting replaced between I mean it is getting scaled down between 0 to 1 so you can see that the maximum value present over here is 1 and then the minimum value will be somewhere between 0 and 1 you can see all the values right so yes this is what it is happening and always remember this feature the the form that I have actually shown you will get applied to each and every feature right so in that particular way it basically works now similarly if you want to perform standardization which is also called as expo normalization this is the basic formula that you get that is X minus X is basically my future date of minus mu is mean divided by standard deviation and you know that why do we use standardization yeah all the features will be transformed in such a way that it will have the properties of so so here all the features would be transformed in such a way that I didn't have the properties of standard normal distribution with mean is equal to 0 and standard deviation of 1 so this is basically the formula that you can see oh yes that is equal to X minus mu bus standard deviation now in order to perform this and again remember the main thing is that the mean will be 0 and standard deviation will be 1 and for whichever feature it is it will be getting transformed or scaled down in that particular values so if I go ahead down and see over here you can see that from Escalon dot pre-processing you have to import this particular library which is called a standard scalar now standard scale ax again I am Telling You it actually scales down a value considering mean is equal to 0 and standard deviation equal to 1 so as soon as I create an object for Stanek scalar and I do fit underscore transform and it passed the attributes like alcohol and malloc you can see that all the values is got transformed considering mean is equal to 0 and standard deviation equal to 1 in that particular way and usually in this scenario will be giving a if it is getting converted in the standard normal distribution it will be get converted like bell curve but the mean will be over here a zero and each and every standard deviation to the right if I go the value will be one you know one two three like that if I talk about the standard deviation in this case so this was how it is basically done very simple way you just have to use standard scaler and in maxilla and trust me guys this is the most popular used library I mean technique like mill masculine standard scale up for most of the problem statement now I will go ahead and try to make you understand when you should use standard normalization and and you should use min max Allah so let us go ahead and try to understand that let us go and understand when we should use standardization and when you should use normalization and most of the scenarios guys whenever you are using some machine learning algorithm which involves Euclidean distance okay and suppose some of the deep learning techniques where you where gradient descent is basically involved you know gradient descent basically means is a parabolic curve where you need to find the best minimal pattern or global minima point right so that particular point in order to retrieve that particular point if we want to have if you want to get that particular point merged quickly you basically have to scale down that particular values so some of the algorithms like K and n K nearest neighbor K means clustering you know all the deep learning and artificial neural network convolutional neural network so in all these particular cases we have to basically perform scaling guys some of the algorithms are you don't have to perform scaling is some somewhere like decision tree is random forests XZ boost all the boosting techniques when I consider the bagging technique and the boosting technique which in was decision tree you don't have to scale down your values because there is no use of scaling down because at the end of the day you are just creating a decision tree right now decision tree is basically divided based on the features if you keep the value high or if you keep the values small it won't affect that much because based on some conditions the branches will be created in the decision tree but definitely for some of the algorithms where you are discussing like KNN k-means clustering linear regression logistic regression because in linear regression also we consider to that gradient descent tourists reach the global minimum point right so they're also you have to do the feature scaling guys now if I talk about normalization and standardization which techniques should be used and when it should be used now then based on my experience so for some of the use case or for many of the use cases where I have actually used standardization which basically means that the mean will be zero and standard deviation will be one it has basically performed better than the min Max kala okay which is basically a normalization technique now it is not like that min max Keller is bad and it is it should not be used for most of the deep learning techniques were using convolution neural networks right and artificial neural network you basically perform in max kala because you need to scale down your values between 0 to 1 now if I take the example of images your images are between 0 to 255 pixels so if you want to scale down that value you always have to do it between 0 to 1 okay and usually for the images it is done similarly for the artificial neural network the neural networks that you create by using any of the libraries like tensorflow and Cara's definitely they would accept the inputs between 0 to 1 which will help them to learn the weights quickly so this is the basic difference between normalization and standardization and I've also explained you how you should go ahead and when you should basically use it but for most of the scenario in machine learning algorithms if I talked about standardization performs well and that is completely based on my experience you know so this was the basic difference between standardization and normalization and I hope you have understood it I have also played you with the help of code now guys if you are looking for some carrier transition advice you know with respect to data sense how you should be simply move I am basically given a link in the description because I found that YouTube channel very much good because most of the advice that was given by data centers like how they work how did they make the transition will be available in that particular channel so I have given that particular link in the description can go ahead and watch it definitely will give you a whole lot of idea right so this was all about this particular video I hope you like this particular video please do subscribe this channel if you're not already and then I should you have a great day thank you one and all"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Feature Scaling Techniques: Min-Max Scaling, Standardization, Log Transformations",
        "Video Title":"Normalization Vs. Standardization (Feature Scaling in Machine Learning)",
        "URL":"https:\/\/www.youtube.com\/watch?v=bqhQ2LWBheQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/bqhQ2LWBheQ\/hqdefault.jpg",
        "ID":"bqhQ2LWBheQ",
        "Publish Time":"2022-04-24T20:58:17Z",
        "Channel":"Prof. Ryan Ahmed",
        "Channel ID":"UC76VWNgXnU6z0RSPGwSkNIg",
        "Transcript":"hello everyone and welcome to this new video on feature scaling today we will learn about the difference between normalization and standardization let's go ahead and get started so feature scaling is an important task or step that we do prior to training of any machine learning models and the objective is to ensure that all features that we have in our data have the same scale so let's take a look at the practical example let's assume that here i have raw original data set and this data set contains three columns these are interest rates employment score and s p 500 so if you guys kind of scan through the data you would notice that the range or the scale of that data is just completely different interest rates here if you take a look at that quick stat or the statistical summary about my data you will notice that the minimum value here of the interest rates is between 1.5 and the maximum is around 3. if you check out the employment which is again this is not employment percentage per se it's an employment score just scored that indicates how powerful the stock market is how strong it is so if you check out the employment you will notice that the minimum is 40 and the maximum is 70 and the s p 500 between eighteen hundred dollars and three thousand dollars so basically if i decided to go and train an ai model or a machine learning model basically to with this data i need to make sure that all the data sets all the features are scaled they have the same scale so basically i'm not going to have one feature that dominates the other feature so to do that we are going to cover two different techniques or strategies so the first strategy is what we call it normalization so normalization is conducted to ensure that features could range between 0 and 1. so all i'm looking for is to go back here and take a look at the interest rates employment s p 500 and just make sure that all of them range between 0 and 1. so min max values here would be 0 1 0 1 0 1 that's the objective so let's learn how we can actually calculate that so to do that this is simply the equation i'm going to get any number i'm going to subtract the minimum value and then i'm going to divide by the maximum minus min so i'm x minus x min divided by x max minus x min and that is going to give me the updated or normalized value so luckily i don't want to go and actually write like you know like a large code large lines of code that contains or kind of perform that process or even write this equation all i need to do is to basically leverage a library known as learn library and if you guys hang in there until the end of the video i'm going to show you a detailed video explanation where we're going to walk you through an actual code that perform the exact same process here in psychic learn so these are simply how the three lines of code where we can use them to perform what we call it min max scalar so min max scalar is simply normalization so what we do first is we say from scalar dot pre-processing i'm going to import my min max scalar class and then i instantiate an object out of my class i say scalar equals to min max scalar and then i grab that object apply the fit transform method on it pass it along the data and that data here is going to be stock underscore df which stands for data frame and then i'm going to override that data and just generate a normalized or scaled output okay so let's take a look at an output an actual example from a normalization process so if you guys see here here i have my pair plot this is simply the different scatter plots between employment and s p 500 and essentially with every every feature that i have in my data so after you perform normalization so here this is the raw data this is the normalized data what you guys notice is the range kind of the scale of my data is completely different now now employment used to range between 40 and 70 now it range between zero and one if you check out interest rates same deal interest rates used to be between 1.5 and three and now it's between zero and one so essentially if you take a look at the normalization process output all what i'm doing is i'm scaling my data to range between zero and one if you take a look at the statistical summary of the data you will notice that here i had the minimum value of the interest rate was 1.5 maximum was 3. if you check out interest rates now it's between 0 and 1. same deal as well for employment it's between 0 and 1 and s p 500 between 0 and 1 and that's the main objective of the normalization process so let me show you an actual example where we go and actually substitute in this equation kind of go through a bunch of runs so what i'm going to do is i'm going to use four sample data points i'm going to grab again that's from the raw data i'm going to grab a random number that would be 2 206 i'm going to grab the average value i'm going to grab the maximum and the minimum okay so these again are i've been able to obtain from that table right and let's go ahead and let's pick maybe that randomly selected number which is 2206 and go and substitute in this equation so if i substitute in this equation you will notice that 2 206 minus the minimum value which is 1800 divided by maximum minus minimum which is again 3 000 minus 1800 and what i'm going to get is 0.338 so i'm going to go here and put that value here so 2206 in the raw data has an equivalent of 0.338 in that scaled or normalized version of that data what if i substitute the average well if you take the average put it here again run through it you will end up with 0.432 if you grab the maximum value which is 3 000 and you substitute here you will notice that the value here the normalized value will be one if you grab the 1800 and you put 1800 here 1800 minus 1800 will be zero and that's why you will get zero here in the output so basically again the objective of the normalization process is to transform the min max value from whatever number that is to be between zero and one and that's what i'm getting here and i was able to confirm that again by substituting in this equation okay what about standardization well standardization is conducted to transform the data to have a mean of zero and a standard deviation of one please note that this is drastically different compared to normalization which is what we covered before and standardization process as well is known as what we call the z-score normalization and essentially we try to shift our data to have an average or a mean of zero and standard deviation of one okay so what about the equation the equation is simple i simply get get any data point i subtract the average so this is the average or the mean value of my distribution of my data and then divided by the sigma which is my standard deviation so here if you go ahead and see how we can actually perform that in psychic learn library i'm going to do the following i'm going to say from dot pre-processing i'm going to import the standard scalar and then i'm going to instantiate an object here so i'm going to say scalar equals to standard scalar and then i'm going to apply the fit transform method on my object pass it along the old data or the raw data to generate a brand new data set and that is going to be my standardized data set so let's take a look at an actual example so what you guys see here on the left hand side that's again the raw data on the right hand side here that's the standardized data after we did the transformation so what you guys notice is here the raw data for employment for example range between 40 and 70 in the standardized form it ranges between minus like three and three some somewhere along those lines and this is quite important when we perform the standardization process we don't care about the min and max value we care about the average of the distribution or the mean of the distribution and the standard deviation of that distribution so what you guys notice here is if you check out kind of plot the statistical summary about my data you will notice that here the average was 2.2 in my raw data in my updated standardized form it becomes everywhere becomes zero all the mean becomes zero if i get the standard deviation well the standard deviation becomes all of them becomes one and that's exactly what i'm looking for so let's take a look at an actual example to kind of go again select four random points and just go and substitute in this equation so here i have a randomly selected value of 2 206 if i substitute 2 206 minus 2319 divided by 193 i will end up with point point five eight three i'm just going to put it here if i grab the average value which is two three one nine and i substitute in this equation two three one nine minus two three one nine which is the mean i would get zero and that's the overall intent here the idea is the output here was going to be zero okay and of course the maximum value could be any anything else it could be 3.5 here for example and the minimum is going to be minus 2.67 so always remember kind of the key takeaway is a normalized data set will always range between zero and one however a standardized data set will always have a mean of 0 and standard deviation of 1 but it can have any upper and lower values okay we don't really care about the upper lower values we care about the average or the mean will be zero standard deviation or the dispersion away from the mean is gonna be one okay so now to the big question okay when should i use standardization versus normalization when why do i even need it well so scaling which includes standardization and normalization or normalization per se is required when you use any machine learning algorithm that requires you to calculate the gradient so if basically when we train any machine learning model let's say an artificial neural network in order to update the network weights i need to calculate the error and i need to propagate the error back okay using the gradients essentially to update my weights and whenever i do that i need to make sure that i'm performing the scaling and the reason is is because i'm calculating that gradient i wanted to make sure that all the features have the same scale so i won't have a feature that will dominate the other feature okay and okay what examples for example can share me with me examples well i have linear regression logistic regression artificial neural networks deep neural networks all of that will require you to calculate the gradient and if you require that you need to make sure that as part of the pre-processing phase that you are running or performing standardization or normalization please note that scaling is actually not really required when you perform distance based or three based algorithms for example if you're using k-means clustering if you're using support vector machines if you're doing k-nearest neighbors decision trees random forest xgboost all these algorithms which are distance based and kind of or three based algorithms we don't really need to calculate the gradient so we don't care about standardization or normalization you can just go ahead and run the algorithm all right so when to use standardization versus normalization so generally speaking there is no right or wrong answer when it comes to that in case of artificial neural networks for example normalization is preferred since we don't assume any data distribution beforehand so in general standardization is preferred when data follows a gaussian distribution so if you have data that follows gaussian distribution it's better kind of again general guideline to use standardization and standardization is preferred over normalization when there are a lot of outliers in the data okay all right so what i'm going to do next is i'm going to go ahead and shift to the jupiter notebook and show you guys the detailed code where we are going to grab data set and perform standardization and normalization on it from scratch again if you guys like this video so far i would really appreciate it if you just give it a like and subscribe as well for more videos let's go ahead and get started all right so here we go now i have my jupiter notebook and here i have the standardization versus normalization basic process first i'm going to go ahead and import all our key libraries and data sets so first i'm going to import pandas as pandas is primarily used for data frame manipulation and it's going to be really powerful when we deal with tabular data okay again think of it as excel basically in python environment here and then i'm going to import matplotlib and cbor and matplotlib and cborn are primarily used for data visualization and if you press shift and enter on your keyboard that is going to run or execute the cell and once you see a number here that indicates that that cell ran successfully next i'm going to go ahead and use pandas to read my csv file so i'm going to say pandas pd dot read underscore csv and then i'm going to grab the s p 500 stock data and if you press shift enter here we go here i have my data here i have my interest rate i have my employment and i also have the s p 500 price please note that basically these three columns here are at our different scales right and that's why i wanted to actually scale them up i wanted to normalize them or standardize them so let me show you but before we do that i'm going to go ahead and plot the pair plot so if i say sns dot pair plot sns is simply um again seaborn and pairplot is really powerful plot as you guys can see here that can show me a different scatter plots between every single feature that i have in my data for example here i have interest rates versus s p 500 price and all these circles here all these dots that's every data point that i have in my data so for example if i go back every row will basically be here a data point essentially here i have employment versus s p 500 and so on so what you guys notice is well the range here the scale basically is different right so here i have the interest rate range between 1.5 and 3 employment range between 40 and 50 and so on if you wanted to basically plot the statistical summary about the data all i need to do is to say grab my pandas data frame and apply the describe method on it and it will generate a table like this and here i'm just saying i'm rounding all the values to two basically data points here so what you guys notice is i have the minimum value is 1.5 to 3 for interest rates the employment value here is between 40 and 70 and the s p 500 is between 1800 and 3000. so let's go ahead and learn how to do normalization in psychic learn so as you if you guys recall normalization just scales the data to range between zero and one that's all i'm looking for and this is the equation i grab any number i subtract the minimum and then divided by the max minus minimum so what i'm going to do is i'm going to import the data again here i read the data again and that's how you perform normalization in psychic learn what you do is you say from sklearn.preprocessing i'm going to import min max scalar and then you're going to instantiate an object here out of my class and then you're going to apply the fit transform method on my object pass it along the pandas data frame and then overwrite your pandas data frame again and if you press shift enter it might take some time here we go and because here when we do the standard scaler is it's going to convert it basically into a numpy array and that's why i just need to shift it back to a pandas data frame so here i'm using the pandas data frame constructor method pass it along here my stock df and i'm just kind of think of it i'm recreating my pandas data frame again with my normalized data set here i have the interest rates employment and s p 500 the name of the columns shift and enter and what i could do next is if i check out or plot the pair plot okay for my data and the statistical summary of my data as well what you guys notice here is well the employment values and interest rate values and the s p 500 all of them right now range between 0 and 1 which is exactly what i'm looking for so basically it looks like the actual operation the normalization operation was successful let's confirm that here with our describe using the described method what you guys notice here is the minimum value for all of them are zeros and the maximum values for all of them are one which is perfect okay let's go ahead and learn how to do standardization in sklearn or second learn so as i mentioned what we do here is we try to set the mean value to be zero and the standard deviation to be equals to one and this is the equation kind of we substitute in we get any value we subtract the average or the mean and then we divide by the standard deviation if you run it shift and enter here we go and what i could do here is i'm going to use scalar i'm going to say from scalar dot pre-processing i'm going to import standard scalar and then i'm going to instantiate an object called the scalar and then i'm going to use the fit transform method on my object which is scalar pass it along stockdf to generate my new updated pandas data frame shift and enter and then shift and enter again to create our pandas data frame here using the data frame constructor method and if you press two shift enters again you will run the pair plot here we go so what you guys notice here is when we actually plot the pair plot for my data you would notice that the min and maximum values right now actually we don't know we don't care about it so you notice that the min max value for example for interest rate range between like let's say minus three to let's say three here the s p 500 price was like between minus two to let's say 3.5 something like that and the idea here is again when we do standardization all we care about is the average the mean and the standard deviation min max value could be anything and that's what we can confirm here as well so you would notice that the mean is all zeros standard deviation is one in all of them however if you check out the min and max value you'll notice that they could be essentially any value okay all right so that's it that's simply all i have for this video i hope you guys enjoyed it and see you guys in future videos"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Feature Scaling Techniques: Min-Max Scaling, Standardization, Log Transformations",
        "Video Title":"Normalization in Data Transformation | Min-Max &amp; Z-score Techniques with example",
        "URL":"https:\/\/www.youtube.com\/watch?v=lggqjmQzsGI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/lggqjmQzsGI\/hqdefault.jpg",
        "ID":"lggqjmQzsGI",
        "Publish Time":"2023-10-10T12:50:04Z",
        "Channel":"Gate Smashers",
        "Channel ID":"UCJihyK0A38SZ6SdJirEdIOw",
        "Transcript":""
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Implementing Feature Scaling with Scikit-Learn",
        "Video Title":"Python Feature Scaling in SciKit-Learn (Normalization vs Standardization)",
        "URL":"https:\/\/www.youtube.com\/watch?v=6eJHk8JYK2M",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/6eJHk8JYK2M\/hqdefault.jpg",
        "ID":"6eJHk8JYK2M",
        "Publish Time":"2023-08-06T18:31:58Z",
        "Channel":"Ryan & Matt Data Science",
        "Channel ID":"UCKq-lHnyradGRmFClX_ACMw",
        "Transcript":"today we're going to be taking a look at how you can apply a feature scaling to data within python through the help of scikit learn now what feature scaling allows you to do is help improve your models and their performance by converting numerical values into the same exact scale now I'm going to show you two different methods on how to do this both normalization and also standardization now what normalization allows you to do is have numbers from 0 to 1. now the formula that scikit-learn uses is you take your x value minus the minimum and you divide that by the maximum minus the minimum and the package that you're going to be using is min max scalar now standardization on the other side of things has a mean of zero and a standard deviation of one think of kind of like a normal distribution that looks like this over here now this is done by taking X minusing the mean and dividing it by the standard deviation and unlike normalization which is bounded between 0 and 1 fact you could have negatives and you can have positive numbers much larger than one think of maybe two or three since you can have larger numbers standardization is much more preferred when you do have outliers within your data with that being said I'm going to jump on my computer right now and let's code these through some examples okay so let's start off with importing pandas as PD should be super familiar with that especially watch other videos on the channel so importing over there a next thing we want to do is create our data frame now I already have some data for you guys I have it down below in my GitHub it's 500 hits CSV so PD dot read CSV and then I'm going to put over here 500 hits dot CSV now this one you're gonna have to add some encoding in here just because how the file was created and pulled it I talked about this another video but just make sure it's Aladdin one like this again feel free to copy this code exactly and now our data frame is defined now I am going to drop two columns on this one um so many dropping and I'll show you what actually let's show you what this data frame looks like first so I'll just show you real quick before you drop those so this is a header five uh so essentially this is a bunch of different baseball players most of them have over like 1500 or 2000 hits I pulled it from ESPN obviously if like player years games at bats runs it's doubles triples home runs RBIs walks strikeouts stolen bases caught stealing batting average and Hall of Fame uh data here doesn't matter too much I just want to show you guys what the initial side of things look like before we end up changing it with both the standard scaler and also the min max scaler so at least you guys can see on that side of things now the columns I'm gonna drop are player um because we can't change this out essentially that these are going to be all strings over here and we need numeric values so I'm going to drop that one over here and I don't really need Cod stealing I did another video with this data and it doesn't really matter too much to me so what I'm going to just do is the DF equals DF dot drop and then you just have to put in here what columns so I'm going to say columns equal and then in here player all caps the comma over here and then CS that's and then those are now dropped so just to show you a little bit more info now that I've dropped these two columns you just put in DF dot info and you can see all these over here and batting average is afloat which makes sense you can see over here decimal zero three six six zero three three one three four five three ten three twenty nine and then int again so all data that we can quite change up the next thing I want to show you guys is a describe so DF dot describe and I'm gonna round this as well so we're just gonna do round and you can put three on that side of things uh so you can see how this changes up over here so like the mean years is 17 with this data uh games 2048 at that 7511 runs 11.50 so like these numbers are all over the place right because you have something like a batting average which is 289 uh for the average over here and then on the other side of things right Excel base is 195 strikeouts a47 hits all the way at 21.70 so we're gonna standardize this all across the board now what we're gonna do is use ilock and I'm going to assign X1 and X2 I don't want this also the hall Fame over here because this is the desired outcome uh so what I'm going to grab is these 13 over here and label them now X1 I'm going to put through a standard scaler and then an X2 I'm gonna put through the min max so you guys can see the difference on that now the code is going to be the same on these so all I'm going to do is DF right DF dot eye lock and then a colon over here we're going to put a comma and say 0 through 13 and I'm just going to copy and paste this both of these they're exactly the same but that way you guys can see how this data will change up here at the very end and with that being said we can start applying standard and min max Skilling so I'm going to start with standard and pretty easy to import so all you have to do is from SK learn Dot pre-processing [Music] we're going to import and then stand scaler scalar like this and then once this is imported you need to call your standard scaler so I'm just going to do is scale I'm going to put standard like this um usually I just put scalar but just because I'm going to differentiate between the two just want to show you guys how that works and then just call this so I'm just going to copy this over here we're doing nothing too special on this one right and now what we're going to do is fit the data if you're familiar with a lot of sklearn we do that all the time so what I'm going to do is exit one equals and I'm going to say scale standard dot fits transform and I'm gonna just throw X1 in here now okay which is great up next I'm going to turn X1 back into a data frame um so it's currently not a data frame but we can do that pretty easily with pandas so I'm just going to do X1 equals PD dot data frame like this then we're going to just put X1 in here and then we have to Define all these different columns that we're going to use I already pre-typed this out so I'm going to copy and paste it um of that's just make sure everything is correct we closed it out here at the very end boom we have that over there and now we can take a look at this again so let's just do over here like X1 X1 dot head so you guys can see what this data looks like over here so our first Five results over here so here is 2.5 1.79 1.791 1.4 right home runs negative and makes sense right because essentially what we're doing on this side of things is we're making our mean zero and our standard deviation one which is kind of cool to see on things like whoever this player was over here hit a lot of triples right a lot of triples didn't hit a lot of triples so it's actually below average and just to show you how this works as well we're just gonna put over here X1 dot describe like this and I'm gonna do a rounding of three so round three just to clean up the data a little bit and here we go right so our counts are still 465 across the board our mean now is going to be zero which is great because now we can see where this data lies and our standard deviation is going to be one now you can see the minimums and maximums are gonna be different for everything across the board which is going to be different than our min max which I'll show you guys how to do that shortly but for example years right our minimum is negative 2.1 but our maximum is is 3.2 that's based off of the standard deviation over here you can see the 25 50 and 75 it's all going to vary across the board quite a bit um but this data is well well cleaned up uh especially on this side of things right like like go back over here this was all over the place right like hits if you take a look at the mean 2170 but now hits our mean is zero and we've built it around this in division of one okay so hopefully you got that down remember this is standardization now we're gonna go into normalization now normalization we're going to have everything go from zero to one and you'll see the Max on all these is going to be about one and our minute is going to be zero now our standard deviation and also our mean are going to be completely different but again another way to kind of format the data for that way you put it into your models so very similar process as the one before so we're going to go into SK learn pre-processing so sklearn dot pre-processing like that and we're going to import min max scalar now that is imported on that side of things so again scale I'm just going to put min max and I always just like put scalar or scale either way and we're going to put this over here min max scalar and what's kind of cool is you don't have to always do zero to one I'm just gonna do this for this example because I prefer that but I'll show you how you can change that so you have something called feature range here and then we're going to put equals and I'm gonna just do zero one but feel free to change this to however you want for your own models but in this example I'll do 0 to 1. next I'm going to Define X2 again so and scale that so we have this X2 we're going to say scale min max and we'll do fit transform so pretty much identical is the one from earlier right from the standard scalar and now X2 is based off of the submit Max scalar now I'm going to just copy and paste this code essentially what I'm doing is like above so X2 is pd.dataframe X2 and just copying over all these columns I just want to type them out right and now we have our data frame with X2 so if we just go over here and do X2 dot head and run that you can see now right 0.86 0.73 0.6 we're not going to have anything larger than one now if you remember from earlier right this triples over here was 4.38 on the standard scalar side of things which is really high right down here 0.95 again super high but it's not over one and then you can see down below and number three 0.205 if we go to number three over here negative 0.25 so just a different way to represent your data especially when you're going to be building out your models and what's kind of nice too is if we go over here the X2 dot describe and just like earlier I'm going to do a round of three so just round three you can see we still 465 across the board or I mean it's going to be different for everything right instead of zero we have 0.403 0.363.343 our standard deviation is also different instead of one now we have 0.184 0.179 right and our minimum across the board is zero with our maximum being one so just to put these side by side just to show you guys how that works if we just do X1 over here again mean different standard deviation different and our data is now cleaned up hope you guys enjoyed this video if you did make sure to subscribe to the channel as it does take a while to produce these videos by the way if you want to use a min max scalar in a model I have a video over here where I applied this to a k n"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Implementing Feature Scaling with Scikit-Learn",
        "Video Title":"Feature Scaling | Machine Learning with Scikit-Learn Python",
        "URL":"https:\/\/www.youtube.com\/watch?v=mmnLkKYvGG8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/mmnLkKYvGG8\/hqdefault.jpg",
        "ID":"mmnLkKYvGG8",
        "Publish Time":"2019-04-04T22:46:16Z",
        "Channel":"Normalized Nerd",
        "Channel ID":"UC7Fs-Fdpe0I8GYg3lboEuXw",
        "Transcript":""
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Implementing Feature Scaling with Scikit-Learn",
        "Video Title":"Easily use Feature scaling with Sklearn | Data Preprocessing In Python episode 6",
        "URL":"https:\/\/www.youtube.com\/watch?v=2PLvHJTyUAc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/2PLvHJTyUAc\/hqdefault.jpg",
        "ID":"2PLvHJTyUAc",
        "Publish Time":"2021-09-18T22:00:01Z",
        "Channel":"Islander Intelligence",
        "Channel ID":"UCq4rE6ASeDC_IV2O-ngzAbQ",
        "Transcript":"what's up everyone i hope you're all having a fantastic day today's video is going to be all about feature scaling so sit back relax with your favorite snack and let's get started shall we hey honors i hope you're all having a fantastic day as always my name is wayne mckean and for those of you brand new to the community the island robotics community is all about using python to learn application machine learning and artificial intelligence through demonstration and today's video is going to be about like i was just saying feature scaling now there are two types of feature scaling there's standardization as well as normalization we're not going to be dealing with normalization really just to understand feature scaling is standardization also known as standard scalar now i can sit here all i want and be like one of those people that have those fancy little things going on right here but instead i'll be showing all of you real world applications of standard scalar starting off with is by heading over to a jupiter notebook where we're going to be having the data well the islander wrote if i can speak correctly we're going to be having the eyeliner data pre-processing data discovery module up on the screen really what reason being is because jupiter notebooks allows us to see the whole head method where if you were just to run in pi charm or any other text editor or ide we would only get a small snippet of what is being produced by the head method and really we want to be able to view the head method to its full potential so that we can actually make a decision on which columns need feature scaling so honestly what we can see right off the bat is if we look over here at total bedrooms actually let me zoom in real quick all right right there that's exactly what i want we can see that total bedrooms has values ranging from 129 all the way up to 280 but for some reason in row 2 we have this huge jump to 1106. now what's what's going to happen when we create our algorithm or model is that that one column or that one row actually is going to get way more than any of the other rows within that one column we really want to prevent that because it's going to make our data set really biased towards that one row where in reality we really want our machine learning model to produce the most accurate predictions without any biases so what we're going to be doing is standard scalar which is going to be preventing that by essentially taking the whole mean of that column and it's going to therefore change all the values not the shape the shape is going to stay the same but it's going to change all the values to represent the mean value of that whole row and the way that we're going to be able to do this is actually very simply by we're going to be coming over to a python console where i have it pretty much set up with pandas and we have defined our data which is going to be a pandas data frame and after that what we're going to want to do is actually we're going to want to say from sklearn dot pre-processing import standard scalar now we're going to next be creating a brand new instance of standard scalar by saying scalar e no equals standard scalar and there's gonna be nothing within these parentheses but the next line we're going to actually be fit transforming our data to the scalar object so we're going to say new scalar equals scalar dot fit transform of data at column total bedrooms all right now we also have to reshape that column into by saying array dot reshape negative one comma one now we can leave new scalar just how it is if you would like but really what you have to think about is new scalar and our original data set are considered two separate entities so we have to combine them by simply saying data dot drop at columns equals total bedrooms and we're also going to say in place equals true so that way all the changes we make to this data set is going to be finalized next thing we're going to want to do is actually create a brand new column of total bedrooms by simply saying data at total bedrooms equals new scaler and just like that when we say print head well no not head data at total bedrooms dot head we will get this output which is exactly what we want so you can see our first instance was originally not negative 0.97 but as you all can see that there is a more of a gradual increase of what's going on within this one data well within this one column which is exactly what we want all right so that was a lot of code as well as we're not going to only want to be doing this feature scaling or standard scaling to one column but we're going to want to do it to several different columns that we feel fall within the rules that i just talked about now for you islanders are just getting into machine learning artificial intelligence or maybe just getting into programming those rules may be feeling very overwhelming to you luckily for all of you we have one really cool trick that's going to help you out with all this all right so we're back over here within the python script and what we're going to be doing in order to actually achieve this one little trick is we're actually going to say scalar equals ir dot feature scalar alright yes within the community's python package we have this module called feature scalar and what it does is allows us to pass in a pandas data frame in our case it's going to be called data and it's going to evaluate the whole data set and make decisions on which columns should have the standard scalar applied to them once we've done that what we're going to want to do is also we're going to want to say data equals scalar dot check all right and now in order to actually view which columns had those changes applied to them we're going to say is print data at scalar dot check all right well not check my bad next one all right we're going to also want to view these columns within the head method all right and then once we've done that we're going to be getting one error but i want you all to see which error i am talking about and that is this near bay all right the reason why we're getting an error on near bay is because we have not yet encoded the data set and since we're viewing the whole call well the whole data set instead of just one individual column like we're doing just in that example we're really going to want to encode that column for our data set so the way we're going to do this is i just want to delete this so it doesn't confuse you guys and then we're going to say encoder equals ir dot encoder we're going to pass in data and then we're also going to say data equals encoder dot check and then once we've done that we will now be able to view what are the changes within our data set and as you all can see we have a good amount not only did total bedrooms have the standard scale applied to them but a lot more like total rooms medium income median house value and that is only just the tip of the iceberg of what can be applied with this already islanders so for all the python packages that we worked with in today's video go ahead and check out the description bar down below to get the pip install commands for all those packages and for those of you that go down there you're going to notice something really special our python community package islander day pre-pressing is now published to pi pi so go ahead and check out the pip command down description bar down below alright antlers i have a question for all of you i want to know what type of python ides or text editors do you currently use i'm considering switching my ide from pycharm to another ide so i really would love to get all sorts of different input leave it down in the comment section down below and if you want to check out more machine learning videos then go ahead and check out the videos popping up on the screen as well as go ahead and check on will click on the logo to subscribe to the channel if you have not done so yet i'll see you in the next video happy coding i love you all"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Implementing Feature Scaling with Scikit-Learn",
        "Video Title":"MinMax Scaler and Standard Scaler in Python Sklearn",
        "URL":"https:\/\/www.youtube.com\/watch?v=G7xRVW9CfHY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/G7xRVW9CfHY\/hqdefault.jpg",
        "ID":"G7xRVW9CfHY",
        "Publish Time":"2022-01-23T08:29:35Z",
        "Channel":"Koolac",
        "Channel ID":"UCau7fBZr_fSNb4CzbGWA1Rw",
        "Transcript":"hi in this video we are going to talk about data normalization and how to scale our data in python programming language and also why do we need to scale our data so first of all let's print the x train and if i run the code you can see we have two columns which we have talked in the previous videos this is the age column and this is the salary column but as you can see the scale of these two columns are drastically different but maybe you ask so what what's the problem? the problem is this: because the scale of these two columns are drastically different maybe some models, not all models, some models are going to be deceived by these different feature scales why because maybe they think that this column which the scale is drastically bigger than this column is much more important so in order to help the model in order to not be deceived by the scale of these two columns we should scale our columns i mean we should bring our features to similar scales in this video we want to talk about two well-known methods for scaling our data the first one is min max scaler and the second one is standard scalar so now let's talk about min max scaler this is the formula of min max scalar first we need to subtract our data our data by the minimum of that column and then we should divide that by the maximum minus minimum of that column and this gives us a result which is the transformed of our x of our initial x i mean so now we have scaled our x so in order to do min max scalar in python programming language we should simply type from sklearn dot preprocessing dot pre-processing import min max scaler then we need to create an object of this min max scaler and for example suppose that we want to name that object scalar and then we simply type scalar dot fit transform because we want to fit and also transform our x train and then we are going to update our x train I mean we are going to store the result in a variable again called x train it means that we are going to update our x and then we simply type scalar dot transform and not fit because we are not going to fit on our x test we are we are just going to transform our scalar on our x test and then we are going to update our x test and remember that you should just fit your scaler on x train not x test you should just transform on x test and now if I print x train if I run the code again you can see the scale of these two columns are now similar but let's talk about another famous scalar which we call it standard scalar which the transformed value I mean x prime is equal to x I mean each value subtracted by the mean of that column divided by the standard deviation of that column and this gives us a result which is the transformed of our x so in order to scale our data with standard scalar in python programming language first we should import the proper packages so we simplify from sklearn dot pre-processing import standard scalar and then we should create an object of standard scalar and for example suppose that we want to name that object scalar and then we similarly type scalar dot fit transform on x train and we are going to update our x train and also we are going to simply type x train dot transform on our x test and then we are going to update our x test and now if I print x train again and if I run the code you can see again the scale of these two columns are similar but maybe you ask which one is better is min max scalar better or standard scalar is better? the general answer is it depends on your data but usually standard scalar is performing better than the min max scaler in the next tutorial we are going to talk about k-nearest neighbors algorithm in python programming language"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Text Data Preprocessing: Tokenization and TF-IDF Vectorization",
        "Video Title":"Text Preprocessing | tokenization | cleaning | stemming | stopwords | lemmatization",
        "URL":"https:\/\/www.youtube.com\/watch?v=hhjn4HVEdy0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/hhjn4HVEdy0\/hqdefault.jpg",
        "ID":"hhjn4HVEdy0",
        "Publish Time":"2020-04-29T18:41:37Z",
        "Channel":"utsav aggarwal",
        "Channel ID":"UCSz5IBDVS2zlPcZofdZfELg",
        "Transcript":""
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Text Data Preprocessing: Tokenization and TF-IDF Vectorization",
        "Video Title":"Natural Language Processing|TF-IDF Intuition| Text Prerocessing",
        "URL":"https:\/\/www.youtube.com\/watch?v=D2V1okCEsiE",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/D2V1okCEsiE\/hqdefault.jpg",
        "ID":"D2V1okCEsiE",
        "Publish Time":"2020-05-02T17:45:56Z",
        "Channel":"Krish Naik",
        "Channel ID":"UCNU_lfiiWBdtULKOw6X0Dig",
        "Transcript":"[Music] hello all my name is krishnak and welcome to my youtube channel so guys today in this particular video we'll be seeing tf idf in the previous video i have already discussed about bag of words how you can use bag of words to convert your sentences into vectors now in this particular video i will be discussing about tfidf that is term frequency and inverse document frequency understand tf basically means term frequency this is the formula okay i'll discuss about this particular formula in some time inverse document frequency is basically this this is the formula and finally we multiply term frequency and inverse document frequency to convert the sentences into vectors now one of the disadvantage of bag of words i told you that there is no semantic meaning much semantic difference because either we have values like ones or zeros all the values all the features basically have one value right that i had discussed in my previous video itself if you have not seen i'd suggest please go to my playlist the link is given in the description of that specific video okay now just understand that we'll just try to see how tf idf is actually derived and finally how do we get our features you know with different kind of vectors and how now semantic meaning is there in each and every sentences now i'm just going to take a very simple sentence which i've actually taken my previous example of bag of words in sentence one i have good boy okay now this sentence one sentence to sentence three is basically derived after you do you know limitization or stemming limitization or stop words right after do you do that i've got some words in sentence one i've got good boy and sentence two i've got good girl in sentence three boy girl good i've got like that okay now based on this particular word first of all we'll go and find out the histograms histograms basically means that okay i should not write bag of words here instead i should write it as t f idf okay i'm going to write it this okay now frequency i'm just kind of going to count out how many times good is present good is present three times voice present two times from the sentences girl is present two times now we will try to convert this whole sentences into vectors which looks like this you know after applying tf idf technique now in tf idf the first technique is called as term frequency now what does term frequency actually mean term frequency is basically given by a formula number of repetition of words in a sentence number of repetition of words in a sentence divided by number of words in a sentence okay only one sentence i am not saying sentences number of repetition of words in a sentence divided by number of words in a sentence now i have these three features i'm going to write it over here and with respect to this will try to find out the term frequency of good boy and girl in sentence one sentence to sentence free okay now in sentence one i have good boy now if i want to find out term frequency number of repetition of word in a sentence right how many times good is present over here only one time one time good is present divided by total number of words in sentence how many words are present in the sentence too so i can write the term frequency of good in this whole corpse in this whole course right or in this particular sentence is basically one by two i'll not say corpse but we should consider sentence okay is one by two similarly in the sentence one boy is also present right what will be the term frequency of boy again number of repetition of words in a sentence how many time boy is present one total number of words is two one by two then if i go and see for girl how many times girl is present zero times so this will become zero right similarly i'll go and compute for sentence two in sentence two i have good then girl so good is actually one by two because i just have two words girl is one by two basically this is one by two uh the total number of words are two girl a word is actually present one so one by two is over here in sentence three with respect to boy girl and good i can just write one by three one by three one by three okay so this is how easily we actually calculate the term frequency okay now comes the important time important thing that is called as inverse document frequency okay now inverse document frequency i will again take the words all these particular words good boy and girl and then i'll try to calculate the inverse document frequency now inverse document frequency basically says log off right number of sentences understand how many number of sentences are there three in every situation for every word the number of sentences will be three it will be fixed divided by number of sentences containing the word okay now if i want to compute for sent in good in sentence one right and uh suppose i want to calculate the inverse document frequency for good first of all we'll go and write log off how many number of sentences are there three right so i've write written three then how many sentences contain this word good you see a good is present here here here so the count will be 3 so log 3 by 3 is nothing but 0 if i want to find out the inverse document frequency i'm basically using this formula i'm getting the value as 0 for good and remember this value will be 0 itself now with respect to the second word boy how many number of sentences are log of 3 okay fine how many time boys are present boy is present 1 and 2 in sentence 1 and sentence 3 so log of three by two right so this is my inverse document frequency for boy word now inverse document frequency for girl word is again log three by two because two times because the girl is present okay and the total number of sentences three right i have got this inverse documents frequency pretty much easy now what is my final goal i have to multiply term frequency multiplied by inverse document frequency so what i am going to do i am going to take this good boy and girl these are my features now with respect to sentence one i have to multiply with this with these values so in f1 good that is over here and this is my feature one right in sentence one again i'm taking sentence when i'm just traversing this like this sentence one is coming here and good boy girl is coming over here now with respect to sentence one i will be multiplying this value with this this value with this this value with this right so in feature one you can see good one by two div multiplied by 0 is nothing but 0 with respect to y 1 by 2 multiplied by log of 3 by 2 so here i'm getting this value in case of girl this is 0 so i multiplied by this is nothing but 0 okay now similarly with respect to sentence 2 you can see over here again sentence 2 over here 1 by 2 multiplied by 0 is again 0 0 multiplied by log of 3 by 2 is again 0 1 by 2 multiplied by log 3 by 2 is this value now in the third sentence first word again 1 by 3 multiplied by 0 is 0 right 1 by 3 multiplied by log 3 by 2 okay 1 by 3 multiplied by log 3 by rho 1 by 3 multiplied by log 3 by 2. so these two values are come now understand most important thing guys when we are using bag of words we just have values like ones and zeroes but just understand in the first sentence where i have good boy right over here the importance is basically given to the boy you can see by this because this value is coming as higher when compared to the other values in the future right we are saying something about the boy now with respect to sentence two also you can see good girl good is not given that importance but girl is definitely given this particular importance you know so there is some semantic meaning we are trying to bring some semantic meaning in the third sentence also again you can see because this good is present frequently in every sentences right it may have the same meaning what about boy and girl now here see again boy and girl are given the more importance now just try to compute these values and you will be getting a different different values you will not just be getting ones and zeros here you will be getting in decimals like point one seven point three seven point four seven now this is just with the example of three sentences i have just taken a simple example just imagine when you have huge number of sentences at that time how this particular thing will work in a wonderful manner right and always remember i'll be having an output feature this will be my dependent feature now after i convert this is basically my vectors with respect to the sentences now i will take this independent feature give it to my model for training this output will be treated as a dependent feature and this is how we actually compute tf idf that is term frequency and inverse document frequency so i hope you like this particular video i hope you have understood this strategy please make sure that you refer again and again and yes uh this is all about this particular video i hope you like it please do subscribe this channel if you're not already subscribed see you all in the next video have a great day thank you all bye"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Text Data Preprocessing: Tokenization and TF-IDF Vectorization",
        "Video Title":"Text Representation Using TF-IDF: NLP Tutorial For Beginners - S2 E6",
        "URL":"https:\/\/www.youtube.com\/watch?v=ATK6fm3cYfI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ATK6fm3cYfI\/hqdefault.jpg",
        "ID":"ATK6fm3cYfI",
        "Publish Time":"2022-08-17T14:05:36Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"In this video, we will talk about TF-IDF representation for text in NLP. In the last video we looked at Bag of words, Bag of n-grams. If you have not seeing those videos, I highly recommend you watch them first before watching this video. We have been looking at news article classification problem, where a news article is given to you, and you have to classify it into one of the companies, whether this article is for Tesla or Apple, etc. And in Bag of words model what we did was we'll take an article and then design this vocabulary. Doc, vocabulary is basically all the words from all the news articles and then we will take count of individual word in a given article. For example in article 1, word Musk is present 0 time, that is present 32 times and so on. And just by looking at this vector right now, you can actually tell that first 2 articles are Apple articles because you see these terms, iPhone, itunes they appear 7 times, 3 times, 6 times, 3 times. Whereas in the other 2 articles, see Musk appeared 15, 3 times, 1 time. So based on these word count, you can say yes this one is for Tesla and this other articles are for Apple. But look at this other terms that, price, market, investor, now these are very generic terms that can appear in any article. I used to work for Bloomberg and we actually were working on extracting company from earning estimate document. And you will find this kind of generic terms too often in all the companies you know, like stock, market, company profit, loss this is generic. And the presence of these terms in your bag of words model can suppress the meaningful words. So here for I Apple, iPhone, iTunes is the relevant term and for Tesla, Musk and Gigafactory are relevant terms. But these other generic terms are suppressing the influence of these relevant terms, and in this example I'm just showing you 4 that, price, market, investor 4 terms but actually there could be 100 such terms and look at what happens to the vector. In the first and third vector these terms are present with a equal amount. See 32 31 45 44. So ignoring all the terms if you just look at the vectors and you give this vectors to a computer, computer will think article 1 and 3 are similar. Actually they are different right? One is from Apple, the third one is from Tesla. But if you look at just the vectors, see this 7, 3 is not matching here, it's 0 0. But see other terms are kind of equal and there are 4 of them. In reality there could be 100 such terms in your vector, and your computer will think, okay these two are actually related articles you know. But that is not the case. So how do you ignore these terms, that's the question? See you don't want to ignore them completely but still you somehow want to ignore it. Someone will come and argue that okay that is a stop word. Why don't we just remove all the stop words? Fine, we removed that. But then how about all these terms: price, market, investor. So stop words can help, but to a certain extent. We need to do some additional thing. So let's think about this together. Let's pause this video and think what can we do, or what should be our approach okay? You just pause the video and you will be able to come up with that idea. So one idea is we figure out in how many documents the term that appeared. So that appeared in 3 documents out of 4 and I'm not talking about the individual word count in a given document. I'm saying in how many documents this term that appeared. It appeared in 3 documents article 1 2 3 out of total 4 documents. Similarly Gigafactory appeared in only 1 document and iPhone appeared in 2 documents. This is something called document frequency. Mumber of times term t so t here is that, Gigafactory, etc. Number of times term t appeared in all the documents and if this number is higher, if some term is appearing in majority of documents, we should lower its influence because it's like a generic term. But if some word is appearing in only few documents such as Gigafactory, see only 1 document, iPhone only 2 documents, then that term is important and we need to give it a high score. So we need to come up with some scoring mechanism here. And obviously the scoring mechanisms should be such that you know higher the term appears in all the documents, the influence would be lower. And you can do that by doing inverse document frequency. So if that term is 3, so you invert it okay? Then the number goes down, and you don't do simple inversion but you do this formula: total number of documents divided by number of documents term t is present in. For example total documents are 4. So numerator in our ratio is is fixed: 4 But denominator is how many times that appeared in all the documents. Well 3 times. Total number of documents are 4, that appeared in in 3 documents. So 1.33 Total number of documents are 4, Giga factory appeared in only 1 document. So this number is 4. So now see when the term appears in less number of documents, the score you are getting is higher. See this 4 is higher than 1.33 and this is called IDF inverse document frequency. There is something additional to this formula, which is called you take a log of this. Why do we take log, I will explain in a minute or so. But hold your thought for now, and in your brain you should be thinking if the term appears in more documents, the final score for that will be lower because you want to reduce the influence. And you will come up with this kind of vector which will be called IDF vector 0.1 to 0.12 and so on. I mean I I'm just showing this. We don't use IDF. We use TF- IDF, which I have to still explain. But for now, I hope the concept of IDF is clear in your head. We also need to take into account the word frequency, because that is also important correct? But we don't want to just take a word frequency like bag of words because see some articles could be bigger. Let's say there is a 5 000 word article and there is only 10 word article small article, you want to be fair and kind of normalize things. So what you should do is total number of time term t is present in document which is just a simple word count that we took in bag of words model, divided by total number of tokens in that document, and that we will call term frequency. So here t is the term and d is that document in which you are counting that term frequency. So here for example the term frequency in article 1 for the term market, will be 48 divided by total number of words in that article. Let's say in that article total number of words are 1000. So the term frequency for market in article 1 will be 48 divided by 1000 and TF-IDF is just a multiplication of these two terms. And we will use that in today's tutorial to do our coding. So we'll do again a text classification for e-commerce goods and we'll be using TF-IDF representation. Here I'm just showing some sample numbers. I did not actually compute it, but what I want to show you here is terms like iPhone, iTunes, Gigafactory will have higher numbers 0.9 .8 Whereas terms like that, price will have lower number okay? One more thing I want to clarify is sklearn uses slightly different formula for IDF to take into account the 0 division possibility. So what they do is they add constant 1 in numerator and denominator both, and then the result they add one more time. So it's just a little customization on your IDF formula sklearn uses that. This screenshot is from the sklearn documentation. Now let's talk about why we used log. You can ask this question to your dear friend who is sitting next to you and that dear friend is actually Google. So if you just Google this question you'll find the stack overflow answer where they say that log is used to dampen the effect of IDF and unki has given a very nice answer here. So I'm just going to take a screenshot of that answer and show it here, credit goes to stack overflow which is let's say in a work in a document you have a term called computer, which is relevant for your given NLP problem. And if the term appears 1 million times, you know that that term is important is and and I'm talking about that one term appearing in your document right in your specific document. So now if it is 1 million or 2 million it is all same, correct? Doesn't matter. So that's why we kind of take a log of that just to kind of reduce the effect of it. If you know from your log function, your log function goes something like this you know, log of 1 is 0, log of 10 is 1, log of 100 is 2. I'm talking about log to the base 10 and the curve kind of flattens out as you go further in your x-axis, and the influence that you want to have in in terms of your IDF you want to reduce it because as you have more terms the effect is same. So you can just reduce the influence. Now the limitation of TF-IDF model is similar to the previous model that we saw before, which is a as you increase dimensionality, the sparsity increases. As your vocabulary increases, obviously the sparsity increases. It's a discrete representation of your text. It doesn't capture the relationship between the words. The relationship between the word is captured by word embeddings and sentence embeddings. But all other models they suffer from this problem where you have relationship between the words, it's not captured because here you're taking a term and you're just taking a count out of it. And it suffers from out of vocabulary problem as well. Let's do some coding now. I opened jupyter notebook and imported TF-IDF vectorizer from sklearn and I'm going to take 1 sample corpus and the meaning of corpus is the collection of documents in your NLP application. I'm just going to take some simple dummy sentences and will create TF-IDF vectorizer out of it. So the way you create the instance, this is TF-IDF vectorizer is a class and you just create an instance of it. And you will just say fit. So when you do fit, you are creating that vector basically right? And then you're kind of training basically and then the creation happen when you say transform. You can do fit transform as well, just one call. I think let's do that. Fit and transform and we will call it transformed output is that okay? And once that is done the v object will get its vocabulary. So I want to print a vocabulary first, and I'll just use print so that I can see everything in like 2 or 3 lines. See all this vocabulary have these indexes. For example already the term already at a is at 0, Apple is at 6 and so on. Now I will print IDF score. So v this parameter v if you do like dir v you will see all the members, and you will see IDF for example as one of the members. IDF smooth IDF actually we are using smooth IDF and so on. So let me first print just a simple IDF score out of it, and for that you can either iterate through this particular directory v.vocubalary or if you want to go in sequence, see if I iterate through this directory, I'm kind of going randomly over the places. But I want to go in sequence where I take the zeroth index element first and so on. And for that we can do v. get feature names out And that will print things in order basically, all your features, all your vocabulary already 0 am is 1 okay? When you do TF-IDF vectorization, these things are actually your features and based on the features you will calculate your vector. So I will just call this all feature names, and then I will iterate through it. So I will say for word in all feature names, I will first get the index and index is v.vocabulary .get because it's a dictionary you can call get method and you get this particular index right? Like 27, 11 whatever. And when you get that index, uh you can then say v.idf that will have your idf score and you can retrieve specific score for a word by using the index operator, and you can just print that. You can print let's use the format strings here, and we will say word and after word we have the score okay? So this is the score. Now you will notice one thing which is this term is is present everywhere, see. In most of the document it is present except this one. So you expect this TF-IDF or let's say IDF score for is to be lower, and the term like Apple is appearing rarely. So the score of Apple, Tesla, etc. should be should be higher. So check this is is 1.1 whereas Amazon or Apple is 2.5 So it is working, and we are more interested in the the whole TF-IDF score. So what I will do is now I will print that and that score is present in transform output. But before I do that, I will let me just print first 2 sentences from the corpus okay? So these are the first two sentences from the corpus, and I want to print the corresponding TF-IDF vector for these 2 sentences. So that that is present in transform output correct? We got that in transform output so transform output. to array It's a sparse matrix so you have to convert it to an array and you just do 2. So this is the first one and this is the second one. Now watch this uh let's see we are looking at is okay and is is at I think position 17 right? So is if you look at it here, it is 17. So is position is 17. So there are total 6 here, so the position 17 will come let's see where so 6 6 so this will be 0 1 2 3, okay 0 1 2 3 4 actually there are only 5 I see. So this will be 15 16 17, it will be this one. I think this one actually. Oh see 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17. So see the IDF TF-IDF score for is in the first sentence is . 10 and the score for Thor for example is how much? So where is my Thor? Thor is 27 So this is 17 18 19 20 1 2 3 4 5 6 7 See, Thor score is 0.24 So it is more than is. So just consume this. I mean you'll get an idea that the things are working as expected. The score for the term which is appearing in too many documents is lower and the score for the rare terms which appear in individual document, is higher. I mean that's that's the whole point I wanted to make. If you remove certain sentences, for example right now, the score for is IDF scored for is is 1.11 If i remove certain sentences you will notice that the score will go up. You can experiment. We want to move to our actual problem that we'll be solving in this coding tutorial which is e-commerce text classification okay? And I took the dataset from Kaggle, the source credit is given to this particular page. This dataset has 4 classes. I mean the actual classes might be more, I have trimmed it down and we have the description of an item that you sell it on let's say Amazon. So Amazon item description Amazon or any other e-commerce website item description, and what category, is it electronic, household box, etc. okay? So it looks I have I have this other notebook that I usually prepare before my coding video. So see it looks something like this, you can see right? So let's say there is a text and there is a label. There is a text this text when you read it, you realize it's electronic right? SATA hard disk type, and when you read this one you realize it's a book. So we have this one and what we want to do now let me just show you the CSV file. So I got the CSV file, number of categories are total 4 okay? and see these are the text description So simple CSV with 2 columns the description of an item and what category that item belongs to okay? So let's import pandas. So how to import pandas? Import pandas is pd How do you read CSV? We have done this like I don't know 10000 times So you read e-commerce cv you store it in your data frame df. shape you can print that and then do df.head What I do is I usually do f.label .valuecount just to check if there is a class imbalance. And no there are 6000 items in each of the categories. As I said like I trim down the dataset, so that there is no imbalance. And now what we will do is map these label categories to numbers. Machine learning models understand only numbers. So how do you do that? So df.label.map you can use a map function and, you can apply uh the dictionary mapping. So what you will say is okay if it is household item, map that to a number 0. Just random number you know 0 1 2 3 4 If the category is books the number is 1 and so on. And that will be my label number column, and the way you do that is you'll have to specify the new column like this. This is a new column that you're creating in pandas. Pandas is an awesome library. You can create new columns like this by applying transformation on existing column, and I will check my df head and it works like a charm, okay? We are ready for train test split as usual this should be in your muscle memory. Import it from sklearn the train test split method and we have done this so many times that I don't feel like writing it. I'll just copy paste here. You are doing train test split of df.text is your main actually it is your dependent variable and df.label number is your independent variable like a target variable correct? So this is x test x train this is y test y train and we'll split train and test into 80% 20% category. 20% samples go to test data set Random state parameter, I'm just randomly initializing it to num some number and then we are doing stratify. Actually if you don't do stratify still okay because no actually we we have to do it okay, otherwise you might get imbalance in your x train x test So this is a good practice and when you do that, it's gonna do train test split and I'm just going to just print the shape of x-train x test took something like this, 4 800 samples in your test dataset. Okay and then just some value checking in in your y train I am doing value counts, and you can see that there is a balance you know every category has 4 800 training samples, and same thing you can do for y test right? So y test value counts and y test also has equal number of samples from all the categories. Now I will use few classifiers from sklearn to train my model. I will first use KNN, then Random forest, then Naive Bayes. I just want to show you if you use different classific classifier, what is the performance okay? So I will just import some necessary libraries and folks I'm doing copy paste. I don't want to type and waste my time because we have done this enough number of times in this particular playlist. So make sure you follow this playlist in a sequence. If you are randomly watching this video, some of these things might not make sense. But if you follow the playlist in sequence it will be more meaningful. We'll import sklearn pipeline and sklearn pipeline allows you to create a pipeline where in a first stage you can have your TF-IDF vector, and the second stage you have your KNN classifier. And then you know you'll do clf.fit so x train y train and then you do prediction right? Once the model is trained, you will say predict x test and once you get your y predicted you want to compare y predicted with your y test and the way you do that is by printing classification report. And you always supply truth first and then the prediction. And I usually use print because otherwise the formatting won't be appropriate for classification report. So right now, it is training the model in this fit comment. And then in the predict comment, it will do prediction on the x test dataset, it will get y predicted output and y predicted it will compare it with the actual truth, and based on that, it has printed this classification report. The result looks pretty good. These are the precision and recall metrics for class 0 1 2 3 okay, you know all those classes household books and so on, and these are the F1 scores. See F1 score of 95 96 97 this is pretty good. If you don't know what is precision recall go to YouTube search codebasics precision recall, you will find a video very easy video, which explains all these metrics and I will just print x test and what was my y test for first 5 sample okay? And I want to also know my y prediction for first 5 samples and see 0 2 3 1 3 0 2 3 1 3 only this one it got it wrong. Other categories it predicted fine okay? So zeroth category Lal Haveli Designer Handmade this looks like clothing item to me, or household so that is 0. And you can see that household is 0, books is 1, electronics is 2 right? So this cotton thing, for example GOTOP classical retro cotton is 2 and 2 category is electronics. Okay so electronics. It is predicting it fine. Sometimes if the label is not clear, what you can do is you can print the whole thing and you need to supply this index okay? This is the index that will give you the holes. See you're talking about DSLR cameras right, electronics maybe this is leather, so maybe it's some kind of case for the camera. I don't know. Yeah, but anyway it is classified under electronics category. The third one is see the third one is this one right? So this one is for let's say camouflage polyester multifunction winter face mask, style mark this is some mask, so household item. So number 3. Oh number 3, okay sorry clothing and accessory. Numbers 3 is clothing and accessories, so the y test was that and prediction was also in in line with the truth. Now I will use multinomial classifier as well, and check the performance. So you can just copy paste this whole thing here okay, and one additional thing you need to do is your Naive Bayes classifier you need to import, and use that one here in the second stage of your pipeline. And you want to check how the performance looks like, and you compare. So see here KNN gave 95 96 97 97 94 95 97 90 almost similar okay? Not too much different. But now I want to try a Random forest classifier okay? So I will do Random forest classifier and you can see this could be trial and error. Usually for text problems, I start with Naive bayes. But based on your problem, your dataset you know one or the other classifier might perform well. There are some general guidelines you can follow. Okay text classification usually use multinomial Naive bayes. But in this particular case what's happening is see Random forest is giving you probably the best performance, 96 98 you can compare all these metrics with these these other metrics from the other classifiers. And we concluded that the Random forest classifier is giving you the best performance. Now when I trained this model, I did not do any preprocessing. I did not remove stop words and I did not do lemmatization. So remember in the last tutorial, we we designed a function that can do this preprocessing. So I will do that now. I will train this same classifier but using preprocess text, not the raw text. See this classifier is trained on a raw text. How do you know? Well see we use in train this place df.text df .text is whatever we got in our CSV. We did not remove stop words or we did not do lemmatization. I want to do that. So I will import a SpaCy library, and I will define a function called preprocess. Now we have seen this exact function in the previous video. Hence, I am not going to go line by line. I will use my most powerful weapon of copy paste. Here you go token by token. If it is stop word or punctuation word you just ignore it. If it is not, you do lemmatization. And then you join filter tokens and you create a string okay? So this will do your uh preprocessing and then on the text column, you will use apply function and you will say pre-process. Apply and map they are kind of similar function, and this will create a new column called pre-process text. It took me close to 10 minutes to execute this sentence. So make sure you have enough patience. You go for a coffee break or a walk while this is executing, and I got this new column created and this has a pre-processed text. So let's check 1 or 2 specific example. For example the zeroth text right, the zero the first one is this, and if you look at the same thing in pre-processed text, you will find that see a study in simple, see a study a in is removed. The is removed, punctuation dot is removed. See here, then chair has a firm. It will be say chair firm. So has a firm is removed. So it got rid of stop words and some and it did lemmatization and so on. And we'll do the train test split, using the pre-processed text column here. Again the random state is same because of that the the samples that we got in the previous scenario, where we are using the raw text will be similar so that we can do apples to apple comparison when we are doing this prediction. And I am going to copy paste the same code for Random forest like from here okay? The same code I will just copy paste which is my most powerful weapon, and I will train the model. This model is trained using pre-process text. The numbers that you saw above, these are using the raw text. And you will notice the performance is slightly better. So F1 score 97 98 98 and 99 correct? Here see, it's little less when you use raw text. So we can conclude that in the case of this e-commerce classification problem, with this particular dataset the pre-processing is helping. General guideline is you should do pre-processing. But in some cases if you don't do pre-processing, it is still okay. So it just varies from case to case. I hope you like this video. This is this is all I had. I'll see you in the next video, and make sure you check video description because I'll be adding an exercise in next few days. So by the time you are watching this video, the exercise will be available. Learning coding, machine learning NLP is like swimming you know. I keep on saying this boring thing again and again, to help you realize that only with the practice, you can master the skills. Just by watching video, nothing is gonna happen. So you have to practice when I'm teaching you code. Along with me do a coding practice and also work on an exercise. So check video description below. I think next next few days, I'll be adding the exercise. So if you're watching the video right now, maybe you can check the video description after few days, and I will have exercise ready for you! Thank you! [Music]"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Text Data Preprocessing: Tokenization and TF-IDF Vectorization",
        "Video Title":"4.8. Feature extraction of Text data using Tf-Idf Vectorizer | Data Preprocessing | Machine Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=MpyTHF2o834",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/MpyTHF2o834\/hqdefault.jpg",
        "ID":"MpyTHF2o834",
        "Publish Time":"2023-10-20T14:30:11Z",
        "Channel":"Edvard Ohanyan",
        "Channel ID":"UCLlkZ7IuhOq5XH1CniNrEwg",
        "Transcript":"hello everyone my name is Edward this module is all about data collection and prep processing this video is about future extraction of Text data in this video I will explaining you what is exactly mean by Future extraction and how we can implement this future extraction of Text data using the F Ida vectorizer so I will be also be showing you can implement this in Python these also contain Amazon part first of all let's try to understand about this future extraction the mapping from textual data to real values vectors is called future extraction I would like to explain you about some basic think about machine learning here before going into future extraction basically in machine learning we will feed our machine learning model with a lot of data and the model can find the patterns in this data and it can learn from it as a result of which it can make new predictions this is how a basic machine learning algorithm or model works but when we have the data in the form of text it will be kind of hard for a computer or a machine to understand the text Data whereas it can easily understand the numerical data we have to convert this text to data to numerical data this is where future extraction comes into play We convert this textual data to to Future vectors future vectors are nothing but the numerical representation of this textual data this is called as future extraction once we convert this text to data to numerical data it is now compatible to go into the machine learning model we need to discuss some few terms in term and Future extraction the first one is bag of vs as you can see here bag of vs represent the list of unique wordss in the Text corpus Corpus means collection of words let's say we have a paragraph This future extraction algorith tries to create a list with all the unique words it removes the repeat wordss and it creates a list of all the uni words present in that particular text purpose then we will use our TF IDF vectorizer TF IDF represents term frequency and inversed document frequency in the next slide I will explain you what exactly meant by this term frequency and inverse document frequency first try to understand what does this TF IDF of vectorizer do as you can see here I have mentioned it here that to count the number of times each word appears in a document we create a list of all the words in the paragraph or in the document and we count the number of times the words repeats you may have doubt how does count the number of words can help us convert this text into numerical data I will give you an example let's say that we are building a machine learning model that can predict whether a male is a spam mail or normal mail we all would have encountered this spam mail in our daily life and we can say that the spam Maes contain the words like like offers free discounts and such kind of things and the normal mail the maale send by our family members or our colleages does not have this kind of words when you count these wordss it can tell the machine learning algorithm like this particular kind of label as this kind of watch for example the label for this spam in prediction we will be spam Maes and normal mail site and here these families will be the labeled this can tell our model that this spam M as the voice like free discount offers Etc this is how counting the wordss can help the machine learning model to understand what is present in the data set that is another important thing to not here when we do this vectorizer it does not understand the context of the paragraph it just tries to count the number of words number of times the words it repeated and it does not understand the context there are also some other methods which we will discuss in our NLP topics which is natural language processing topics in that part I will EXP explain you what are the other methods to do this but in machine learning we frequently go with this vectorizer Concepts I have told you what does this vectorizer do now let's understand more about this term frequency and inverse document frequency we will be using a TF ID vectorizer and TF stands for term frequency you can see the formula here the formula for term frequency is a number of times term T appears in a document div divided by the total number of terms in the document there is a word called as offer in that particular data set this VOR result will count how many times this word offer has been repeated in the document and it will divide it by total number of wordss present in the document this is called as term frequency this can tell us what are the important words are KN this idea stands for inverse document frequency and the formula for this is log capital N by small n I have mentioned what has been made this two n and C the capital n stands for the number of documents and small n stands for number of documents a term T has appeared in you can refer the formula basically what it tells us the Ida value of a rare word is high whereas the IDF of a frequent V is low why we have this inverse document frequency value is that they can be words like the Articles and other kinds of things words like these are the ET these words would be repeated a lot of time and we don't want to give a significant Focus to this words this is where we use this inverse document frequency where if a word repeat a lot of times that word will have a small value this will tell us the machine learning model that the word is not [Music] significant then both of these values will be multiplied TF IDF value and each term this is nothing but our future vectors it represents the numerical value as as you can see here we got some numerical values from these formulas this is how we can convert the text data to a numerical data I hope you have understood what we have discussed now I will show how you can implement this in Python now let's look into Google collab and do it the in the code section now about the data set first ID unique ID for a news article two title three author four text and five label real news or fake news we have taken this fake news prediction data set this data set contains news article and it contains two types of news one is the fake news and another one is the real news you can see the datas of the data set here I am going to explain you how we can convert this text data to numerical data I will just give you a short introduction of what we are doing here now we have the necessary libraries so import naai SMP import pandas SPD and import Ray Ray stands for regular expression it is very useful for search searching the text in a document now from nl. Corpus import Stop words nlka stands for natural language tool kit Corpus generally means the body of that particular text the important content of the text and stop words stop words basically means those words which does not added much value to a paragraph where text these can be the words such as the A and so it can be the Articles or it can be the words like where what when those kinds of words those words are stop words we need to remove this kind of stop words from our data set because it does not added much value about the context of the data that's why we need to import this topic using this function and now from NL dot stem do pter import Porter stemmer stem we will perform a function called a stemming this steming takes a word and removes the prefix and suffix of that word and Returns the root word of it this is the general description about stemming portal stemmer this function is used to stem our words it basically gives us the root for a particular word and now we write from sklearn do future extraction do text import TF IDF vectorizer this TF IDF vectorizer function this is the V we are going to use to convert the text into future vectors future vectors are nothing but numbers this is the function we will be needing we just imported the libraries now let's run this and move on now we have need to download these top wordss from analytic Library so import NLT now ntia do download parentheses and codes stop words now I run this this will download all the stop words present in here now let me show you what are these top words so I will print these words print keyword parentheses stop wordss words parentheses quotes English so these are the stop wordss I have told you as you can see in these words here I run this this words does not added much value to our data set during the steming proced we will also remove all these words from our data set these are nothing but the stop wordss now let's continue our code once we have imported all the dependencies we have downloaded this top words module now this is data collection and data Pro processing part now I click text button and to right data pre processing now we need to load this data set into a bondas data frame now I create the comment here and to write load the data set to a pandas data frame pandas data frame loads the data set into a more structured table I will create the variable as news data set newscore data set so we need to use the function which is equal to PD do read CSV now let's go to our data set and copy the path so right click upload on the pl data set now let's go to our data say train. CSV and copy the part and pass here so I will run this and move on sorry we get invalid syntax sorry single codes put yeah now let's check the number of rows and columns in the data set so new data set variable do shape I run this so we have 20,800 news articles and five columns so we have 20,800 news articles and five columns now let's try to print the first five columns of this data set I create a comment here and to write print the first five rows of the data frame you can do that by using the function hit so news data set variable do heit function I run this and see the result as you can see here this is the ID which we have seen above first we have the ID column which gives the serial number of the news then we have the title of every news then we have the author of that news and we have the text in this data set this is the detailed explanation of this news and we have labeled so label the labels will be either one or zero one represents fake news zero represents real news this side the sample of our data set now we need to check where the some values are missing in this data set now I create a comment here and to right count the number of missing values in the data set you can do that by using the function is null so new data set variable dot is null and Dot sum so I run this and see as you can see here there are no missing values in the ID this serial number column there title column we have about 558 missing values the outer column we have about 1,957 missing values and text column we have about 39 missing values let's understand why these things are missing basically it means while we are preparing the data said we might not get the title of a news or auor of a particular news or the text of a particular news that Ison for some news we may have only the title and author in that case the text will be absent so similarly we have the title on the text for some news but the outer name is missing while we are preparing the data set that's the reason in this case we have a very large data said because it has almost 20,800 news articles these missing values are not that much important we can drop these missing values or we can replace it will null string but if a lot of the values are missing we will use some methods like imputation in that imputation method we will do some processing to replace those missing values with appropriate values in this case we don't need to do that because we have enough data set to train our machine learning mod model if we have a very small data set in that case we cannot drop the values because we already have very small data set and in that if we drop some values we cannot make good predictions but in this case as we have a large data set we can replace the missing values with null string null strings are nothing but empty string now I create a comment here and to write replacing the null values with empy string so news data set variable which is equal to I am creating the new variable which is the same as our data frame name so news data set do fill now parentheses this will fill the missing values this field your name fills the missing values with whatever we need to fill so I run this sorry Wicked value error sorry now we don't need run this this will your name fills the missing values with whatever we need to fill here I will mention a null string so I put single Cotes null string can be mentioned by just putting single Cotes or double codes this is a null string now I run this and move on this replace the missing value with null string this is one of the important step in our project for our prediction we are going to include title and author now we will combine this title and author together and this is the data we are going to use for prediction we are not going to use the text because this these text are so large it can be a hook paragraph and it takes a lot of time for processing now I create a comment here and to right merging the outer column and title column these are the two main important features required for us so I will mention data frame name new data set I will create a new column named this content so I put square brackets single codes content column I am going to store this comined value of title and Alor in that column let's mention in here so this is equal to news data set variable author BL basically this plus symbol concatenates to strings I want to separate them with the the space I want to combine with outer column and title column now I run this and move on now print our data set and see the result news data set variable content column I run this as you can see here first we have the outter name because we have mentioned the outter name and the title next so we are going to use particular data to make the prediction we will be using this content data and the labels to make predictions now we need to separate this particular data which is the content column and labels separately let's do that now I create a comment here and to write separating the data and label I create the variable as X which is equal to new data set variable dot drop parentheses columns label and axis is equal to one basically I want to remove this label out of this data frame I don't want to give this label together with the data in this data frame I remove this label and store all this data in one variable and then store this label alone in one variable this drop function is basically removes the label column from our data frame this drop function is basically removes the label column from our data frame if you are removing a row from a data you need to mention as axis is equal to zero but in this case we are removing the column and by mentioning the axis is equal to one now I am creating another variable called as y in which I will store all the label Y is equal to news data set variable square bracket single codes and label this will load all the label value to my new variable called as y now I run this and move on so I can print this X and Y variables print keyword parentheses x variable and print y variable now I run this and see the result as you can see here from our original data frame all of these values excape the label will be stored in X variable and label value will be stored in one y variable we are not seeing this other columns such as title outter because it will be not shown because of the reason it is showing these three dots it basically means there are several columns between it these columns are nothing but our title outter text and content columns that's why it's just printing the content column alone because there won't be enough space to print the anti thing so we have totally 20,800 rows and five columns now we need to the stemming procedure now I create a te text here and to right stem in stemming is the process of reducing a word to its root word I have told earlier stemming will remove all the prefix and suffixes because they don't give much meaning to our text for example let's say that we have several words such as actor actress acting so example oor oess acting for all these words the root word is act right act word all these words the prefix and suffix will be removed on the main root what will be captured that is what we are going to do that in the case of stemming this is a very important step because we need to reduce the vs as much as possible to have better performance on our model once we do this procedure of reducing the words to its accurate root world we will perform the function of vectorizing in that vectorizing step we will convert these wordss to their respective future vectors future vectors are basically the numerical data once we convert this text Data to a numerical data we can fed it to our machine learning model here you just need to understand that steming is the process of reducing a word to each root we Import in the Porter stemmer Library Above This is function we are going to use for this purpose now I create a variable named this bardcore stem this is equal to border stemmer parentheses this P stemmer function will be loaded to this port stem variable now I run this and move on now I create a function Dev Define this Dev means basically Define in some cases you need to rep replicate a particular code snippy we cannot do the same code again and again for simplifying several lines of good to simple word we use this function now I am creating a function called a stemming steming parentheses in parentheses you need to mention some input value so content end point we will be by this procedure we will create a function and the name of that function is steming and in the bracket we need to give some input value this word conent particularly represents the input I want to name this input as content it's not that this is the same word that we specified above as the column name these are the different words this function we are going to use for stemming our text Data this seems very complicated to you but it's actually very simple now I create a variable named as stemed content St content is equal to Ray Ray this means regular expression Library we have importing this above it is useful for searching paragraph for text I am using this function called the sub R sub function function and parentheses sub basically means substitutes values in this I have mentioned this upper Arrow so we need to put uper row upper row this means an exclusion I am mention a set which contains small a to small Z so square brackets and single codes then we mention capital a capital A to capital Z I need to get all these alphabets in our data set can be numbers commas codes and a lot of things for this we don't want all of those things we just want all the alphabets and words for this we are taking this set lower at two lower Z and upper case a to upper case z this is our set and we are excluding upper row everything that is not present in the set there we mention space so comma single codes space basically means if there is numbers or commas codes do will be replaced by a space I want all these to be replaced by content so comma content this content basically means the text I want to fit this content column this content column basically represents the title and outer column comined I want to fed this data to this steming that is the purpose of this step which to remove all those things from my content except for alphabets and words I want to this processing in my content which is basically the compined form of the auor name and text name once we have done that we need to convert all the alphabets to lower case letters so Stam content is equal equal to St content do lower some words can be in uppercase letters I want to convert all those to lowas letters because it can create some problems in processing the machine learning model or other processing may think that appas letters means some significant things I convert all the words all the letters to lower case letters it is very helpful once we done that we are splitting into respective list all these words and text will be converted to list so stand with content variable is equal to stand with content do split it will be splitted and it will be converted to a list and [Music] now St content variable is equal to square brackets or stand VAR stem parentheses and we will write word now we are stemming this word we are taking each word and we are doing this steming function the steming which means taking a word and reducing it to its root word we need to do it in all the words but in this case we are removing the top wordss we have earlier printed these top wordss now I am just using a for Loop to pass in my data frame so stemed content is equal to Q bracket for St variable do stem word for v in St me tment content if not word in stop words do wordss English I delete this I remove all the words top words and choose only the words that are not the stop wordss as you can see here for word in stem content in my content I want to take the word if it not in this top words Library I am taking all those wordss that are not Stoppers and I am performing the function Port stem which means Port stemmer now St content variable which is equal to single codes the join function T content variable once I have done that I am joining all the words this is the stemming function so return stamate content finally we will return our processing text which is tamid content now I run this and move on now we have created this function so news data set variable content column is equal to news data set conent both apply function steming so I am taking the content column which is the joint form of the outer name and title name of the newes article and I am applying this taming function this will do all the processing in this taming function and it will return that to a new column called as content the same Colum column in this data frame now I run this and this will take a minute or two because the processing has to be done for a lot of words another important thing here if you practice this code here there is a good chance this will throw an error because you need to pay close attention to the this symbol here the symbol in some systems this Unicode symbol is cannot be replicated if you face some error in your code what you should do is download the collab file from the description once you are making the code just make sure that this symbol is the same now we have applied the steming procedure once let's once now let's print this stamp content so print keyword news data set variable and content I run this and see the result now as you can see here we don't have any uppercase letters you can see this cont before processing we have a capital letters in almost all the words and then we have a lot of functions Cotes Etc all those will be removed the words will be converted to their respective root words all the stop Words which are insignificant will be removed from this data set that is the reason for doing this steming procedure now let's create again the X and Y we just need this content value for our processing I am not using this text column because if we use this T will be a lot of processing because each of these text will be a h paragraph and it will take a long time for processing but if we do the train with this title and author alone it will yield a very good result and it is very easy for us I will just separate this column now I create a comment here and to right separating the data and label so X variable which is equal to news data set variable content do values I want the content value to be in this data set in X variable I want the labes to be in y variable y VAR variable which is equal to news data set content do values sorry label now I run this and move on we don't have to make this particular cell we don't have to just make this excillent cell I made this because I wanted to show you that the data set can be splitted by this I am just getting this content column and the label column now I print X variable I run this and see the result this x variable will be our data there we have a lot of values previously we have all the columns such as ID title outter Etc so I want only this content column type storing this content column to X now let's print to Y variable print y variable this will show the label I run this so 1 0 1 0 one one so one means fake news and zero means real news you can check the shape of y y do shape shape of Y is nothing but 20,800 in this particular variable y we have 20,800 values all those values cannot be shown here that's why has shown this is not short form by three dots this is the thing we are going to fit our machine learning model we have this content column in X and label column in y hour we have have this content column in X and label column in y now this value is still in textual form I have told you earlier that computers canot understand text we need to convert all these text into meaningful numbers that the computer can understand we will be using this input TF idea vectorizer vectorizer function now I create a comment here and to right convert the textual data to numerical data now I will create a variable called as vectorizer using variable we store Vector is a function so this equal to TF IDF Vector Reserve now I run this and the move on now you can fit the value vector function do fit and fit X variable now let's keep the text X variable is equal to vectorizer do transform x y variable we are using special Vector it is called TF vectorizer function TF stands for ter frequency IDF inverse document frequency it basically counts the number of times a particular word is repeating in a document in a particular text paragraph or something that repetition tells the model that it is a very important word and it assigns a particular numerical value to that word that is the purpose of this tfidf it stands for inverse frequency sometimes a word which is repeated several times does not have meing in it let's say that we are building a system that predicts whether review is a positive review or a negative review for a movie let's say that we are doing this for the Rangers movie there are a lot of reviews in the internet and we are analyzing all the all these reviews when we perform the same things here all the reviews contains the movie and name Rangers in that case the repeated word which is Avengers does not add any value to it because or the previous it definitely going to contain the mov and name this inverse frequency function finds those values which are repeating many times and it detects that those words are not significant and it reduced its importance value but this term frequency finds the number of repeating significant words and it will be give a particular value to to it by this they create future vectors future vectors are nothing but numbers X variable is nothing but this content column all these words will be converted to their respective future vectors we don't need to do in the case of Y because y variable is already a number y variable is already our label we are fitting X variable transform this function will convert all this value to their respective Futures now I run this and move on now let's see a sample of X so print keyword parentheses and X variable before it was text right as you can see here before it was only text now it is converted to their respective future vectors based on the function of TF ID Vector Reserve this is how you can convert a text to A numbers that the system can understand I run this now we need to feed this data to our machine learning model to predict whether this news is a real news or a fake news let's get to know each other in more detail and do it on my channel fake news prediction using my machine learning with python so I hope you have understood all the contents covered in this in this channel I will see you next video thanks for watching"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Implementing Text Preprocessing with NLTK and Scikit-Learn",
        "Video Title":"Hands-on NLP with NLTK and Scikit-learn: Building Text Preprocessing Pipeline with NLTK|packtpub.com",
        "URL":"https:\/\/www.youtube.com\/watch?v=3_oq1jABXeY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/3_oq1jABXeY\/hqdefault.jpg",
        "ID":"3_oq1jABXeY",
        "Publish Time":"2018-08-06T10:42:09Z",
        "Channel":"Packt ",
        "Channel ID":"UC3VydBGBl132baPCLeDspMQ",
        "Transcript":"you today we're beginning a new section now advanced topics in topic classification using concepts like tf-idf latent semantic analysis and support vector machines in this section we're gonna take a look at number one how we can build a text pre-processing pipeline with ni otk number two creating hashing based features from natural language number three classified documents into twenty topics with LSA number four document classification with tf-idf and SVM's in this video we're going to focus on how we can build a text pre-processing pipeline with n ot K in this video I want to introduce this pipeline thing with two steps number one you send and you to pass data around functions or to structure your text pre-processing graph using decorators so why are we talking about pipelines again and I thought we solved this last section with the secular and pipeline function so what ii learn pipeline function can do is it does allow you to branch so you can't do two things in one go using the same outputs of the previous step you always have to go one input into model one outputs and you take that put it into another model meaning that you cannot take the output of one step and feed in two different models with two different outputs and then maybe either combine them or discard one of them and use the other one you can't do that right furthermore it's also sometimes hard to combine NLT k and ii learn functions and then LT k is obviously purpose design for natural language processing so we want to use it as much as possible so i'm gonna show you today what are the NLT k functions that is commonly used to pre-process natural language to make it structured language data so that it's ready to be vectorized and fed into machine learning pipelines number two how we can use to send and you generators in Python standard library to create a data so a text pre-processing graph let's get right to it so some housekeeping we're going to import any otk and suprisingly and we also set a corpus this corpus is simply a few sentences from the secular documentation nice and easy the first thing we want to do is to understand the different and ltk functions that are commonly used for tax pre-processing so their sense tokenize which splits a text into sentences and while this might seem very intuitive actually because let's say ellipsis is 3-foot stops and then you have the distinguishes between that's and one-foot stop and also what happens if there's no space after the first stop what happens or how do you take care of parentheses etc so cent tokenize contains all these rules and you can just deploy it and it would nicely separate it into sentences and then work tokenized we're very familiar with we this breaks up a sentence into words easy POS tagging stands for parts of speech tagging so what this does is it tax words in whether it's a noun or adverb so these position tags can be used to inform downstream models about what this word is and help on inference and then nu chunking is trying to extract now phrases so is a little bit like two Rams and three grams but instead of blindly taking two words that's you know come after the other or three words that come out to the other any chunking tries to intelligently extract phrases that make sense given what the word given the corpus so how do we create this pipeline so we first need a pipeline decorator which takes in a function app in that pipeline decorator we have a defined in a function called start pipeline which just simply passes on arguments and two keyword arguments we first call and assign the function with the arguments into new function and it would call and mixed on the function so doing this allows us to feed in a function that is generator and calling next allows it to then feed data through so there are two key words when you remember here so number one is send which sends data into the targets in the target functions we can use the yield keyword to get back to data from sin so what this allows us to do is to then pass data from one stage to another and also pass the data to all the targets instead of just one target like what we have in scikit-learn so if we wrap all of the NL TK steps with function like this and use the pipeline decorator we can then create a text pre-processing graph like this one where we have ingest tokenize sentences tokenize words and then put again or pipe the outputs of tokenized words into two different stages where we one first print it and then we then pass it further on to parts of speech tagging and in any chunking and then print it again we run this we can see that after splitting it into sentences and words we then print it and then feed the rest of the results into parts of speech tagging which is what happened here so for example strategy is a noun and it is a proposition and then we also have chunked the nouns so we have things like psy PETA parse is a chunk now or we can have parentheses here with or binary or current information and that's all there is to it so we've just learned how to use the generating functions send and you then the Python standard library to pass data around functions and also structuring a text pre-processing graph which allows us to take the outputs of one step and pass it to multiple new models steps instead of just one as opposed to the second learn pipeline and we do this by using decorators you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Implementing Text Preprocessing with NLTK and Scikit-Learn",
        "Video Title":"Text Preprocessing | tokenization | cleaning | stemming | stopwords | lemmatization",
        "URL":"https:\/\/www.youtube.com\/watch?v=hhjn4HVEdy0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/hhjn4HVEdy0\/hqdefault.jpg",
        "ID":"hhjn4HVEdy0",
        "Publish Time":"2020-04-29T18:41:37Z",
        "Channel":"utsav aggarwal",
        "Channel ID":"UCSz5IBDVS2zlPcZofdZfELg",
        "Transcript":""
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Implementing Text Preprocessing with NLTK and Scikit-Learn",
        "Video Title":"Python Sentiment Analysis Project with NLTK and \ud83e\udd17 Transformers. Classify Amazon Reviews!!",
        "URL":"https:\/\/www.youtube.com\/watch?v=QpzMWQvxXWk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/QpzMWQvxXWk\/hqdefault.jpg",
        "ID":"QpzMWQvxXWk",
        "Publish Time":"2022-05-05T14:46:00Z",
        "Channel":"Rob Mulla",
        "Channel ID":"UCxladMszXan-jfgzyeIMyvw",
        "Transcript":"in today's video we're going to walk through a natural language processing project from start to finish by doing sentiment analysis on amazon reviews sentiment analysis it's the use of natural language processing to identify the motions behind text we're going to walk through a traditional approach to sentiment analysis using python's natural language toolkit or nltk and then we'll implement a more complex model called roberta that's provided by hugging face we'll do some analysis of how the different models perform and we'll even explore using some pre-trained pipelines for making sentiment analysis really quick and easy hi my name is rob and i make videos about data science machine learning and coding in python i'm going to share everything we do today in a kaggle notebook so you can find the link in the description copy that notebook and explore all the stuff that we would do today if you do enjoy this video please consider subscribing liking and follow me on twitch where i stream live coding all right let's get to the code okay so here we are in a kaggle notebook you can see this is the basic outline of what we're going to be talking about today we're going to be doing some sentiment analysis in python and we're going to use two main techniques the first one is the older kind of way of approaching sentiment analysis with a model called vader and this uses a bag of words approach and then we're gonna look at a pre-trained model from hugging face that's a roberta type model and this is a more advanced transformer model that we're going to see how the results compare between those two models and then we're going to also explore a hugging face pipeline but before we get into that let's talk about the data and do some basic analysis with the natural language toolkit which is a great library for python i'm going to show you over here to the right side that we are going to use this data set that is a bunch of amazon fine food reviews so these are text reviews for food on amazon as well as the rating that out of five stars that the reviewer gave them and all of this is in csv format that we're going to pull in so before we get too far into it let's do some of our imports we are going to let's give it some space here we're going to import pandas as pd we're going to import numpy it for some plotting we'll import matplotlib pi plot as plt and we're going to import seaborn as sns let's set a style sheet that we'll use for our plots and then we're going to just to start out here import nltk which is that natural language toolkit we'll be using for the start let's go ahead and read in our data so let's comment here and say read in data and we're going to read in from the input directory there is this reviews.csv and after i read this in i can show you here just in head command on this that in this data set we have each row is the unique id we have the product id user id profile name and then the real interesting stuff here is the score so this is out of a one to five star rating how many stars the reviewer gave this item and then the text it's a little small to see here but you can see if i just do the text row and i show us the first one it's actual text with the review that was written by the reviewer for this product and we're going to be running our sentiment analysis on this row of data in this entire data set but actually this data set if i print the shape is quite large there are almost half a million reviews here and just for time's sake let's down sample this data set so i can do that pretty simply by just doing a head command on this and taking the first 500 rows and then if we print the data frame shape after that we'll see that it's 500 rows but you could scale up this project very easily to all half a million products if you wanted to run a more intense analysis so then i'll just put the data frame head command here so we can see and reference back to what columns we have available to us now let's do some quick eda just to get an idea of what this data set looks like so we'll take this score column which we know to be a value between 1 and 5 and we're going to do a value counts on this this gives us the number of times each score occurs and then we'll sort the index and we'll do just a bar plot of this kind will be a bar plot and the title is going to be count of reviews by stars and let's also do a fig size of 10 by five plot that and let's add a label to it so i'm going to do a some line breaks here to clean this up and we're going to call this our axis and then i'm going to set the x label as review stars and we will do plt show all right so we can see here that most of the reviews are actually 5 stars but then it kind of goes down it has a little uptick in the number of one star reviews we have so the very biased towards positive reviews in our data set that's good to know before we get any further now um the next thing we'll do is just some basic and nltk stuff and we'll start by just taking one example review so let's do example equals this text column and just pick the 50th value as an example and we'll print this example all right so what did this person say they said this oatmeal is not good it's mushy soft i don't like it quaker oats is where you go okay so uh seems to be negative sentiment here but before we get into that let's just see some of the stuff that nltk can do out of the box um nltk can t tokenize this sentence so let's taste this example and run l ntk word tokenizer and all that basically does is splits this into uh the parts of each word in the sentence now it looks pretty clear clean at the beginning but then you can see that um i don't do an n apostrophe t is split so this is a little bit smarter than just splitting on spaces in the the text and this will give us our actual tokenized results in natural language processing often you need to convert the text into some format that the computer can interpret and tokenizing that is the way that you do it so let's make this the tokens and then take the tokens and let's just show the first 10 so we can remember what this looks like all right so now once we have these tokens another thing nltk can do out of the box is actually find the part of speech for each of these words so let's run nltk's pos tag for part of speech tagging and we'll run this on each of these tokens now we can see that we have each token and we also have its part of speech so these part of speech values are codes and we can actually load up an example page that has some examples of what each abbreviation means so let's go back here to our example not here so you can see that oatmeal is nnn and in our part of speech tagging that means it's a noun a singular noun so each of the values in this text has now been given its part of speech so let's call this tagged and then let's just show the first 10 again as our example um now it can actually we can take it the next step from this and take these tags part of speech and put them into entities so nltk we can do a chunk on this and then n e chunk so this uh takes the recommended name entity chunker to chunk the given list of tokens so it takes these tokens and actually will group them into chunks of text so let's run it on this to see what it looks like [Music] what are we getting here oh yeah so we need to store this because we're running in a notebook and then we'll actually run entities dot p print for pretty print of this so you can see that it's chunked this into a sentence and noted here that this is an organization some other interesting stuff about the text that can be extracted out automatically using an nltk all right so that's just a basic primer about nltk but we want to get into sentiment analysis so we're going to start by using vader vader stands for what does vader stand for it stands for i wrote up here balance aware dictionary and sentiment reasoner so this approach essentially just takes all the words in our sentence and it has a value of either positive negative or neutral for each of those words and it combines up it just does a math equation and for all the words it'll add up to tell you how positive negative or neutral that the statement is based on all those words now one thing to keep in mind is this approach does not account for relationships between words which in human speech is very important but at least is a good start so we also remove something called stop words stop words are just words like and and the and words that really don't have a positive or negative feeling uh to them they're just for the structure of the sentence all right so let's do some uh sentiment analysis using this vader approach we're gonna do from nltk sentiment sentiment import sentiment intensity analyzer and then we're going to also import from tqdm notebook import tqdm this is just a progress bar tracker for when we're going to do some loops on this data i also made a video about tqdm that you can watch if you're interested and then we're going to make our sentiment analyzer object by calling this sentiment intensity analyzer creating it and calling it s i a and that's going to be what we're the object uh let me make sure oh yeah this needs to be from tqdm notebook input cdm and now we have our sentiment intensity analyzer object we can run this on text and see uh what the sentiment is based on the words so let's run just on some examples let's say i am so happy an exclamation point we can see that this vader approach has made this has tagged this negative as zero this these are scales from zero to one so zero negative neutral point three and positive point six eight two so mostly positive now there's also this compound score which is an aggregation of negative neutral and positive this count com compound value is from negative one to positive one representing how negative to positive it is but if you want more detail you can take the breakdown of this negative neutral and positive so it did a good job it made this it tagged this as being mostly positive let's try the opposite so s-i-a polarity scores of this is the worst thing ever all right now we see that the polarity score polarity score for this is mostly negative and neutral and nothing positive and then this compound score is net point negative 0.62 so more on the negative side than positive very interesting now we can run sia on our example like that we had before remember our example which was this oatmeal comment let's run that on the oatmeal comment and see what it is okay so it's pretty high neutral but also some negative and the overall compound score is negative no positive score so we want to run this polarity score run the polarity score on the entire data set so basically looping through this data frame we have every text field we wanna run this and grab the polarity scores and we can do that with a simple loop so we're gonna do for d which is going to be just our row or we can just say four row and t qdm df dot itter rows and then our total is going to be yeah so then this should work i think and then we're gonna take um the row text and this will be tech our text and then we're also gonna take our we're going to call it my id which is the rows id column and then let's just break here to make sure we have this correctly oh that's correct this is going to be for i row in tqdm iter tuples uh it arrows and then we'll also make the total of this the length of the data frame so that when we see our progress bar it's out of 500 we're going to want some way to store these results so let's make a dictionary called res for results and every time we loop through we'll take my id we'll store in the my id part of the dictionary the polarity score score polarity score of the text right and then yeah that's it let's run this so really fast it ran it's done and um now we have this result uh dictionary with each id the negative neutral positive and compound score of each but we want to store this into a pandas data frame because that's easier to work with let's do that really quickly by just running pd.dataframe on this dictionary pandas can take in a dictionary pretty easily except for it's oriented the wrong way so let's just quickly run a dot t on this which will flip everything horizontally and now we have an index which is our id and then our negative neutral positive and our compound score for the sentiment for each of those values all right let's call this vader's that's our vader's result and then let's also let's take this vaders and let's reset the index and rename that index as our id so we can merge this onto our original data frame and then we're going to take vader's so vader's will now be this and then we'll also take vader's and we're going to merge it on our original data frame and how we'll do a left merge so now basically we have our data frame but with our scores and we also have all the other values from our original data set including the text so if i run a head on this we can see now we have sentiment score and metadata all right so let's see um let's see if this in general is in line with what we would expect so we're gonna make some assumptions here about our data that if the score of the item that the reviewer gave it is a five star review it's probably going to be more positive of text than if it was a score of one one star review is going to have more negative connotation than a five star review and we can do that by just doing a simple bar plot so let's use seaborne i think i imported seaborne yeah i imported keyboard before and we're going to do a bar plot of this data where our data is vader's let's call this plot vader results and our x value is going to be the score which remember is the star review of the the person and then compound is going to be our y value and that's the negative one to positive one overall um sentiment of the of the um text then let's set the title to be compound score by amazon stars review and then we'll show this what did i do wrong here i spelled compound wrong comp bound there we go okay so one star review has lower compound score and a five star view is higher and it's actually exactly what we would expect the more uh positive that the compound becomes that's the more um well by each score that was given it's more and more positive of text respectively and that's uh that's good that just kind of validates what we're looking for we can even break this down instead of looking at the compound score we can look at the positive neutral and negative scores for each so we're going to do that by doing something like sns bar plot data is vader's again x is score again and then let's do the positive and see what this looks like all right so this is the positive score and let's actually make three of these side by side uh left being positive neutral and then the negative to the right and we'll do that with matplotlib subplots so this will make a 1 by 3 grid of our results and we will call this axes put this first one here which will be our positive then we want our neutral and then we want the negative and this is going to be in position one two and three and then let's also set the title so we remember what these are positive neutral and negative and plots show this [Music] oh this needs to be ax equals and i need to change each of these there we go now we have um let's see what we have here let's make this a little less wide we have the positive positivity is higher as the score is higher in terms of stars the neutral is kind of flat and the negative goes down it becomes less negative of a comet as the star review becomes higher great this just confirms what we would hope to see and shows that vader is valuable in having this connection between the score of the text and sentiment score and that it does relate to the actual um the actual rating review of the reviewers uh let's do a tight layout just because i see some overlapping here of of the review of the y-axis labels but i think this is good all right so now we're going to take it up a notch our previous model just looked at each word in the sentence or in the review and scored each word individually but like we mentioned before human language depends a lot about a lot on context so if i say something uh we'll see a sentence that could have negative words actually could be sarcastic or related to other words in which way it makes it a positive statement so this uh vader model wouldn't pick up on that sort of relationship between words but more and more recently these transformer based deep learning models have become very popular because they can pick up on that context so we're going to use from hugging face which is one of the leaders in these types of models and gathering them and making them easily available we're going to import from transformers now this is hugging faces library you could pip install transformers to get this on your local machine or um of course you can just run it in a kaggle notebook like we are right now so let me make sure this works from transformers we're gonna import our auto tokenizer now this is gonna tokenize similar to what we showed nltk can do and then from transformers we're gonna import auto model for let's auto complete here for sequence classification you can see that there are a lot of different types of models that hugging face has and then we're also going to import from sci pi special um soft max which we will apply to the outputs because they don't have soft max applied and this will smooth out between zero and one all right special spell that correctly right and then we're going to pull in a very specific model that has been pre-trained on a bunch of data um for sentiment exactly like we're trying to do this is provided by hugging face and when we run the auto tokenizer in the auto model sequence classification methods and load it from a pre-trained model it'll pull down the model weights that have been stored and this is really great because we're essentially doing transfer learning this model was trained on a bunch of twitter comments that were labeled and we don't have to retrain the model at all we can just use these trained weights and apply it to our data set and see what comes out so anytime you do this the first time you will see that it needs to download all of the weights this is expected and now that's finished now we have a model and a tokenizer that we can apply to the text so let's remember what our example was before now this is this oatmeal comment when our polarity score from the old type of model look like this let's call this the vader results on example remember negative neutral and we want to run this though on using the roberta model that we've pulled so we just need to take a few steps so before we can run it on roberta model and that's uh first thing is encoding the text so we're going to take our tokenizer that we pulled in we're going to apply it to this example and return return tensors is going to be pt for pi torch and then encoded let's call this you can see here this is the encoded text so this is taking that text and putting it into ones and zeroes that embeddings that the model will understand we'll call this our ink coded text then we're going to take that and we're going to run our model on it it's just that simple so we're going to take this encoded text run our model on it and this will be our output and then you remember how we so this is what the output looks like it's a tensor with our results and then we're gonna take that output take it from being a tensor and make it into numpy so that we can store it locally so let's detach this and then numpy and store this as scores and then the last thing we're going to do is just apply that soft max to the these scores that we imported soft max layer um like this now if we print our scores we see that we have three different values in a numpy array now these are similar to the last type of model that we ran so basically this is the negative the neutral and the positive score for this text so let's make a scores dictionary where we will store this and we'll put in roberta negative is going to be the first value and then we'll just do like this negative neutral and positive and this will be 0 1 2 and we'll print this scores dictionary need a comma here there we go all right so the roberta model much more than the vader model thinks that this comment is negative which from reading it seems to make sense this is a very negative review of this product so already here we can see sort of how much more powerful roberta is than just a vader model let's go ahead and run this on the entire data set like we did before with the vader model so we can do this pretty easily by just making a function out of the code that we did before called polarity scores roberta where it takes an example like our code did before and it runs all of this and it just returns scores dictionary so now we could run this on one example of text and get this scores dictionary like we had written the code for above and we're going to enter iterate over the data set just like we did before so let's take this code from abort above where we iterated and we have our we're going to call this our vader result because we'll still run the vader text on this and then we'll also have our roberto results which is gonna be the polarity scores roberta function that we had written on this text and we'll break here after the first one just to see how it ran on the first iteration through so we see we have our veda results and we also have our roberta results it's exactly what we wanted and we also want to combine these so the way we can combine two dictionaries is there's a way to do it with the newer version of python but we're running an older version so we'll just do it like this and we'll call this both that's both results let's also go and rename this from negative neutral and positive to be explicitly named that they're vader vader results and i'm just going to copy this code which will basically rename these to vader and then the key name instead of right now it's just negative positive and then we will combine these two okay so running that for one iteration it looks like our results look good and we want to now just run it through all 500 examples so i'm going to take out this break and it's gonna run through oh we also need to um actually store this into the dictionary that we're gonna store with the id and we're gonna store both now here's i know this is gonna break uh because i ran this before but i want to just show as an example when it does break all right it did break on one of these examples because the text had some issues with it and it wasn't able to run through the roberta model so instead of debugging this all right now and it has to do with the size of the text itself there's certain size of text that's just too big for the model to handle we will skip those and we will skip those by adding a try accept clause here so it'll run through except for when this runtime error occurs in that case we'll just print out a message so we we know that it broke for id this id now we're going to rerun and let it go through all 500. okay so that's done running and you can see that did break for two examples um we could we could have lowered the amount of size of those and and it would have worked but this gets us a good result for now now it was pretty slow running keep in mind that's because i was running it only on a cpu these roberta models and transformer models are optimized to be run on a gpu and if i was to go here and turn on the gpu and the preferences i could have run a lot faster but it works for this case just to run it on a cpu and now let's actually take the results of this and make the results dictionary similar to what we did before so this line of code which takes these results runs of transforms on it let's call this results data frame and it will merge back on the main data set now if i do a head command on this we can see now we have our vader scores all four of them and our roberta scores for each row in the data set uh really quickly let's come compare scores across or between models and we can do this using seaborne's pair plot i think this would be a nice way to look at it so uh pair plot lets us see comparison between each observation and what each feature looks like so i'll show you here by running it on this results data frame and then we're just going to provide it the variables we want it to look at and those will be this vader negative neutral positive and the let's remove the vader compound because i don't think that's really needed to compare and we're also provided the roberta columns and we're saying these are the ones variables we want to compare uh let's also color it so the hue color of each dot is going to be by the score which is that one to five star scores and let's also give it a palette something where we can easily see the difference between each values um okay so this is vares i think and actually this combine and compare is what we're doing now okay so a lot going on here but one thing that we notice here is the five star reviews are this purplish color and if we look at vader the positive reviews are more so to the right on for these five star reviews for the roberta model you can see it's way over to the right and then we can see that there are some correlations between the roberta model and the vader model it's a little hard to see exactly if there are correlations but one thing that becomes very clear is that the vader model is a little bit less confident in all of its predictions compared to the roberta model which really separates the positivity and neutral and negative scores for each of these predicted values but if you look here this um positive and neutral like the roberta model has very high scores for the five stars and most of these one stars are very low in positivity um sentiment scoring so that's pretty cool let's also review some examples this is going gonna be pretty cool because now that we have uh sentiment score and uh we know the five-star ranking of the review we can look and see where the model maybe does the opposite of what we think it should so one way we can do that is we just take this results data frame and we query where there's a one star review so score equals one these are all our one star reviews and then we'll sort the values by this roberta positivity score and for ascending let's make this false so the highest positive score positivity score with the rank rating of the value being 1 will appear at the top and then we'll take the text of that and just do a values and print out the top value so what we're looking at here is a text that is said to be positive by the model but is one score by what the actual reviewer gave it it says i felt energized within five minutes but it lasts about 45 minutes i paid 3.99 for this drink i should have just drunk a cup of coffee and save my money so this is very nuanced sentence and you can see that it starts off being sort of positive i felt energized it lasted 45 minutes the model is getting confused and thinking this is more of a positive statement than we can tell that this is saying negative by the end of the statement so that's interesting and it makes sense let's do the same thing with the with the vader score so look at the most positive score for a once rating it says so we canceled the order it was cancelled without any problem that is a positive note so they actually were used the word positive and without any problem seems positive but it is a negative review and it's being a little sarcastic i guess a little tongue-in-cheek and the model does not pick on up on that especially the vader type of model which is only looking at a bag of words um for all of this sentence and and the score of each word individually let's also look at uh negative sentiment five star review and let's do this with the roberta model first so we'll switch to a five-star review and we'll look at the top negative sentence it says this was so delicious but too bad i ate them too fast and gained two pounds my fault okay so it is sort of a negative sentiment but a positive review that's kind of funny that that one came up and then we'll do the exact same thing for the vader and it happens to be the exact same one so the both models got i guess you could say confused but maybe this actually is a negative sentiment for a positive review so maybe it did a better job than what we would expect to see all right so we've explored a lot of things with sentiment analysis so far the one extra bonus piece that i want to show you is just using the hugging face transformers pipelines and this just makes everything really simple and i want to make sure i noted this you can read about it on their website but you basically can just import from transformers library a pipeline um and you need a spell pipeline correctly there we go and then we can make a pipeline with called sentiment pipeline with this pipeline and there are a handful of things that we could feed it um that it it will automatically be set up tasks that it's automatically set up for and we want to do sentiment analysis this will automatically download their default model and embeddings for uh this pipeline and you can just run sentiment analysis with two lines of code super quick and easy uh you can also go in here and change the model that it uses in the different tokenizer but the nice part about this is you don't have to set anything up you just do this it'll download the model it'll give us our default sentiment pipeline and then we can just run text on it okay so that's done running so i could say i love sentiment analysis you can see it's a different format and the output it gives by default but it's saying this is positive with a very high confidence um i'm going to type in make sure to like and subscribe here and that also is positive just to do a bad example we'll say boo and that does show as negative so it's working all right so that's it for our sentiment analysis project tutorial so there we have it we walked through two different types of models that you can use for sentiment analysis explored some of the differences between them we actually ran it on a whole corpus of data 500 different reviews from amazon you could scale this up and run it on all half a million examples and see what insights you can find so thanks again for watching my videos make sure you subscribe so that next time i release a video you'll be notified and i'll see you in the next one bye"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Implementing Text Preprocessing with NLTK and Scikit-Learn",
        "Video Title":"Hands-on NLP with NLTK and Scikit-learn: Use OS Dataset, and What Is the Enron Dataset|packtpub.com",
        "URL":"https:\/\/www.youtube.com\/watch?v=2gRpRHCCn18",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/2gRpRHCCn18\/hqdefault.jpg",
        "ID":"2gRpRHCCn18",
        "Publish Time":"2018-08-06T10:42:09Z",
        "Channel":"Packt ",
        "Channel ID":"UC3VydBGBl132baPCLeDspMQ",
        "Transcript":"you today we're beginning a new section about applying natural language processing techniques to do the task of spam classification in particular we're going to do the spam classification task with the data set on emails in this section we take a look at number one using an open source data sets and what is the Enron data sets let's talk about loading the Ameren data sets into memory we're going to take a tour over at organization lament ization and stop word removal as a pre-processing step to the text we're going to then convert the text into numbers in a process called feature extraction into a bag of words representations using the scikit-learn library and finally we're going to implement a very simple naive Bayes model with n ot K so this video will first begin in looking at what datasets we're going to work with and what is the N written data sets in particular I want to focus on talking about how data anchors natural language processing and machine learning and also finding understanding and downloading that and rinse map data sets so you might have been driven to this course because you heard a lot of new developments and breakthroughs around artificial intelligence using machine learning and every day there seems to be a new application a new benchmark has been broken by some AI what it gets less often reported is the data that enables this scale of machine learning training in fact no matter how complex your machine learning model is or how good your machine learning algorithm is you've got the right data to train it on you don't have the most important assets in AI for people who want to learn about AI techniques whether it's a natural language processing or not one of the biggest gifts is that we can nowadays find datasets online very easily and they're open source and licensed in a way that you can download them practice and experiment with your Nash language processing skills so Ameren is such a datasets is actually they're owed a dataset from more than 12 years ago but it's still a very good way for us to get our hands dirty in real world data and to practice what it means to train a natural language processing pipeline let's go take a look at an error in data sets if you search for Enron's spam data sets on Google the first link brings you to the very simple website so what it is it's simply saying there's a readme and then there is the Ameren data set in the pre processed form and there's also Enron's spam in a raw form so going from raw messages to pre-process messages is somewhat covered before but there's also a more advanced chapter later in this course to talk about how you can go from completely wrong messages to process messages the first thing to do with any data sets is to understand how it is collected so here it says you can see all the processing details in the paper spam filtering with naive Bayes which naive Bayes and I encourage you to search and have a look at this paper but this paper is naturally very theoretical so this text document also describes a little bit how the data set is collected and how the pre-processed data in the per processed a subdirectory is and in particular we also notice that the author's marked spam as spam and then they use the word ham to refer to non spam messages if you download the pre processed form of the interent data sets you can see that it splits into both ham and spam folders I put this data under the code samples that come with this course and if we just simply go into have you can see that each message is an email so first up here I think somebody has sent an email with farm pictures attached and here it's more of a long conversation between a few people from 1999 so wow that just looks like a bunch of browsing a bunch of downloading things I cannot emphasize enough how important it is for us to find the right data sets to practice our natural language processing skills on and how important it is to understand how these data sets came to be a lot of the art of machine learning and artificial intelligence is about how you pick the right model or the right data and engineer the right features from the right data"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Feature Engineering for Machine Learning",
        "Video Title":"What is feature engineering | Feature Engineering Tutorial Python # 1",
        "URL":"https:\/\/www.youtube.com\/watch?v=pYVScuY-GPk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/pYVScuY-GPk\/hqdefault.jpg",
        "ID":"pYVScuY-GPk",
        "Publish Time":"2020-05-25T03:30:05Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"feature engineering is a very important process in the field of data analysis and machine learning and I am very excited to announce this new series of tutorials where I will be going over different feature engineering techniques and we'll be writing Python code along with the exercises with that let's first understand what exactly is feature engineering there was a study published by Forbes magazine on what data scientists spend most of their time on and from this chart it was very clear that they spend majority of that time in data cleaning because in the real world the data looks pretty messy as shown in this funny picture here and you need to clean that data so that you can feed that clean data to your machine learning model machine learning model will not work if the data is messy even if you are not using machine learning and doing simple data analysis still you need to clean data so data cleaning is very very important and that's where data analyst and data scientist spend most of their time on and feature engineering is something that helps you with data cleaning let me give you example let's say you are studying the home prices and these are the home prices in the city of Bangalore in India to do the analysis on this one of the things you can do is you can create a new column which will be price per square feet just to kind of get a feel of your incoming data set now this data set can be coming from internet from the variety of sources and it can have errors okay it can help problems so by creating this new column you are trying to analyze if you have any data errors all right and once you have this new column now you can use either your domain knowledge so if you have a business manager that you are working closely with he would say that in Bangalore you cannot get a home at a rate of price per square foot equal to 3,500 square foot so when you see this particular data point highlighted which has price per square foot as 500 it clearly looks lion like an error okay and this error you detected by using a domain knowledge you can also use visualization and by plotting this bar chart this data point clearly comes out to be different than the rest of the data point you can also use mathematics and statistics where you can use techniques like to standard deviation or to detect these kind of spatial observation now this data point here is called outlier and these three techniques listed on the right hand side are all basically the outlier detection techniques if you have followed my pandas tutorial we also looked at how to handle missing values sometimes when you receive your incoming data set it might have missing values here for these two data points the bedrooms are missing so what you can do is just take a median which will be three and fill those missing values okay so handling missing values is also very important during the process of data cleaning we also had a tutorial on one hot encoding in my machine learning series where we replace the text data with numeric data here the text data is down so these are the property prices in different towns in the state of New Jersey and when you want to build a machine learning model the model doesn't understand text so you have to convert that into numeric values and we converted this text column into these three numerical columns and this process is called one hot encoding so these three techniques that we covered which is outlier detection one hot encoding and handling missing values these are the few examples of feature engineering so feature engineering is basically a process of extracting useful features from raw data using mathematic mathematics statistics and domain knowledge we have seen advancements especially in the area of neural networks where you can auto detect the features the meaningful features all right so the examples that I showed you we use domain knowledge mathematical statistics to extract meaningful features out of our data set but these processes were manual but there have been some automated ways of detecting these features and we'll be covering all of that in this feature engineering tutorial series so I hope this gets you excited enough and I will see you in the next tutorial thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Feature Engineering for Machine Learning",
        "Video Title":"Intro to Feature Engineering with TensorFlow - Machine Learning Recipes #9",
        "URL":"https:\/\/www.youtube.com\/watch?v=d12ra3b_M-0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/d12ra3b_M-0\/hqdefault.jpg",
        "ID":"d12ra3b_M-0",
        "Publish Time":"2017-10-30T17:59:21Z",
        "Channel":"Google for Developers",
        "Channel ID":"UC_x5XG1OV2P6uZZ5FSM9Ttw",
        "Transcript":"[MUSIC PLAYING] JOSH GORDON: Hey, everyone. Welcome back. Features are the way you represent your knowledge about the world for the classifier, and today I'll walk you through techniques you can use to represent your features and utilities TensorFlow provides to help. You use a dataset from the US census as an example, and the goal is to predict if someone's income is greater than $50,000 based on attributes like their age and occupation. The dataset is stored as a CSV file, and previously we've seen how to use the column values directly as features. But today we'll use feature engineering to transform them into a more useful representation. As we go, I'll visualize what these transformations do using a tool called Facets, and you can find a link to it in the description. You'll also find complete code to train a TensorFlow estimator on this dataset. OK, let's get started. Let's begin with a numeric attribute like age, and think about how we can use it to predict income. Now if you think about how age correlates with income, our first intuition is that as age increases, usually so does income. And the simplest way to represent this would just be to take the raw numeric value and use that as a feature. Here we're building a list of features we use to train the model, and each of these is stored as a feature column. This contains data about the column from the CSV file and how to represent it. Here we'll write a feature that just uses the raw value of age, and this string corresponds to a column in the CSV file. Now what can go wrong with this approach? Well, if we think more closely about age, we realize it's not in a linear relationship with income. The curve might look something like this. It's flat for children, then increases during working age, and decreases during retirement. A linear classifier, for example, is unable to capture this relationship. That's because it learns a single weight for each feature. To make it easier for the classifier, one thing we can do is bucket the feature. And bucketing transforms a numeric feature into several categorical ones based on the range it falls into, and each of these new features indicate whether a person's age falls into that range. And now a linear model can capture the relationship by learning different weights for each bucket. Let's see how this looks in Facets. Conveniently, there's a live demo that runs in the browser with our census data preloaded, and each individual from the CSV is visualized as a dot colored by income. If you click on a dot, you can see stats about the person. Now let's bucket by age, and you can adjust the number of buckets to make it more or less granular. How you choose the number of buckets is up to you, and ideally, you'd want to use your knowledge of the problem to do this well. In TensorFlow, we can create a bucketized feature by wrapping a numeric column from the CSV. And here we're specifying the number and the ranges of the buckets we'd like created. Once this is done, we can add the bucketized feature to the list used to train our model. Now let's see how to represent a categorical feature, and I'll use the education column as an example. Because there are only a few values, the best way to represent this is just use the raw value. And here we'll create a feature column that says education can be a single value from this list. Of course, you could also read the values from a file on disk rather than writing them out in code. Now using the raw value is the right thing to do when there are only a small number of possibilities. We'll cover the case where there are thousands of possibilities in a moment. First, let's take a look at feature crossing. Feature crossing is a way to create new features that are combinations of existing ones, and these can be especially helpful to linear classifiers, which can't model interactions between features. Here's what this looks like in Facets. I'll take our age buckets from before and cross them with education. Under the hood, you can think of a true-false feature being created for each bucket that tells the classifier whether an individual falls into that range. Now these buckets can be informative, and here we see some groups are likely to have a high income, and others low. In code, using a feature cross works the same way as before. We'll cross our age buckets with education and add it to the list of features to use. A feature cross can generate many possibilities quickly, which is why they are often represented under the hood with a hash. A hashed feature column is one way to efficiently represent a categorical feature with a large vocabulary. More importantly, you can use these as a way to make your data easier to work with because they free you from having to provide a vocabulary list. In this example, we'll represent the occupation column from our CSV file by using a hash with 1,000 possible values. Notice we don't have to provide a vocabulary list, and to avoid collisions, I've set the hash size so it's larger than the number of items in the vocabulary. Here's how this works under the hood. Normally, a categorical feature is represented as a one hot encoding. That means there's one bit for each possible value in the vocabulary. And we can create a lookup because we know the vocabulary list in advance. Now if we don't know the vocab, we can use a hash function to compute the bit automatically. The downside is there could be collisions, meaning different items are mapped to the same value. Hashes can also be used to limit memory usage at the cost of adding some noise to your training data. If you have a large vocabulary, it can be memory intensive to use that as input to a neural network. A hashed column can be used to limit the maximum number of possibilities, but I prefer them simply as a tool to save you programming time. Finally, I'd like to mention embeddings, and these can be less intuitive than the other techniques, but they're a powerful way to work with categorical data in a deep learning setting. You can think of an embedding as a vector that represents the meaning of a word. And we can visualize a dataset of word embeddings using the TensorFlow Embedding Projector, and there's an online demo you can find in the description. Here we're looking at a dataset of 10,000 words, each of which is represented by a vector with many dimensions, projected down to 3D so we can see them. You can search for words in the box to the right. And if you experiment a bit, you'll find similar words are often close together. For example, all of the words in this cluster are cities. What's neat about embeddings is that they're learned automatically in the process of training a DNN. And to make that happen, all you need to do is write an embedding column. Here we'll create an embedding for education with 10 dimensions. Now embeddings are helpful if you have a categorical column with a large vocabulary and you want to compress the representation so the classifier learns general concepts rather than memorizing the meaning of specific words. For example, imagine if the census data had a column called job title. There are thousands of different jobs, and an embedding could be used to help your classifier learn that words like programmer and software engineer often mean the same thing. OK, hope this was a helpful intro, and thinking about how to represent your features is one of the most important contributions you can make to a machine learning experiment. Feature columns are great because they let you experiment with different representations in code and make advanced features like embeddings accessible. As a next step, I'd recommend you try the code in the description and see if you can modify it for a problem you care about. Thanks for watching everyone, and I'll see you next time. [MUSIC PLAYING]"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Feature Engineering for Machine Learning",
        "Video Title":"Introduction to Feature Engineering in Machine Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=DkLQtGqQedo",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/DkLQtGqQedo\/hqdefault.jpg",
        "ID":"DkLQtGqQedo",
        "Publish Time":"2022-02-05T17:15:00Z",
        "Channel":"Prof. Ryan Ahmed",
        "Channel ID":"UC76VWNgXnU6z0RSPGwSkNIg",
        "Transcript":"hello everyone and welcome to this new lecture in this lecture we are going to cover an introduction to feature engineering so let's get started all right so what do you mean by feature engineering so machine learning algorithms require training data to train if you guys remember in order to train an artificial in an artificial intelligence model or machine learning model what you need to do is that you need to have training data and to train this model either in a supervised fashion or in an unsupervised fashion and the problem is is that most of the data that is available right now in the world they are unstructured data they have missing information they are messed up there are duplicates they are out of there is no schema for it it's just a mess so the point is of feature engineering which is one of the most critical tasks that data scientists do is to kind of clean up the data and make it ready to be fed to an ai machine learning algorithm so feature engineering is a critical task that data scientists have to perform prior to training the aiml models so as a data scientist you may need to first highlight important information in the data you need to remove and isolate unnecessary information so for example if you find some outliers in the data you need to get rid of these outliers and as well you need to add your own knowledge and your own expertise to the to alter the data appropriately appropriately as well so feature engineering is an art of introducing new features that weren't existing before again you have raw data and what you wanted to do is that you wanted to kind of tweak the features somehow tweak what you have somehow in order to come up with usable features that will train the model and you will be able to have a model that's able to generalize too so data scientists spend and that's very very important and i want 80 percent of the time performing feature engineering and i'm going to show you guys when we go to the modeling section is that actually building an extremely advanced let's say deep learning model is not as challenging as you think especially obviously if you're building it from scratch obviously yes but leveraging all the tools that you guys have available such as sagemaker or any of the other let's say you know like frameworks like you know like tensorflow 2.0 or keras they made it extremely easy to build you know like like a network with millions and millions of weights so actually training the model is not a challenging part at all so the actual 20 is they actually data scientists spend around 20 of this time just training in the model performing hyper parameter optimization the main work existed okay in the data how can i give the model the real data the important data and how do i do feature engineering and that represents 80 percent of their time all right so performing proper feature engineering is crucial to improve aiml model performance all right so what are the proper questions to ask so as a data scientist you need to answer the following questions first what are the capability of the machine learning model that i have which features should i select can i add my domain knowledge to you basically use less features so if for example if i wanted to drop let's say one column or two and maybe add or come up with something new from the data that i have can i come up with new features from the data that i have at hand or and then at the end what should i put in the missing data location so sometimes we have some missing data and unfortunately we cannot feed in a model with on with missing data you need to fill in this missing data first and then train your model so that would be a challenging part so that's why it's important to choose features that are most relevant to the problem adding new features that are unnecessary will increase the computational requirements needed to change the model and that's why we call it curse of dimensionality so you actually need to be very very very you need to make sure that you are not just you know like yeah i collected bunch of data and just to feed it into the model without actually understanding what's happening in the background and without removing any unnecessary columns that you might have or data column that you might have there are numerous techniques that could be used to reduce the numbers of features such as compression encoding the data and we're going to cover a technique of what we call it feature reduction technique it's called principle component analysis or pca for short we're going to be covering it in the future basically if you have let's say 10 features i can compress them somehow and make them let's say five features using pca technique or principle component analysis technique all right so let's take a look at a feature engineering example let's assume that i have this data okay so you know here i have customer id here i have my customer name here i have the location and here i have basically the output or my target which is i wanted to predict i want to know if that customer has clicked on an ad or not so let's assume that we have you know for example you are a company and you know like you know you posted an ad on facebook for example and you have collected data about your customers and you wanted to know does let's say bird uh clicked on an ad or not let's say did like chanel for example clicked on um on an ad or not and that's it so if the customer clicks on the ad that would be one if the if the customer does not or did not click on the ad that would be zero so that's the data you've collected and you wanted to clean it up you want to do some featured engineering so first of all i let you guys guess first any any idea what should we do here okay so let's start with the first one first machine learning models does not accept any names like that so first i need to encode it so i need to take usa canon and friends and convert it into a bunch of numbers so i need to encode that column okay second one is well i have a missing data point here i cannot feed in a machine learning model with this missing data and here i have to do some formatting again yes i have to replace it with let's say one for example in this case and then i can here if you guys take a look at it you will find that basically these two rows are duplicate so i need to remove one of these duplicate entries all right so just imagine you wanted to do that first before feeding in this algorithm to train your aiml model all right so what are the feature engineering engineering techniques available obviously there are tons of them we'll try to cover as much as possible just to make you guys ready for the exam so first imputation handling outliers binning log transform one hot encoding feature split scaling and many many more we're gonna be covering all of these bare minimum in this in the next couple of lectures and i hope i'll be adding additional as well feature engineering techniques if i see any of these questions appear on the exam somewhere all right so the question is what tools should i use to do some feature engineering so basically you can kind of go to routes first one is using jupiter notebooks and using amazon sagemaker so if you wanted to do some ad hoc for example analysis you can create your own code and if you guys are you know not familiar with that please refer to the previous um lecture where we covered jupiter notebooks and and psychic learn and all that i'm sorry my previous section and we showed you guys a lot of examples and basically you can use again sagemaker and jupyter notebooks to do that and on the other hand you can use as well glue if you guys remember we covered glue before which to do some etl jobs so if you wanted to do for example some transformations and so on you can use glue as well and here if you use glue that would be repeatable and reusable application as well however here that would be ad hoc you know like if you want to do it basically every once in a while all right and that's all i have for this lecture i hope you guys enjoyed it in the next lecture we are going to cover amazon sagemaker ground truth that's all what i have please enjoy aws machine learning certification course and i will see you guys in the next lecture"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Feature Engineering for Machine Learning",
        "Video Title":"Feature Engineering Techniques For Machine Learning in Python",
        "URL":"https:\/\/www.youtube.com\/watch?v=GduT2ZCc26E",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/GduT2ZCc26E\/hqdefault.jpg",
        "ID":"GduT2ZCc26E",
        "Publish Time":"2021-12-18T13:44:00Z",
        "Channel":"Greg Hogg",
        "Channel ID":"UCJublDh2UsiIKsAE1553miw",
        "Transcript":"hey guys it's Greg welcome back today we have an awesome video pretty much every feature engineering technique for training machine learning models I'll explain how to do that in Python code we are doing this in a co-op notebook so take a look in the video description if you want to follow along with that but either way we've got one dimensionality reduction using PCA two pre-processing and scaling methods three categorical encoding so dummy and one hot variables four we've got bidding grouping aggregating features together five clustering so here's the map of California for example we can cluster the latitude and longitude and get this kind of pretty picture out of it it doesn't have to be that geographical data but that makes the most sense visually and finally we've got feature selection combining all of this different stuff together and only taking what you want to okay if you only want to learn one or a particular subset of these ideas I do have marked in the chapters Down Below on all these different uh topics so just go and learn what it is that you want to learn or if you want to follow everything along with me I've got it here at the start with initial setup but before you go off and do your own thing with this video I want to point out that we are using the California Housing data set that is in the link in the description down below so that's the particular Kangle data set whether you're doing it like this or you click that link and download it it's the same data set but the point is we're not using this one that is in Sample data California Housing over here we actually have to download it straight from kagle like this okay so with that being said um I put a lot of effort into this video to code everything up and explain it so if you don't mind dropping a like in this video video I'd really appreciate it I know this is going to help a lot of different people and that would be great if you could support me and others that way H cuz we're all learning together and if you're not subscribed to the channel this is a really good time to do that but with that being said let's get started with our initial setup so firstly we have to grab that California Housing data set there's two ways to do it you go to the video description click that link download the zip folder and then you can actually go ahead and upload that zip folder up here or what I do is I use my kagle API and so I go to my kle profile generate API token it downloads this k. Json thing and then you can upload that or put it in the working directory of all the stuff that you're doing whatever environment that you are using and then you can just run this block of code here in collab and it'll do all of the downloading and UNS it for you so I am going to do that it complains at me because I've already done it before but I can go over here and click yes just to kind of redo this stuff it inflates or whatever housing. CSV so the important part is we've got this California housing data set right there now now that we've got that we can readen the CSV into a pandas data frame so import pandas aspd we'll make this variable DF equal to the read pandas read CSV of housing. CSV we're going to immediately drop any rows that have any null values that's what this part does and then we can just call DF to Output that we see we've got 20433 rows by 10 columns just to explain the data set if you don't AR aren't already familiar with it if you are skip over over the section for sure but basically each of these rows are a particular area in California and we have a bunch of different information for each of those areas so for each of these different areas we've got the longitude the latitude the housing median age so of all the houses in that area we can uh get all their ages get the median of that and so that happens to be 41 for this one uh we can sum up the total number of rooms so 880 total bedrooms is 129 population household the income uh the median income in that area and then finally up and two we have median house value which is generally what this data set is used to predict we generally use a bunch of these different variables to try and learn a function that Maps these variables into the predicted median house value okay so we'll do that for everything that we do in this video we're also going to use this is why we got the extra data set the different data set here is it has this ocean proximity this categorical variable here which we will do in the uh the categorical encoding section okay so we have our DF here the first thing that we're going to do since we're training a machine learning model or many of them that are going to learn uh basically a transformation of these features into trying to learn the median house value from these features well we're going to split into a train and test data frame so that we can train our machine learning models on that dat on that data frame and then we can test in uh our models on our test data frame as well okay so with this what this does here DF is equal to DF do sample set Frack equal to one what this does here is basically it shuffles the rows here and we set random state to two that's uh that's if you want to reproduce the same results you should use the same uh random seed value here or if you want a different results you could change the value there okay and then what I do is just make train and test the first uh train is going to be the first 177,000 rows because remember it has a the the data frame itself has 20,00 433 rows so I can do a decent split of roughly like 80 80% in the train and maybe 20% or so in the test data frame I get the first 17,000 in train I have to do this reset index thing don't worry too much about that it just kind of it just makes it so that this index is still correct and then what we do is get test DF is 17,000 onward and then we can reset that's index as well so we got our train data frame is all the columns and then the first 177,000 rows and then test data frame is all of this stuff and the the randomization randomly split um the the train and test information okay so now what we're going to do all our machine models are going to be about using the median house value to predict that and so the training the training output basically we say we use y for output y train is just going to be that particular column so we get the column this actually returns a panda series and so we can do train DF sub the median house value we convert that to a numpy array which is just going to be a flattened all 177,000 values and we do that for both y train and Y test and we can output their numpy shapes okay so these we are going to use very heavily these are all of our observed or values or outputs we have 177,000 uh observed outputs for the train and 3,433 for the test so the first thing we'll do here is create something called a baseline model and we're going to do that by making it just the average median house value in in the train data frame okay so the point of what we're generally trying to do here is learn just some sort of mapping between these inputs and this uh this output over here the median house value we're trying to predict that and so usually we'll try and learn from these features and use those but we don't necessarily have to we can actually just make a baseline model so a very simple model for terms of comparison uh where we just calculate the average of everything so this is the train data frame if we calculate the average of all of these median house values will be left with some number and it's it's not a very good prediction because obviously we should be using some of the features that we have available to us like longitude and magnitude but we don't have to so we'll make our first Baseline model which is simply the average median house value in the train data frame and for all of our models we're going to be evaluating them based off of the error metric the mean absolute error and we'll import that from sklearn or pyit learnmetrics okay so we get the average median house value in Train by this is a panda Series so we take the train data frame sub median house value and that has a panda series have an object called or a function called mean and so we can call that to get some value here and then we can get the test predictions which is just all of our predictions for the test set and that's what we're always trying to do here when we we train models off of the train data frame that's what we did by taking the average and then we compare our prediction one by one to all these different actual test values okay so the first prediction we are trying to guess this that was our best possible uh but what we instead predicted is whatever this average actually is and just to show what this average is uh very quickly we can do that with this so every time we're predicting simply 20,000 uh sorry 27,1 189 as is the house value and we can see that's wrong in many places it's actually pretty close to this value but it's far off the majority for sure but what we do is we we make the list we just get the list of that average value and then we multiply that by the length of the test data frame so that we're going to have and we know that's sorry about that we know that it's going to have 3,433 predictions and so it's really just the average value uh 3,433 times and so that'll be our predictions for everything we get the mean absolute error of those test predictions right right there and Y test so we're comparing one by one the difference between the Baseline model which is just one value over and over again versus the actual y test which is again just these values here in the test data frame and we come up with some number which as we can see is not defined clearly I'm not running this stuff as much as I have to run that run that and we get 90,9 52 it means on average we off by the absolute value of $90,000 which is not terrible I mean it's definitely not a very good model uh but we want to do a lot of different methods to improve upon that we're going to start with dimensionality reduction in PCA and by the way the point of this video is not necessarily to actually get uh better values than this although we definitely will I'm not trying to show you how to make the best machine learning model on this particular data set I'm showing you all the different techniques so that you can do what you think is right and you know all the different techniques available so you can try and get the best model for whatever your problem is okay so firstly we're going to start with dimensionality reduction with PCA observe the correlation between total rooms total bedrooms and households if we get to the train DF all of these columns so total rooms total bedrooms and households were left with just a smaller data frame and then we can get the correlation between those where the correlation is always going to be a value between negative one and one these are actually all positives and this makes sense because if it is a positive value that means for example as the number of total rooms increases the number of total bedrooms tends to increase as well so if we look at total rooms with total bedrooms we get 9 31023 and that value is the same as that one over here because it's total bedrooms and total rooms total bedrooms and total rooms it's the same thing so what we're seeing here is there's a very high positive correlation it's almost it's very close to this perfect linear pattern of when the total bedroom is going to increase the total rooms increases as well why am I telling you this well that's because maybe we don't actually need all of these different variables maybe we don't need total rooms and total bedrooms and households to try and predict uh the the median house value because there's so much relationship and information captured in them already so one option here and of course you could just simply use one of these different features um you could just only use total rooms for example but something that's most likely a little bit better is to do something called PCA which we'll look at very shortly so what we're going to do first here is set up our Matrix to make a machine learning model of our input with just total rooms total bedrooms and households as inputs so without PCA at all for now we import numpy as NP and we'll make xtrain 3 equal to that uh that same data frame there and the the train so that's just those three columns and then we convert that to numpy and we what we're left over with is a 17 ,000 row by three column nump array that makes sense now we do that exact same thing uh for X tests and I just put a three on it to remind you that it's with the with these three variables and so we get our test information as well now we have an input Matrix where it's the number of examples by the number of columns the number of examples by the number of columns so 177,000 in the train and 3,433 in the test what we're going to do now is get an error of a random forest model and it doesn't matter if you know much about ROM force or not uh but on just XT train 3 y train and then testing that on X test 3 and Y test okay so that's a lot of words but basically we're going to uh import the rain and Forest regression model it's going to we're going to be doing regression that's a it's it's predicting the median house value so it's predicting continuous value that's what regression means we get the forest base I'm just writing Bas as in a base model is equal to the random Forest regressor where we'll just make these variabl for fun the N estimators is equal to 50 Max deps is five don't worry about don't worry about that stuff if you don't understand it and then we fit it with xtrain 3 so we feed it in our input Matrix uh for Te for train sorry we train it uh we feed it in the corresponding inputs in the training and then we can then now that we have a model okay so we have our forest base as a model trained one and we can get Forest based test prediction so the same thing we are making for the average model where we just getting uh instead of the the average just every single time we're actually going to be using a random Forest to make a prediction now and so we feeded in our X test information we feed it in our input and then we can get the mean absolute V value or mean absolute error on y test comparing that to the forest models based predictions okay so if we run that we're going to see it's not defined I keep forgetting to run stuff X test and then X train 3 is now to find I guess I'll run that one funny okay so we run that model and then we get 81,9 196 so on average our our model was off by about $882,000 instead of $90,000 like our average Baseline model was so clearly we learned 91,000 actually where that was off okay so our random Forest did a lot much better obviously but now let's actually do a PCA and see if this helps for this remember for this particular data set and this particular model that we happen to use does it help well we can train PCA which is just a linear uh dimensionality reduction technique you can also do something called autoencoders and some other weird ones if you want to look at them uh so PCA is a linear dimensionality reduction technique from SK learn. decomposition we'll import PCA and we'll train a PCA where we set it's number of components equal to two so that means uh whatever you feed me in here I'm I'm feeding you in uh three different columns that's why I call it xtrain three here and we're getting back just two columns which is supposedly just better information more concise information into two columns we then we don't need these three columns we can just get it in two two better ones so we can use PCA to transform xtrain 3 and X test 3 and xtrain PCA and X test PCA and so as we see if we call the pca. transform on those inputs we will be left over with 17,000 X3 but 17,2 and 3,433 by 2 instead let's see if that does any better this random Forest is doing the exact same thing as before we'll get a random forest and then we'll fit it with extrem PCA instead on the PCA data feed in that y train Forest PCA test predictions is equal to Forest pca. predict we feed it in that X test PCA and back we get we happen to have and I always forget what I'm actually running here but we get the absolute error on this model is 79 something okay so the other one was 82 for this particular model and maybe it would change things if we change this value or this to a linear regression or whatever but the PCA did happen to help for this model so that is one particular way of doing uh feature engineering techniques is with dimensionality reduction that's how to do it with PCA in particular okay I quickly added in xra three here to show you what happens if we pre-process or scale it beforehand from SK to pre-processing will import standard scaler normalizer and minmax scaler there're three different tools to do that and we'll also get our graphing Library here if we make a scaler equal to for example the standard scaler and we fit that with xtrain 3 we we fit it feed it in this we'll get xtrain 3 scaled by transforming using that scaler to transform xtrain 3 we transform this thing and then we get this thing out called Xtreme 3 scaled and it's it looks exactly the same except just these the values look different and in particular we see two things well one they they can be negative now and that's just that that happens to happen from standard scale or these ones might not do that um but in general they are all every single value here is pretty much on a closer range to each other and what I mean by that is these are all in the thousands as we see and these are all in the hundreds as we see that is a big problem for many machine learning models not all of them some SK learned stuff might not even care but any deep learning algorithm is going to really care that these values are just so much bigger than these ones and that's because you know if we change from 2,700 to 1200 that's a 1500 value drop that's huge compared to 574 versus 214 we only drop this by 3 350 or so 360 so why do we care that this change is so much bigger than that one well that just says that this feature this this column is more important than this other column and if we really wanted to do that maybe we could keep it but in general we assume in setup that all features are pretty much equally important and so to do that we have to do for example this standard scaler thing where now all these all these values here are between -3 and 3 or so these are all between 3 and 3 or so why am I saying three and not like 7 and negative. 7 that's because we are doing what standard scaler is is uh performing a standardization thing which means we are subtracting we calculate the mean of this column and we calculate standard deviation of this column we take this value and we subtract it by its mean divide by standard deviation we take this value we subtract it by its mean standard de divide by standard deviation and we do that for all of the values in that column we do it for this column where we take this we calculate the mean for the column and then we subtract by the standard deviation of the column we do that for all of those values and then they will all be roughly in if we were to graph it this is just the First Column after the transformation these are between as we can see -3 and three or so almost everything is that and if we were to do say look at the first uh second column actually they again they moved a little bit more to the negatives but most of them are between -3 and 3 not everything but most are and so that makes it a lot easier for these machine learning algorithms to figure out what the heck is going on it treats these treats these Colum equally basically okay so we can use standard scaler uh that that does this particular transformation we don't technically need between ne3 and three or so but we could do normalizer instead which for this particular data set uh we'll see this is just looking at the First Column uh this makes the values between zero and one we might get different Behavior if our values were allowed to be negative in the first place because note that all these are positive uh or we could do a mmax scaler and I'm not really going to show you uh the different ones like all the differences between these uh that's what SK learn is for and so I have this over here if you want to read the documentation on why you might want to use one over the other and what exactly they're doing uh then you can read about it there or I might make a future video on that um but the idea is that most values you are going to be uh standardizing and uh if you happen to know that there's already a maximum out there so for pictur for example for picture data all the values are going to be between 0 and 255 and so your pre-processing could just be divide everything by 255 note that in that particular example I just said uh all the values are between 0 and 255 they're all already on the same sort of range why are we dividing by anything why aren't we just happy with the values in general nural networks are still and many machine learning Al algorithms are still happier with values between zero and one or so and negative3 and three or so or something small like that it's just easier for a lot of things okay so those are our different options uh each of these produce a different uh algorithm and you can produce a different output you can look here I'll include this in the do in the description down below if you want to see which one you want to use but that's the idea you just kind of try them out and see what works best overall okay it's important to note that I did actually rerun The Notebook above so if you see slightly different numbers that would be why um but anyway for now note that we were doing the exact same transformation at least the same Transformer object on every column that we were using we picked standard scaler and here we fed in the whole xtrain 3 here there is times when you might want to pick um you know maybe du only the First Column a standard scal on that one maybe a normalizer on the second column you can feel free to play around with that it takes a little bit of manipulation and numpy to to kind of get those arrays separated and then joined back again later I just wanted to let you know that's an option that you can do and sometimes we we apply different transformations to different columns like that but for now we're going to do a standard scaler on xtrain 3 and we do that with we'll get X test 3 scaled as well so that we have X train down here we have XT train scaled and X test scaled I don't know why I didn't include the three on that but I guess that's okay and then we make a random forest with the scaled information to be that same random Forest we fit in XT train 3 scaled uh and Y train we get the test predictions for that model with that random Forest there do predict with the X test 3 scaled and then we check the absolute error comparative y test and those predictions which gives us 82270 uh for this example okay and so if we look up above and compare this to exactly the same model but without any St scaling being done this is 824 that one the other one is uh 82 to okay so they're very very similar it turns out that just this random forest model doesn't really care too much about the scaling but I promise you neural networks care a lot so we might want to do that for those and some other machine learning models as well now it turns out that that PCA thing we did earlier actually requires uh at least it prefers some sort of standard scale or some sort of scaling operation like that before the data is fed in before we just fed it in the Raw values uh now we want to actually scale the data then do PCA then train a random Forest on that and now that we have three steps here it's probably the best uh best choice to actually use this psychic learn pipeline object from SK learn. pipeline import pipeline where we'll make a scale then PCA then pipe uh then Forest pipeline so we do standardization then PCA then random Forest to be the pipeline with standard scaler and then PCA with number of components of two and then a random forest regressor with that same thing as before note how we're passing in these objects which is just like bracket bracket here we're not fitting we're not passing any sort of inputs this is just defining the pipeline of what can be trained and then and then predicted with later so we can fit it so we get this scale PCA pipe forest and we call fit on that with it still before X train 3 and Y train okay so we feed in those RW inputs what it does is it learns this standard scaler okay it knows what to do uh to it it learned its parameters so that it can perform that transformation then we did a PCA so it can perform that PCA transformation with that data actually being scaled because when we call fit what it does is it fits this object and then it passes it in to this object which it then fits that and then it passes into this object which then fits that so it fits all of these in the full pipeline so we call fit and then we can get the predictions with it but now that we fit with just that thing. predict with the XS 3 that input matrix it's going to do the all those Transformations so on predict it's going to scale that data it's going to apply PCA to that data to make it two columns and then we're going to actually apply this previously learned random Forest to that to get the mean absolute error which totals uh that is not defined because I haven't run that up there and that is going to get an error of 80 8,224 which as we look above is uh not very different from the other ones still what is going on here uh that is just because standard scaler may not be the best possible object you might want to use normalizer instead so let's try normalizer and see if we get a better result we do this and although this comment is technically wrong right now it doesn't really matter now we get 77 okay that is that is better than all the other ones so it turns out that normalizer happens to be a better uh transformation to perform on this data okay so I'll just change the comment again to that normalization and above to normalization as well okay now we're on to something called categorical encoding which generally we want to convert this into dummy or one hot variables dummy is for the pandas version and one hot is for the numpy version although it's doing the same thing we'll look at that shortly there is other ways of encoding categorical variables what this assumes is that each different category oral variable for example red green and blue are treated entirely differently and that actually makes sense for RGB that's why we make colors out of RGB because the there are three primary colors but then instead think about more colors if we had yellow and green well they actually have similarities to red and blue and and the primary colors so what I'm trying to say here is that we might not necessarily want to do this dumy encoding for everything for example for words words have similarities to each other we use something called embeddings for words but we're going to use dummy encoding uh for for this because it does make sense we have low dimensionality meaning we have uh what we're looking at is if we get the train data frame sub ocean proximity that's that categorical column uh from way above and we look at this column uh it's just it's just that column there and then hopefully I can find this again very quickly here we have uh we pd. getet dummies on that column and that returns a data frame when we get the head of that that returns this object right here where it is going to have since we called that on train DF it'll be 177,000 rows and for each of these different columns it has uh well it has it now has one column for each of these different variables that were in that list so all those different columns are we have one for near Bay we have one for Island and all of the different options we have right there okay so why do we want this well now we can say hey this row is it's one of these one of these possible things and the one that is it's a near Bay this row is one of these many possible things the one that it is is an inland so we call or Inland whatever um so we call this either dummies because uh actually I don't know the term but I think it's a statistical thing and uh one hot when we're when we are to convert this thing to a numpy array really this is just one Vector here which is a this is one hot this is hot as in it's the thing that's not cold this is cold this is cold this is cold this is cold and this is hot and so it's one hot because this is this is a one okay so that's the idea it's just a way of decoding to this row that this is this is this type of thing and this row is this type of thing and this row is this type of thing now importantly uh how many different values do we have for each of these different things well firstly before we look at this code If You observe the frequency of categories we can look at the value counts of train DF Sub sub ocean proximity that's just that categorical variable of above we look at the value counts which shows how many this this total value should be the number of rows that we have in the data frame and so we have 7,522 of this one which is an hour less than the ocean or like an hours drive to the ocean or something like that we have 5,48 inlands we have 2,172 NE I 1,895 near Bas we only have three islands so what we're actually going to do here after we just do a simple concatenate to append these new columns to our train data frame so we train DF PD duck and cat train DF train dummies across the uh the column AIS AIS is one we combine all of those to get uh basically just the dummies or one hot from this this variable here so we append that and then since we don't have many isets here we're going to draw drop Island train DF drop Island in place is true meaning we don't have to do uh we don't have to do train DF is equal to that thing if we do uh in place is true we do it on AIS equals 1 and we train DF do head we output that and we can see it is all of these columns except uh we removed Island okay so this uh this data frame isn't overly useful as it is uh we'll change that very shortly for now we're just going to do exactly the same thing on the test data set so we'll get the test dummies which is the data frame of that gets the pd. get dummies of the test DF sub ocean proximity test dummies. head gets that data frame there and then we append the dummies and drop Island on the test data frame so test dummies is H that same thing I don't know why I would need that I definitely don't need that so test DF is equal to PD do and Cat of test DF test dummies across axis equals one test do DF drop Island in placees true accesses one test DF head I haven't ran a few of these things for a while so I'm actually just going to go back from the start and start running these operations and now that we have um so now that we've concatenated that we have those columns on the train test uh train and test data frames so uh we can get this we're just going to try and make a machine learning model based off of only those columns of course in the end we will use some other columns as well but for now we're just going to make a model that only looks at these different categories and see how that does so we get X train dummies which is just that train DF to numpy this part's a little bit weird so we get all rows and then we want -4 up until the end well if we count back here we get uh min-1 -2 -3 -4 we get this column so it starts at Min -4 that column and then we want to go to the end so we say all rows which is all of this information but then all only columns we want is just this chunk here so it's just this chunk here uh except this is the head so it's all that information and so if we were to look at the shape of this thing it is 17,000 by4 so we did that for the training information we can do that for the test and so we do test DF that's Den by all the rows ne4 up to the end and that's the test dummies. shape we will make a linear regression model just for fun uh we don't have to do that but I'm choosing to do a linear regression on the X train dummies and Y train we make that our linear dummy model we get the linear dummy test predictions which is that model predict with the X test dummies and we get the mean absolute error with Y test and linear dummy test predictions to show that it is in total I don't know why I'm not just running the stuff as I go um 77,000 195 okay so that's actually a pretty good model like that's actually a really good model that only uses uh the a one hot encoding or dummy encoding of these variables okay so it just use the ocean proximity in its fullest capacity to return a pretty good model next up is something called binning and I'm going to use other terms for it like grouping and aggregating really just referring to the same thing which is if we're to look at train DFS up housing median age for example not the median value this is one of our inputs we range from about 1 three to like 55 years or so okay so why is this important well there's no reason that we have to use this column as it is as just its values or even just scaling of it uh what we can do is maybe sort of move things into bins and so if I was to draw a line right down the middle which is uh say 30 or so okay so if I draw a line here what I can do is basically shove everything in here into one big bin and say that it's less than 30 or it's bigger than 30 or equivalently bigger than or equal to 30 so we get two big bins out of this which is everything is either less than 30 or it is greater than 30 if I do train sub train DF sub median age less than 30 this is adding a new column into the data frame we make that equal to train DF sub housing median age is less than 30 so it applies that operation uh to every single every single Row in there and then it returns a Boolean okay so what this thing what this thing is here is a panda series of booleans where it's either this thing this first one is happens to be less than 30 the second one happens to be less than 30 third one is bigger than 30 okay so we get this Boolean series out of it and then we convert that into an INT and so that'll make the falses zero and the trues uh will be one and then train DF sub uh uh train DF do head after that we now included this column here so we basically just bin uh so there's no reason we could have just made this a binary variable we could have of course uh made this multiple variables like say between zero and 15 15 and 30 and 30 and so on then we'd have to do that one hot encoding thing again um that's that's the main idea though is basically we just kind of group these things into different bins and treat them as different ideas now housing median age is bigger than 30 is kind of something I made up uh but for pretend for now that that's actually makes sense that uh some realtor went over some really uh famous real realtor said something stupid like uh any anything less than anything older than 30 is just completely useless like it's just so much worse um and so that would actually have a strong effect and so this is something reasonable to do to give uh the Zero versus the one value very uh different feelings and it's not just about this kind of this gradual increase it's more about the fact that it's all either less than 30 or it's all bigger than 30 that's why we're doing this it's probably not going to work super well but it would better for other examples but that's the idea so again uh we could of course combine this with other features but for now we're going to make a model just out of that variable and see how that does we'll get the X train with just median AG that should be really median age less than 30 but that's fine uh so we get that we convert that to numpy and for these SK learn models they always have to have this kind of other column shape like number of columns so if if we just grabbed this technically it would have been 177,000 flat we reshape it into this is the same as doing uh 17,000 by 1 but I'm going to leave it as negative 1 in case the number of values uh happens to change over time so we do that we reshape it and we get 70,000 by 1 which is all either zeros uh sorry I must have forgotten to do this and then we get this which is all either zeros or ones that same thing as we had before we you can do the exact same thing with test which uh not really going to show how to do that but it's the exact same operation but on the test information and then we train a model with just that information I'm going to start to gloss over what I'm doing here because you've seen this a bunch of times now I'm just whatever training and testing uh data set I'm setting up here uh that's the one that I'm using for the model okay so we do that and then we make a model out of that we're going to do a linear regression just for fun and we can see we got 90,000 uh 90.7 which if we compare that to the Baseline model at the very top that model so it's 90.7 compared to 90.9 okay so it's ever so slightly a better guess than the average so it's clearly it's it's means something to to do that so that is the idea of uh bidding and grouping you could do it into more more variables you could of course combine this with other variables uh but that's that's the idea now we're moving on to clustering where first we're going to plot a map of California uh I'm very certain we are that is definitely doing nothing so I'm going to kill that and we'll do a scatter plot of we'll do the longitude on the X and the latitude on the Y and this is California right there it's shaped like that so there's are all the points we can make a k means model from longitude and latitude and get the Clusters so what are clusters well they're just groups and so clusters it's really like a label it's saying No this group is different than that group and it's different than that group if we from K learn. cluster import K means that's just one algorithm for clustering and it's important for this because it actually has a predict method you can learn a k means and then you can predict that uh on a different data set than you learned from a lot of algorithms uh clustering algorithms can't do that um but we get XT Trin lat long which is just train DF subat longitude latitude that's numpy we make a k means to be a k means with the number clusters you know I put seven but there's no reason I couldn't do three and then we we fit that with uh the input just the latitude and the longitude and we can get the labels K means. labels uncore and so this is saying the first one belong to this group and the second one belong to this group and the third one belong to this group the numbers have no meaning other than they are different groups than each other there's it's not like uh three group three is closer to group two uh than it is to group zero like it's just different values are different groups entirely now let's plot a colored map of California we'll do uh P plot. Express is PX my favorite Library px. scatter with the x is train DF sub longitude Y is train DF sub latitude and the color so this is so much easier to do this in plotly rather than map plot lip we make color equal to K means. labels under score so what that does is it colors here each of the different groups so here these are all the fours these are all the ones these are all the sixes these are all the the twos these are the fives okay so it makes this it makes this uh this grouping here and I forgot I hadn't run that again because we actually only made three groups and so here this is the these are the twos these are the ones these are zeros and so it grouped them according to these values here which is really really cool so you can do this uh clustering idea with you know other variables as well which won't produce as as cool visualizations as something like a latitude and a longitude uh but I just wanted to show you this so it really made sense what we were doing um and often by the way you might also want to do uh the same sort of scaling stuff before running a k means or some other clustering algorithms as well okay so now we're going to make X train clustering uh which uses just the one hot encoding from the cluster viel so how do we take this and make a model out of this well really these are categorical variables this is a two these are all twos these are all ones and these are all zeros so what we're going to do is that same sort of dummies thing uh where we don't have that many for this we have three columns and so all of these this is if it's zero this is if it's one this is if it's two these are the zeros these are the ones these are the twos so we do that same thing extreme clustering is pd. getet dummies on the that a series so that's just a way of transforming that into a column not even sure if I need that but um so we we get a a pandas data frame out of those labels and then we convert that to numpy so that this is that one hot encoding thing and I definitely call that one hot here when I'm looking at it at in a numpy format like this so that is how we make uh the Clusters turned into at least one way there's others um but one way to get uh turn clusters into meaningful descriptions so we'll now predict the clusters of the test data and create X test clustering using one hot encoding so we get X test latitude longitude or test DF which is test DF sub longitude latitude I don't know why it's kind of confusing how I said lat long and then long lat but um we get that to numpy which is just the longitude and the latitudes we get the clustering uh which is pd. get dummies of the series I should have organized this organized this a little better but basically we do cing St predict on the X test last long and then we do the series of that and then we do the get dummies on that so what gets the labels it turns it into a series it converts that into a data frame which is one hot and then we do tumpi to get the full shape is going to be that by three instead okay so now we'll check the error of a linear model that uses only the cluster one hot encodings how does a linear model do with just knowing that it's kind of organized like this is this even related to the median house value uh let's find out so we make a a model and it's got 80 88 so it's not bad uh but what if we were to increase the the cluster the amount of clusters what if we actually got more information out of saying uh maybe these ones versus these ones versus these ones so if we were to go back to that seven if we were to do seven here uh so train it plot it on the train and then uh make the matrices out of it and then we were to make another model we got way down to 75 okay so that's how important this information is is we can make this is just a linear model out of all just out of these different clusters of information we' learned so much about the median house value based off of this these areas and feel free to play around with that value uh of number of clusters if you want to so that really cool you of course you don't have to do geographical data U that's just a usually you won't uh but if you do have geographical data that can be uh a useful way to get better predictions out of it uh or at least different predictions there's it it we didn't even make a model that just used the vitude and longitude as is you could do that and maybe it did better I don't know this is just a cool clustering technique that will have its applications for many other variables okay so finally we're going to do feature selection which is just a combination of features so we'll observe the shapes of XT train clustering XT train scaled and dummy X train so all this is if if you didn't watch this part basically this is uh data that's from clustering this is data that's uh some variables scaled and this is uh the dummies from that'll actually be the dummies from the geographical information okay so what we can see here is clustering has the seven columns the scaled has those three columns and the dummies those are those four different uh categorical variables so we conat concatenate the training arrays side by side to make one big xra full input Matrix so we do the numpy do concatenate of X train clustering xra 3 scaled and ex string dummies across axis one to get this full thing so we can see these are 70,000 rows 70k rows 17K rows so is that and then we just kind of concatenate uh we just put these seven and then we put these three and then we put these four so you can see that the shape is like this so it's this is a First Column second column third column up until the first seven are this the next three are this and the next last four are these ones here okay so that is our full information here this is just one way why why am I doing this well this is just one way that we can combine multiple features uh and to make a model okay we could of course drop one of these columns at a different one I just picked some random stuff because that's what feature selection or combining features is about just play around with stuff figure out what works well and only use what you need to use okay so we get our full Matrix there I'm going to make sure yes that's already run we can look at the shapes of the testing information so it's the same thing but the tests and then the test as well this full testing array we check the eror of a random Forest this is the combination of these features we'll make this random Forest we'll fit it with that train full and compare that to White train we'll do test predictions where we predict the uh random Forest clustering that's not the right model guys so uh okay you're going to watch me fix this which is basically making a a random Forest we'll call that full this is going to be full based off of random Forest full and this is going to be full test predictions and yeah so it didn't actually matter what the name was but that was definitely not that was definitely not the right name and in total here we get a model that seems to be uh at 67k okay so that is easily better than anything we looked at so far uh it's better than our Baseline coverage it's better than uh just the clustering just the dummies uh so we combine all of this stuff together to make that and just for fun here let's even uh try to make an even better model you know what let's not use these features let's try and let's try and figure out uh some new ones so I'll do maybe just an initial one can we do better than 67,000 uh I bet you we can if we try and change those parameters and we get bum bum bum 60k okay so immediately uh you know this these are things you always have to do which is train the parameters of a model we could fit that a lot better as well but that's the idea so uh if you aren't subscribed I think you probably figured out right now that that's a good idea drop a like if you did get value from this I'd really appreciate it and yeah let me know what you want to see in the future I hope this one was a good one CU a lot of people are asking for it and so I I put in a lot of work to make sure that I got that out and I look forward to uploading this I I'm really happy if you to see it I hope it does well and yeah I'll see you later guys"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Feature Selection Methods: Filter, Wrapper, and Embedded Methods",
        "Video Title":"Filter vs Wrapper vs Embedded Methods Explained with Examples | Feature Selection Methods in ML",
        "URL":"https:\/\/www.youtube.com\/watch?v=Sc-TNxW3PiI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Sc-TNxW3PiI\/hqdefault.jpg",
        "ID":"Sc-TNxW3PiI",
        "Publish Time":"2024-01-26T18:11:53Z",
        "Channel":"Learn with Whiteboard",
        "Channel ID":"UCF-uUxo43IPpnEwnR62WGlA",
        "Transcript":"in this video we are going to understand the difference between filter methods vs wrapper methods vs embedded methods one by one exploring their objectives processes techniques advantages and disadvantages so let's get started firstly think of yourself as a picky eater navigating any buffet then you wouldn't just take everything on a plate right filter methods are just like this only focusing on specific traits of each feature is it spicy is it crunchy does it contain gluten or not and further you use statistical tests and measures to identify features that seem relevant based on their correlation with your target variable think chi square tests information gain and mutual dependence methods so to summarize here the objective is to have quick elimination of irrelevant features and in terms of process features are evaluated based on statistical measures or mathematical functions further when to use filter methods a when you have higher dimensional data with a. larger number of features b. the pre-processing step as it acts as a preliminary filter before diving into more intricate methods next wrapper methods which on the other hand are more like trial and error imagine building your plate one bit (or bite) at a time testing each feature and seeing how it affects your overall dining experience these methods train predictive models with different combinations of features and then choose the set that makes the tastiest model ideally choosing the one with the highest accuracy or lowest error think of methods like forward selection and recursive elimination and to summarize here the objective is to evaluate subsets of features as a group and in terms of process it employs predictive models to assess feature subsets further when to use wrapper methods a. when dealing with model specific optimization b. small to medium data sets next embedded methods blend the best of both worlds they're like having a star chef guide you through the menu highlighting hidden gems and subtly shaping your meal these methods build the model and perform feature selection simultaneously think Lasso regression, L1 regularization, and tree based methods they shrink the weight of irrelevant features or prune them all together resulting in a leaner meaner model here the objective is to incorporate feature selection within the model training process and in terms of process features are selected as the model learns during the training further when to use wrapper methods a. when dealing with integrated learning b. when dealing with large large data sets as it efficiently handles substantial amount of data during the model building process lastly let's compare the advantages and disadvantages of each feature selection method so starting with filter method in terms of advantages they are computationally cheaper as compared to others also they have the fastest running time with the ability of good generalization it is also easily scaled to high dimensional datasets in terms of disadvantages first of all no interaction with classification models can happen for feature selection and secondly it mostly ignores feature dependencies and considers each feature separately in case of univariate techniques next wrapper method in terms of advantages it interacts with the classifier for feature selection secondly more comprehensive search of feature set space can happen with it thirdly it considers feature dependencies and is offering better generalization than filter approach and in terms of disadvantages it surely has high computational cost alongside long running time it also poses higher risk of overfitting as compared to filter and embedded methods lastly it is computationally more unfeasible with increased number of features further in terms of embedded methods they're computationally less expensive as compared to wrapper methods they offer faster running time as compared to wrapper methods and interacts with the classification model for feature selection lastly it offers a lower risk of overfitting as compared to wrapper in terms of disadvantages the identification of a small set of features may be problematic in this method with that i hope this video was helpful to you and served value if you like my content feel free to smash that like button and if you haven't already subscribed to my channel please do as it keeps me motivated and helps me create more content like this for you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Feature Selection Methods: Filter, Wrapper, and Embedded Methods",
        "Video Title":"Feature Selection Wrapper and Embedded techniques |  Feature Selection Playlist",
        "URL":"https:\/\/www.youtube.com\/watch?v=za1aA9U4kbI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/za1aA9U4kbI\/hqdefault.jpg",
        "ID":"za1aA9U4kbI",
        "Publish Time":"2022-03-29T13:30:09Z",
        "Channel":"Unfold Data Science",
        "Channel ID":"UCh8IuVJvRdporrHi-I9H7Vw",
        "Transcript":"hey guys welcome to unfold data science my name is aman and i am a data scientist so this video is the continuation of my last video where i was talking about feature selection techniques in python okay so i had explained one method known as filter method in this video i am going to explain you and show the python demo of wrapper method and embedded method so all these are different different buckets of feature selection in one bucket there are different different techniques so we will be discussing what are there in wrapper techniques and what are there in embedded bucket of techniques right so let's start guys without any further delay now wrapper technique i gave some hint in the last video itself wrapper technique is basically a technique where you wrap features together okay wrap features together and try to find out which combination of feature is working good for you okay and what is embedded method method guys in embedded method you generally use machine learning models to choose your features here in embedded in wrapper also sometimes you use model but in embedded method you actually run a machine learning model you train the model and then you see which feature is making sense we will see the difference in a moment okay but first of all let's try to understand what does wrapper mean right so guys suppose you have let's say f1 okay f2 okay f3 okay f4 and f5 five features now you want to do a feature selection you don't know which feature is good for your model which feature is not good for your model so what the wrapper technique will do is it will create a combination of feature to see how the performance is coming out for example it can combine f1 f3 and f4 and try to fit a model and see how this combination is working okay or else it can f2 and f4 combination it can you know send to a model and see how the performance is coming out so different combinations based here i created a three feature combination here i created a two feature combination so how these combinations are created that is a critical part to understand okay so let me explain you guys how these combinations are created there are basically three ways three approaches okay one approach is known as forward selection approach okay other approach is known as backward selection approach okay and third approach is known as bi-directional which means it is forward and backward both okay in terms of feature selection how do you select the feature now if i talk of forward selection guys what happens in forward selection is one feature first the model will be fit on a null feature set which means there will not be any feature in the first iteration or first model you can take it as the base model in the next iteration one feature at a random will be taken f1 f2 f3 f4 f5 any one feature will be taken for simplicity let us assume that feature is f5 okay in the next iteration what will happen is so in in this iteration in second iteration only f5 will be run then f4 will be run f3 will be done f2 will be run and f1 will be run in the next iteration with f5 all the combination once f5 and f1 then f5 and f2 then f5 and f3 then f5 and f4 like that combinations will be done so let's say f5 f3 is run just for example and next iteration one more feature will be added so what you are seeing is in the forward direction in the forward direction the algorithm is adding one one feature and hence this method is known as forward selection or addition of features i will show you in python how it is working in a moment just to give you a theory of practice um you know theoretical explanation of how it is one feature it will start with empty blank zero features it will go to one feature then two features then three features and then to all the features okay or wherever the criteria is met remember guys this one feature when i say this one feature right this i have written here f5 but it will be run for f3 f4 f2 f1 all and whatever is best that will be picked west in terms of accuracy score or whatever criteria you give when i say f5 plus f3 here do not take it literally it will be tested on f5 plus f4 f5 plus f1 f5 plus plus f2 and so on and so forth whichever is best will be chosen and then in the next iteration one new feature will be try to fit with this okay so that is how it will work now try to understand how backward selection will work okay in what word selection what will happen is suppose we have five features okay so there will be let's say one model on f1 plus f2 plus f3 plus f4 okay one model will be fit on this one model will be fit on f1 plus f3 plus f4 plus f5 one model will be fit on f1 this time we will leave f4 out f2 plus f3 plus f5 okay and this time we will leave f5 out f1 plus 2 plus 3 plus 4 and in one iteration f1 only will be left out f2 plus f3 plus f4 plus f5 so how many models we are running here guys 5 models total this is your one iteration one iteration okay so how many models you are running five models out of these five models whichever the best comes in terms of result right that you take to the next for example let us say this goes to the next stage okay and then out of this again you remove one one feature for example one will be f2 plus f3 plus f5 other can be f1 plus f2 plus f3 and so on and so forth okay and at the moment you get a combination of three features which gives you the best score for example if you are looking for three features right then you say that f2 plus f3 plus f5 is the best combination of my features so in both these examples one thing you notice guys we are wrapping multiple features inside a kind of wrapping paper okay and then sending it to the model to decide which feature is working best for my model and hence whatever i explained you is called wrapper based technique okay and this bi-directional is a combination of forward and backward where model can drop the feature or add the feature based on the need okay i am going to show you all these in python and if we talk of embedded right in embedded it is actually training the model and while training the model finding out which feature is working best for you okay for example when you fit a random forest model or when you fit a ridge regression or when you fit elastic net right so you get some idea about which feature is working good which feature is more meaningful which feature is more valuable for you right based on that you choose your features now let's go to python guys and see some examples of how these things are implemented okay so first of all you have to install a package called called ml extender guys if you don't have some of you might face little issue in ml exchange try to do it with pip3 it will not be an issue than this and from ml exchange you have to import something known as feature selection and from feature selection you have to import something known as sequential feature selector you remember i was explaining now forward selection backward selection all those will happen through sequential feature selector okay so i'm importing as sfs and then you have to import one model i am putting here k nearest neighbors okay you can import any other model if you want to use that in this place okay and then one simple data iris data source and target number of neighbors four you can make it two three whatever and here you say i'm putting it twice no need to do this okay so sfs canon model i'm taking number how many features i want three features forward is equal to false forward false means i want to use a backward selection first let me show you the forward selection first okay make it true okay floating is false i will tell you in a moment what is floating scoring is accuracy and cross validation let me make it 0 to start with now let's run this if i run this guys you will see that the model what i am saying to the model is you can choose three features okay how many features i am telling model to choose three features okay i am saying forward is equal to true means do the forward propagation which means start with one feature then take the two feature combination and then find the three feature combination okay and scoring criteria i am taking accuracy cross validation i am not doing and once i do that and i plot the result in a pandas data frame this is very important guys see this here feature id is three featured so first it took one feature pickle with okay as i was showing you in the theory video one feature it took then it took one feature two feature combination okay so this entire thing is one iteration at reason number two the output is this petal length petal width iteration number three the output is this simple length petal length little width simple with brittle and petal width okay so it started with one feature that it went to two feature then inverted three features because i am saying give me three features now let me say give me two features in forward selection let's do this it will give you two features and best feature combination is feature number two and feature number three this is what your forward selection is starting with feature number two and then the model is saying you you know what petal width and pixel length are the best combination for your model okay and here i can change few parameters okay for example here i can make forward is equal to false which means it will go for backward um backward way backward method which means it will start with all the features then go with reducing the feature let's see this see here it is starting with three all three features okay all four features basically then go into three features combination of four features combination of three features combination of two features in the end you are asking for two features hence it is giving you two features this is your score okay and this is your this is your cross validation score this is your average score because i have made the cross validation as zero if i make it cross validation as two you will see two different scores in cross validation see here cross validation first time score second time score first time score second time score and this is your average score the the concept is same same thing is happening here okay now guys you need to understand the other parameters here right so as you saw cross validation is equal to two means i want to do the cross validation also and how many times two times suppose three then three cross validation if you don't know cross validation concept guys there is a video you can watch it the link is right here okay and scoring i am taking accuracy you can take any other scoring as well forward backward you understood k features means how many features you want as the output here i am saying to give me two features if i say give me three features it will give me three features see this it started from four and now it is giving me three features which three features these three features okay and this floating if you make this true right then what it will do is it will do a bi-directional collection of features it can go it will remove one feature it can add that back as well for example if you see like this here i am here there is only four features in this data set not much scope for addition and removal but if you do for a larger data set right and if you make floating is equal to true then it will give you you know combination of picture sometimes it will add sometimes it will remove so this is also one one option you can check and you can you know use it so we have covered the wrapper method uh forward selection backward selection and bi-directional selection okay you can test with some other data before that i want you to must have a look at this link here guys check this page i will i will upload this in in my google drive you can obviously take this link this link is nothing but ml extend where where from i have taken all these examples is that same page only okay similar extent feature selection there are some interesting things you can plot the feature you can visualize in better way um you can do cross validation you can run some other models many things you can do okay so this is one section of this what i wanted to cover forward backward and bidirectional you must have heard about something known as recursive feature elimination if not there is a video on unfold data science guys link is here this is an important concept you must watch now what is recursive feature elimination the cursive feature illumination is very similar to backward selection only one difference in recursive feature elimination the criteria to remove the features is coefficients of the model or from internally it is decided but here you are saying take the knn model and then decide you are passing your model okay but in recursive feature elimination it will happen automatically through the rfe or rfcv method in python okay so for example here if you can see i have imported the code of recursive feature illumination right so these are common things i have loaded the boston data and then i am saying that do the recursive feature elimination see here sql and this comes from scalar not from ml extern so sql on feature selects in rfe and rfe comes with other flavor known as rfe cv okay so recursive feature elimination you can run here i am running linear regression you can run any other model also okay and then if i run this right entire thing you will see that in boston data six features it's giving me as output number of features six so wherever you see one right it is telling me those features are making sense for me i should use those features okay and you can use other version of rfp called rfe cv and you can say cross validation you can how many times you want to do cross validation let's say two times you can write that here and it will run with a cross validation score okay so cross validation simple concept right it will hold out some set and test on that set so this is about recursive feature elimination very very similar to backward selection just a minor difference in how it chooses the variable to throw out okay so this is about recursive featuring messenger now i want to explain you something regarding uh embedded methods which is like using models to choose the feature so this is also easy to understand guys if you see i'm importing from sklearn lasso lasso is a degradation l1 l2 there is a video on for data science you can watch that okay and lasso that fit extra in y trend is nothing but from the boston data set and this is your prediction and then you see your errors right and coefficient estimate and in coefficient estimate obviously whenever you see a very very low value for example this one right you understand that this this particular variable is not very useful for you and you remove that variable that is why it is known as embedded embedded means you are training the actual model you are doing the actual task and deciding which feature is not useful for you this is one ridge method guys uh lasso method there are two more things one is called ridge regression and one is called elastic net this is an assignment for you to check that you must check that okay similar ways slight difference if you have confusion what l1 l2 video okay and one last thing in embedded method i want to cover here guys random forest to select feature very very simple you take random forest and then forest automatically gives you the feature importance right plot that feature importance and see which feature is least important for example here if you notice here chas is coming very low zn is coming low you know rad is coming low so all these features where importance is low right these features are kind of less important for you you can consider the removal of these features okay so what all we covered guys we understood the theory behind wrapper method forward backward bi-directional we did that in python we understood the theory behind rfe that i had explained before as well you can watch that video for reference also and then we saw some methods for embedded please explore ridge and elastic net that also is useful for feature selection and reduction take this file from the google drive try with your own data try all the methods filter wrapper embedded see what is working best for your users please give me a thumbs up guys if you like this video drop me all your comments see you all in the next video guys wherever you are stay safe and take"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Feature Selection Methods: Filter, Wrapper, and Embedded Methods",
        "Video Title":"Feature Selection | Wrapper | Filter | Embeded Intrinsic Method in Machine Learning by Mahesh Huddar",
        "URL":"https:\/\/www.youtube.com\/watch?v=GJnSBhxWEEg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/GJnSBhxWEEg\/hqdefault.jpg",
        "ID":"GJnSBhxWEEg",
        "Publish Time":"2022-12-14T16:40:16Z",
        "Channel":"Mahesh Huddar",
        "Channel ID":"UCPi23Ql765_5smMj2-r0X4g",
        "Transcript":"welcome back in this video I will discuss what is feature selection what are the different type of feature selection and what are the advantages and disadvantages of feature selection in machine learning before we go to the feature selection part first we will try to understand what is machine learning machine learning is a subfield of artificial intelligence that allows computers to learn without being explicitly programmed so in machine learning what we do is we will give data to the system and it will learn from that particular data and later the learn model can be used to classify or predict the future events usually machine learning model is built with the help of a data set a data set is usually represented in the form of a table with rows and columns the rows represents the examples and the columns represents the features or you can say that attributes or input variables in this example you can see here the last column is a price that is the we want to predict the price of that particular house here given some features the features are address number of rooms House age and owner given these particular features we want to predict what is the price of that particular house in this case now uh the very first question comes in front of us is so what is feature selection feature selection is a process of selecting most relevant subset of features from the original set of features by dropping the Redundant noisy and irrelevant features that is given a set of features sometimes what happens is that particular data set may contain some redundant features noisy features or irrelevant features so we need to remove all those particular things and we need to come up with the subset of features which are most relevant to that particular problem definition so this particular feature selection is used in many of the applications like object detection natural language processing remote sensing image retrieval and so on now the next question comes in front of us is why do we need to do feature selection as said earlier the given data set May contains a lot of redundant noisy and irrelevant features so what we need to do is we need to remove those particular things otherwise the trade model will become too weak as well as it may generate some misleading patterns over here toward that particular thing what we do is we will remove all those particular features from the uh feature set over here let us take an example to understand why do we need to do feature selection this is the data set I have already explained in the previous slide here we have four features and the price is what we want to predict now if you want to predict the price of that particular house we will try to use each of these particular features now let us say that the first feature is address the address does matter when you want to predict that particular house price of that particular house because uh the if the house is present in a remote area then the price will be less if it is present in the you can say that urban area or something like that the price will be more in that case the number of rooms does matter here because the number of if the less number of rooms are there the price will be less if more number of rooms are there the price will be definitely more in that case the age of that particular house also matters here because the new house will be having more price and the old house will be having the lower price in that case but coming back to this owner when we want to predict the price of that vertical house the this column or can say that this feature doesn't matter much here that is the owner of that particular house so what we need to do in this case is we need to remove this particular owner and we need to retain only these three features over here so once you remove that particular owner it will look something like this it contains only three features and the price is the what we want to predict in this particular case here now what are the different methods to do feature selection is uh mainly feature selection methods are divided into two parts one is called as the supervisor feature selection and the second one is unsupervised feature selection supervised feature selections are again divided into three groups filter methods wrapper methods and embedded methods in this case now we will try to understand each of these particular methods one by one in unsupervised feature selection method we use these particular methods on the top of unlabeled data here given some set of features what we do is we will try to assign some rate or the weight for that particular feature with the help of the entropy variance capacity to maintain the local similarity and so on using these particular things we will try to assign either rate or a weight to that particular feature the one which is having the more weight that will be selected and remaining will be removed from that particular feature set so this now coming back to supervisor feature selection method these methods are applied to the label data in this case so what we do is we will try to select those features which will maximize the performance of the supervisor model now uh this particular supervisor feature selection methods are divided into three groups as I said earlier the first one is a wrapper method here so in wrapper method what actually happens is we will be given some set of features from this particular set of features we will generate the subsets over here subset of features you can see now we will give that particular subset of features to the machine learning model the same thing will be repeated again and again for all subset of features here and finally the one set of feature which will give you the maximum performance that will be considered as a final feature set over here now the question is how to generate this particular subset of features to generate subset of features there are two methods are there either we can use forward wrapper methods or we can use backward wrapper methods over here in the forward wrapper method we will start with the empty set and then we will go on adding one feature at a time so that we can measure the performance of machine learning model so we will start with the empty set we will add one feature to that one and then we will create a model similarly we will add one more feature and then we will create another model and we will go on monitoring that particular performance of the model the one model which will give you the maximum performance that will be considered as the final feature set over here in backward wrapper methods it is opposite to the forward wrapper method because in this case we will start with all features in the first case now what we do is we will go on removing one feature at a time and then we will try to monitor the performance of the supervisor machine learning model the one which will give you the maximum performance that will be considered for uh considered as the final feature set over here so these are the two methods we use to generate the subset of features in this case now coming back to the next method that is the filter methods in filter method what we do is given some set of features we will try to identify the correlation of that particular feature let us say that X is one feature we will try to identify the correlation of feature x with respect to Target variable here now uh if it is correlated with the target variable we will retain it otherwise we will remove that particular thing now the next question comes in front of us is how to find the correlation between a variable X and the target here so there are a lot of statistical tools are available like chi-square test Information Gain Fisher score PSN correlation Anova and so on all this particular statistical tools are available with the help of this we will try to identify the correlation of a input variable with respect to Target if it is highly correlated we will retain it otherwise we will remove that particular feature from the feature set over here the last feature selection method is intrinsic or embedded methods there are some machine learning algorithms of which how this particular feature for example decision tree in this entry while building the tree what we do is we will start with the feature which is having the most Information Gain and then we will start building the tree from there onwards when it comes to the second level from the remaining features we will try to select the one feature which is having more information gain and so on so what we do here is rather than considering all features we will consider only that feature which is having the importance over there and then we will start building the machine learning model over there so this is the third set of features selection method over here now the last part of this particular discussion is what are the advantages and disadvantages of feature selection method there are a lot of advantages are there with respect to the feature selection the very first Advantage is it will reduce the overfitting because we are not using all the features over here the features with the redundancy noisy and irrelevant features were removed here so definitely we will not get the model into overfitting case or something like that second one is for sure it will improve the accuracy of the model because we have removed the misleading and unimportant data from the given data set and it will reduce the total time required to train that particular model because if you have a more number of features then it will take lot of time to train that particular machine learning model but in this case we have removed the irrelevant features so because of that definitely the training time will be less year now coming back to the next one that is the disadvantages definitely we will use this particular model to remove the irrelevant data but if you have a data set with very high dimensional case for example let us say that some thousands of features are there in such cases if we try to use these kind of methods they will take lot of time to get the better feature set over here so that is the one disadvantage what we will get when it comes to feature selection methods over here in the next video I will discuss how to implement feature selection method in Python with a simple data set I hope this particular concept is clear if you like the video do like and share with your friends press the Subscribe button for more videos press the Bell icon for regular updates thank you for watching"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Feature Selection Methods: Filter, Wrapper, and Embedded Methods",
        "Video Title":"Types of Feature Selection Methods &amp; their Working || Overview of Filter, Wrapper &amp; Embedded Methods",
        "URL":"https:\/\/www.youtube.com\/watch?v=iRjx63W4Sg8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/iRjx63W4Sg8\/hqdefault.jpg",
        "ID":"iRjx63W4Sg8",
        "Publish Time":"2020-12-23T09:35:15Z",
        "Channel":"EmergingTech with Dr Javed",
        "Channel ID":"UCmCC4h9S87GVLYjJora3mXw",
        "Transcript":"asalam alikum and very good day my name is Dr jav ibal welcome to my new video on types of future selection methods and their work this video is part of my lecture series on machine learning for more videos on machine learning you may subscribe my channel before this I have created a video on overview of feature selection and dimensionality deduction method in machine learning if you have not watched that video you may watch on my YouTube channel or the link given in the description this slide shows three types of feature selection method which are used in a machine learning the name of those of three feature selection methods are filter methods rapper methods and embedded method first we will look into what is filter method this is the flow diagram of filter methods of each selection we start with our data set with all features then we select best subset of features using Pon correlation linear discriminant analysis Anova or kcare and then we apply any machine learning algorithm and finally the performance of the model is evaluated filter methods measure relevance of features by correlation with the target class this method of feature selection is faster the next type of feature selection is rapper method in rapper method we start a data set with all features then generate subset of features features are seared through backward feature elimination forward elimination uh feature ranking are different type of method then the machine learning algorithm is applied based on the inference from the learning algorithm a different set of feature subset is generated so this process continues with different subset of features and finally we select the features that have better performance so here we add our remove features in the model based on the inferences from the previous model the next method is embedded method in embedded method uh we start with all features then we generate subset of features next apply learning algorithm and the performance of model is evaluated if the performance of the model is not set satisfied we again regenerate a subset of featur and this process continues in iterative fashion the embedded methods are Fusion of filter and rapper method it adds L1 regression and L2 regression to add in order to evaluate the performance of the model so basically we have three type of feature selection methods and each method has its own advantages and disadvantages the ultimate objective here is to select a subset of features that significantly enhance the performance of the computer with less number of feature this slide shows my recommendation on feature selection here we start with the data set of all features we generate a subset of features then we apply learning algorithm and after learning algorithm we evaluate the performance of the model if the performance is not satisfactory we repeat this process we again regenerate a subset of features apply learning algorithm evaluate performance so whenever the performance of the model is satisfactory we keep the best performance results so most of the time in our experiments we apply featur selection in such a manner this slide shows the working of feature selection and feature subset selection through animation we start with data set of all features the features may be huge in number here the feature set have 11 features after that we pass the original data to the feature selection method feature selection applies here a technique to select a subset of feature so in our scenario we have selected top five features those five selected features are then passed to the learning algorithm and based on those selected features the algorithm makes the decision this scenario is about the classific problem I have considered three classes in the experiment positive negative and neutral So based on selected features the performance of algorithm may be evaluated finally in this session I have discussed different type of feature selection method the name of feature selection methods are filter method rapper method embedded method and finally I have shown working of feuture selection through any animation I hope this video will help you to understand the working of different types of feature selection method you have to choose the right type of feature selection in order to select a subset of features thank you very much for watching this video for more videos please stay tuned"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Implementing Feature Selection with Scikit-Learn",
        "Video Title":"Feature Selection for Scikit Learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=wjKvyk8xStg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/wjKvyk8xStg\/hqdefault.jpg",
        "ID":"wjKvyk8xStg",
        "Publish Time":"2017-06-25T18:52:11Z",
        "Channel":"Data Talks",
        "Channel ID":"UCQTQ0AbOupKNxKKY-_x46OQ",
        "Transcript":"Nathaniel here and welcome to a bit of data science and scikit-learn where we learn just a little bit of data science and a whole lot of secular today we're gonna be talking about feature selection so I'm not going to be introducing you to any more models instead we're gonna be building on top of these models one of the common things that you want to do with models is you'll have a model I will have data the data have lots of features you know the medium income of people in this area medium college degree that people get blah blah blah you wanna figure out which features are most important and only include the features that are most important into your model it's called feature selection it's very important so let's just jump right in so again we saw last time this method called transform we're gonna see it again and I want to give you a little bit of intuition behind what it's doing mm-hmm so I'll start off by talking about the variance threshold this is kind of like the most simplistic way you can do feature selection we'll go ahead we'll get an x value here which is just some binary stuff we'll make a variance threshold and we'll fit our variance threshold just on the X data and then we'll transform okay so what does this mean well we see we had three features to begin with each one of these little arrays had store 3 things and it doesn't really know what they are and we end up with two the idea is that the features that had variants below this threshold right here were dropped and all the ones that were above were kept and the important thing here is that it only uses the training data to figure this out so you'll look at your training data you look at highly variant ones and your training data and you'll keep those and you've been feeding your test data those are you know like basically constant functions you'll still have to keep them because remember the game is that you're giving some training data you have to learn on that and then after you're given the train data you take an algorithm and you hand it to your friend and then your friend tries to break that an algorithm with the test today yeah well your friend feeds the test data into this algorithm and has to has to act based on that the algorithm can't update itself you're doing online learning but that's for another time so that's why that's why we have a fit method different method doesn't need to take a y-value because it learns in an unsupervised way it just it just looks at the X data looks at the factors knees which ones have the most variance so this is kind of like our first example of a of an unsupervised learning technique we transform you know it could be called predict predicted would be a fine name for it but transform here is is to really mean that this is not a prediction these are just the original x-values they use just a transformation of the original factors in this case just a reduction there's this quick shorthand which is called fit transform that doesn't both at the same time okay so that's one way to do that there's a couple more ways that we can do we can do univariate feature selection which we'll go into we can do recursive feature elimination and then we can do feature selection using select from the model so these are the three feature selection techniques now I'm going to go through and show you how they're used in secular so with univariate feature selection there's a couple things that we can do here you can select a best you can select percentile you've got a couple of like selects false positives for a it selects false discovery rate this to this type of stuff this just super built-in and you've got something that's incredibly generic I'll give you some intuition on on select K best and that should be able to prompt discovery of what these other things did let's check it out so sliced K bass takes two parameters it takes a K and a scoring function and the scoring function is just it's however it's like how good is this feature that's basically the idea behind the scoring function how good is this feature and I mean it's how good is this feature in the model is perhaps the best idea here and the idea is that we'll just select two features based on how well they do on any sort of test and we can choose the chi-squared test you could choose a mutual information test you can choose and F classification so just an f-1 ratio or something like that um we'll just do a chi-squared test go ahead and fit let me just show you now if you go ahead and transform your data based on this select KK best you'll end up with two features again because we were just electing - now you can use it's like percentile adjust in the same way and generic univariate select just allows you to do different types of selection techniques you could say the worst and the best if you wanted to yeah each of these features has a score which are down here the next thing which I think is a little bit more interesting the last one or save the best for last it's called recursive future elimination okay strucker's they feature elimination so let me just show you what this looks like so it takes an estimator it takes a step and it takes a cross validation method and it also takes a scoring so what this recursive feature elimination needs two halves it needs to have some sort of model that has a future importance whether that's any coefficient or whether that is a future importance given to you by random forests it needs that in order to operate what it will then do is it will recursively eliminate features until it reaches the best cross-validation score so we'll start off with all the features and we'll recursively eliminate the worst it will refit the model restore the model see how well it does and if it does better well we'll continue to reduce if it does worse then we'll stop and we can just sure to show you what this looks like so again this this is a little bit different in that this returns you a model and so you can use use the small to predict so it's always a little bit but you should you should just be able to figure this out so you can use this to predict this pretty well we can actually why don't we just go ahead and score this see how well we do score X fly pretty good 99% and using only two features but again we're also using random forests which are powerhouses okay and then finally we have feature selection using select so again let's go ahead and meet these docs here so an estimator it can be both fitted and pre fit which is somewhat interesting a threshold features whose importance are greater than equal to our cat while others discarded this is kind of the idea here and pre fit whether whether the model has been pre fit so what you could do is you could go ahead and train a train a model like this lasso cross validation so lasso is linear regression with l1 penalisation so you go ahead and pre train that and then out once you've trained it you can go ahead and select the features that are most important or you don't have to so we're just going to go ahead and select from model I believe what was the let me check this year the initial threshold none median looks like something something like this so 1 times 10 to the negative 5 or something like that so that's our initial threshold so we'll go ahead and we'll go ahead and select from model and we'll check out the shape so our our iris dataset had a lot of stuff here so it had 4 features that reduced it down to 3 down here we'll go ahead and use lots of cv well select from model will do the transform blah blah blah let's check out what happens here so again we start off with 13 and then we drop down to 10 features and in the way that we've selected them okay so I hope this has been useful future selection is done very very commonly these ways are generally the most accepted ways to do it the thing is that I I use the most I love univariate future selections super easy super easy to wrap your head around recursive feature elimination is fine as long as your hull doesn't take too long to Train and selecting from model is probably the one I use most often save the best for last ok well I hope you enjoyed and I'll see you next time"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Implementing Feature Selection with Scikit-Learn",
        "Video Title":"#89: Scikit-learn 86:Supervised Learning 64: Feature selection",
        "URL":"https:\/\/www.youtube.com\/watch?v=bm3fQ7g-EPM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/bm3fQ7g-EPM\/hqdefault.jpg",
        "ID":"bm3fQ7g-EPM",
        "Publish Time":"2021-05-19T09:51:44Z",
        "Channel":"learndataa",
        "Channel ID":"UCzUpHQAfj3ulmJkTAfBtXLw",
        "Transcript":"hello and welcome to learn data it's great to have you on this channel i'm nation in this video we'll look at the implementation of feature selection methods that we discussed in previous video so briefly uh these were the methods that we looked at from the different sets so we have variance univariate based on your variate statistics recursive select from model and sequential feature selection so we'll try to uh work on at least one of these from this flowchart so let's get into jupyter notebook and uh start coding here i've imported these libraries so we have numpy uh matlab uh we probably don't need we could try plotting uh some of the feature importances as see how they look so matplotlib and then we have all these others including the new feature underscore selection so let's run this and these are the versions that i'm working on and for data we'll create uh iris dataset later on but for now for some of the examples we'll use this data set and for this i've already calculated the variance so let's say the column one so let's say this column one is feature one this feature two feature three and then you have these that are on the variances so we can set the thresholding based on the variances to drop the features so let's do that in the very first first code remove features with low variance and for this we can create a variable select sel feature underscore selection dot variance threshold and threshold is equal to because we know that 0.16 is the smallest lowest threshold and here we can see that most of the values are just two there's just one one there so we can say that if values are less than 0.17 then drop all those values and here then we can say select scl dot fit fit to x array and then scl dot transform x so now as we can see from the original array we have dropped the first column of twos and we just have these remaining two columns so that's uh you and low variance and now let's look at the univariate uh feature selection so univariate feature selection and for this one uh first we'll look at select k best select k best and this would be for classification task so maybe one more okay here we use because we are using from the classif or classification uh we can select we need to select the proper scoring method and for that we'll use uh f underscore classif so feature underscore selection dot f underscore class if and then selection is equal sel is equal to feature underscore selection dot select k best and score underscore function is equal to f k is equal to 2 and k is equal to 2 means that we need two features in the output after they are selected and to check what are the input parameters for this we can always look in the help menu and here we can see this is the score function and k is the number we can provide to select how many features and it calculates the scores and calculates the p values as we saw in the previous video so let's uh continue and here then we can fit this so scl.fit x and y so we'll use the same x and y that we have used earlier and then perform this scl dot transform x and so uh in this case now the [Music] uh for x let's run x so this is our original data and from this we dropped the last column uh based on the score that was found in this uh method if you look at acl dot score i think okay let's let's see what how to get the score from this so run this get there and in the help menu scores so it says a score with s so if we go back sel dot scores underscore so here we can see that the lowest score was for the third column and that's why that column was dropped and the other two columns are there still there in the output and before we move further let's changed this a little bit now we'll try to look at the regression so select k best uh and this is for regression we could just convert the y values to float and see how that works out so for uh this what we need is f is equal to feature underscore selection dot so for selection uh if you remember we can use uh the regression so f underscore regression and the rest of the code should be the same so i'm going to copy that paste here so then we have sel is equal to feature underscore selection select kbs score function is that we have k2 then select fit on x and transform x so again in this case uh we get the same output uh so here we could change this as type load 32 and so we get uh the same result so here you could try using some different types of continuous data and see or what type of output you get next let's look at sequential feature uh selection so for that we'll use the data which is the iris dataset and we look at the classification so data and for this we will going to use x comma y is equal to data sets dot load underscore iris and return x underscore y is equal to true so here we have four features so if we look at x dot shape we have four features and we'll try to see if we can eliminate unimportant features from these four features so the first item we look at sequential feature selection and we'll look at forward selection and for this we create the classifier that we are going to use so clf is equal to linear underscore model dot rich classifier and then scl is equal to feature underscore selection sequential feature sequential features selector and clf and underscore feat features features to select so how many features do we need in the output we have four total so let's say we need three features in the output and then we can fit this sel dot fit x and y so here our y is of ones zeros and two so there are three classes and x would be floats so yeah x are the continuous values right there so let's delete that and then so when we run this so that's the fit is done now we can look at scl.get underscore params and within this you can get all the information that was within this particular model and to get specific information about the features we can look at acl get underscore support and here we can see the these three features were kept and the last feature was the first feature was dropped so if we look at x sorry if we look at x that's how the x looks so we have 5.1 3.5 and so on so let's keep maybe zero to first three rows of that so that's what we have now if we transform it so scl dot transform transform x and again if we look at just the first three rows then we can see that uh last three were true so those would be these three and those three are in the output after the transform is applied and the first column was dropped so that's how the forward selection process works and i in if we look at the help menu for this so if we if we go back here and if i copy this for selector and we can specify the direction so in the parameters here the default direction is forward so what we can do is copy this and if we were to perform a backward selection uh how would uh what features would it select would that select if you type backward then run this again run this let now we don't need to run this so let me close that and so we got that we have these so now it says backward and and here again we get the same out same output so the first column is dropped the last three columns are retained in the output now after this we look look at select from model so let's do that [Music] one select model and for this one okay so for this we'll use the same iris data set we'll create a function uh will create we'll initialize the variable f with this classifier svm dot linear svc c capital c is equal to point zero 0.01 penalty is equal to l one and dual is equal to false and then use that in the select from model so sclf is equal to feature underscore selection dot select from model capital s f and m uh then we have that there and then we can fit this clf dot fit x comma y and cliff dot transform x so let's look at the shape so as you can see we have just three features in the output and if we were to check what column was eliminated we can do that so let's look at the first three rows of the original data which is that now if we use the transform feature and look at again the first three rows and all the columns we see that in this case the last column was dropped and the first column is there so as in the previous uh method we saw that the first column was dropped in this case the last column was dropped now let's look at a pipeline and a pipeline we'll use the same code that we have typed above so we'll make it easier which is okay let me copy this uh just this first two lines and we'll use that in this code right here so clf is equal to pipeline with a capital p and then here within a list we can specify the tuples feature underscore selection and here i'm going to copy so this entire thing we could also directly paste it here and then copy all of this and then paste that in here so we have that particular step in the pipeline done the next one is the actual classification task so classy and for this we'll use the rich classifier linear underscore model dot ridge classifier and now the pipeline is done we can now perform the fit clf dot fit x comma y and here we we get a warning increase the number of iterations so you could try that max underscore outer is equal to one e3 maybe one e4 okay so this is uh the fit that we have done now we can look at what information is there we can pull information from the pipeline as well so for example you can check the coefficients of the features within the pipeline for that you type if you type clf.get underscore params it will give you the entire dictionary with all the key value pairs right here so for what we are going to do is we are going to use the feature selection so for this i'll type feature underscore selection and then when you run this you get these and then from here we can look at estimator underscore c underscore dot coeff underscore and so these are the coefficients and we can also look at the support values so clf feature underscore selection and dot get underscore support and here we can see that the first three features were kept and the last feature was dropped and we can also check that by running in the transform clf feature underscore selection dot transform x and let's look at just the first five first three rows so here as you can see in the x we have x 0 to 3 x we have these columns and the transform matrix we have just the first three so the last column was dropped which is denoted here by the false now with this what we can do next is we let's try plotting the future importance on on the same data set now let's try to plot future importance plot and for for this we'll use this time we'll use our trees so extra trees classifier so we'll import the library from sk learn import ensemble okay so once we have that we can now type tlf is equal to ensembl dot extra trees classifier and n estimators let's say we have maybe 10 trees and we can drive clf dot fit x comma y and then we can get clf dot clf we can type that in a variable so maybe feature feature importance is equal to clf dot teacher underscore importances with s fea underscore imp so those are the future importances now before we create the plot what we'll do is we need the feature name so i'm going to load the dataset again uh data sets dot load underscore iris and from this we can now get the data dot feature underscore names and so we have those names so we can use them and for the plot then we have feature importance in this video so we'll type plt dot bar data dot feature underscore names comma fva underscore imp and let's run this so that's the plot we can change the x rotate the x takes so x takes rotation is equal to 45 and we can also add a label to y axis y label and this is feature importance so that's uh the future importance we get using the extra trees classifier so that was it for this video i hope in this video you got an hands-on experience on how to use different methods uh that are available in scikit-learn for performing feature selection on a data set these were the examples that we tried on the toy datasets but i encourage you to try these on some different types of other real-world data sets and see which type of method works on what type of problem because two different methods can give different types of features in the output after selection process is completed so the criteria for choosing of the method for performing the serial feature selection is also important uh in the future videos will combine the material from this information from these videos and also from the ensembl methods uh to work on a real world project where we will try to predict some interesting uh prediction values so until then please like share and subscribe i hope to see you all in the next video until then thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Implementing Feature Selection with Scikit-Learn",
        "Video Title":"Optimize Your Machine Leaning Models: Feature Selection Techniques with scikit-learn in 5 Minutes",
        "URL":"https:\/\/www.youtube.com\/watch?v=T19pJev9BPg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/T19pJev9BPg\/hqdefault.jpg",
        "ID":"T19pJev9BPg",
        "Publish Time":"2024-06-13T12:00:34Z",
        "Channel":"TechInFive",
        "Channel ID":"UC70A0M1tWqMEbiQPRbe_Omg",
        "Transcript":"[Music] Turk in five python in practice master real world projects in 5 minutes ever wondered what feature selection is and how to effectively apply it in your machine learning projects welcome to Tekken 5 today we're diving into an essential Topic in machine learning feature selection selecting the right features not only improves the accuracy of your models but also makes them more efficient and easier to interpret features selection also known as variable selection or attribute selection is a process used in machine learning to select the most relevant features from your data this means we filter out any unnecessary or redundant data points that don't contribute much or might even decrease the accuracy of our models by reducing the number of input variables we not only speed up the training process but also enhance the performance of the model this simplification helps in avoiding the curse of dimension ity improves model interpretability and often leads to a better understanding of the data's underlying patterns moreover feature selection can significantly impact your model's performance a model trained with carefully selected features can outperform a model using all available data especially when dealing with high-dimensional data sets it reduces overfitting making the model more generalizable to new unseen data moving on let's explore the different techniques of feature selection these can be broadly categorized into three main types filter methods wrapper methods and embedded methods each has its unique approach and application scenarios filter methods are based on statistical tests to evaluate the relationship of each feature with the target variable common examples include using correlation coefficients Ki squared tests and Information Gain rapper methods involve selecting features as part of a search problem where different combinations are prepared evaluated and compared with a model techniques like recursive feature elimination and forward feature selection fall under this category embedded methods perform feature selection as part of the model training process and are usually specific to given learning algorithms examples include lasso and ridge regression which incorporate penalties on feature coefficients in summary filter methods are ideal for quick analysis with limited resources wrapper methods offer the best performance at a higher computational cost embedded methods supported natively by certain algorithms strike a balance between performance and efficiency let's now turn our Focus to how we can Implement feature selection using psyit learn a powerful python library for machine learning psyit learn offers several modules specifically designed for feature selection which can greatly simplify and to automate the process today we'll focus on two popular methods select K best and select from model these tools allow us to efficiently select the most relevant features based on statistical tests or model performance first up is Select K best which selects features according to the K highest scores from a given statistical test based on a specific criterian like the Ki squared test which is ideal for categorical data will apply the kai squared test to select the top two features from the iris data set this method is especially useful when you need a straightforward way to identify the most impactful features rapidly next let's look at select from model which selects features based on the importance weights of any estimator such as a linear model or tree based model in this case we'll use a random Forest regressor appropriate for the continuous Target in the California Housing data set select from model is particularly effective when working with models that inherently provide feature importance scores allowing you to retain only those features deemed most significant by the model itself as we wrap up let's quickly recap what we've covered today we've explored how to use psyit learns select kvest and select from model for Effective feature selection in the machine learning projects feature selection is a powerful tool to enhance your model's performance and efficiency by applying these techniques you can improve training times increase model accuracy and make your models easier to interpret stay tuned for our next video where we'll dive deeper into more advanced machine learning techniques until then keep experimenting and exploring the possibilities with psyit learn thank you for watching and happy learning stay tuned for more videos on Tekken 5 [Music]"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Feature Engineering and Preprocessing",
        "Topic":"Implementing Feature Selection with Scikit-Learn",
        "Video Title":"Advanced Predictive Techniques with Scikit-Learn &amp; TensorFlow\u2013Feature Selection Method|packtpub.com",
        "URL":"https:\/\/www.youtube.com\/watch?v=fnupA1nWrHM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/fnupA1nWrHM\/hqdefault.jpg",
        "ID":"fnupA1nWrHM",
        "Publish Time":"2017-12-07T11:50:37Z",
        "Channel":"Packt ",
        "Channel ID":"UC3VydBGBl132baPCLeDspMQ",
        "Transcript":"welcome to this new section in this advanced predictive analytics course here we will be talking about features these are techniques that will allow us to improve our predictive analytics models in two ways we can improve our models in terms of performance of the performance metrics of our models or in we can view this as improving in predictive power and at the second way in which we can improve our models is in understanding we can get a better understanding of the relationship between the features and the target the variable that we are trying to predict so let's see what we are going to do in this section in the first video we will talk about feature selection methods and these are methods for selecting features that are likely to help with prediction in the second video we will talk about dimensionality reduction dimensionality reduction are techniques to reduce the number of features that are used in the model sometimes we can simplify our model by including less features and dimensionality reduction techniques are are the ones that are used for that purpose the most widely use the mention allottee reduction technique is principal component analysis or PCA in in the second video we will do an example of that in the third video we will do what is known as feature feature engineering and we will we will be creating new features from the existing ones that we have in our data set and finally in the last video we will try to improve our models with feature engineering and now we can improve the model in two ways we can get better performance better predictive power of our model or we can get more understanding we can gain insight about our data set in the relationship between features and target so these are very useful techniques for building and understanding better your predictive analytics models ok this is the first video of this section feature selection methods this is the plan for this video first we will talk about feature selection then we will discuss the three methods for feature selection the first one is very simple it works with tummy features and we can identify and remove dummy features with low variance the second method is identifying important features by performing statistical tests and the last one that we will discuss is a recursive feature elimination okay so when building predictive analytics models most of the times some features won't be related with the target so they by definition can't help in prediction now the problem is that including irrelevant features in the model can introduce noise and add bias to the model so feature selection techniques are a set of techniques to select features that are most relevant and useful for our model in the sense that they will help either with prediction or with understanding of our model okay so the first one the first technique of feature selection that we will talk about is removing dummy features with low variance so as you know the only transformation that we have been applying so far to our features is to transform the categorical features using the one hat encoding technique so if we take one categorical feature and use the 100 kala technique we get a set of dummy features we we get a set of tommy features and then we can examine if these features have variability or maybe they don't have our ability so features with very low variance are likely to have little impact on prediction now why is that imagine that you have a data set where you have a gender feature and that's a 98% of the observations correspond to just one general just to female so it is very likely that these column this this feature won't won't have any impact on on prediction because all of the cases are just of one of a single category so there is not enough variability so these these case has become like candidates for elimination and in such features should be examined more carefully now you can take the following approach you can remove all the features that are either 0 or 1 in more than X percent of the samples or what what you can do is to establish a minimum threshold for the variance of sorts of such features now the variance of such features can be obtained with this formula P times 1 minus P where P is the number or the proportion of one's in your in your dummy features we will see how this works in the notebook the second way is to identify important features statistically and in these methods is to make use of some statistical tests for identifying and selecting relevant features so for example for classification tasks we can use an ANOVA F statistic to evaluate the relationship between numerical features and the target which will be a categorical feature because this is an example of a classic classification task or to evaluate the statistical relationship between a categorical feature in the target we will use the chi-square test to evaluate such relationship and in scikit-learn we can use the Select K best object and we will see how to use that object in the notebook okay the third approach for identifying important features and removing the ones that we don't that that we think are not important for our model is called recursive feature elimination or our fe so our fe is something and that also can be applied in scikit-learn and we can use this technique with models that calculate coefficients like linear or logistic regression all models that calculate something that is called feature importance as we saw in the first video of this course and like random forests provides us with these feature importance metric so for models that don't calculate these things either coefficients or future importance s this this method cannot be used for example for KN models you cannot use this recursive feature elimination because this works by first the first step in using this technique is to predefined a number of features that you want to use in in in your model now after you have decided how many features you want to include this method fits the model using all features and then based on the coefficients or based on the future importances the least important feature are eliminated and then this procedure is recursively repeated on select on the selected set of features until the desired number of features to select is eventually reached okay so now we will also perform an example of this in the jupiter notebook and of course there are other many other methods you know to select important features in in your models like l1 feature selection threshold methods 3 based methods and all of these you can do in cycle earned but we will stick with 3 first that we discussed and now let's go to the jupiter notebook to see how we actually apply these methods in scikit-learn ok here we are in the jupiter notebook these are the objects that we are going to import including the traditional libraries and here we will use both well we will use first these credit-card default data set and in this in this L we are applying the traditional transformations that we do to the raw data in here I am distinguishing between the dummy features that we have in our data set and the numerical features because we will apply different methods depending on the class off of features that we have okay here we apply the scaling operation for future modeling and the first method that we talked about in the presentation was removing dummy features with low variance so now we can get the variances from our our features using the VAR method and then let's see the variance is only for the dummy features now as I told you in the presentation you can choose for example a threshold for the variance so you can say for example that you will consider only the dummy features that have a variance over 0.1 in that case with such threshold of 0.1 the two candidates for elimination are they 5 and and pay 6 in this case okay so this would be the the first approach for removing in unnecessary features features with dummy features with low variance okay now the second approach that we talked about is statistically selecting the features that are related with the target and we have two cases this is a classification task so we have two cases we have categorical features which would be the dummy features and we have the numerical features now first let's let's perform the statistical tests in the dominant features so for performing this statistical test we are going to import this object the Chi 2 squared from the feature selection soft module in scikit-learn we will also use the select K best which is the object that we will use to perform this statistical tests in all of the dummy features okay so we instantiate this object called dummy selector we pass the statistical test that we want to apply and Here I am I am passing this argument as K as all because I I want this statistical test to be applied to or all the dummy features after Stan she ate in this object I run the fifth defeat method and here I have the the chi-square scores and these numbers doesn't make a lot of sense but the thing that you have to know about this is that this isn't a statistical test and the larger the number the stronger the relationship between the feature in the target now if you remember your statistics class this is a hypothesis testing setting so we can calculate now also calculate the p-values and we can remove we can say that the features where the p-value is greater than say 0.05 we can say that those features are not related with the target now in this case as you can see we get very small p-values for all the features so this statistical test is telling us that actually there is a relationship between the target in all of the dummy features so under this methodology we shouldn't eliminate any of these dummy features now for evaluating the relationship between a numerical features in the target we can use another statistical test called the F class if n we use again this this this object we pass the statistical test that we want we pass the number of features that we want to select in this case I want the test to be applied to all numerical features and then I am using the fit method again with the numerical features and the target and these are the p-values that I get from this application of this statistical test PFS a physical test and then I can select which of the numerical features have a p-value greater than 0.05 which is the usual threshold for statistical test and the results here are that these three features amount for bill amount five and bill amount six these three features are likely be irrelevant or to not be related with the target okay so we have here these three candidates for elimination okay now if you want you can eliminate these three features or you can apply some of the techniques that we will see in the individual videos of this section in order to to deal with these with these features okay so this was the second technique that we discussed in the presentation and this is the recursive feature elimination the third technique in for this example we will use the random forest a classifier model remember that we have here twenty five features okay so let's say that we want to select only twelve features let's say that we want a model that uses only twelve features so we we are using about half of the features then we can use this object from psyche learn our Fe from feature selection module we can use this to actually select these twelve features using the recursive feature elimination technique okay so we instantiate this object by passing which is the estimator that we want to prefer that we want to use and the number of features that we want to select now remember that the random forest provides us with a metric of feature importance so we can actually use this model with the with this technique of recursive feature elimination okay after using the feet method now on the whole dataset what we get is the selector support so we get true for the features that included in our model the twelve that we wanted and we get false for the ones that should be eliminated so according to this according to this object into this method the twelve most important features that we should include in our random forest model in order to predict the target or this limit violence HP all the will amount in pay one two and three pay amount one two and three and the features that should be eliminated according to this method are these because they are not very relevant according to this method and into this model to predict the target so now now what we can do is to evaluate you know the simpler model the one with the twelve features with the phone model that the model that we have been using so far so after you know evaluating these after feeding and evaluating these models we can calculate the metrics using cross-validation so we are here in this in this example we are using 10-fold cross-validation to get an estimation of the performance of these two models okay the one with remember this electron model is the one with the twelve features according to the recursive feature elimination technique and this is the full model the model with the 25 features and these are the results look that the the the full model has a recall of 0.36 and the model which includes only 12 features has a recall of 0.35 now it is less here so so in terms of recall the best model is is still the phone model but notice that only using half of the features will give us almost the same the same performance so we get about the same performance as you can see the values are really really close now you can decide as the analyst if you want if you want to use the full model or if you want to use all the simpler model that is up to you and in terms of accuracy as you can see we get almost the same accuracy still a little bit more accuracy with the full model but now you have a technique to the side if you want to use a a more complicated model a model that uses more features or you can use a simpler model with these with these techniques okay so in this video we learn about some feature selection methods and how to use them in scikit-learn you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Introduction to Supervised Learning",
        "Video Title":"Supervised vs. Unsupervised Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=W01tIRP_Rqs",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/W01tIRP_Rqs\/hqdefault.jpg",
        "ID":"W01tIRP_Rqs",
        "Publish Time":"2022-07-27T11:00:30Z",
        "Channel":"IBM Technology",
        "Channel ID":"UCKWaEZ-_VweaEx1j62do_vQ",
        "Transcript":"Supervised and unsupervised learning are two core components in building machine learning models. So what's the difference? Well, just to cut to the chase: supervised learning, that uses labeled input and output data, while an unsupervised learning model doesn't. But what does that really mean? Well, let's better define both learning models, go deeper into the differences between them and then answer the question of which is best for you. Now, in supervised learning, the machine learning algorithm is trained on a labeled dataset. So this means that each example in the training dataset, the algorithm knows what the correct output is. And the algorithm uses this knowledge to try to generalize to new examples that it's never seen before. Now, using labeled inputs and outputs, the model can measure its accuracy and learn over time. Supervised learning can be actually divided into a couple of subcategories. Firstly, there is a category of classification. And classification talks about whether the output is a discrete class label such as \"spam\" and \"not spam\". Linear classifiers, support vector machines, or SPMs, decision trees, random forests - they're all common examples of classification algorithms. The other example is regression. The output here is a continuous value, such as price or probability. Linear regression and logistic regression are two common types of regression algorithms. Now, unsupervised learning is where the machine learning algorithm is not really given any labels at all. And these algorithms discover hidden patterns in data without the need for human intervention. They're unsupervised. Unsupervised learning models are used for three main tasks, such as clustering, association and dimensionality reduction. So let's take a look at each one of those, starting with clustering. Now clustering is where the algorithm groups similar experiences together. So a common application of clustering is customer segmentation, where businesses might group customers together based on similarities like, I don't know, age or location or spending habits, something like that. Then you have association. And association is where the algorithm looks for relationships between variables in the data. Now association rules are often used in market basket analysis, where businesses want to know which items are often bought together. You know, something along the lines of, \"customers who bought this item also bought \", that sort of thing. The final one to talk about is dimensional ... dimensional reduction. And this is where the algorithm reduces the number of variables in the data, while still preserving as much of the information as possible. Now, often this technique is used in the pre-processing data stage, such as when autoencoders remove noise from visual images to improve picture quality. Okay, so let's talk about the differences between these two types of learning. In supervised learning, the algorithm learns from training datasets by iteratively making predictions on the data and then adjusting for the correct answer. While supervised learning models tend to be more accurate than unsupervised learning models, they do require all of this up-front human intervention to label the data appropriately. For example, a supervised learning model can predict how long your commute will be on the time of day and thinking about the weather conditions and so forth. But first you'll have to train it to know things like rainy weather extends the driving time. By contrast, unsupervised learning models work on their own to discover the inherent structure of unlabeled data. These models don't need humans to intervene. They can automatically find patterns in data and group them together. So, for example, an unsupervised learning model can cluster images by the objects they contain - things like people and animals and buildings - without being told what those objects were ahead of time. Now, an important distinction to make is that unsupervised learning models don't make predictions. They only group data together. So if you were to use an unsupervised learning model on that same commute dataset, it would group together commutes with similar conditions like the time of day and the weather, but it wouldn't be able to predict how long each commute would take. Okay, so which of these two options is right for you? In general, supervised learning is more commonly used than unsupervised learning, and that's really because it's more accurate and efficient. But that being said, unsupervised learning has its own advantages. There's two that I can think of. Firstly, unsupervised learning can be used on data that is not labeled, which is often the case in real world datasets. And then secondly, unsupervised learning can be used to find hidden patterns in data that supervised learning models just wouldn't find. Classifying big data can be a real challenge in supervised learning, but the results are highly accurate and trustworthy. And in contrast, unsupervised learning can handle large volumes of data in real time. But there's a lack of transparency into how that data is clustered and a high risk given accurate results. But wait, it is not an \"either\/or\" choice. May I present to you the middle ground known as semi-supervised learning. This is, well, a happy medium where you use a training data set with both labeled and unlabeled data. And it's particularly useful when it's difficult to extract relevant features from data when you have a high volume of data. So, for example, you could use a semi-supervised learning algorithm on a data set with millions of images where only a few thousand of those images are actually labeled. Semi-supervised learning is ideal for medical images, where a small amount of training data could lead to a significant improvement in accuracy. For example, a radiologist can look at and label some small subset of CT scans for tumors or diseases, and then the machine can more accurately predict which patients might require more medical attention without going through and labeling the entire set. Machine learning models are a powerful way to gain the data insights that improve our world. The right model for your data depends on the type of data that you have and what you want to do with it. And the choice between supervised and unsupervised learning is only the first step. If you have any questions, please drop us a line below. And if you want to see more videos like this in the future, please like and subscribe. Thanks for watching!"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Introduction to Supervised Learning",
        "Video Title":"ML 3: Supervised Learning with Examples | Regression VS Classification",
        "URL":"https:\/\/www.youtube.com\/watch?v=vbp2mco2c7U",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/vbp2mco2c7U\/hqdefault.jpg",
        "ID":"vbp2mco2c7U",
        "Publish Time":"2021-09-13T04:00:47Z",
        "Channel":"CS & IT Tutorials by Vrushali \ud83d\udc69\u200d\ud83c\udf93",
        "Channel ID":"UCapyHygDZ_pEJ5pwwZZKV3w",
        "Transcript":"good morning everyone so this is rishali and welcome to csn it tutorials by richarli so in last session we learn about introduction examples of machine learning and we also learn about training and testing data sets so i have mentioned the link of that videos in below description box so in this session we will learn about the types of learning this is the next point in your first unit so let's see which types of learning are there so following points are covered in this session that is types of learning introduction of supervised learning then working of supervised learning algorithm steps of supervised learning algorithm then there are some types of supervised learning the their types their advantages and disadvantages and differences between their types so let's see one by one okay so the first type is types of learning the first point is types of learning so in machine learning there are basically three types of learning that is supervised learning unsupervised learning and reinforcement learning in this session we will learn about the first type that is supervised learning so what is mean by supervised learning see supervised learning is a first type of machine learning which machines are trained by using some label data means we will pass as a input to the particular machine and the input is in the format of label data for example see in this diagram here suppose there are two types or there is the types of input okay which input include label data and their image for example say here this is circle and the label is circle this is the image of rectangle and the label is rectangle this we will pass the input to the machine that is labeled data means we will provide the input data as well as correct their output data to the machine so this is a concept of supervised learning algorithm so supervised learning algorithm to find out the mapping function okay they map your input variable that is x to their output variable means that map the image of circle the label with the circle so this is called as mapping okay so just remember this thing the concept of supervised learning is we will pass label input data to the machine for training purpose now see here this is just a working of supervised learning algorithm so in supervised learning models are trained by using label data so where the models learn about each type of data see here this is called as label data and these are the labels and this combinedly is called as input so in label data there will be an image of hexagon and their label is hexagon the image is square the label is square okay and there is again another images triangle and their image label is triangle so we will pass this label data to the machine for the training purpose so machine learn their features their structure and the characteristics of each and every image okay and after that by using testing data set we will test the particular images with the particular machine that particular machine can be trained by properly or not okay so this is a working of supervised learning algorithm now the first step is you need to train the data okay there is a labeled input data set you should pass those data set as an input to the machine right so first thing is you need to train the machine suppose we have a data set that is different types of shapes right that is square rectangle triangle and polygon so in training data set machine can be trained by using their features for example if the given shape has four side and all the sides are equal then it will be labeled as a square again if the given shape has three side then it will be labeled as a triangle and if the given shape has six equal sides then it will be labeled as a hexagon so this is called a training this machine can be trained by using features of those particular images okay and the features is mapped with the particular labels this is called as training data set so first these are the steps of supervised learning algorithm see here first you need to determine the types of data set okay whether your dataset regarding to the students regarding the shapes of the particular images then fruits images etc so you need to identify what is your particular training data set then you need to collect and gather label training data only okay because in supervised learning algorithm we will use labeled training data only now your complete data set can be divided into the three parts that is training data testing data set and validation data set okay so we will use suppose 75 percent images for training purpose and for suppose 25 percent images for testing purpose okay now the fourth step is determine the input features of training data set okay this machine can be trained by using some suitable algorithms like we will use support vector machine decision tree this kind of algorithm for training purpose so machine can be trained by using the features of those images right after that you need to execute the algorithm for the training data set right and after training is completed you need to test different images with the machine suppose you should give the particular square image to the machine then the particular model predict the result that this image is called as square suppose when you pass the circle image to the machine but your model is not trained with the circle image right so it shows you that the particular error will be generate right now suppose you should pass the triangle image to the machine now model predicted that the particular image is called as triangle so this is called as supervised learning algorithm if the particular model generate the accurate result so that can be decided by the accuracy of the model how the particular model predict the correct output which means the model is accurate clear so these are the steps of supervised learning algorithm now there are two types of supervised algorithm one is regression and another is classification so here there is just a simple difference between regression and classification so classification means you need to classify the data either a temperature is cold or hot right so this is called as classification but regression means the value is continuously change for example temperature so temperature is continuously changed as per the morning afternoon or evening right so this is called as regression so let's see in detail so the first type of supervised learning algorithm is regression now what is mean by regression so regression algorithm is used if there is a relationship between input variable and output variable okay and it is used for the prediction of continuous and real variable for example weather forecasting temperature right temperature is continuously change now next one is age okay your birth date and age both variables are depend on each other means according to your birth date you should calculate the age right so these things are depend on each other and this the ages continuously change right so this is called a circulation then there is a market trends means stock price share price this is also changing values and that will be depend on the market then salary price right so this is a concept of regression and for calculating the regression we will use different analysis algorithm like linear regression logistic regression support vector decision tree and random forest regression so we will learn all those algorithms in your third unit your third unit is called as regression okay next now the next type of supervised learning algorithm is classification so classification of algorithm is used when output of variable is categorical which means there are only two classes such as yes no male female true false etc so this concept is called as classification and there are different classification analysis algorithm like random forest decision tree logistic regression and support vector machine so we will learn this algorithm in your second unit your second unit is classification okay now see these are the some examples of classification like determining the email is spam or not then outcome can be binary classification or logistic regression then something is half full or not rainfall tomorrow or no rainfall right so these are the examples of classification now in classification there are two types of classifiers are used one is binary classifier one and another one is multi-class classifier okay so this algorithm which implements the classification on the particular data set okay and that's why they use the classifier so binary classifier means if the particular classification problem has only two possible outcomes that is yes or no means there is only two possible outcomes male or female spam or not spam cat or a dog right so this is called at that time this data can be represented by using binary classifier okay and multi-class classifier means if the classification problem has more than two outcomes which is called as multi-class classifier for example types of music types of crops mean there are multiple types that are there right so that can be denoted by using multi-class classifier now the advantages and disadvantages of supervised learning algorithm so with the help of supervised learning algorithm we will predict the particular output okay on the basis of your experience means in supervised learning algorithm we will pass labeled input data that's why we will predict the correct output that will be matched with the particular label so this is the advantage of supervised learning algorithm now in supervised learning algorithm we will solve also some real world problems like fraud detection spam filtering etc now the disadvantages of supervised learning algorithms are this model is not suitable for handling the complex task if the particular data set labels are not given okay at a time supervised learning algorithm is not used it required only label data and supervised algorithm cannot predict the correct output if the test data is different from the training data set means suppose you should train the machine by using apple images okay and for testing purpose you should use mango images so at that time it cannot generate the correct output okay it can't predict the correct output so this is the disadvantages of supervised learning algorithm and sometimes training required lots of computation time for supervised learning algorithm it must to first follow training phase okay after that follow the testing phase so sometime it required a time then we need to enough knowledge about class of objects means for supervised learning algorithm we need to require the particular classes of objects means for example the particular image of fruits and their labels okay we have the approximate knowledge of each and every thomas so these are the disadvantages of supervised learning algorithm now see here this is just the difference between regression versus classification this is just diagrammatical representation in classification we will classify the data okay this is in middle range this called as classifier and in regression the value is continuously change okay and that will be depend on input and output variable so regression means system attempt to predict the value input based on past data that is real number continuous number okay for example temperature for tomorrow etc but in classification prediction are made by classifying them into the different categories like types of cancer right and whether it is cancer yes or no so we will categorize the data so this is the difference between regression and classification so hope so you should understand about the concept of supervised learning algorithm so if you like my videos please subscribe the channel thank you keep practicing"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Introduction to Supervised Learning",
        "Video Title":"Supervised vs Unsupervised vs Reinforcement Learning | Machine Learning Tutorial | Simplilearn",
        "URL":"https:\/\/www.youtube.com\/watch?v=1FZ0A1QCMWc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/1FZ0A1QCMWc\/hqdefault.jpg",
        "ID":"1FZ0A1QCMWc",
        "Publish Time":"2020-11-20T15:30:00Z",
        "Channel":"Simplilearn",
        "Channel ID":"UCsvqVGtbbyHaMoevxPAq9Fg",
        "Transcript":"hello everyone welcome to this tutorial by simply learn in this video you will learn about an interesting machine learning topic that is supervised versus unsupervised versus reinforcement learning let's discuss each of them in detail and understand when to use these algorithms along with their applications now there are a number of algorithms used in the field of machine learning to solve complex problems each of these algorithms can be classified into a certain category so the different types of machine learning algorithms are supervised learning unsupervised learning and reinforcement learning now let's look at the definition of each of these learning techniques supervised learning uses labeled data to train machine learning models label data means that the output is already known to you the model just needs to map the inputs to the outputs an example of supervised learning can be to train a machine that identifies the image of an animal below you can see we have a trained model that identifies the picture of a cat unsupervised learning uses unlabelled data to train machines unlabeled data means there is no fixed output variable the model learns from the data discovers patterns and features in the data and Returns the output here is an example of an unsupervised learning technique that uses the images of vehicles to classify if it's a bus or a truck so the model learns by identifying the paths of a vehicle such as the length and width of the vehicle the front and rear end covers roof hoods the types of Wheels used Etc based on these features the model classifies if the vehicle is a bus or a truck reinforcement learning trains a machine to take suitable actions and maximize reward in a particular situation it uses an agent and an environment to produce actions and rewards the agent has a start and an end state but there might be different parts for reaching the end State like a maze in this learning technique there is no predefined target variable an example of reinforcement learning is to train a machine that can identify the shape of an object given a list of different objects such as square triangle rectangle or a circle in the example shown the model tries to predict the shape of the object which is a square here now let's look at the different machine learning algorithms that come under these learning techniques some of the commonly used supervised learning algorithms are linear regression logistic regression support Vector machines K nearest neighbors decision tree random forest and knife base examples of unsupervised learning algorithms are K means clustering hierarchical clustering DB scan principal component analysis and others choosing the right algorithm depends on the type of problem you're trying to solve some of the important reinforcement learning algorithms are are Q learning Monte Carlo sarsa and deep Q Network now let's look at the approach in which these machine learning techniques work so supervised learning takes labeled inputs and Maps it to known outputs which means you already know the target variable unsupervised learning finds patterns and understands the trends in the data to discover the output so the model tries to label the data based on the features of the input data while reinforcement learning follows trial and error method to get the desired solution after accomplishing a task the agent receives an award an example could be to train a dog to catch the ball if the dog learns to catch a ball you give it a reward such as a biscuit now let's discuss the training process for each of these learning methods so supervised learning methods need external supervision to train machine learning models and hence the name supervised they need guidance and additional information to return the result unsupervised learning techniques do not need any supervision to train models they learn on their own and predict the output similarly reinforcement learning methods do not need any supervision to train machine learning models and with that let's focus on the types of problems that can be solved using these three types of machine learning techniques so supervised learning is generally used for classification and regression problems we'll see the examples in the next SL slide and unsupervised learning is used for clustering and Association problems while reinforcement learning is reward based so for every task or for every step completed there will be a reward received by the agent and if the task is not achieved correctly there will be some penalty used now let's look at a few applications of supervised unsupervised and reinforcement learning as we saw earlier supervised learning learning are used to solve classification and regression problems for example You can predict the weather for a particular day based on humidity precipitation wind speed and pressure values you can use supervised learning algorithms to forecast sales for the next month or the next quarter for different products similarly you can use it for stock price analysis or identifying if a cancer cell is malignant or benign now talking about the applications of unsupervised learning we have customer segmentation so based on customer Behavior likes dislikes and interests you can segment and cluster similar customers into a group another example where unsupervised learning algorithms are used as customer churn analysis now let's see what applications we have in reinforcement learning so reinforcement learning algorithms are widely used in the gaming Industries to build games it is also used to train robots to perform human tasks and with that we have come to the end of this video on supervised vers vers unsupervised versus reinforcement learning I hope you like this video If you enjoyed watching this video then please subscribe to the simply learn Channel and hit the Bell icon to never miss an update thank you for watching and keep learning hi there if you like this video subscribe to the simply learn YouTube channel and click here to watch similar videos turn up and get certified click here"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Introduction to Supervised Learning",
        "Video Title":"Supervised Learning - AI Basics",
        "URL":"https:\/\/www.youtube.com\/watch?v=g9oESGzcA84",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/g9oESGzcA84\/hqdefault.jpg",
        "ID":"g9oESGzcA84",
        "Publish Time":"2023-08-01T13:33:25Z",
        "Channel":"LearnFree",
        "Channel ID":"UCeYUHG6o0YguM-g23htdsSw",
        "Transcript":"supervised learning in AI supervised learning is one of the three main types of artificial intelligence it's all about teaching computers to learn using examples during supervised learning the computer is provided with lots of examples and told the right answer for each one the computer learns from this information allowing it to now guess the correct answer for new things that it's never seen before for example let's say we want the computer to learn how to recognize different kinds of fruit we can show the computer pictures of apples oranges and bananas and tell it which fruit is shown in each picture once the computer has seen enough it starts to recognize the features that make each fruit unique like the shape color and texture now when we show the computer a new picture that it hasn't seen before it can make a prediction about what kind of fruit it is based on what it's learned supervised learning can be applied to lots of things such as detecting credit card fraud identifying spammy emails and even diagnosing medical conditions computers can learn to recognize patterns and make predictions and their promise is that they can help us to address some of today's biggest challenges and help us to create a better world to learn more about AI check out our website at gcfglobal.org GCF global creating opportunities for a better life"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Simple Linear Regression",
        "Video Title":"Video 1: Introduction to Simple Linear Regression",
        "URL":"https:\/\/www.youtube.com\/watch?v=owI7zxCqNY0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/owI7zxCqNY0\/hqdefault.jpg",
        "ID":"owI7zxCqNY0",
        "Publish Time":"2015-08-30T19:11:54Z",
        "Channel":"dataminingincae",
        "Channel ID":"UCbdsyFBdG0Fmjroqk-ZoPcg",
        "Transcript":"welcome to our first video on simple linear regression in this video we will discuss the two main objectives of simple linear regression and also we're going to run an example that will help us start understanding how should we interpret the coefficients of a simple linear regression model simple linear regression or regression models in general have two main objectives the first one is going to be to establish if there's a relationship between two variables we're going to be talking about a positive relationship between two variables if they tend to move together in the sense that when one increases the other one increases as well and conversely in a negative relationship we find that if one variables values increase the other variables values tend to decrease more specifically we're going to be talking about statistically significant relationships between the two variables but we will get back to this later let's talk about some examples on average we expect that people or families that earn higher income will generally spend more of a given product in this case we're talking about a positive relationship between income and spending we could also analyze and test if there's a relationship between wage and gender we could ask if men are more likely to earn higher wages than women case in which we're talking about gender discrimination which is negative and we don't want it but we can actually use the regression models to test if that relationship exists another example that I hope you find odd is relationship between a student's height and that same students exam scores we should expect no relationship to exist and we can use for regression models to test that our second objective is going to be to forecast new observations and what we mean here is can we use what we know about an existing relationship to forecast unobserved values let me give you a couple of examples for instance if we know that our sales tend to grow over time and we actually even know how strong this relationship is and we know how fast our sales grow we could use this information to predict or to forecast what will our sales be you over the next quarter also if we have data on stores and we know how profitable different stores are and we know what is the relationship between how much competition a store is facing in a given location or what the population of a location is and how those variables impact the stores profitability we could use what we know about previous stores to evaluate the profitability of a new inexistence store of course we don't know how profitable the new store is going to be but if we know how much competition it's going to face and we know how much people live nearby the store then we can use this information to forecast that stores profitability in general we're going to be talking about two different roles that variables play in regression models the first one is going to be the dependent variable this is the variable whose values who want to explain or forecast and we call it the dependent variable because its values depend on something else and we will be denoting it as Y the other role is that of the independent variable and this is the variable that explains the other one and we say that its values are independent hence its name we will denote this value as X or this variable as X when we use simple linear regression models we call them linear because the magic is that we're using a linear equation and from your high school years you might remember one of these where Y is a function of X and there's a term that is added to the function another one that multiplies the x in the stats role we like Greek letters and we're going to be using a slightly different notation our linear equation is going to have a beta0 term which we're going to be calling the intercept or the constant and the beta one which is the term that multiplies to X and we're going to be calling the coefficient of X or the slope of X and as a recap we call these linear equations because they will appear as a straight line if we plot them in I be dimensional plot let me show you an example of a linear equation and remember our model is y equals beta sub 0 plus beta 1 times X and let's simply give numbers to those betas and let's say that we have a linear equation y equals 4 plus 2x let's analyze what the 4 means 4 is the distance from the horizontal axis at which the line crosses the vertical axis in this case if X had a value of 0 y equals 4 and we see here that the line is crossing the vertical axis 4 units above the horizontal axis the slope in this case is a 2 and it means that for every unit increase of X Y will increase twice as much or two times as much we see here that X increased from two to four while Y increase from eight to twelve twice as much let's further understand this and wonder what happens if we change the intercept in particular let's use a larger intercept let's change the intercept to a nine we see that we're moving the line upwards meaning that we're increasing the distance from the horizontal axis to the point where the line intersects the vertical axis we could also have a negative intercept which would imply that the line is intersecting the vertical axis below the horizontal axis at a negative number if X were zero here y is minus two now what happens if we change the slope if we change the slope we're changing the sensitivity of Y on values of X meaning how fast or how slow Y will change when a unit of X is changed if the slope were five that means for every unit of X Y is going to increase five times so you see that we have a much steeper slope because Y is growing much faster than it was before when we had a two what would happen if we had a zero slope this would mean that it doesn't matter what value X has y will always be four and if you go all the way around to a negative slope a minus three in this case we have a downward slope and this represents that Y will decrease three units for every unit of X now in this case we're talking about very crisp and clear straight lines but data in the real world does not behave like this data in general is going to be a serious of X&Y observations that on average may follow a linear pattern and that's why we use linear regression models to represent them but the line is not always and actually most generally not intersecting or passing across any of these rather there are going to be errors and that we can measure which are the distance between the dots in the real life and the actual linear regression and what the linear regression is going to do is try to draw a line that minimizes these errors but the important aspect we're going to take away now from this is that our linear regression model must include this error into it so this is how our linear regression model looks like it is y equals beta 0 plus beta 1 times X plus the error term so let's recap Y is the dependent variable the variable whose value depends on all other parameters in the equation X is the independent variable that helps explain the variance in the Y beta 0 is our constant term or intercept meanwhile beta 1 is the slope coefficient for the X and epsilon which is a Greek term to symbolize something that is very very small is going to be our error term which we're trying to minimize now let's look at an example with actual data we will try to explain a family's consumption of a given product so think for a moment of any product it could be cereal it could be toys it could be books and try to wonder what determines a family's consumption of this product and you may want to pause this video now and spend a couple of minutes thinking about what could you use to explain a family's consumption of a given product alright maybe you thought about attributes such as how large the family is and in general larger families would necessarily have to consume more but if you recall our early example we talked about a positive relationship between income and consumption and this is what we're going to have in the data we're going to use our data is going to be a series of 40 observations of 40 different families and their weekly income and consumption of a given product so we have three columns in our data the first one is going to be an observation ID which generally has no meaning whatsoever and the other two columns are going to be income and consumption so how could we use these two columns in a simple linear regression model remember our model has a dependent variable and our independent variable and in this case we're trying to explain consumption based on income data so consumption is going to be your dependent variable and income is going to be your independent variable and of course the assumption we're making and what we want to test is if income is good enough variable to explain consumption if we throw this into a statistical package such as Gretel this is what we observe and for now don't worry about all the numbers you see here we will learn about them later focus on the coefficients which are 2 and Gretel's indicating that our constant term is 49 point 13 34 and that the coefficient for income is 0.85 to 7 3 6 if we wrote this into our equation we would find that consumption equal to 49 point 13 + 0.85 times income plus the error term which is going to be very important now what do these numbers mean for our model which try to see how did income explain consumption remember our model is this one right here now let's start with 49 point 13 if income where 0 consumption would take this value consumption will be forty nine point thirteen so using this we could interpret the intercept as the consumption level of a family with zero income now this makes little sense unless we assume that there's a state grown program that offers financial aid to families with no income such that thanks to this a they can have some consumption but more generally the intercept will not have an intuitive interpretation meaning that in most cases we will actually be ignoring it now let's talk about the 0.85 this means that for every unit increase in income consumption will grow by 0.85 we can call this the marginal effect meaning that on the margin income will grow 0.85 for every unit increase in income and let's put some dollar numbers to this let's say your family's income is 100 more so 100 times point 85 means that for every $100 of income a family earns more per week in this case the consumption will grow on average and expected of eighty five dollars worth noting the slope will always have an intuitive interpretation which is the sensitivity of the dependent variable on changes in the values of the independent variable and we will finish our example by showing the initial observations and the fitted linear model in the same plot here in this plot the horizontal axis is income and the vertical axis is consumption the 40 red dots you see represent each of the 40 observations of the families for which we had data on income and consumption you can notice that they follow a growing trend but there are not at all in a straight line what we did however was draw a straight line that closest resembled the pattern followed by the dots and this is the blue line which shows the fitted values from our linear regression what our estimation procedure did was try to draw a line that minimized the error between the red dot and the blue dots but we will talk more about that later this is all for this video thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Simple Linear Regression",
        "Video Title":"Simple Linear Regression: An Easy and Clear Beginner\u2019s Guide",
        "URL":"https:\/\/www.youtube.com\/watch?v=gPfgB4ew3RY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/gPfgB4ew3RY\/hqdefault.jpg",
        "ID":"gPfgB4ew3RY",
        "Publish Time":"2024-12-08T13:14:54Z",
        "Channel":"DATAtab",
        "Channel ID":"UC3UwrWtAFlAkFl_3Nia756g",
        "Transcript":"this video is about simple linear regression this is the second video in our playlist on regression analysis if you like you can find the content of this video in our book statistics made easy so let's start what is a simple linear regression simple linear regression is a method to understand the relationship between two variables you can infer or predict a variable based on another variable for example you can predict the annual salary of a person based on the years of work experience so a simple linear regression can help us understand how salary changes with an increase in years of experience the variable we want to inere or predict is called the dependent variable the variable we use for prediction is called independent variable let's look at an example imagine we want to predict house prices the dependent variable the one we want to predict is of course the price of the house the independent variable the one we use to make the prediction could be the size of the house in square feet of course you can also use more than one independent variable for example the construction here or features like whether the house has a swimming pool the number of bathrooms and the house size but in this case it would be a multiple linear regression because you have more than one independent variable more on that in my video about multiple linear regression okay but how do we calculate a simple linear regression first of all we need data so we collect information from 10 houses including their size in square feet and the price they were sold for now we can use this data to calculate our regression model here Y is the dependent variable house price and X is the indep dep variable house size and we want to use our data to determine the coefficient b and a but how do we do that let's visualize our data using a scatter plot on the xais we plot our independent variable the house size and on the Y AIS we plot the dependent variable the house price each point is there one house with the respective house size and the house price okay now we want to summarize all this data using a simple linear regression to do this we draw a straight line through the points on the scet plot but the line we draw isn't just any random line it's the line that tries to minimize the arrow or the distance between the actual data points and the line itself if we add up the length of all the red lines we get the total error our goal is to find the regression line that minimizes this error but how do we actually calculate this line This is where the equation of the linear regression comes into play in the equation B is the slope of the line the slope shows how much the house price changes if the house size increases by one square foot a is the Y intercept telling us where the line crosses the Y AIS so if we have a house with a size of zero the model will predict a house price of a of course predicting the price of a house with zero size doesn't make sense however every model is a simplification of the real world and in the case of simple linear regression our model is defined by a regression line with a specific slope B and an intercept a let's look at this example in that case our intercept is 100 so we Answer 1 100 for a but how do we read the slope for this we take a one unit step in the independent variable for example if we move from 1 to two then we observe how much the dependent variable changes with this one unit increase in this case if the independent variable increases by one unit the dependent variable increases by 50 units so our B is 50 okay but how do we calculate b and a there are two ways to do this we can calculate them by hand or using statistical software like data tab let's look at this example how can we calculate b and a by hand to calculate the slope B we use this formula R is the correlation coefficient between X and Y so in our case the correlation between house side and house price we get a correlation coefficient of 0.92 s y is the standard deviation of the dependent variable house price and SX is the standard deviation of the independent variable so house size so in this case our B is 10 18.35% formulas for standard deviation one divides by n and the other by n minus one without diving into the details now almost all statistical software uses the formula with n minus1 to calculate the standard deviation however for calculating the regression coefficient B we use the formula that divides by n if you like a more detailed explanation of the standard deviation feel free to check out my video on that topic all right once we've calculated B we can find The Intercept a using this formula here Y Bar represents the mean of the house prices B is the slope which is calculated and xbar is the mean of the house sizes substituting these values The Intercept a comes out to be 60,000 91944 So based on this data we have now calculated the coefficient b and a if we insert the numbers for b and a we get this equation if we enter Z for X the house size we get 6,919 44 which is our intercept if we increase the house size by one square foot each time we get a house price that is $18.35 higher each time okay before we start with the last important topic the assumptions for a simple linear regression let's check the results with data tab if you like you can load this sample data set the link is in the video description we want to calculate a regression so we click on regression here we can now select our dependent variable house price and the independent variable house size now let's look at the results we will focus on this table which shows the key information we need if you're curious about the other tables you can click on AI interpretation for a corresponding table or check out our video on multiple linear regression where we explain these details in dep okay in this table we can see the calculated regression coefficients for the constant we called it intercept and house size the values match exactly with the ones we calculated by hand the intercept and the slope in the results table for a linear regression you'll also see the P value what does the P value tell us the P value helps determine whether the relationship between the independent variable and a dependent variable is statistically significant to test whether the relationship we observe is Meaningful or just due to random Chens we start by stating the null hypothesis there is no relationship between the independent variable and the dependent variable if the P value is small typically smaller than 0.05 we reject the null hypothesis this is suggesting a significant relationship between the variables if the P value is large typically greater than 0.05 we fail to reject the null hypothesis indicating The observed data may have occurred by chance with no strong evidence for a relationship so in our case the P value is highly significant indicating strong evidence of a relationship between house price and size all right in this example it's pretty obvious a bigger house size typically costs more however there are cases where the relationship isn't that clear and what about the assumptions here are the key assumptions number one linear relationship in linear regression a straight line is drawn through the data this straight line should represent All Points as good as possible if the relation is nonlinear the straight line cannot fulfill this requirement number two independence of Errors the errors so the differences between actual and predicted values should be independent of each other this means that the aror of one point doesn't affect another number three homoscedasticity or equal variance of Errors if we plot the arrow on the Y AIS and the dependent variable on the xaxis their spread should be roughly the same across all values of X in other words the variance of the arror should remain constant in this case the assumption is fulfilled but what about that case here we observe unequal variant at low values of X the errors are small while at high values the variance of the arrow becomes much larger number four normally distributed errors the arrows should be normally distributed the normality of the arrow can be tested both analytically and graphically however be cautious with analytical tests for small samples they often indicate normality and with large samples they quickly become significant because of these limitations graphical methods such as the QQ plot are more commonly used today if you use data tab you just need to click here to check the assumptions so let's just go through how the assumptions are checked in practice to check for a linear relationship you can use a scatter plot plot the independent variable against the dependent variable if the points form a clear straight line pattern a linear relationship exists if not the relationship is likely nonlinear in our case we observe a clear linear relationship to test if the errors are normally distributed you can use a QQ plot or one of the several analytical tests with a QQ plot the residuals should fall roughly along a straight line indicating normality if you use an analytical test check whether the calculated P value is greater than 0.05 if it is not there is evidence that the data are not normally distributed the choice of test often depends on your field of research however as mentioned the QQ plot is increasingly preferred as a visual and intuitive way to assess normality independence of Errors can be tested using using the Durban Watson test which checks for autocorrelation in the residuals if the calculated P value is greater than 0.05 it indicates that there is no significant autocorrelation in the residuals and the independence assumption is likly satisfied homoscedasticity can be checked using a residual plot where the predicted values are plotted on the xais and the residuals or errors on the Y AIS the residuals should show a consistent spread across the plot a funnel shape indicates hat Tois catastic meaning the variance is not constant in our case the plot looks acceptable though not perfect if these assumptions are violated the regression results might not be reliable or meaningful and the predictions could be inaccurate so always check these assumptions before drawing conclusions from a regression model okay now we have a good understanding of what regression analysis is we know what a simple linear regression is and in the next video we'll dive into multiple linear regression thanks for watching and see you there in just a moment"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Simple Linear Regression",
        "Video Title":"Linear Regression in 2 minutes",
        "URL":"https:\/\/www.youtube.com\/watch?v=CtsRRUddV2s",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/CtsRRUddV2s\/hqdefault.jpg",
        "ID":"CtsRRUddV2s",
        "Publish Time":"2021-11-28T07:49:01Z",
        "Channel":"Visually Explained",
        "Channel ID":"UCoTo2gtN527CXhe7jbP6hUg",
        "Transcript":"an important task in machine learning is prediction given some information summarizing an independent variable x for example the height of a person predict the value of another dependent variable y like their weight usually we have a training data set that is a table that contains values for both x and y that we want to use to infer what the function g should be we can then use that learned function g to predict the values y for new values of x not seen in training finding the right function g is called regression and the easiest way to do this is to assume that this function g is a linear function hence the name linear regression here alpha is the slope of this line and beta is the intercept the simplicity of linear regression makes it really attractive we're only dealing with linear functions here so no complicated neural networks involved if you are faced with a prediction problem linear regression should really be the first thing that you try to do linear regression we just need to pick the alpha and beta that makes this line fit the data as much as possible one way to quantify the fit of a line to a bunch of data points is to consider where the point in the training data set is and where it should be according to this line take the square of the difference and then take the sum over all data points to find the alpha and beta that make this quantity as small as possible let's take the gradient set it to zero and solve for alpha and beta congratulations you have just solved a prediction machine learning problem from scratch if now you want to predict the weight of a new person you just plug their height into this formula in practice you don't have to do this derivation by hand if you feel more sophisticated you can fire up python import psychic learn declare a linear regression model and feed it to training data and call the fit function to get predictions you can just use the function predict in addition to being simple and tractable linear regression is also very extensible for example you can easily adapt linear regression to fit a model with more than just one variable as input in the same vein if you are not satisfied with the linear relationship you can augment your data with nonlinear features another compelling aspect of linear regression is the fact that we can interpret the coefficients that we get for example if the slope is positive x and y move in the same direction and if the slope is negative they move in opposite directions good luck trying to find a meaningful interpretation of the weights of a neural network this was linear regression in two minutes if you like the video please make sure to like and subscribe and see you next time"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Simple Linear Regression",
        "Video Title":"An Introduction to Linear Regression Analysis",
        "URL":"https:\/\/www.youtube.com\/watch?v=zPG4NjIkCjc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/zPG4NjIkCjc\/hqdefault.jpg",
        "ID":"zPG4NjIkCjc",
        "Publish Time":"2012-02-05T17:53:08Z",
        "Channel":"statisticsfun",
        "Channel ID":"UClD8c_piy1nrJySPJUgyivg",
        "Transcript":"this tutorial is an introduction to regression there is an X variable and a Y variable in this case the independent variables on the x-axis and the dependent variable is on the y-axis and we try to form a relationship between these two variables and draw a line in this case a straight line and over the next series of videos I'll explain what all this means what we try to understand is as the independent variable is moving or changing what happens to the dependent variable does it go up or does it go down how does it change if they move in the same direction if the independent variable increases and the dependent variable increases as well like this we say there's a positive relationship if on the other hand as the independent variable increases and the dependent variable decreases like this we say there's a negative relationship the line would look like this go downward in the linear regression we try to make a line a line to make a linear regression the key is on line right there a straight line you can also do curved lines but for the this topic is all straight lines to actually conduct regression I take observations and always plot some more observations in your random play I'll stick them in here like that and I try to find a line that will fit a straight line that fits through all these different points and this is called my regression line and it's based upon the least squares method and in the end I want to minimize the difference between the estimated value and the actual value I want to minimize my error errors this line will have a lot of errors if I compare the actual to the estimated value and again the point is to minimize these errors or make them as small as possible now let's imagine I put study time on the x-axis or make that my independent variable and the dependent variable becomes grades or GPA as study time increases grades should go up there is a positive relationship in regression we develop these equations like this in this case y hat is estimated grades and it's based upon or it's equal to B naught plus B 1 times X where X is study time be not we derive mathematically and it is the y-intercept b1 we also derive mathematically and I'll do in a later video and it's the slope of the line in this case the slope is positive in the next video I'll discuss how you develop these equations now if I change the x-axis to time on face book we see a negative relationship more time on face book grades will suffer and go down a negative relationship what we're estimating is still grades estimated grades is equal to B naught minus B 1 times X where X is time on Facebook B naught is still the y intercept the y-intercept and it is a calculated value the slope of the line is negative B 1 because it's downward sloping negative relationship and as I said before all show you how to calculate this equation in the next video the X is the independent variable the Y is the dependent variable the X is what we control what we manipulate what we change and the dependent variable is the outcome so study time is the independent variable is what we control and manipulate and your grades are dependent upon how much you study now this looks really ugly and it's what I'll talk about in the next video but I'll step you step-by-step through it and hopefully make it simple for you you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Multiple Linear Regression",
        "Video Title":"Multiple Regression, Clearly Explained!!!",
        "URL":"https:\/\/www.youtube.com\/watch?v=EkAQAi3a4js",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/EkAQAi3a4js\/hqdefault.jpg",
        "ID":"EkAQAi3a4js",
        "Publish Time":"2022-11-18T19:11:48Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"stat Quest stat Quest stack Quest stat Quest yeah it's that Quest hello I'm Josh starmer and welcome to stat Quest stat Quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about multiple regression and it's going to be clearly explained this deck Quest Builds on the one for linear regression so if you haven't already seen that one yet check it out alright now let's get to it people who don't understand linear regression tend to make a big deal out of the differences between simple and multiple regression it's not a big deal and the stat Quest on simple linear regression already covered most of the concepts we're going to cover here you might recall from the stat Quest on linear regression that simple regression is just fitting a line to data we're interested in the r squared and the p-value to evaluate how well that line fits the data in that same stat Quest I also showed you how to fit a plane to data well that's what multiple regression is you fit a plane or some higher dimensional object to your data a term like higher dimensional object sounds really fancy and complicated but it's not all it means is that we're adding additional data to the model in the previous example all that meant was that instead of just modeling body length by mouse weight we modeled body length using mouse weight and tail length if we added additional factors like the amount of food eaten or the amount of time spent running on a wheel well those would be considered additional Dimensions but they're really just additional pieces of data that we can add to our fancy equation so from the stack Quest on linear regression you may remember the first thing we did was calculate r squared well the good news is calculating r squared is the exact same for both simple regression and multiple regression there's absolutely no difference here's the equation for r squared and we plug in the values for the sums of squares around the fit and then we plug in the sums of squares around the mean value for the body length regardless of how much additional data we add to our fancy equation if we're using it to predict body length then we use the sums of squares around the body length one caveat is for multiple regression you adjust r squared to compensate for the additional parameters in the equation we covered this in the stat quest for linear regression so it's no big deal now we want to calculate a p-value for our r squared calculating F in the p-value is pretty much the same you plug in the sums of squares around the fit and then you plug in the sums of squares around the mean for simple regression P fit equals 2 because we have two parameters in the equation that least squares has to estimate and for this specific example the multiple regression version of P fit equals three because least squares had to estimate three different parameters if we added additional data to the model for example the amount of time a mouse spends running on a wheel then we have to change P fit to equal the number of parameters in our new equation and for both simple regression and multiple regression p mean equals one because we only have to estimate the mean value of the body length so far we've compared this simple regression to the mean and this multiple regression to the mean but we can compare them to each other and this is where multiple regression really starts to shine this will tell us if it's worth the time and trouble to collect the tail length data because we will compare a fit without it the simple regression to a fit with it the multiple regression calculating the F value is the exact same as before only this time we replace the mean stuff with the simple regression stuff so instead of plugging in the sums of squares around the mean we plug in the sums of squares around the simple regression and instead of plugging in p mean we plug in P simple which equals the number of parameters in the simple regression that's two and then we plug in the sums of squares for the multiple regression and we plug in the number of parameters in our multiple regression equation bam if the difference in r squared values between the simple and multiple regression is big and the p-value is small then adjusting tail length to the model is worth the trouble hooray we've made it to the end of another exciting stat Quest now for this stack Quest I've made another one that shows you how to do multiple regression in R it shows all the little details and sort of what's important and what's not important about the output that R gives you so check that one out and don't forget to subscribe okay until next time Quest on"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Multiple Linear Regression",
        "Video Title":"Statistics 101: Multiple Linear Regression, The Very Basics \ud83d\udcc8",
        "URL":"https:\/\/www.youtube.com\/watch?v=dQNpSa-bq4M",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/dQNpSa-bq4M\/hqdefault.jpg",
        "ID":"dQNpSa-bq4M",
        "Publish Time":"2014-12-02T05:22:05Z",
        "Channel":"Brandon Foltz",
        "Channel ID":"UCFrjdcImgcQVyFbK04MBEhA",
        "Transcript":"- [Brandon] Now as I'm sure you know and experience, the world is a very complex place. So when we're looking to predict the value of a variable, oftentimes we can get better predictions if we use more than one other variable to make that prediction and that leads us to multiple regression. Now I am going to assume you have some familiarity and some comfort with simple linear regression which we covered in the previous series. So if you're still a bit shaky on just simple linear regression, I would go back, review that and then come back to this series. So without further ado, let's go ahead and get to learning. So as always, let's start out with a problem and a dataset that we're gonna use for the next several videos. And this is called the regional delivery service. So let's assume that you are a small business owner for Regional Delivery Service, Incorporated, or RDS for short, who offers same-day delivery for letters, packages, and other small cargo. You are able to use Google Maps to group individual deliveries into one trip to reduce time and fuel costs, just like UPS would or FedEx or the Postal Service does. Therefore some trips will have more than one delivery. Now as the owner, you would like to be able to estimate how long a delivery will take based on two factors, one, the total distance of the trip in miles and two, the number of deliveries that must be made during that trip. So we're looking to estimate how long a delivery trip will take based on the distance and the number of deliveries during that trip, so two factors. So to conduct your analysis you take a random sample of 10 past trips and record three pieces of information for each trip, one, the total miles traveled, two, the number of deliveries during that trip and three, the total travel time in hours, which is what we're trying to predict. So you make a table that looks like this. So we have miles traveled, num deliveries, which is the number of deliveries, and then travel time in hours along the top. We have labeled them X1, X2 and Y. Now X1 and X2 are special types of variables we'll discuss here in a minute. And Y is a distinct variable, we'll talk about here in a minute. So we can see the first trip we traveled 89 miles. We had four deliveries on that trip, and the total time was seven hours. The second trip was 66 miles. We only had one delivery and the travel time was 5.4 hours. So here are our 10 trips. Now remember that in this case, you would like to be able to predict the total travel time, so that's the right column so in the orange-brown color there, using both the miles traveled, which X1, the first column, and number of deliveries, which is the second column X2 of each trip. So the question is, in what way does travel time depend on the first two measures, miles traveled and number of deliveries. So travel time is the dependent variable and miles traveled and number of deliveries are independent variables. Now one note here, some prefer predictor variables and response variable instead of independent and dependent variables respectively. Since most stats textbooks use independent variable and dependent variable, I am going to stick to that. But I do subscribe to the case of predictor variables and response variable. Just keep in mind that depending on the textbook you're using and your professor and things, you may hear both or one or the other. So what about multiple regression? So multiple regression is just an extension of simple linear regression, again which we talked about in the last series. So remember in simple linear regression, we have a one to one relationship. So we have a dependent variable and we're going to utilize an independent variable to explain the variation in that dependent variable or make predictions about that dependent variable. Now in multiple regression, we have a many to one relationship. So we still have one dependent variable but we can have two or more. So in this case we have four on the screen but we can have more than that or just two or three or whatever independent variables that are all being utilized to explain the variation or predict the value of the dependent variable. So we go from a one to one relationship, one independent to one dependent to two or more independent variables and one dependent variable. Now having more independent variables complicates things a bit. So we have to have some new things to consider. The first is that adding more independent variables to a multiple regression procedure does not mean or necessarily mean the regression will be better or offer better predictions. In fact, doing so can actually make things worse. This is called overfitting. So let's say we conduct a multiple regression procedure and our model explains 65% of the variation in the dependent variable. Well for some reason we don't like that. We think well, we can do better than that. So we start adding in more independent variables. Well adding more independent variables will explain more of the variation in the dependent variable but it can do so under false pretenses. So adding more variables will always explain more variation, but it can open up a whole Pandora's box of other problems that we definitely want to avoid. So we'll talk about that more as we go, but I just wanna float it out there that dumping more variables into a multiple regression procedure is not the way to go. The idea is to pick the best variables for the model. We'll talk about how to do that in future videos. The other concept is that the addition of more independent variables, see a pattern here, creates more relationships among them. So not only are the independent variables potentially related to the dependent variable, they are also potentially related to each other. Now when this happens, it is called multicollinearity. Now it's a mouthful of a word to say and I stumble it over it sometimes but hopefully we'll get better at it as we go. So it's called multicollinearity when the independent variables are correlated with each other. So the ideal, the perfect world is for all the independent variables to be correlated with the dependent variable but not with each other. And again, we'll talk about overfitting more as we go. We'll talk about multicollinearity more as we go forward and just keep in mind that the ideal is for the independent variables to be correlated with dependent variable but not with each other. Now because of multicollinearity and overfitting, there is a fair amount of prep work to do before conducting multiple regression analysis if one is to do it properly. And in a future video, we will walk through all those things step by step so that you form the best model you can. So things like correlations, scatter plots, and some simple regressions between the independent variable each one of them, and the dependent variable just to see how they're related. So to do multiple regression properly, really running the multiple regression is the very last step. There's a lot of prep work to do before doing that and again we'll talk about it as we go. So as we talked about before, adding more independent variables creates more relationships among all the variables. So we have this many-to-one relationship. Now in our problem we have a dependent variable that is the travel time. We are trying to predict the travel time of these trips. Now we are utilizing two independent variables that we selected. We have miles traveled, that's our X1 and then we had the number of deliveries or num deliveries which is our X2. Now we'd like to utilize those two independent variables to make predictions about the dependent variable. Now by setting it up this way, we also create a third relationship, and that is the relationship between the two independent variables themselves. So we don't have just two relationships, independent and dependent. We now have a relationship between the independents. And having that relationship sets up the potential multicollinearity risk. So we're gonna have to see when we do this problem, whether or not these two independent variables are correlated with each other. And the easy way to think about this is that if these two independent variables are related to each other, we're really not sure which one is explaining the variation in the dependent variable. So if I put sea salt and table salt in my dinner, all I know is that it tastes salty. But I can't tell the difference necessarily between the two because they're both salt. They have the same relationship to my now salty dinner. So we wanna have a distinction between the independent variables so that it explains something different. We have a different relationship with the dependent variable over here on the right. So we will walk through that analysis as we go forward. Now let's look at this situation. So here we have one dependent variable and four independent variables. So we know we have the four relationships with each independent variable and the dependent variable. So right there we already have four variable relationships. But we're not done. We have to account for all the relationships between the independent variables. And that's six more. So now with four independent variables and one dependent variable, we have 10 relationships we have to consider. Now you can see as each independent variable is added, these relationships become very numerous. So part of the art of multiple regression is deciding which independent variables make the cut and which do not. And we'll talk about that as we go of course. So the bottom line is that some independent variables or sets of independent variables are better at predicting the dependent variable than others. And some independent variables contribute nothing. So we'll have to decide which independent variables to include in our model and which ones to exclude. So again, the ideal is for all of the independent variables to be correlated with the dependent variable, so the orange lines, but not with each other, so the colored dotted lines here. So this slide is not something you have to really commit to memory, but I just wanna show you sort of where the multiple regression model comes from. So we have our multiple regression model which is Y equals beta sub zero, plus beta one X1 plus beta two, X2 plus et cetera, et cetera, et cetera. P just means the number of variables we have plus epsilon. Now over here on the left what we have are the sum of linear parameters. So if beta sub zero, which is our intercept and then we have beta one, X1, which is one variable and its weight, then we have X2, beta two, which is another variable and its weight, et cetera, et cetera. So it's just the sum of some linear parameters. But over here on the right we have our error term. So we've seen this before in simple linear regression. So we have an intercept plus a set of linear parameters plus an error term. Now for the multiple regression equation, we have the expected value of Y equals everything we see up at the top but there is no error term. Well why is that? That's because in the multiple regression equation, the error term is assumed to be zero. So zero is zero and therefore it's not on the end of that equation. The one we're gonna be familiar with is the estimated multiple regression equation. So again when we're using sample data, it's never gonna be perfect. We're estimating, so we have to use the estimative multiple regression equation. So Y hat is the predicted value of Y equals B sub zero plus B1 X1 plus B2, X2, et cetera. As you can see it follows the same form as the multiple regression equation above it and everything that you see at the bottom is just an estimate of what is above it. So B zero, B one, B two are all the estimates of beta zero, beta one, beta two, et cetera and then Y hat is the predicted value of the dependent variable. So again, this is not something you need to really commit to memory, but I just want you to see the pattern of how these multiple regression equations are gonna look and we'll talk about in the next slide sort of what they mean. So let's go ahead and look at an example. So this is a multiple regression equation you may generate based on some analysis you conduct. So if Y hat equals 6.211 plus 0.014 X1, plus 0.383 X2, minus 0.607 X3. This is a standard form of a multiple regression equation you may generate. Now if we look at our estimated multiple regression equation so we have Y hat equals B zero plus B1, X1 plus B2, X2, et cetera, if you look at that and compare it to the equation at the top, you can see that they're very similar. We just have some stand in numbers that we have to interpret. So we have our variables, so X1, X2 and X3 are our variables and we can see that they're in place there at the top and of course in the equation at the bottom. Then we have some coefficients and an intercept. So 6.211 is our intercept which corresponds with B sub zero in the equation below. Then we have .014 there in the blue that corresponds to the first coefficient on the bottom, et cetera, et cetera. So we follow the same basic form. Intercept plus some coefficients paired with a variable. So a coefficient with our first variable, a coefficient with the second variable, in this case a coefficient with the third variable. And again, these are all estimates of the multiple regression model. So how do we interpret the coefficients in multiple regression? They're interpreted a bit differently than they are in simple linear regression. Let's take this example. So we have Y hat equals 27 plus 9X1 plus 12X2. Everything we see here is in thousands of dollars. So X1, that's our first variable stands for capital investment in the thousands of dollars. So X2 stands for the marketing expenditures in thousands of dollars. That's there in the blue. And of course Y hat is gonna be our predicted sales in thousands of dollars. So everything is in thousands of dollars. We have to keep that in mind as we go about interpreting it. So in multiple regression each coefficient, so we have our nine and our 12 up there, is interpreted as the estimated change in Y corresponding to a one unit change in a variable when all other variables are held constant. So what does that mean in this problem? So in this example, $9,000 is an estimate of the expected increase in sales, which is Y, corresponding to a $1,000 increase in capital investment which is our X1 up there. So remember, everything's in thousands of dollars so I was making sure it's actually in dollars here at the bottom. So $9,000 is an estimate of the expected increase in sales corresponding to a 1,000 increase in capital investment, which is X1. Well why's that? With the big coefficient, with our X1 variable up there, it is nine. So if we increase X1 or X1 is the number one, we have nine times one, so that's nine times $1,000. That's $9,000, assuming we hold the X2 over here on the right constant. And that's how we interpret the coefficients in multiple regression. We could flip that and say well, $12,000 is an estimate of the expected increase in sales Y, corresponding to a $1,000 increase in marketing expenditures when capital investment is held constant. So a one unit increase when everything else, all the other variables are held constant. And again, we'll be doing this more in future videos so we'll get some practice at it. But that's the basic idea of how we interpret coefficients in a multiple regression equation. Let's go ahead and do a quick review and then we'll be done with this first video. So multiple regression is an extension of simple linear regression. Two or more independent variables ar used to predict or explain the variance in one dependent variable. Two problems may arise however, overfitting and multicollinearity. So overfitting is caused by adding too many independent variables. They account for more variance but really add nothing more to the model. When multicollinearity happens when some or all the independent variables are correlated with each other and it becomes hard to tell which is actually predicting or explaining the variance in the dependent variable 'cause they're so similar. In multiple regression, each coefficient is interpreted as the estimated change in Y, the dependent variable, corresponding to a one unit change in a variable when all other variables are held constant. And again, we'll practice that more as we go. So this was our first video, just the very basics of multiple regression. We'll be doing more videos in the future and walking through the process by which we examine our variables. We look at relationships among them before we even ever get into conducting the multiple regression using a statistics package like SPSS or R or Excel or whatever. There's a lot of pre work to do and that's what we're gonna cover in the next video. So I hope you found this first video helpful. If you like the video, please give it a thumbs up, subscribe, share it or whatever you wanna do, spread the word. I just do these to help people learn. So I hope you enjoyed it and I'll see you again in the next video. (light music)"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Multiple Linear Regression",
        "Video Title":"Multiple Linear Regression: An Easy and Clear Beginner\u2019s Guide",
        "URL":"https:\/\/www.youtube.com\/watch?v=i3IadpjctWg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/i3IadpjctWg\/hqdefault.jpg",
        "ID":"i3IadpjctWg",
        "Publish Time":"2025-01-08T10:38:25Z",
        "Channel":"DATAtab",
        "Channel ID":"UC3UwrWtAFlAkFl_3Nia756g",
        "Transcript":"hi my name is Hannah and this video is all about multiple linear regression this is the third video in our playlist on regression analysis therefore if you don't know what a regression analysis is or what a simple linear regression is just take a look at the first two videos if you like you can find the content of this video in our book statistics Made Easy the link is in the video description so let's start what is a multiple linear regression a multiple linear regression is a method for modeling relationships between variables it makes it possible to inere or predict a variable based on other variables an example let's say you want to find out what influences a person's salary you take the highest level of Education the weekly working hours and the age of a person you now investigate whether these three variables have an influence on the salary of a person if they do you can predict a person's salary by taking the highest educational level the weekly working hours and a person's age the variables we use for prediction are called independent variables the variable we want to inere or predict is called the dependent variable but what is the difference between a simple linear and the multiple linear regression as we know from the previous video in simple linear regression we use just one independent variable to predict the dependent variable for example if we want to predict a person's salary we use either if a person has studied or not the weekly working hours or the age of a person multiple linear regression on the other hand uses several independent variables to predict or in feere the dependent variable therefore the difference between a simple and a multiple linear regression is that in one case only one independent variable is used and in the other case several both have in common that the dependent variable is matric matric variables are for example the salary of a person the body size or the electricity consumption so unlike simple linear regression multiple linear regression can include two or more independent variables but what impact does that have on the regression equation in the case of linear regression this was our equation we had one dependent variable Y and one independent variable X now in multiple linear regression we have more than one independent variable but don't worry the coefficients b and a are interpreted similarly to those in a simple linear regression if all independent variables are zero the value a is is obtained so we get a value of a for the dependent variable y furthermore if an independent variable increases by one unit the associated coefficient B indicates the corresponding change in the dependent variable okay let's make one small adjustment going forward instead of Y we'll use y head but why in the previous video we learned that regression aims to model the dependent variable as accurately as possible however when working with real world data there's always some error in other words the True Values often differ from the predictions now y hat represents the predicted values from the regression model While y denotes The observed actual values great your gradually becoming an expert now there are four topics to cover the assumptions of regression regression how to calculate a regression with data tab how to interpret the results and finally how to handle categorical variables in regression by creating dummy variables let's start with the first topic so what are the assumptions the first four assumptions of multiple linear regression are similar to those of simple linear regression but there is an additional fifth assumption let's briefly recap the first four assumptions and then we'll go into more details on the fifth assumption let's start with the first one linear relationship in the case of the simple linear regression we were able to test this assumption easily a straight line is drawn through the data this straight line should represent All Points as good as possible if the relationship is nonlinear the straight line cannot fulfill this requirement in simple linear regression we have one independent variable and one dependent variable making it straightforward to plot the data points and the regression line in contrast multiple linear regression involves multiple independent variables which complicates the visualization however you can still plot each independent variable separately against the dependent variable to gain an initial sense of whether a linear relationship ship might exist number two independence of Errors the errors so the differences between actual and predicted values should be independent of each other this means that the arrow of one point doesn't affect another one we can test this with the Durban Watson test number three homoscedasticity or equal variance of Errors if we plot the errors on the Y AIS and the predicted values from the regression model on the xaxis their spread should be roughly the same across all values of X in other words the variance of the error should remain constant in this case the assumption is fulfilled but what about that case here we observe unequal variant at low values of X the errors are small while at high values the variance of the errors becomes much larger number four normally distributed errors the errors should be normally distributed we can test this with a QQ plot or with analytical tests if you like you can check out my video test for normal distribution for a deeper dive and what about the fifth assumption no multicolinearity first of all what is multicolinearity in regression multicolinearity means that two or more independent variables are highly correlated with each other as a result the effect of individual variables cannot be clearly separated why is that a problem let's look at the regression equation again we have here the dependent variable and there the independent variables with the respective coefficients for example if there is a high correlation between between X1 and X2 or if these two variables are almost equal then it is difficult to determine B1 and B2 if both are completely equal the regression model cannot determine how large B1 and how large B2 should be this means that one independent variable can be predicted from the others with a high degree of accuracy an example imagine you're trying to predict theice prze of a house to do this you use the size of the house the number of rooms and some other variables usually the size of the house is related to the number of rooms large houses tend to have more rooms so these two variables are correlated if we now include both in our regression model the model will struggle to decide how much of the price is influenced by size and how how much is influenced by number of rooms this is because they overlap in the information they provide and this is multicolinearity in this case it becomes impossible to reliably determine the regression coefficients if you just want to use the regression model for a prediction the presence of multicolinearity is less critical in this context the focus is on how accurate the prediction is rather than on understanding the influence of the individual variables however if the regression model is used to assess the influence of independent variables on the dependent variable there should be no multicolinearity okay but how do we detect multicolinearity if we look at the regression equation again we have the variable X1 X2 and up to variable XK we now want to determine whether X1 is nearly identical to any other variable or a combination of the other variables for this we simply set up a new regression model in this regression model we take X1 as the new dependent variable and keep the others as independent variables if we can predict X1 accurately using the other independent variables X1 becomes unnecessary its information is already captured by the other variables we can now do this for all other variables so we estimate X1 by using the other independent variables we estimate X2 by using the other variables and we estimate XK by using the other independent variables okay but what is a method to detect multical linearity for all K regression models we calculate R sared which is the so-called coefficient of determination what is the coefficient of determination R SAR if X1 is the dependent variable and the other independent variables are used as predictors R squ explains how well the independent variables explain the variability of the dependent variable therefore a high r squared in this context suggests that X1 is highly correlated with the other independent variables this is a sign of multicolinearity using R squar we can calculate the tolerance and the variance inflation Factor short VI basically the vi is 1 divided by the tolerance if the tolerance is less than 0.1 it indicates potential multicolinearity and caution is required on the other hand and a vif value greater than 10 is a warning sign of multicolinearity requiring further investigation typically statistical programs like data tab provide the tolerance and variance inflation Factor VI values for each independent variable okay but how to address multicolinearity there are two common ways to address multicolinearity number one remove remove one of the correlated variables so choose the variable that is less significant and remove it or number two combine variables create a single variable by combining the correlated variables EG taking an average if you're using data Tab and calculate the regression you just need to click on test assumptions here you can see the table with the top Toler an and the vi all right let's work through an example on how to calculate a multip linear regression and then look at how to interpret the results our goal is to analyze the influence of age weight and cherol level on blood pressure so blood pressure is our dependent variable while age weight and cholesterol level are our independent variables to calculate the regression we check go to data.net and copy our data into the table if you like you can load the sample data set using the link in the video description we want to calculate a regression so we click on regression now we simply click on blood pressure under dependent variable and age and weight and cholesterol level under independent variable afterwards we automatically get the results of the regression we will now disc discuss how to interpret the individual tables if you need an interpretation of your individual data you can just click on AI interpretation at each table and you will get a detailed explanation of your results and if you want to test assumptions just click here but back to the results let's start with the most important table the table with the regression coefficients and then take a closer look at the model summary table we will focus on these three columns here we can see the three independent variables AG weight and cholesterol the first row represents the constant so in our regression equation let's replace X1 X2 and X3 with their corresponding names so we want to predict blood pressure based on a person's age weight and cholesterol levels okay in the first col colum we see the unstandardized regression coefficients these are our coefficients from the regression equation now we can calculate the blood pressure for a given person let's say a person is 55 years old has a weight of 95 kg and a cholesterol level of 180 then our model would predict a blood pressure of 91 so for example if we look at the variable h 0.26 means that for each additional year of age the blood pressure increases by 0.26 units assuming other variables remain constant and what about the standardized coefficients the standardized coefficient tells us the relative importance of each independent variable after standardizing the variables to the same scale why is this useful our model includes variables measured in different units such as age in years weight in kilograms comparing their unstandardized coefficients can be misleading because these coefficients are influenced by the units of measurement for instance if weight is measured in tons the coefficient would be larger if we measured in grams it would be smaller Additionally the values for age in years are generally smaller than values for cholesterol so you cannot directly compare their unstandardized coefficients with each other in contrast the standardized coefficient remains consistent regardless of the units this allows for direct comparison of the relative effects of different variables for example we can see that cholesterol level has the largest standardized coefficient indicating that it has the strongest influence on blood pressure in in our video about simple linear regression we explained the P value in detail the interpretation is similar in multiple linear regression to summarize the P value shows whether the corresponding coefficient is significantly different from zero in other words it tells us if a variable has a real influence or if the result could just be due to chance if the P value is smaller than 0.05 it means the difference is significant in our case all P values are smaller than 0.05 so all variables have a significant influence perfect let's move on to the next table the model summary table first we get the multiple correlation coefficient r r measures the correlation between the dependent variable and the combination of the independent variables what does that mean here we have the equation for linear regression once the coefficients are determined we can sum all up and calculate the predicted values y head of the dependent variable so if we use our example data we have the real blood pressure data and we can predict the blood pressure data with the regression model the multiple correlation coefficient R is now the correlation between the predicted values y hat and the actual values Y in other words the multi multiple correlation coefficient R indicates the strength of the correlation between the actual dependent variable and its estimated values therefore the greater the correlation the better the regression model in our case an R value of 0.27 indicates a strong positive relationship okay and what about R SAR R squar is called the coefficient of determination r squared indicates the proportion of variance in the dependent variable that is explained by the independent variables the greater the explained variance the better the model's performance for example an r squared value of one would mean that the entire variation in blood pressure can be perfectly explained by the variables age weight and cholesterol level however in reality this is really the case and r^ 2 of 0.52 means that 52% of the variation in blood pressure is explained by the model what is the adjusted R squ the adjusted R squ Accounts for the number of independent variables in the model this provides a more accurate measure of explanatory power when a model includes many independent variables the regular R SAR can overestimate how how well the model explains the data in such cases it is recommended to consider the adjusted R squ to avoid overestimation okay and what about the standard error of the estimate the standard error of the estimate measures the average distance between the observed data points and the regression line a standard error of the estimate of 6.6 indicates that on average the model's prediction deviate from the actual values by 6.6 units so if we predict a person's blood pressure using their age weight and cholesterol level our prediction will on average deviate by 6.6 units from the person's actual plot pressure okay if you want an interpretation of the other tables simply click on AI interpretation earlier in this video I mentioned that independent variables in regression analysis can be nominal okay but what are nominal variables noral variables are variables with different categories like gender with male and female or vehicle type but how do we use nominal variables in a regression model as independent variables let's keep things simple let's start with variables with two categories imagine we have the variable variable gender with the categories male and female now we can code female as zero and male as one the category coded with zero is our so-called reference category all right let's take a look at the regression equation suppose the variable X1 represents gender then B1 is the regression coefficient for gender but how do we interpret B1 we said zero is female and one is male so let's just insert this for X1 for a female individual we have 0 multiplied by B1 and for a male individual we have one multiplied by B1 accordingly B1 represents the difference between males and females now that we've discussed how to handle variables with two values let's explore how to approach variables with more than two values let's say we want to predict the fuel consumption of a car based on its horsepower and vehicle type to keep it simple let's say there are only three vehicle types sedan sports car and family van thus we have a variable vehicle type with more than two categories however as we know in regression we can only include variables with two categories so what's the solution this is where dummy variables come into play dummy variables are artificial variables that make it possible to handle variables with more than two categories for the variable vehicle type we create a total of three dummy variables is sedan is sports car and is family van each of these dummy variables has only two possible values zero or one a value of one indicates the presence of the specific C category while a value of zero indicates its absence instead of having one variable with three categories we now have three variables with two categories each these newly created dummy variables can be included in the regression model okay but what does this mean for our data preparation initially we have one column labeled vehicle type where the individual vehicle types from our sample are listed the first entry is San the second is also sedan the third is a sports car and so on from this column we create three new variables for the first vehicle which is the sedan we assign one under the sedan and zero under the others as it's neither a sports car nor a family van similarly the second vehicle is also a seeden the Third vehicle however is a sports car so you assign a one under sports car and zero under the others by doing this we've successfully created our dummy variables one important thing to note the number of dummy variables you create will always be the number of categories minus one so in our case we have three categories so we basically only need two dummy variables why is that the case if we know a vehicle is a sedan we automatically know it is neither a sport sports car nor a family van similarly if we know it's a sports car we can inere that it's not a seedan or a family van finally if it's neither a Sean nor a sports car we know it must be a family van this means we can express the same information with just two variables instead of three including all three variables would make the regression model overdetermined okay but don't worry if you're using data tab it will automatically create the dummy variables for you for example if we select fuel consumption as the dependent variable and horsepower and vehicle type as the independent variables we can then see the three categories here and can select which one to use as the reference category for example if we choose cedan as the reference dummy variables will be created for sports car and family van when we we examine the results we will see the two variables vehicle type sports car and vehicle type family van along with horsepower now that we have a solid understanding of regression analysis including simple linear regression and multiple linear regression we are ready to move forward in the next video we'll explore logistic regression in detail thank you for watching and see you there shortly"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Multiple Linear Regression",
        "Video Title":"Multiple linear regression - explained with two simple examples",
        "URL":"https:\/\/www.youtube.com\/watch?v=AP_K7SaKkIE",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/AP_K7SaKkIE\/hqdefault.jpg",
        "ID":"AP_K7SaKkIE",
        "Publish Time":"2021-09-04T14:34:44Z",
        "Channel":"TileStats",
        "Channel ID":"UCZQqrFW2VkirBF2a-aV1Fdw",
        "Transcript":"welcome to this video about multiple linear regression in this lecture we have a look at the basics about multiple linear regression and learn how to interpret the estimated parameters and how to use them all to make predictions in the previous videos about linear regression we have only used one explanatory variable to predict response variable when we have only one explanatory variable in our model it is called simple linear regression in contrast when we have two or more explanatory variables it is called multiple linear regression for example let's say that we would like to predict the price of a car and use age and mileage as explanatory variables since we use more than one explanatory variable it's called multiple linear regression in the first lecture about linear regression we used the age to predict the price of cars in thousand euros in total we collected the price and age of six similar cars when we fitted a simple linear regression model to the data the optimal values that minimize the sum of squared residuals was an intercept of 30.57 and a slope of negative 3.55 we can interpret the slope as the price is reduced by 3.55 000 euros each year however the age of the car is of course not the only thing that determines its price for example car number 2 and three have the same age but the price is very different in addition these two cars have the same price but car number six is one year older than car number five let's say that we have also collected information about the mileage of the six cars this column shows how far the car has been driven in the units of thousand miles if we plot the price against the mileage we see that the price is reduced when the mileage is increased the additional information about the mileage now explains why the price of these two cars are very different even though they were produced the same year car number two has a lot fewer miles compared to car number three this explains why car number three is cheaper than car number two we can also explain what con number five and six have the same price even though car number six is one year older than car number five this is because car number six has less miles even though it is one year older let's use simple linear regression to create the model where mileage is used as the explanatory variable the slope has been estimated to negative 0.23 this can be independent as for every thousand miles the price is reduced by 0.23 000 euros or by 230 euros so far we have used simple linear regression to grade two models where mileage and age have been used as explanatory variables separately imagine that we will plot price age and mileage in the same figure so that we have a three-dimensional plot if we then fit a plane to the data we would perform multiple linear regression with two explanatory variables by using multiple linear regression we can create just one model where both age and mileage are included as explanatory variables by using a software the following intercept and slopes were estimated for the model the intercept represents the price when both the age and mileage are set to zero in this case we can interpret the intercept as they estimate the price of a brand new car which is here 32 460 euros note that this value is a bit different from the intercepts we estimated earlier when we used mileage and age separately this is expected since the three models are quite different and are based on different data however when we look at the estimated parameter value associated with the age there is a big difference between the two models this is due to that when we include the mileage in the mall we remove the effect of the mileage when we study the age therefore when the mileage is constant then the price is reduced by 1540 euros each year this could be seen as how much the price is expected to fall each year if the car is not driven at all even though we do not use the car the price will still decline by 1540 euros every year because of the fact that the car gets older when we only use the age in the mall the mileage is indirectly affecting the price because all the cars generally have more miles the same is also true for the mileage when we also include the h in the mall the estimated parameter is changed from negative 0.23 to negative 0.15 this change is due to that we remove the effect of age when we start the defect of mileage for example if we compare cars at the same age the difference in price between these cars is then dependent on the mileage if the age is kept constant the price is reduced by 150 euros for every thousand miles in comparison if you only use mileage in the mall the age is indirectly affecting the price because cars with more miles are generally older note that the estimated parameter for the age is about 10 times bigger than estimated parameter for the mileage we can interpret this as driving 10 000 miles has about the same effect on the price as if the car gets one year older without being used once we have established a multiple linear regression model we can use it to predict a price given a certain age and mileage for example let's say that we'd like to predict the price of a car that is two years old and that has been driven 50 000 miles we plug in the values for the age and mileage in the equation and do the math the estimated price of our car is 21 880 euros we'll now have a look at another example where we have data on this historic blood pressure of four men and four women at different ages let's plot the data with different colors for men and women for example this data point represents a 22 years old man with his historic blood pressure of 132 whereas this data point represents the 45 years old woman with a blood pressure of 115 let's fit a simple linear regression model by using h as the exponential variable we see that the line does not fit very well to the data the reason for this is that we have not included the variable gender in the model which seems important because the women generally have a lower story blood pressure compared to the men let's also include the variable gender in the equation and use the software to estimate the parameters note that gender is a categorical variable on a numeric scale with two categories when working with categorical variables in linear regression one has to decide which of the categories that should be set as the baseline i hear arbitrary selected a category woman as the baseline which means that women are coded with a zero whereas demand are coded with the value 1 for example if we use this equation to predict the systolic blood pressure of a woman at the age of 47 we should set the variable h to 47 and gander to zero because the women were coded as zero if we plug in these values and do the math we see that the woman at the age of 47 is predicted to have a cestorial blood pressure of 118.9 if we like to predict for a man at the same age we simply set the variable gender to 1 since men were coded as 1 in this example the corresponding systolic blood pressure of a 47 years old man is therefore 131.8 since we have set the variable gender equal to zero for the women this term can be ignored in the equation if we like to predict historic blood pressure for women we can therefore describe this historic blood pressure for the women by the following simplified equation says the member coded as one this term is simply 12.9 because it is the product of 12.9 and 1. we can therefore describe the systolic blood pressure for the man with the following simplified equation the only difference between the two equations is that we add the value 12.9 when we predict this is early blood pressure for the man this can be independent as men have on average as historic blood pressure that is 12.9 greater than the women let's move 12.9 a bit to the left so that we have the following equation if we add these two numbers we get the following simplified equation for the men we can now start to interpret the estimated parameters in the following equation 99.2 represents the intercept for the wim man since the code of women with a zero the intercept in linear regression including a categorical variable therefore represents the intercept of the category that has been set as the baseline since women were selected as the baseline category in this example which means that they were coded by zeros the intercept corresponds to the intercept for the women the estimated parameter for the age is 0.42 which means that the systolic blood pressure increases by 0.42 when h is increased by one unit in other words a person who gets one year older is expected to increase its systolic blood pressure by 0.42 note that we here assume that both men and women have the same slope later we'll see how men and women can have different slopes the last estimated parameter represents the difference in intercept between men and women men have an intercept that is 12.9 bigger compared to the women by adding these two numbers we see that men have an intercept of 112.1 which is 12.9 bigger than the estimated intercept for the women after we have worked out the intercepts for the men and women we can add the fitted regression lines to the data note that these two lines fit a lot better to the data compared to when we use just a single line for both men and women also note that the two lines have the same slope which means that we assume that the blood pressure increases at the same rate for men and women during aging in this case it seems reasonable that men and women have the same slope because according to the data it seems like men and women increased their historical blood pressure at the same rate during aging however suppose that the data would look something like this by fitting separate lines to the data for men and women it is clear that they do not have the same slope line for the man has a much steeper slope to allow that men and women have different slopes we could add a so-called interaction term to our linear regression model which means that the two explanatory variables are multiplied if we use the software these are the optimal parameter values for this model if we set gender to zero we can simplify the equation to this which predicts this is total blood pressure for the women note that the slope is close to zero which tells us that women barely increase the sisterly blood pressure during aging if you set the variable gender to one we can use the following equation to predict systolic blood pressure for the men if we rearrange this equation a bit and add the numbers inside the brackets we'll get the following simplified equation to predict this total blood pressure for the men the third term in the equation therefore tells us that the intercept for the man is 3.0 higher than the intercept for the women and the last term tells us that the slope is 0.33 higher for the men compared to the women let's use the equation to predict the sisterly blood pressure for a man at the age of 45 if you plug in the age in the equation and the coded value for a man we see that the predicted systolic blood pressure is 130.8 we get the same prediction if we use the graphical method based on the regression line for the man we see that the 45 years old man is predicted to have a sisterly blood pressure of about 130 if instead will predict for a woman at the same age we only change the variable gender to zero and do the math which results in a value 113 a woman at age of 45 is therefore predicted to have a systolic blood pressure 113 this was the end of this lecture about the basics of multiple linear regression thanks for watching"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Linear Regression with Scikit-Learn",
        "Video Title":"Machine Learning Tutorial 3 - Linear Regression Python Implementation with Scikit-Learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=VcrW7BCJLK4",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/VcrW7BCJLK4\/hqdefault.jpg",
        "ID":"VcrW7BCJLK4",
        "Publish Time":"2019-11-08T03:00:05Z",
        "Channel":"ProgrammingKnowledge",
        "Channel ID":"UCs6nmQViDpUw0nuIx9c_WvA",
        "Transcript":"we'll talk about the implementation of linear regression using scikit-learn my name is Ron acquiesce and this video is a collaboration with programming knowledge to watch more videos on machine learning and programming do subscribe to the channel in this video we'll implement linear regression using the scikit-learn library to learn more about what is linear regression you can check out the link to the video in the description the entire code and the data set can be downloaded using the link in the description which will direct you to this github page after this download the data directory and store that in your projects folder let's start with the implementation I'm using a Jupiter notebook here but you can implement the same in a single Python file as well first we start with importing all the libraries and the dependencies that are required we need the pandas library to manipulate the data set next we need we import the matplotlib library to visualize our data and the results we use the PI plot here and lastly we need the linear regression model from the scikit-learn library which is the main dependency so from SK learn dot linear model we import the linear regression class now we start with reading our data into the code using pandas make sure that the data type 3 is in the projects folder we use the read csv function here because our data is in the csv format we move inside our data directory and use the advertising data set it is just check if the spelling is right and yes that should be good now to see what the data looks like we use the head function which is data dot head as you can see here the column unnamed zero is redundant and hence we need to remove it to remove a column we can use a drop function in pandas we have to remove the unnamed column named zero and we specify the access equal to 1 here axis is equal to 1 to remove the entire column and that is equal to 0 to remove only an index as you can see in the output the unnamed 0 column is being rim has been removed all right now our data is clean and it is ready for linear regression for simple linear regression let's consider only the effect of TV ads on sales before jumping right into the modelling let's look at what the data looks like we use matplotlib a popular Python plotting library to make a scatter plot read a set of size of the plot which can be 16 comma 8 then we generate a scatter plot is mascara function in which we have the TV ads and the sales let's color the scatterplot with a black dog with black dots and as you can see there is a clear relationship between the amount spent on TV ads and the sales let's see how we can generate a linear approximation of this data first we convert these values into vectors and then store them into two variables so X is equal to data of the TV ads their values and we convert them into vectors using the D shape function which is minus 1 and 1 then we do we do do the same for the sales which is date of sales and their values which are converted into vectors now after this we use the fit function of the linear regression class to fit a line on the x and y values let's name the variable reg which is linear regression object and then we call the fit function on x and y the minimization of the cost function using gradient descent works behind the scenes here behind the fit function to learn more about the cost function and how gradient descent works you can check out the introduction to linear equation video in the description below now we have fit a straight line to the data set and let's visualize this using a scatter plot again now since the code for visualizing the best fit line is long I'm going to copy\/paste it but the entire code will be available in the github repo now here first we predict all the values on the X data set and then we use those predictions to make a line on the scatter plot here the dots will be in black and the line will be in blue the X label will be the money spent on the TV ads and the y label will be the sales from the graph it seems that a simple linear regression model can explain the general impact of amount spent on TV ads and sales this is how we implement linear regression in scikit-learn life using the second learn library if you have any doubts with implementation please use the comments to ask the doubts and also make sure to check the code in the github repo thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Linear Regression with Scikit-Learn",
        "Video Title":"Linear Regression Python Sklearn [FROM SCRATCH]",
        "URL":"https:\/\/www.youtube.com\/watch?v=b0L47BeklTE",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/b0L47BeklTE\/hqdefault.jpg",
        "ID":"b0L47BeklTE",
        "Publish Time":"2019-07-25T15:43:15Z",
        "Channel":"Python Marathon",
        "Channel ID":"UCub4qT8Sgm7ytZsO-jLL4Ow",
        "Transcript":"In this video we will learn how to use scikit-learn for linear regression. The first thing we need to do is import. We will import a Pydataset to use in this example and let's get panda's and numpy imported as well. Next, we import the things we will need from scikit-learn so the linear regression comes from linear underscore model and will also import from scikit-learn the train test split. and lastly let's get matplotlib in there so we can visualize this model. So, first of all let's get our data. We will be using the pima women dataset. This is about Native American women living in Phoenix. What we're going to attempt to do is use triceps skinfold measurements to predict BMI. So, first what we want to see is if this data looks approximately linear, so we're gonna use the pandas plotting capabilities to plot BMI against skin. So, here on the y axis we have BMI and we have the skin fold measurement and that looks pretty linear so we're gonna go ahead and proceed with our linear regression model. Now we are going to do a test train split. We are doing supervised learning; basically we create the model using only the training data and then we use the model to see how well it predicts the testing data. It will choose a random place to split the train test data; usually at about 70% train 30% tests. And real quick let's go ahead and visualize what that train test split looks like. We're gonna plot this in a scatter plot first. We'll do x_train and y_train and we'll do plt dot scatter with X_test y_test. and we'll make the training data read and the testing data green. Alright, so let's plot that so you can see exactly what I mean. Everything in the red here will be used to create our line or linear model, and that line will then be tested on everything in the green. Okay, so now let's go ahead and actually create our linear model. So, first we'll do LR is equal to a linear regression object and then we'll do LR dot fit and we will plug in the X_train and y_train and we'll have to reshape X_train because the input must be two-dimensional so a reshape of -1 1 should be just fine. And there it is. Now let's use this model to predict on our test data. We will plot that against the scatter plot of the actual test data, so we'll do prediction is equal to LR dot predict and we'll plug in our X_test... and of course what we have to reshape that... so this is what our model predicts the X_test or in other words the skinfold BMI will be and then we'll do PLT dot plot and we'll plug in the X_test and the prediction, make that a blue line and then we'll do a scatter plot of the X_test against the actual y_test then we'll make that green again. Okay, so there it is, it seems to do a decent job at following the overall trend but there could be a few outliers. Maybe this guy up here is an outlier? Alright so there's our model, now suppose we want to see how the model will predict a specific skinfold measurement... say 50. Let's plug that in and see what our model predicts. So with the skinfold measurement of 50 this predicts that the BMI of that woman will be 39 point 5 4 and you can see that in our model right here. At 50 the line goes up to about 39 at that point and that's exactly what this model is doing and how it is predicting. Alright, now we will score the model using SciKit-Learn's built-in score function. So, I'll just plug in X_test and Y_test and it will score how accurate that is. You can see the output is about 0.399. Now, the max it could get would be 1.0. I want you to think about what that score means. Is this a good score? Is this a good model? Write me what you think in the comments below. So there you have it, that is how you can use pythons scikit-learn to create a linear regression model. Please check out some of my other Python videos and please subscribe for more Python content. :D"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Linear Regression with Scikit-Learn",
        "Video Title":"Simple Linear Regression in Python - sklearn",
        "URL":"https:\/\/www.youtube.com\/watch?v=feDJkDaNuOk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/feDJkDaNuOk\/hqdefault.jpg",
        "ID":"feDJkDaNuOk",
        "Publish Time":"2022-05-03T13:15:03Z",
        "Channel":"RegenerativeToday",
        "Channel ID":"UCzJgOvsJJPCXWytXWuVSeXw",
        "Transcript":"hello everyone this is vishweeta in my last video i explained how a simple linear regression works in this video we'll work on a linear regression problem using python cyclic learn library you will see how easily you can perform machine learning using this library we are going to use the famous iris data set we can load that data set directly from c1 library load data set iris so this is the iris data set we have simple length simple reads period length period width and spacious and look for a simple linear regression problem we do not need all these variables if you remember this diagram from my last video look in simple linear regression we have only one dependent variable and one independent variable right so we need one training feature this is the machine learning language training feature i will explain it later and we need one label this is one the dependent variable so we will keep only two variables and exclude the rest of them because we do not need them so iris iris fiddle length you know width let's see so for an iris data set now we have only piddle length and piddle width as per this equation we need one x and one y and this m and c are the training parameters that we will find out we will calculate later on so let's see iris this film length is our x and the spiral width is our wine petal width so using x and y we need to find out m and c the training parameters so that if we have x we can predict or estimate y we will explain some more later as we mentioned in our last video that we need to have some relationship some correlation between x and y to be able to perform a linear regression so let's see if this x and y have some kind of correlation so import math plot leave that pi part as plt so plt dot skater x y look at it we actually have some correlations some good positive correlation plt dot x label pedal length and plt.y label uh pill width before we move on to any machine learning we usually split the dataset into training and test data set so we can use the training dataset for training purpose to train the model and we can test the model using the test data set to do the splitting we have the function train test speed so from sk learn dot model selection import train test split now x train into this x strain x test y train y test then train test split x y okay now we have to give a test size 0.4 and random state uh 23 what test size 0.4 means it means that it will keep 40 of the data for testing purpose as x test and y test and why random state 23 for that let's see x train look at the x strain here look at this row number 77 29 92 23 so it takes the random rows it doesn't take top sixty percent or bottom sixty percent for the training data or top forty percent of the bottom forty percent for the testing data it takes it random rows to make up that sixty percent or forty percent if you run this train test split again it will take different random growth so if we want to recreate the same splitting we have to use a random state and this 23 is not constant you can use any other number but when you regenerate the data you have to use the same number this screen is one dimensional in psychic learning libraries the machine learning algorithms take two dimensional data so we need to reshape this x t x strain and x test to two dimensional data so for that we need to import numpy libraries first and then x screen we need to np dot array is to convert it to array x train dot reshape minus 1 1 look extreme here it is it's two dimensional now look at it we are going to do the same with x test okay x test x test became two dimensional as well so data is original so we need to import the linear regression from scikit-learn library first from sklearn dot linear model import linear regression now let's say the linear regression model to a variable linear regression so this is the model we need to fit our x crane and y train and our dot fit we need to feed our x screen and while train to this linear regression model what this will do we talked about how we keep changing m and u and we find the optimum m and c for this best fit line to fit this straight line to the dots when we fit x strain and y train to the linear regression model this is what happens it finds this m and c to find his best fit line generally m is called the slope of the line and c is the intercept line but in machine learning m is called the coefficient and m and c are called the training parameters when we fit x strain and y train to this linear regression model this is called the training that means we actually trained this model and after this feeding process is done that means the training is done this lr model should have the training parameters m and c okay let's see lr dot intercept so we have the intercept and look at the coefficient lr dot square m or not n this is the m the coefficient or the slope for the linear model now we already have m and c so if we have the x now we should be able to estimate the y let's see if we can estimate the y y red plane so first we take m times m times x train plus c so okay instead of this let's put underscore y correct frame so this is how it looks like okay but for scikit-learn library there is a predict formula so you can use directly the predict formula internet credit x train we already fit x screen and y change to the error that means error should be trained and it should be able to predict now okay let's do y right train one now one correct pin one you can see here okay i will flatten it okay so this one is one dimensional so let's make this one one dimensional two so that we can compare dot flatten so it will become one dimensional so you can see that the values are exactly the same one point seven three three zero point three one five one point seven two three zero one three one five one point nine eight one point nine eight so these are exactly the same thing so you do not need to calculate like this you can simply use the predict function and it will calculate it for you let's check if we found the best fit line okay let's just copy this and paste it here x strain y train because we actually used x plane and y train right so i'm using that control c x train and i will use y try to train one here okay and that color red so we plotted x screen versus y frame and then explained versus this prediction now see we found our best fit line this red line this is the best fit line isn't it instead we can actually put the line now if we have the pita length using the model we can actually get the pillar width even if the piddle grid is not directly on the dot but at least it will be close enough isn't it if it's here at least we'll find the value here so it's going to be at least close enough we used x strain and white and all this time now what to do with x test and y test we used extreme and y trains to train the model right here but the model have never seen x test and y test so we can test this model using x test now let's see the way it was able to predict for extreme if it can predict the same for x test let's copy this and x test instead of extreme let's see x test y test one so yeah indeed predict something but let's see how right or how good the prediction is we are going to plot the same so x test y test and this time we're going to do x test and y prediction on test data so look at it it was able to find the best fit line for x test as well though the linear regression model never have seen x test so it can generalize it can predict pedal width for any pedal line so this was a simple linear regression where we had only one training feature pedal length to predict piddle width but it doesn't have to be on the one training feature linear regression can work when there are multiple training features as well and we call it multiple linear regression in my next video we are going to work on a multiple linear regression where we will predict the housing price using several training features all right see you in next video thank you so much for watching"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Linear Regression with Scikit-Learn",
        "Video Title":"Mastering Multiple Linear Regression in Scikit-Learn: A Step-by-Step Guide",
        "URL":"https:\/\/www.youtube.com\/watch?v=R2Zb5s_RrDU",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/R2Zb5s_RrDU\/hqdefault.jpg",
        "ID":"R2Zb5s_RrDU",
        "Publish Time":"2023-10-06T14:30:09Z",
        "Channel":"Ryan & Matt Data Science",
        "Channel ID":"UCKq-lHnyradGRmFClX_ACMw",
        "Transcript":"in today's video we're going to be taking a look at how you can create a multiple linear aggression within python with the help of the S kit learn library now what essentially differentiates a multiple linear aggression from a simple well in fact there's going to be multiple independent variables also known as features that outcome the dependent variable and in this video I'm going to show you an example within the sport of baseball there's going to be many factors that determine how many wins a baseball team actually has at the very end of the season so you can take a look at things from the Batters perspective such as like a team's aggregate total of hits doubles triples or home runs or you could even take a look at from the pitcher perspective right what's the team ER or the Whip or how many strikeouts or walks does the pitcher or pitching staff in general have at the end of the season now the form is going to be a little bit different uh our our simple is typically going to be yals MX plus b again very very basic uh but with a multiple it's going to be something like y = M1 X1 + M2 X2 + M3 X3 it could continue to go on depending on how many uh variables you use all the way up to the very end which you do plus b which is going to be your intercept hope that isn't too confusing it's going to be pretty simple once you see how this code specifically works now I'm going to jump on my jupyter notebook right now and code this with you guys all right let's get started so the first thing that you want to do is Import in a few different libraries but we're going to start with importing in pandas as PD shift and enter runs the cell above creates a brand new one down below then we're going to import random and then we're going to import numpy as NP and um we're going to use random and numpy essentially for our data creation so I'll show you how we're going to do this I generated this code with chat GT p and that way you guys don't have to download any csvs and shouldn't have any issues recreating what I do in this video so let's start off with that so we're just going to put data equals to and we're going to essentially put two like this right and then we're going to say for blank in range of 500 right and then we're going to start building out our different columns in this future data frame and future it's not a data frame yet um so we're going to have team name first right and then we're going to say that's F and put this team and you don't need to know specifically how all this code works for this video because this video is more based around multiple linear regressions than it is essentially building out a random data frame but hopefully this does help that side of things we're going to move that over here that's we're going to say season BR in 201023 and we're going to say wins random and in going to see 50 through one 10 and let me just explain real quick what these randoms are going to do is essentially assign anything from like this one is 2010 through 2023 or 50 through 110 as you have like a season for baseball team the number of wins then we're going to take a look at losses which is just rate 162 if you're familiar with baseball minus the wins over here we're going to do hits equals random Rand in we're doing 1200 through 1600 we're going to do doubles equals random. Rand ins say 200 through 350 we're going to say triples equals random Rand 10 through 40 and then two more and essentially we're going to be pretty much done with these say home runs equals random Rand in 100 through 250 and then lastly strikeouts equals random Rand in 1,500 okay now we're going to do a few different adjustments to this data frame that way we can actually have a kind of a linear model based around what's happening over here otherwise it'd be truly random and you're not going to really have good results with that so we're just going to say hits adjusted equals hits plus wins minus 80 times five okay then doubles adjusted equals Doubles Plus we'll just copy this over here we're going to times it by two this time then we're going to do triples adjusted equals triples plus and that's just going to be WIS minus 80 and lastly what we're going to do we'll wave two more home runs adjusted equals home runs and then just copy this and this over here for doubles should be a two not a three and lastly we're going to strikeouts adjusted equal strikeouts and this is going minus we're going to multiply this by 10 okay and now we're going to have to append it some data so we're going to say data. append again any of this is confusing don't worry about it because literally we're just going to use this for our data in this video feel free to skip ahead if you have your own CSP that you want to try this out on and no worries on that but this is kind of fun it's adjusted and just copy all these right doubles adjusted triples adjusted home runs in the strikeouts again also all generated from chat GTP okay so now that is over here which is good and shift and enter and hopefully this works with no errors now we have to Define essentially our column names so we're just say columns equal and then we'll put over here like team season wins losses hits doubles triples runs SS runs strike outs like that and lastly let's create our data frame so DF equals pd. data frame throw in our data in columns equal columns and just rewrite you don't need to know any of this right here this data I mean you should know how to create columns and essentially build this into a data frame but right here a little bit more complicated stuff in um just again creating the data that we're going to be using now just to show you what this essentially looks like little pandas practice DF do head right we have our team so team h u JB D Seasons wins losses hits doubles triples home runs and also strikeouts now let's take a look at what just a very basic plot would be and going to import two more libraries Seaborn as SNS and then import matplot li. pyplot as PLT now you don't need background knowledge on both of these for this video again but I think it is helpful and you should know at least the basics of plotting you can use chat GTP and you can create a lot of different charts but at least having the fundamentals does help and um what we're going to be plotting is hits and wins and then just data equals DF again a full video on caborn if you do want to check that out not M plot libia but it will be in the future and um there we go okay so you can see it's a linear relationship right as hits go up for a team in general it's expect that they win more games and there's not many teams that win 110 games again just random data that I generated right um but you can see that linear relationship over here and I also want to show you that a Nega linear relationship between two different variables so ss. LM plot we're going to throw in over here our x equal strikeouts y equals wins theta equals DF and then we'll just do plot. show again and you can see that there's a negative relationship between strikeouts and also wins and baseball this is kind of changing a little a little bit it probably isn't as dramatic as a as a line of this is with strikeouts and also wins but for the video again random generated and also why not right we need some practice so okay with that being said we have our original data frame over here DF let's get into our multiple linear regression now if you're familiar with running different models in psyit learn we got to drop some categorical data and over here we have our team name we don't need the team name the team name should not impact essentially how many wins the team has in a season also at the same time I don't really think we need the season now baseball had a shorten season because of Co um this data does not really factor that into it so it's really not necessary on that side of things and then I don't think we really need losses so just some good practice to dropping some columns do apologize you coded those up over there but it's a good practice right so let's drop those first we'll just build a new data frame df2 and we'll just put over here DF do drop you put over here columns equal to and then just throw them in over here so first name is going to be team and you're going to have over here season right and lastly we're going to put over here losses and then we'll put axis equals 1 and guess what if we go over here to df2 head look we have now winds hits doubles triples home runs and strikeouts which is great and I'm going to also do one thing because we're going to start splitting our data between our X and also our y I'll explain that in a second but I'm just going to put over here df2 columns and it's going to make this much easier for me just to copy over stuff so we have all these over here and let's split up our data so essentially if you're familiar with some machine learning models we need to have our X and then also our y so essentially what our X is going to impact our y so hits doubles triples home runs and strikeouts that's all going to impact the amount of wins that a team has in a season and again I know baseball there's a lot more things to look at than this is very simplistic but I just wanted to make it easy for you guys in this video so we're going to set our ax to hits doubles triples home runs and strikeouts and R Y which is often known as a Target is going to be our wins so let's split up this data frame with that so first we're going to say x equals DF and then let's just copy essentially all this over here and just throw it in and what's kind of nice is when you put columns already single quotes and commas so it can save you some time right and then y I'm just going to say is going to be equals to DF and we're just going to put our winds over here that's our other column so now our X is everything else our y so we're good right and before we run our linear regression we still have another step that needs to be taking place we need to split up our data again so we're going to split up our data into a training set and then also a testing set and that's done through something called a train test split essentially you don't want to train all your data on the full set that is available out there you want to have some of it just for training and and then some of it for testing purposes so let's do that so the first thing we're going to have to do is from sklearn do model _ selection import train test splits so now that is imported in and by the way again if you're brand new to pyit learn which some of you are some of you have been watching that all the categorical stuff you'll learn really fast like where to look these things up like I I still look up essentially where all the different libraries are within scit learn and I'm sure it'll ingrain my memory over time but it just takes two seconds to research like train test split you Google search ons kit and then you essentially get this code that you can put in over here so just a little bit of tidbit because I know we have imported quite a lot of different libraries and if you've seen some of the other kagle projects there's way more in there I don't memorize those I just Google search them and throw them in so okay a little bit tangent there now we're going to do X train we're going to do our X tests which if I can type correctly we're going to do our y train and then y test we're going to say that's equal to train test splits X Y test size equals 0.2 and we'll do a random State random State equals and then we're going to put in Miguel caber's number he just retired essentially today um I'm going to look it up and it's kind of embarrassing I think it's 24 but I could be wrong um it is number 24 see I should have went with my judgment um but we're going to put that over here Miguel Caba watched him a lot when I was growing up playing baseball so okay random 824 test size 0.2 one other thing to call out to is when you build out your models you're going to see more normally capital x lowercase y make sure you do that kind of standard practice it will work if you have lowercase x or also uppercase y but everything that you see online capital x lowercase y so make sure to kind of ingrain that essentially what we're doing with this over here we have our train and our test right y train y test we split it up over here our test size is 20% So 20% of our data is testing set 80% is going to be our training set and then random states that way you guys can copy and get the same exact results I have so okay now that is essentially built in over here we can work on our linear regression let's bring in our linear regression so from SK learn. linear model Import in linear regression okay so now that you have that over here then we need to call our linear regression so essentially I always just put LR some people put model whatever like it's fine linear regression and the reason why I put LR is when I'll do like a CLE project or just in general um if I deal with multiple models I'd rather just have the initials and that's just my personal preference so we have that over here I'm not going to put any parameters in here we'll talk about more parameters um with regressions in future videos as well as like hyper parameter tuning and things like that but I'm just going to put very basic right LR equals linear regression now you have to fit your data so you put something lr. fit and you're always going to fit your training set of data so we're going to have X train and you're going have also y train and that now fits it and then you get this little icon down below which is very basic right now because there's no parameters in there and also we're not doing any pipelines s pipelines get a little bit of fun these get expanded out quite a bit all right so now our data is fitted to this linear aggression we can kind of see how well it performs and there's a few different metrics on this now you want to be super lazy and not import anything in you can just do lr. score over here and this is going to get you your R2 scored or also known as R squ now to get your pretty accurate version of it you want to do your X test and then also y test but you can also do this on your training set so you can see like we have 0.87 over here which is pretty good overall I'll show you how to do it as well uh the proper way which I personally prefer because it doesn't confuse me what this score means and then you can see our train was a little bit higher than our test so let's bring in a few different methods and the a few different ways we can do this is through sklearn metrics so we're let's import those in so from s learn. metric import we're going to bring in mean AB solute error we're going to bring in mean squared error and then just to show you that it's the same thing we're going to do R2 sore and I also did that in the simple linear regression video maybe you didn't watch it but uh we'll show you that over here so okay now that we have all those I'm going to build out a few more cells and let's get going the first thing we're going to do is do a y prediction and this might be a little bit confusing but just stay with me on this we're going to do predict and we're going to throw in our X test all right so let me explain why we're doing a y prediction you might be thinking like what isn't it just y test well with your model over here you have a predict function right and we're throwing in our X test so this is going to be our prediction result based off of X test this y prediction is going to be different than your y test and this is a way um through over here me error means squared error and then you're in your R2 score you can see how accurate your model specifically is so you'll see this also a lot with classification problems we see like a y PR over here uh and you have your model name. predict and you throw in your X test it threw me off a lot in the beginning when I first started machine learning um but it's been ingrained so I just always want to call this out in videos so that way you guys can understand why this specifically is happening so okay let's take a look at the first one over here which is going to be our mean absolute error so I'm just going to copy this because I am lazy right now as I'm recording a YouTube video of course and then we're going to throw our y test in here and then throw in this y prep right and then you can see that we have a score of 4.67 now our mean absolute error it measures the average absolute difference between the predicted values and then also the actual values now there's another way that you can do this and it takes a look at the squared values and I guess should know it's a mean squared error so this is going to be a lot higher score right just copy this over here throw this over here and we get 33.25 and lastly we've already done this but just to show you for R2 score can throw this over here right y test y predict and 08735 and look at that when I talked about X test and Y test 08735 the same exact thing personally I prefer just having R2 score over here because in my mind it it it recognizes that rather than over here where score it's kind of ambiguous so that's my preference if you want to just put in score and not Import in R2 score all good there again just something I do all right lastly we're going to be taking a look at coefficients and also your intercept now since this is a multiple linear aggression you're going to have multiple coefficients associated with it but you're still only going to have one intercept now the coefficients are going to be as an array so let's do that right so you can just put lr. coof like this fortunately does have underscore at the end so make sure to do that and you can see the array right 0.013 0.042 0.44 0.05 and then 0.16 and this is associated with each of those things from above right we have over here hits doubles triples home runs and strikeouts right so just the strikeouts for example right the more strikeouts a team has the less likely they're going to win a game and that's based off of this data over here right then you have the hits data right the more hits someone has you go down below the more likely they're going to win a game same with double triples and home runs so that's essentially how this is built out and then let's take a look at our coefficient right so you can just put over here lr. intercept and now we can just take a look at our intercept so uh that's going to essentially be the lowest amount of games over here would win and you can see 49.76 which if you go back over here and we talk about number of wins a team has Right started at 50 to go through 110 so essentially hit that 50 number that was at the very beginning that we called so just kind of wrap everything up together kind of go through this code all right we imported pandas random numpy this this isn't as important again just how we generated some of the data essentially um we used numpy over here this was all generated through chat GTP randoms and then we essentially kind of cheated and kind of pre-built a linear regression with all this over here but again it's for the video for testing purposes built out our columns right and then we built out we have our data which over here right and our columns over here to built out a data frame and not super important you can start right over here first thing I wanted to do is just take a look at an example right hits versus wins which makes sense the more hits teams has more likely they are to win a baseball game then we have strikeouts versus wins uh the more strikeouts the team has the less likely they are to win a game then we have over here we're setting up df2 we're dropping our categorical information of Team season doesn't matter and then losses also does not matter for this model or df2 has been cleaned up grab the columns over here we split between our X and then also our y so that way our X can predict the Y which is going to be our Target then we split up our data between a training set and then also a testing set we brought in our linear regression we fitted this to the training data then we did a few different tests down below to see how accurate our model was mean absolute error mean squared error and then also our R2 score and lastly to see the coefficients we just put LR coefficient and then our LR intercept at the very beginning hope you did learn something brand new in this video if you did make sure to subscribe to the channel it's 100% for free and helps share this video to other people online that are trying to learn data science I upload three videos every single week and if you want to learn even more about s kit learn I have a full playlist right over here which I intend to make over 50 videos just dedicated to this Library so make sure to check them out over there"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Polynomial Regression",
        "Video Title":"Polynomial regression",
        "URL":"https:\/\/www.youtube.com\/watch?v=QptI-vDle8Y",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/QptI-vDle8Y\/hqdefault.jpg",
        "ID":"QptI-vDle8Y",
        "Publish Time":"2020-06-06T14:34:13Z",
        "Channel":"Mike X Cohen",
        "Channel ID":"UCUR_LsXk7IYyueSnXcNextQ",
        "Transcript":"I'm going to introduce you to something called a polynomial regression you will see that in some sense it's basically the same thing as the other regressions we've been working with so far but in another sense it's a little bit different so we have to work with the data in or with the model in a slightly different way ok and actually I'm gonna even gonna start by just reminding you of what a polynomial is so a polynomial is basically any expression that looks like this where we have our data X and then it's taken to increasingly high powers and then we have some coefficients that are scaling the data so a zero is it's kind of not multiplying by X but you could think of this as being multiplied by X to the zero and then we have a 1 times X to the power of 1 a 2 times x squared bla-bla-bla up to a sub n so some coefficient some number a n times X to the nth power and X is our data okay so here are two examples of polynomial expressions so you see this would correspond to a zero this would correspond to a 1 five six here would correspond to a two and so on now for a polynomial you don't need to have every single X term in here you can just imagine that there you know so in this example a sub 2 would be equal to zero so you could you know think of writing zero times x squared but in practice that's just left out same thing with with a zero here is also set to zero so there's another concept in polynomials which is called the order so the order of a polynomial is basically n here it's the highest coefficient so this is an nth order polynomial this is a third order polynomial this is also a third order polynomial sometimes people get confused about this because there's only two terms but the order of a polynomial doesn't refer to the total number of terms it refers to the largest exponent in any of the terms so third order polynomial okay so here's a polynomial regression technically this looks like the simple regression that you saw several videos ago but so you can see this is beta zero times X to the 0 which earlier I was calling L a 0 and I call it beta 0 and now as beta 1 times X to the power of 1 so this simple regression is actually nothing more than a first-order polynomial regression ok but with polynomial regressions we can add more terms here so we can add something like this so we say y equals beta 0 the intercept plus beta 1 times X plus beta 2 times x squared plus the residual term and on and on on and so we can get this to be higher order so let's say beta K times X to the power of K so this would be a chaos order polynomial regression now here's a question how can we fit this kind of model look at these nonlinearities there's nonlinear term so you know these are here X is linear but now we're bringing X to higher and higher powers and didn't I say in the beginning of this section that regression is all about a general linear model so it has to be linear so how can we fit this polynomial regression if we have these squared terms in here which are and higher order power terms which are nonlinear well you have to remember the other thing I said about what needs to be linear and what can be nonlinear so in fact the coefficients are all linear the betas are all linear they're just being regular multiplied there's no squaring happening to the betas so in fact it may not look like it initially but a polynomial regression is a standard regular run-of-the-mill vanilla plane general linear model and that's because we're not doing anything nonlinear with the coefficients it's just the data themselves that are getting raised your higher powers okay so here is what here you see two examples this shows you when you would use a polynomial regression so here we have our data looks like this the blue circles are the data blue circles are the data so you can see that a line is not going to be a really great fit to these data set so this is a second-order polynomial so that means that this polynomial has the form y equals at beta 1 which is the intercept plus beta 2 times X which is a linear term plus beta 3 oh wait I miss counting my betas anyway beta 2 would be times x squared and that gives us this quadratic form here so this part we need the x squared in here the beta coefficient on the x squared in order to get this model to fit and this would be a third order polynomial so you'll probably recognize the general form of x cubed in this data set here so these would be the kind of data that you would use for a polynomial regression now of course that leads to the natural question of what is the appropriate order because when you when you set up a polynomial regression you are given so this X this is one independent variable I can multiple I can bring it up to any higher power that I want we could go up to you know beta 1000 times X to the power of 1000 I mean maybe that's a little high to be sensible but you know there isn't a real hard limit for how many powers we can include here I suppose you could say that the limit would be when K exceeds the number of data points because then we can no longer fit the model but that's only a turns out to be just a soft constraint so how do we know what the what order the polynomial should be so that's a good question here you see an example of this data sets that I showed in the previous slide with an order of two so visually this doesn't look good right I mean we can it's trying you know that the model is trying to fit the data it gets an A for effort but you know if this is just not a good model and here we have increasingly higher orders so here we have a model order of 17 and I showed this slide actually in towards the beginning of this course and this is something about overfitting we are overfitting data I'm gonna talk more about overfitting in regression modeling soon in in several videos from now okay but so how do we know what is the appropriate order that we should use this is too small the order is too small here the order is too large we're overfitting we're capturing too much of noise here okay so the way that we go out about figuring out the appropriate order is through something called the Bayes information criteria it's often just abbreviated as bic so this is Bayes information and criterion and so the idea is that this is a quantity that we can compute for every model order so we get up to K model orders so we can compute K BICS and this is a formula so it's n where n is the number of data points that you have times the log the natural log of the sum of squares of the residuals and I showed you the formula for this quantity several videos ago plus K which is the number of parameters times the natural log of n ok so then what you do is you run through a bunch of different model orders and you fit them all and then you look at a plot of these bits and that's going to look something like this so this is the BIC plotted as a function of the polynomial order and then what you want to look for is a minimum point here so the minimum point is the optimal model order so in this case that would be 3 which is actually consistent with these data and in fact I created these data as a third order polynomial plus noise so it's no surprise that a third order polynomial turns out to be the best fit okay so that's the Bayes information criteria you fit the model with a bunch of different order parameters so you you rerun a bunch of regression models and you take the model that has the smallest Bayes information criteria as being the optimal model order so in this video I showed you how to interpret and compute and set up a polynomial regression"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Polynomial Regression",
        "Video Title":"Polynomial Regression",
        "URL":"https:\/\/www.youtube.com\/watch?v=nGcMl03LPC0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/nGcMl03LPC0\/hqdefault.jpg",
        "ID":"nGcMl03LPC0",
        "Publish Time":"2015-02-23T19:55:32Z",
        "Channel":"Udacity",
        "Channel ID":"UCBVCi5JbYmfG3q5MEuoWdOw",
        "Transcript":"Alright. So we talked through how it works when you've got you're trying to fit your data to a constant function, to a zero order polynomial. But let's, let's at least talk through how you do this in the more general case. This is, this is what I've been doing to, to fit various curves to the data at least implicitly. So, what we're really trying to do is we've got a set of data, x and y. Set n, n examples of x's and their corresponding y's. And what we're trying to find is these coefficients, C0, C1, C2, C3. Let's say if we're trying to do cubic regression where C0 gets added to C1 times x, which gets added to C2 times x squared. Which gets added to C3 times X cubed and we're trying to get that to look a lot like y. Now we're not going to get to exactly equal y but let's pretend for a moment that we could. We have a bunch of these examples and we want it to work for all of them. So we can arrange all of the, all these constraints, all these equations into matrix form. If you're familiar with linear algebra. So the way that we can write this is here are the, here are the coefficients that we're looking for, the C's, and here are what we're going to multiply them by. We're going to take the X one and look at the zeroth power, the second power, the third power. And that equation I'll use my hands cause that's I always, I always need to use my hands when I do matrix multiplication. So you're going to across here and down there to multiply these and add. And that needs to correspond to y1. And same thing this now the second row. Multiplied by these coefficients. Need to give us our y2 and so forth. Alright. So if we arrange all these x values into a matrix, and we'll call it, you know, x. And then we have these other guys. And we'll call this w, like the coefficents. Obviously w stands for coefficent. And we want that to sort of equal This vector of y's. And we basically just need to solve this equation for the w's. Now, we can't exactly solve it because it's not going to exactly equal, but we can solve it in a least squares sense. So let me just step through the steps for doing that. Alright, so let's, so here's how we're going to solve for w. So what we're going to do is premultiply by the transpose of x. Both sides. I mean really what we wanted to do at first is if we are solving for Y, we need to multiply by the inverse of X, but this isn't really going to be necessarily well behaved. But if we pre mulitplied by the X transpose then this thing is going to have a nice inverse. So now we can pre multiply by that inverse. All right. Now, conveniently because this has a nice inverse, the inverses cancel each other. [NOISE] We get that the weights we're looking for can be derived by taking the x matrix times its own transpose, inverting that, multiplying by x transpose and then multiplying it by the y. And that gives us exactly the coefficients that we need To have done our polynomial regression. And it just, it just so happens that we have some nice properties in terms of these x transpose x. Not only is it invertible, but it does the right thing in terms of minimizing the least squares. It does it as a projection. Now, we're not going to go through the process by by which we argue that this is true. >> Does it have something to do with calculus? >> It most likely has something to do with calculus. And we'll get back to calculus later. But in this particular case we can, we're just using projections and linear algebra. And most importantly the, the whole process is just we take the, the data we arrange it into this matrix with whatever sort of powers that we care about. And then we just compute this quantity and we're good to go. >> Okay."
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Polynomial Regression",
        "Video Title":"POLYNOMIAL REGRESSION",
        "URL":"https:\/\/www.youtube.com\/watch?v=Qnt2vBRW8Io",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Qnt2vBRW8Io\/hqdefault.jpg",
        "ID":"Qnt2vBRW8Io",
        "Publish Time":"2017-10-26T18:23:03Z",
        "Channel":"Art of Visualization",
        "Channel ID":"UCHBWJGoZMkhJyElgvuN1U1w",
        "Transcript":"into it we already know a couple of types of regressions we know the simple linear regression which we can see over here then we've also discussed the multiple linear regression which is written out over here and finally we've got the polinomial linear regression which is written out here so notice how it's very similar to the multiple linear regression but at the same time instead of the different variables like X2 X3 X4 and so on xn we have the same variable X1 but it is in different power so instead of X2 we have X1 squ instead of x3 we would have X1 cubed and so instead of xn we would have X1 to the power of n so basically we're using one variable but we're using the different powers of that variable so let's have a look at when you use a polinomial regression when it would come in handy let's say we've got a observation a set of observations which look like this then the line that fits this data is obviously a simple linear regression as you can see it f fits it quite well but let's for a change say that the data set looked something like this so if we try to use a simple linear regression here where is expressed like that you'll see that it doesn't fit quite well so in the middle you've got data underneath and then as you go further the data will be above the line so how can we correct that well we can try to correct that by using a polinomial regression let's have a look so instead of the linear regression we're going to conduct a polinomial regression and that in this case fits perfectly and what is the formula well that is a formula for this particular case yal b 0 so that's the constant plus B1 X1 so that's a simple linear regression part but then we're adding the B2 X1 SAR and the B2 X1 s gives it that parabolic effect so that the curve becomes parabolic and therefore it will fit this data better as you can see polinomial regression is a bit different to simple linear regression and at the same time it has its own use cases so it's all comes on a case- by casee basis you you have a problem and then you might try a simple linear regression a multiple linear regression if you have many variables or you might try a polinomial linear regression and see what happens and sometimes the polinomial regressions do work better for example they're used to describe how diseases spread or pandemics and epidemics spread across territory or across population polinomial linear regressions can be handy there and they also have other use SC so it's a matter of what we expect so it's always good to have more Tools in Your Arsenal and we have one final question left the question is why is it called linear still right so we saw those different powers squared cubed to the power of n and so on why is it still called linear and I'll show you what I mean if you look on the left here it says polinomial linear regression so why is it still called a linear regression if it's a polom regression well the trick here is that when we're talking about linear and nonlinear we're not actually talking about the X variables right so even though they're nonlinear here the relationship between Y and X is nonlinear when you're talking about the class of a regression you're talking so whether it's linear or nonlinear you're talking about the coefficients here so that's the interesting part so whether or not this function which we have here so Y is a function of X right and so the question is can this function be expressed as a line combination of these coefficients that because ultimately they are the unknowns right so your goal when you're building a regression is to find these coefficients find out their actual values so that then further down the track you can use those coefficients to then plug in X and predict y whether it's a linear s simple linear multiple linear regression or polinomial linear regression that's your goal to find these B coefficients and that's why linear nonlinear refers to the coefficients so an example of a nonlinear regression would be if the equation was y = b 0 + B1 X1 divided by B2 + X2 or something like that or B 0 divided by B1 + X1 so a situation where you really cannot replace the coefficients with other coefficients to turn the equation into a linear one in regards to the coefficients not the X values so there you go that's why polinomial regression is still called a linear regression that's your fun fact for the day and maybe you can show off to your colleagues and also because of that the polinomial linear regression is actually a special case of the multiple linear regression so that's just something to also kind of note that this is a version of the multiple linear regression rather than a standalone absolutely new type of regression so I hope you enjoyed today's tutorial and I look forward to seeing you next time until then enjy machine learning"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Polynomial Regression",
        "Video Title":"Polynomial Regression ! All you need to know",
        "URL":"https:\/\/www.youtube.com\/watch?v=PVxpRs25TAA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/PVxpRs25TAA\/hqdefault.jpg",
        "ID":"PVxpRs25TAA",
        "Publish Time":"2020-07-05T14:21:49Z",
        "Channel":"Priyanshu Vats",
        "Channel ID":"UCCAML70J3jlTwA614L3tEow",
        "Transcript":""
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Polynomial Regression with Scikit-Learn",
        "Video Title":"Polynomial Regression in Python - sklearn",
        "URL":"https:\/\/www.youtube.com\/watch?v=nqNdBlA-j4w",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/nqNdBlA-j4w\/hqdefault.jpg",
        "ID":"nqNdBlA-j4w",
        "Publish Time":"2023-06-13T01:44:53Z",
        "Channel":"RegenerativeToday",
        "Channel ID":"UCzJgOvsJJPCXWytXWuVSeXw",
        "Transcript":"my today's video is going to be on polynomial regression we will start with a high level overview of polynomial regression and then being the Implement a polynomial regression in Python's psychic learn library or you will also do a little experiment on overfilling today if you don't know what overfitting is you are going to learn it today so what polynomial regression is it is actually built on linear regression and it is built based on the limitation that linear regression can perform well only if there is a linear correlation between the input variables and the output variables something like this but the relationship can be something like this or something like this or any other shape right a linear regression actually cannot handle this kind of shapes or any other different shapes let's get to a little detail about how polynomial regression work to understand this portion you really need to understand linear regression well I have the video on linear regression and you will find the link in the description box below please check this is the basic formula or equation of linear regression for Simplicity I'm just taking one variable just checking we have only one input variable and one output variable okay and B and C they are the coefficients but in linear regression this is the slope and this is The Intercept but for machine learning language we actually call it coefficient and usually we express it like this Theta zero Theta one we have our X that means that our input variable and as I mentioned before we are assuming that we have only one input variable we have to find out these coefficients or weight and then we have to find out the hypothesis H which is actually our prediction the predicted output variable well but then polynomial regression the hypothesis looks like this actually Theta 0 plus Theta 1 times f plus Theta 2 times x squared so we have only one variable but we just transform this x with square with Cube and makes it three variables to fit into different types of shapes okay and it can be more I'm showing here up to three but it can be up to four or five but polynomial regression can also be root over okay square root cube root or so this was the high level overview of polynomial regression now we are going to implement a polynomial regression in Python here is my implementation so I'm going to explain it step by step okay so first I'm going to import the penis as PD and then I have insurance.csb data set and I am using this pandas library to create a data frame from this data set and you can see if I do tf.head I get the first five rows of this data set I have these variables here age sex BMI children's smoker region and charges here the charges is going to be our output variable or Target variable and as you can see this is a continuous variable this is a regression problem our problem today is using asex BMI children's worker and region all these variables and using them estimate or predict charges now data preparation this is very important you will hardly ever find any data set in the real world where you don't need to prep you have to do some sort of data preparation always first of all I always check for the null values DXL is an a that's 7 so this will give you how many null values you have in each and individual features here you can see we have zero null values and all the features next you can see we have a problem here insects smoker and religion we have the stream values we need to convert them to numeric values because our machine learning models can only use memory values you can see I am first checking in region how many unique values we have there are four and I can see for sex it's going to be only two male and female for smoker is going to be only two yes and no and I'm using this replace function to convert them to numeric values so female becomes one mean becomes two and this in the smoker variable yes becomes one no becomes two and for region this all these uh regions they become one two three and four the next step is defining the input features and Target variables if we look at this DF from the DS if we just drop or remove these charges the rest of them are all input figures right because we are going to use all of them to predict charges so these all of them is going to your input features and if we just catch charges in the DF this is going to be our output feature right our Target variable so DF dot drop columns equals to charges that's our X our input features and the target variables simply df.charges now we need to split our data set because we do not want to use all the data for training we have to keep some we have to keep the portion of the data to train the data set and we will keep another portion of the data separate for the testing or evaluating the model so for that scikit-learn Library we have the train test speed method so extreme X test y train y test equals we call train test speed method and the and the parameters we put X and Y that we just defined and test size 0.25 that means we are keeping 25 of the data for testing purpose and in a random state I'm using one and you can use any integer of your trace and then I need to scale my data what does that mean look we have sometimes these children one two three zero and for smoker zero one again look at this BMI we have 27 33 35 so the range of data here and range of data here are much much different in that case it's a really really good practice to bring all the features in a similar kind of range if you do not do that your model is not going to give any error or anything if you check in my previous video I have done that without scaling the data also and it worked but it's always good practice in ideal case you should do it in polynomial regression there are other machine learning models we are going to use in the future where we don't need data scaling but for polynomial regression we should do it as I mentioned if you do not do it you are not going to get any error but you should do it for a better model so we are going to use standard scalar here from SQL under pre-processing I'm going to import standard scalar then I am calling the standard scalar and saved in the variable scalar for the training data I am doing scanner.me transform because I need to fit the data in the standard scalar and then transform the training data to it and for test data I am only going to transform I am not going to fit it the reason is standard scalar is going through compute the mean and standard deviation of the data we only want to compute the standard deviation and mean for the training data we do not want and we will use the same values of standard deviation mean to transform the X test or test data as well because we don't want to give any information about test data to the model the test data should be totally separate totally unseen to the model okay now the model development polynomial regression is based on linear regression so we will need linear regression so first I am going to import linear regression I'm saving this linear regression method in l i n mean variable and then I will need polynomial features method and then first I'm importing it from sqlr processing and I'm saving this polynomial features method in this variable poly and I'm using degree 6. we need to transform our X strain and X test both to this polynes as we did in the scaling we will also do the same thing here for training data I will fit transform and for testing data I will only transform and not fit and this party we need to create our training data so X polytrain that our final transformed uh input features and our y train finally we will use this linear regression for the final model training so I'm going to fit this data that already fit in the poly I'm including this again in the linear regression model when model training is done now prediction so for prediction I only need X test poly the final transformed input features for the testing data okay so this is how we are going to predict and then let's evaluate so for evaluation you can use several methods I am going to use mean absolute error and I am importing that from scalar.com metrics and mean absolute error y test this is the true value y Test original value from the data set and this is why prediction that we got from the model right we have to pass this two and you can see our mean absolute error this is pretty big now I want to see how big the error comes from the training data so I am predicting the label for input features of the training data X polytrained as well and then I'm calculating mean absolute error for y Train That's the true label for the training data and why create a frame that is the prediction for the training data and you can see if 1970 it's really small and for the testimator it's too big so this is what we call overfilling so what happens here we used polynomial features degree six our model learned the training data really well when your model learns your training data too well it actually does not understand or cannot predict well for the unknown data for the general General data look at this image so when our model learns the training data too well what happens it fits the training data so well like this you can see that and when it gets a new data it just doesn't understand the new year and it doesn't predict well so we do not want our model to feed their training data in this precisely we want it to just go in the middle somewhere so that it can estimate close enough but it doesn't have to fit on that dot but again there is under filling we don't want uh too bad fitting either but here we have an overfilling problem so what we do in this case we just change the parameter here is the solution plug the poly polynomial features before we used degree six now I'm using degree three so I'm putting the lower value in the to the degree and then again I am feeding and transforming the extreme scanner and transforming the test scanner then we fit the training data to the following this polynomial features I call the linear regression again and finally I fit X polytrain the transformed from the polynomial features and while training totally near regression so first I check with X test poly I predict and the mean absolute error I'll give white the true label for the test data and the prediction that which is calculated here and then here I get the mean absolute error is 2819 and for training data I get 28 18 they're pretty close so that's what we want the error for the training data and testing data should be closed a error for test data is not going to be too high and compared to error for training data if you didn't know what overfitting is today you learn something new and I hope this video was useful for you please like comment share and subscribe and thank you so much for watching"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Polynomial Regression with Scikit-Learn",
        "Video Title":"Polynomial Regression using SKLEARN in Python 2021[NEW\ud83d\udd34]",
        "URL":"https:\/\/www.youtube.com\/watch?v=SSmpyoIdJTc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/SSmpyoIdJTc\/hqdefault.jpg",
        "ID":"SSmpyoIdJTc",
        "Publish Time":"2020-03-27T01:49:45Z",
        "Channel":"Financial Programming with Ritvik, CFA",
        "Channel ID":"UCyMifqUrSntvvrrGMaVPkrw",
        "Transcript":"What is up everybody I'm Ritvik Dashora and I'm back with a video and some new learnings. In this video, actually this video is an extension of the previous video which was on simple and multiple regression analysis. This was the code that we actually made in the previous video. I used housing.csv data in order to make it and we saw we saw how to do simple regression analysis and how to use all the parameters that is multiple regression analysis. If you have not not watched this video I would highly recommend - I would highly recommend you to watch this video first and then come back and watch this one. I am NOT going anywhere. The reason is I used a different library in the previous video which was stats model and this one I'm going to use sklearn. The reason I'm using a different library is that we should have a different learning all the time still I'd like the stats model every more than the sklearn and I will tell you what are the pros and the cons of sklearn library. Different people have different opinions on these libraries but I like stats model both and sklearn for regression analysis. Perfect so we can start it. Yeah, but before starting it I would like to have a request if you have not yet watched this, sorry if you have not yet subscribed to my YouTube channel then please click on the subscribe button down below and also hit on the bell icon. Also if you if you go to my youtube channel you can see there is a button here which is \u201caccess to my Google Drive\u201d and if you want to have a free access to my Google Drive where I save all the codes and my notes and the database all the data that I am using like in this video, I'm using housing.csv. I save everything over there. If you want to download it for free, yes, you can just click on this button after subscribing to my YouTube channel and it will redirect you to this google form and it needs some basic information like full name and email address etc I will be using this email address to give you the free access to my Google Drive and that the codes that are saved and the Google Drive are completely free for everyone. You can use them for your personal uses. There are no copyright issues. It's completely free. The one and only condition that has to be satisfied is that you should be my subscriber. I need your support because I have posted a lot of videos on this YouTube channel and I have not yet got enough subscribers. Someone asked me on a video that I am hiding the number of subscribers the reason is that I am not having enough subscribers to show. I am putting a lot of lot of efforts here so I need your support and I need it because I want to have the same motivation all the time otherwise from the next time maybe I will start thinking that I should start posting I should keep on posting videos or not because I am not having a lot of subscribers. Anyways, so I need your support. Please subscribe it please share it with the people that you that you think that are interested to watch this stuff. And also tell them to subscribe to my youtube channel such that they can get a free access to my Google Drive by clicking on this button. So let us start with the code now as I already told you, I will be using \u201chousing.csv\u201d data here and first of all let's see what is a polynomial regression a regression equation and as we are getting polynomial regression analysis in this video, a polynomial degree is basically a regression equation where we don't have degree 1 variables we have degree 2 which is quadratic or cubic or even degree 4 variables. So this is an example of polynomial recreation equation. This actually a quadratic equation so we can see that we have one variable and a square of variable in this case we have a square of the same variable which is X1. This is an example of the polynomial regression and actually it's a simple form and when it is a quadratic equation and I am also using a quadratic equation to do the to write the code here but you can do cubic or power 4 or power 5 degree equation. It's quite easy to tweak it to change it I'll tell you how to do it in this video I am doing a power 2 degree equation which is a quadratic equation and then for your practice you can make it power 3 power for whatever you like so let us start with the with the code input pandas as DF is equal to pd.read_csv Housing.csv and then DF dot, as I already told you I\u2019m using housing.csv fine here and so you can see that we have the same data frame here that we use at the last one now in the last one I use stats model and this one I'm gonna use SKlearn. So also yeah let's do some change in the last one if you remember for the linear sorry for the simple regression analysis, I used RM and MEDV parameters to do the analysis and this one to do to have a change, I'm using LSTAT and MEDV. You can do the same with RM and MEDV for your practice but I'm using I'm doing a like regression analysis on these two variables. Okay perfect so we we just need the first and the last column, we don't need these three columns data. Many people actually tweak the original data frame. I don't like to tweak the original data frame it's good to make a different data frame such that we can do all the work on that on that separate different data frame. I don't want to change, I don't want to touch the original data frame yet so I'm making DF2 now and DF2 which is let's have the same column names is equal to DF LSTAT Similarly, for df2 and MEDV. Perfect so now we have this differently different now now I'll do all the work on this df2 data frame I'm not gonna touch this main data frame which is DF. Let's keep it like this now in the last video I don't remember I didn't use train test split okay perfect yeah I didn't use it so in this video what I'm gonna do is using train test split, which is actually under a sklearn library. If you have watched my neural network videos which I posted one month back I used this train test split methodology in order to split the data into two different data sets one is for training and another is for testing purpose. I'm using I'm doing the same here. I'm splitting into two different data sets. There are quite few ways to do it to split the data but I like train test split because it it does two works the first one is its of course it splits the data and the second thing is actually it randomizes the data as well, so I don't need to import the random library or import a random in numpy library and it does both of the work so it actually reduces my work. So I'm typically that using the SKlearn that we have from SKlearn model selection. Now make all these different variables which is train X test see if you watch my neural network videos on deep learning you would be familiarized with this way of splitting the data okay so that sorry DF2. My x-axis is MEDV and my y axis LSTAT Perfect, so this is X train X test by test Perfect okay all this variables are not in data frame format so let's convert it into data frame format it's quite good to to analyze it in a data frame format so we know how to do it. Let's make a different variable called X train if and it will be PD. dataframe and then X train. Perfect so now we have a data frame yeah the good thing to do to notice here is that we can see that the index numbers 1 49 47 1 2 6 283 152 train test split has randomized the data which is a good thing actually it's a very good thing for doing analysis like regression or neural networks or machine learning because we need to randomize the data otherwise the model will not learn it properly Perfect, actually I should do the same with test also test. we can make we could make a for loop here in order to do it but I like to use this commas it's cool to use it in the same line I'm doing two things I always like to check before proceeding of it so we have split it split the data now. This is the equation that I'm gonna make here we already know that we have our X variable here which is X train here which is which we are gonna use it for training the model but we need X square also which is square of X train how to do it there are multiple ways to do it you can use number library and square the entire data frame which is quite easy to do it but I like another way which is actually using SK learn library and using polynomial features the reason for using this library is actually flat we first imported SK dot pre-processing import polynomial features make a video called polynomial future the reason that I like this one is that you just need to write the degree inside the parentheses if you want to make it a three degree equation the linear sorry the regression equation which is which will be B naught plus B 1 X 1 plus B 2 X 1 square plus B 3 X 1 Q then you you just need to do degree is equal to 3 and everything will be done if you are using numpy liability or to square it then if you have to tweak it what to what degree 3 equation you'll have to change the entire code so that's why I like this this way to do it so polynomial bodies go to polynomial features degree is equal to 2 now let's convert our X and perfect so you can see that actually I should like this yeah perfect so you can see that what it has done is it has converted this dataset into three different elements the first one is this one the first one for example sixteen point six to sixteen point six to the power power of zero which is one sixteen point six to the power of 1 which is the same number sixteen point six and sixteen point not sixteen point six sixteen point 1 to the power of two sixteen point 1 square which is two fifty nine point two one if I make it three then now it's a three degree equation which okay sorry I was getting confused that these numbers are changing it because numbers are changing because I am also randomizing it it is randomizing people so yeah we're right so we can see that if you make it three and we also I have to run it so third no actually this is e to the power of one which is actually thirteen point three also three thirteen point three only so this is thirteen point three to the power of zero thirteen point three ^ 1 ^ 2 ^ 3 etc but in this one I'm doing power to a certain degree to equation which is a quadratic equation perfect so actually let's make the same for testing data set which will be exactly the same no no let's make the model which is like as I already told I'm gonna use SQL library here to do the regression analysis now again one more important thing that I should mention here I have converted the X variable into X square variable using this polynomial features so now actually what we can do is we can consider for example in this one we can consider X 1 as a different variable and X 1 square as a different variable so if you consider X 1 square is equal to say Z so we have a linear equation sorry we have regression equation of y is equal to B naught plus B 1 X 1 plus B 2 Z now if you see it like this in this format you can see that it's a it's a type of linear equation right because I'm I'm having a different variable called Z I'm not saying it as considering it as x squared I'm considering it as Z a completely different parameter so I can use the linear regression analysis for the same thing because I have already converted the x + 2 x squared so what I'm going to use here what I'm going to do here is I'm going to use this x squared data rather than the third variable which is said which is the Z data if would have if we would have a different variable concept so from s scale import linear model I am doing linear model analysis here model is equal to model linear equation fit and then I'll have to input the X train body it will be taking the entire like all the three variables the first one is one which is not the three variables that three parameters the first one is one the second one is the element to the power of one the third one is the element to the power of two which is square of this one and it is considering it as a linear equation so we need to do it on trained at assailant the model is done so now let's make the equation this is the coefficient again we should have two different coefficients it's just ignore this one for now we should have two different coefficient this is the coefficient number one which is b1 and this one is b2 if it were degree is equal to 3 then we would have three different variables let's check it actually you can see we can have we have this is zero this b1 b2 and b3 so this is the coefficient so let me give it a name this just go through this also we need the intercept which is this one this intercept which is actually B not okay so now let's make the linear regression before making it actually I should show you the scatterplot of actual data so import the most useful library for plotting which is matplotlib dot B by the plot as x-axis and y-axis let's use color beautiful so yeah there's the scatter plot now you can imagine that we would have our regression curve to be like this should have a curve like this right so now let's plot our technician cover let's make the regression equation first and then let's plot so in order to make it yeah actually I yeah I will have to have an x-axis some numbers on x-axis and then for those numbers I'll have to have the value of y axis using the regression equation that we made so for that I'll have to improve as and what I'm doing here is actually I'm making an x axis of numbers from say 5 to 50 by changing it by 0.1 I'm gonna use NP dot a range here you can also use NP total in space if you want it will also do the work it will throw in a list actually and this one n P dot a range actually throws an array but you can convert and listen to an array by using 8-bit or array you can use and B dot linspace but I'm gonna use here and P dot a change from 5 to see from 5 to 50 0.1 and for all to all the values and X access the response actually I use the same here same variable Y which is response so I'm going to stick to that response is equal to that the regression equation which is intercept which is B not I'm actually writing this equation now B not plus b1 which is coefficient but of the zeroth element of coefficient pause if you remember it was zero the first one was actually be 1 and the second one must be do so coefficient the first one and multiply by x axis similarly coefficient second element multiplied by x axis squared so we have this linear it's not linear we have this regression curve so now let's plot it PLP dot plot x axis is actually x axis and the y axis as response let's make it red beautiful so it's amazing like this this we can see that that this regression curve has actually covered a lot of data so you have got a very beautiful regression curve which is this red one actually it's written quite beautiful actually I didn't practice it before making this video but actually it's looking quite good perfect now if you remember in order to check the I would say efficiency of this model we have a variable called R square or adjusted r-square in the last one Fe if you remember that there was a very easy way to do it called model dot summary and then we've got our square at the still R square and not just these two parameters we have we had got a like a lot of things like Durbin Watson parameters skewness kurtosis this is the entire and word table T statistics etcetera unfortunately there is no such facility here in order to do it for all square we'll have to use SK learn dot X and like it's a different way to do it I will have to like write a couple of lines in order to extract the R square data but we don't have any facility to get all this details that then this is one of the reason that I like this library which is stats model might ready more than SK done actually you can use you can do this polynomial regression analysis using the stax model library as well the only thing is that you need to do here is rather than doing the like this plus different variables you'll have to create a variable which will be square of the first variable and then you'll have to name it for example name and LS dat squared and then write LS dat square here with illustrating and just keep these two variables and do this analysis and I think that you will get the same results but in that you'll have the facility to use model dot summary and get all the details here which is like quite useful this is the this is one of the reason that I like stacks model library more than skin it but still how to do it on a scale and I'm from SK learn dot matrix and R square R square if we need some prediction data and then by test X is Polly so we've got our R square of sixty five point six three percent which is a lot which is quite good if you see that and the last one all the way did like use different independent variables which was our M&M edv we got the R square data of fifty percent which is actually 51.4% to be precise but in this case we have got it more than 60 percent R squared which is quite good if you do this analysis all stock price data and if you get this high R square which is actually impossible to do it but still if you get it then you make a lot of money if you do this analysis all stock prices you will get the R squared value of around two percent three percent maximum and that is the reason that it's quite difficult to predict the stock prices will have to do more analysis even doodlenet was deep learning cannot do the the best work to rate the stock price data but still there's a lot of work that has been done on the stock price prediction many hedge funds are doing it a lot for example in factor investing my there are some hedge funds that are using the neural networks in order to make different factors they are not considering factors there are like when you cruel momentum etc there they're using neural network and the number of nodes as different factors in the neural network and through that they are like diversifying their portfolio by changing the weights on the number of of the number of the different nodes on the first or the second layer that they're using I know that all this information is cannot be grabbed that easily many of you cannot understand what I'm saying but still there's a lot of work that is being done right now on this quantitative analysis and this is actually one of them that is doing the polynomial integration so perfect we have got a very good R square which is sixty five point six percent so I think we are done with it in the last wheel let me check if I missed something in the last video you can plot these graphs if you want to you can like make an error parameter which is like which will be the subtraction of the response and LS dat I am NOT going to do it like this is easy stuff I already explained it and last one so you can do it so I think we have completed all important things I hope you liked my video and if you wanna if you want to watch this type of videos this type of stuff in future then please subscribe to my youtube channel I need your support also if you have if you know anyone who is interested to know all these things and forward it to him or her and also recommending him or her to subscribe because I need some subscribers before going forward I have a lot of stuff to share I love Python I I used to do Python coding a lot I'm a finance professional and a Python lover so I just try to implement both of these things in order to optimize this industry as much as possible but for that I would need your support to continue the subscribe to my youtube channel I'm I'm giving everything for free all this stuff is for free these videos I used to post these videos like on all the all I would say every week sometimes twice a week so I'm putting a lot of efforts I'm giving all my quotes for free and not the schools a lot of different stuffs that are actually on Google Drive if you go to my Google Drive you will see there are many things that over there anything that you can use for free the only thing that I want from you is that I need some subscribers so please subscribe to my youtube channel so thank you so much I hope you like my video stay tuned for my future videos see ya"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Polynomial Regression with Scikit-Learn",
        "Video Title":"Python for Machine Learning - Polynomial Linear Regression using Scikit Learn - P9",
        "URL":"https:\/\/www.youtube.com\/watch?v=_MNgb7nL0oo",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/_MNgb7nL0oo\/hqdefault.jpg",
        "ID":"_MNgb7nL0oo",
        "Publish Time":"2018-03-25T07:26:38Z",
        "Channel":"technologyCult",
        "Channel ID":"UCYCcf7stRPHTWtu0r4ZoXbA",
        "Transcript":"hi everyone welcome to the 56th session of Python for machine learning in this session we will learn about the polynomial linear regression using the scikit-learn library we will plot like we will get the salary based on the experience we already have our dataset with us so we will execute this and if we see this contains a year of experience and with the salary along with the years of experience you will normally plot like what we have done in the such session from 30 to 35 the detailed is discussion we had about linear regression and in the session number 52 will be I have shown you how to use the polynomial linear regression without using scikit-learn till sampling will execute it and from there we'll start our coding okay okay so till something is done in now what we'll do what it needs to what we need to do is we need to create the regression model as usual and then after that we need to like create so for that you get from SK learn thought linear model import linear regression and then we create the object of linear regression now what we need to we need to create the polynomial features and then for that what we give from SK learn pre-processing import what will import will import the polynomial features and then we create the object of this polynomial features suppose you give a degree of you that we will see once and then will give X under scope only which is equal school poly leg thought it underscore transform and then we features like we have already extracted the features and leverage and I have not shown you how because I've already a plane in the previous session so I'm just statically starting okay and then we need to once this is done we need to show the we need to test the model for that why underscore bread is equals to drink racer but before this will execute these three lines okay we're execute from here fine regressor dot dot what will get predict because we are going to predict the values salary and we need to finish it on predict and then to give Polly Rick thought fit understood transform and we need to pass here fix test thought okay now if you go through your variable Explorer if you open your exfoli not my bread and this is your y test so you can see the model is not 100% ready for dips coming near by values like this is 14,000 and here it is 37,000 731 for this one leg 23,000 is 122 almost there so it is doing like good prediction we can see now if you want to calculate the QC polynomial equation accuracy is how to calculate the accuracy that will give progressive don't score that we have used in the previous session and then figure holding dot poly Lincoln underscore top four and then we need to pass X under spoon next then that will be followed by Bob why let's go test so accuracy is 97% but if we again want to do this change the equation no but before this for the second degree equation what is my like a crystal dot so what is your coefficient here nine five two nine thing see this 95 blue name and this is sixteen point three nine so it is matching and if you give regression thought intercept this is two six two and then it is being round off so it's coming perfectly not in p12 given degree you check it with a degree of three so what will be they my accuracy accuracy is it proving actually and if I want to take the coefficient so what's the coefficient coefficient and also the intercept okay so these are the coefficient and intercept and we will verify with this equation this equation we have got by checking into the Excel I have already shown in the session number 52 okay so here you can see the intercept part is three eight eight six three then minus seven eight if you if you go from here then two zero nine nine which is coming perfectly and then - one two - so this is walking but no if you want do like this is mechanical if you want to execute suppose we want to check for the 60 to be at 100 what will be my the accuracy score so we can use follow but if the data is huge then it's no point in taking this and then intercept then we'll bring the accuracy which is so now we need to simply execute this and so okay so for first-degree the accuracy is ninety seven ninety seven six eight for second-degree decreasing nine seven six six for third-degree it's increasing for four degree it's even increasing a bit more for fifth degree it is like even decreasing further and also if you want to see the coefficient and the intercept so you can see this fifty-nine fo for it is being round off and then two five in the seventy two five nine seven two for the second degree the koi fish intercept is two six this and this value and there is the coefficient is you have to come from back like sixteen point three nine x square plus nine five two nine x to the power of one and then for third degree the value is matching three eight eight for third degree three eight eight six three and here 3 to 6 3 and similarly we'll value if you are coming from this this and this - one two two two zero nine nine seven one eight minus one two two two zero nine eight - seven eight this is all we can get the like value and we can also cross verify which is one of the very important thing like whether the work which you are doing is correct or not this is the way how you can verify your linear model for polynomial linear regression so thanks a lot for watching the session if you feel this video helpful please subscribe to this channel and I'll see you in the next video thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Polynomial Regression with Scikit-Learn",
        "Video Title":"Linear and Polynomial Regression using Scikit-learn [Part 12] | Machine Learning for Beginners",
        "URL":"https:\/\/www.youtube.com\/watch?v=e4c_UP2fSjg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/e4c_UP2fSjg\/hqdefault.jpg",
        "ID":"e4c_UP2fSjg",
        "Publish Time":"2023-05-15T16:00:32Z",
        "Channel":"Microsoft Developer",
        "Channel ID":"UCsMica-v34Irf9KVTh6xx-g",
        "Transcript":"in this video we'll be writing code to train linear and polynomial regression models for our pumpkin data set using scikit-learn we'll continue to add code to the notebook from the previous video you can find that notebook in the GitHub project we've been using for the series under regression then linear make sure you run the code in the notebook before you run the code in this video let's try a linear regression model first our goal is to predict the price of a package of pumpkins given the day of the year if you watched our previous video you saw that we filtered our data frame to contain only pumpkins a variety Pi type and we calculated the correlation between the day of the year and price to be minus point 27. let's see if we have enough correlation to create a meaningful linear regression model we'll start by importing the scikit-learn package next we need to get separate numpy arrays for the input values X which represent the day of the year and the labels Y which contain the price remember that our goal with this model is to predict the price of a pi type bushel of pumpkins given the day of the year we need to reshape our input X because later we'll be passing it to the fit function of linear regression and that expects a 2d array if we don't reshape it it will have shape 144 which is a 1D array if we reshape it then you will have shape 144 by 1 which is a 2d array and that's exactly what we need next we need to split our data into test and train sets the train data set will be used to train the linear regression model and the test data set will be used to check the quality of our results we can then create our linear regression object and call the fit function which trains the model once the model is trained we can make a prediction using our test data if we print the pred variable we can see the prices predicted for a bushel of Pi type pumpkins on different days of the year how can you tell if these predictions are good we can calculate the mean squared error let's run this code to see what our error is our error is 2.77 or 17.2 percent that's a pretty big error so our predictions won't be great another indicator of model quality is the coefficient of determination this value can be anywhere between 0 and 1 and the closer to one it is the better our model fits or the data a coefficient of determination is 0.04 which is very low we can plot our original data points and our prediction line in the same graph the graph we get is consistent with a numerical result the vertical distance between the points and the prediction line does look pretty significant if you watched our linear regression introduction video you'll learn that you can obtain the equation for the prediction line by calculating two parameters the slope of the line and the value of Y where the line intersects the y-axis we can get those values with this code we can use the model we train to predict the price of a bushel of Pi type pumpkins on programmers day to the 256 day of the year we get a prediction back but keep in mind that this prediction isn't super accurate because our mean squared error is so high maybe we can do better with polynomial regression linear regression fits the data using a straight line but our data doesn't always approximate a straight line with polynomial regression our prediction can follow a curve let's see if that will give us better results since we're using scikit-learn we could use pipelines to add one or more data transformation steps before we train our model here we're using the polynomial features class to compute the extra features we need to do polynomial regression in our scenario we just want one extra feature that squares our input X so we'll pass a 2 to the Constructor once we have our new feature we can use our usual linear regression algorithm to find a polynomial that fits the data let's run the cell we now get a mean squared error of 2.73 or 17 which is a little bit better than before but not by much we also print the coefficient of determination 0.08 shows that we don't have a good fit but it's better than before let's look at the data points and our prediction curve in a graph our intuition matches the results we got I can believe that a curved line would fit the data a bit better but not a lot better so we still don't have a great model to predict the price of pumpkins but remember that we've only been using one feature in our input X just the day of the year maybe we can get a better prediction if we use more features what's the next video to find out"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Ridge and Lasso Regression for Regularization",
        "Video Title":"Regularization Part 1: Ridge (L2) Regression",
        "URL":"https:\/\/www.youtube.com\/watch?v=Q81RR3yKn30",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Q81RR3yKn30\/hqdefault.jpg",
        "ID":"Q81RR3yKn30",
        "Publish Time":"2018-09-24T16:59:00Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"regularization it's just another way to save desensitization let's check it out with a new regression stat quest hello I'm Josh stormer and welcome to stat quest today we're going to do part 1 of a series of video on regularization techniques in this video we're gonna cover Ridge regression and it's going to be clearly explained note this stat cuesta seems you understand the concepts of bias and variance in the context of machine learning if not check out machine learning fundamentals bias and variance it also assumes that you are familiar with linear models if not check out the following stat quests the links are in the description below lastly if you're not already familiar with the concept of cross-validation check out the stack west on cross-validation in this stack quest we will one look at a simple example that shows the main ideas behind Ridge regression to go into details about how Ridge regression works three show how Ridge regression works in a variety of situations and four lastly we'll talk about how Ridge regression can solve the unsolvable BAM let's start by collecting weight and size measurements from a bunch of mice since these data look relatively linear we will use linear regression aka least squares to model the relationship between weight and size so we'll fit a line to the data using least squares in other words we find the line that results in the minimum sum of squared residuals ultimately we end up with this equation for the line the line has two parameters a y-axis intercept and a slope we can plug in a value for weight for example two point five and do the math and get a value for size together the value for weight two point five and the value for size two point eight give us a point on the line when we have a lot of measurements we can be fairly confident that the least squares line accurately reflects the relationship between size and weight but what if we only have two measurements we fit a new line with least squares since the new line overlaps the two data points the minimum sum of squared residuals equals zero ultimately we end up with this equation for the new line note here are the original data in the original line for comparison let's call the two red dots the training data in the remaining green dots that testing data the sum of the squared residuals for just the two red points the training data is small in this case it is zero but the sum of the squared residuals for the green points the testing data is large and that means that the new line has high variance in machine learning lingo we'd say that the new line is over fit to the training data now let's go back to just the training data we just saw that least squares results in a line that is over fit and has high variance the main idea behind Ridge regression is to find a new line that doesn't fit the training data as well in other words we introduce a small amount of bias into how the new line is fit to the data but in return for that small amount of bias we get a significant drop in variance in other words by starting with a slightly worse fit Ridge regression can provide better long-term predictions BAM now let's dive into the nitty-gritty and learn how Ridge regression works let's go back to just the training data when least-squares determines values for the parameters in this equation it minimizes the sum of the squared residuals in contrast when Ridge regression determines the values for the parameters in this equation it minimizes the sum of the squared residuals plus lambda times the slope squared note I usually try to avoid using Greek characters as much as possible but if you are ever going to do Ridge regression in practice you have to know that this term is called lambda this part of the equation adds a penalty to the traditional least squares method and lambda determines how severe that penalty is to get a better idea of what's going on let's plug in some numbers let's start by plugging in the numbers that correspond to the least squares fit the sum of the squared residuals for the least squares fit is zero because the line overlaps the data points and the slope is one point three we'll talk more about lambda later but for now let lambda equal one all together we have zero plus one times one point three squared and when we do the math we get one point six nine now let's see what happens when we plug in numbers for the ridge regression line the sum of the squared residuals is zero point three squared for this residual plus zero point one squared for this residual the slope is 0.8 and just like before we'll let lambda equal 1 altogether we have 0.3 squared plus 0.1 squared plus 1 times 0.8 squared and when we do the math we get 0.7 for for the least squares line the sum of squared residuals plus the ridge regression penalty is one point six nine for the ridge regression line the sum of squared residuals plus the ridge regression penalty is 0.74 thus if we wanted to minimize the sum of the squared residuals plus the ridge regression penalty we would choose the ridge regression line over the least squares line without the small amount of bias that the penalty creates the least squares fit has a large amount of variance in contrast the ridge regression line which has a small amount of bias due to the penalty has less variance now before we talk about lambda let's talk a little bit more about the effect that the ridge regression penalty has on how the line is fit to the data to keep things simple imagine we only have one line this line suggests that for every one unit increase in weight there is a one unit increase in predicted size if the slope of the line is steeper than for every one unit increase in weight the prediction for size increases by over two units in other words when the slope of the line is steep then the prediction for size is very sensitive to relatively small changes in weight when the slope is small then for every one unit increase in weight the prediction for size barely increases in other words when the slope of the line is small then predictions for size are much less sensitive to changes in weight now let's go back to the least squares and Ridge regression lines fit to the two data points the ridge regression penalty resulted in a line that has a smaller slope which means that predictions made with the ridge regression line are less sensitive to weight than the least squares line BAM now let's go back to the equation that Ridge regression tries to minimize and talk about lambda lambda can be any value from 0 to positive infinity when lambda equals zero then the ridge regression penalty is also zero and that means that the ridge regression line will only minimize the sum of squared residuals and the ridge regression line will be the same as the least squares line because they are both minimizing the exact same thing now let's see what happens as we increase the value for lambda in the example we just looked at we said lambda equals 1 and the ridge regression line ended up with a smaller slope than the least squares line when we set lambda equals 2 the slope gets even smaller and when we set lambda equals 3 the slope is even smaller and the larger we make lambda the slope gets asymptotically close to 0 so the larger lambda gets our prediction for size become less and less sensitive to weight so how do we decide what value to give lambda we just try a bunch of values for lambda and use cross-validation typically 10-fold cross-validation to determine which one results in the lowest variance double bail in the previous example we showed how ridge regression would work when we want to predict size which is a continuous variable using weight which is also a continuous variable however Ridge regression also works when we use a discrete variable like normal diet versus high fat diet to predict size in this case the data might look like this in the least squares fitted equation might look like this where 1.5 the equivalent of a y-intercept corresponds to the average size of the mice on the normal diet and 0.7 the equivalent of a slope corresponds to the difference between the average size for the mice on the normal diet compared to the mice on the high-fat diet note from here on out we'll refer to this distance as diet difference high-fat diet is either zero for mice on a normal diet or one for mice on the high-fat diet in other words this term alone predicts the size of mice on the normal diet in the sum of these two terms is the prediction for the size of mice on the high-fat diet for the mice on the normal diet the residuals are the distances between the mice and the normal diet mean and for mice on the high-fat diet the residuals are the distances between the mice and the high fat diet mean when Lee squares determines the values for the parameters in this equation it minimizes the sum of the squared residuals in other words these distances between the data and the means are minimized when Ridge regression determines values for the parameters in this equation it minimizes the sum of the squared residuals plus lambda times diet difference squared remember diet difference simply refers to the distance between the mice on the normal diet and the mice on the high-fat diet when lambda equals zero this whole term ends up being zero and we get the same equation that we got with least squares but when lambda gets large the only way to minimize the whole equation is to shrink diet distance down in other words as lambda gets larger our prediction for the size of mice on the high-fat diet becomes less sensitive to the difference between the normal diet and the high-fat diet and remember the whole point of doing rich regression is because small sample sizes like these can lead to poor least squares estimates that result in terrible machine learning predictions BAM Ridge regression can also be applied to logistic regression in this example we are using weight to predict if a mouse's obese or not this is the equation for this logistic regression and Ridge regression would shrink the estimate for the slope making our prediction about whether and out of mouse is obese less sensitive to weight note when applied to logistic regression Ridge regression optimizes the sum of the likelihoods instead of the squared residuals because logistic regression is solved using maximum likelihood so far we've seen simple examples of how Ridge regression helps reduce variance by shrinking parameters and making our predictions less sensitive to them but we can apply Ridge regression to complicated models as well in this model we've combined the weight measurement data from the first example with the two diets from the second example combining these two datasets gives us this equation and Ridge regression tries to minimize this now the ridge regression penalty contains the parameters for the slope and the difference between diets in general the ridge regression penalty contains all of the parameters except for the y intercept if we had a big huge crazy equation with terms for astrological sign the airspeed of a swallow and other stuff then the ridge regression penalty would have all those parameters squared except for the y intercept every parameter except for the y intercept is scaled by the measurements and that's why the y intercept is not included in the ridge regression penalty double bam okay now the next thing we're going to talk about is going to sound totally random but trust me it will lead to the coolest thing about Ridge regression it's so cool it's almost like magic we all know that this is the equation for a line in an order for least squares to solve for the parameters the y-intercept and slope we need at least two data points these data points result in these parameters in this specific line if we only have one data point then we wouldn't be able to solve for these parameters because there would be no way to tell if this line is better than this line or this line or any old line that goes through the one data point all of these lines have zero residuals and thus all minimize the sum of the squared residuals it's not until we have two data points that it becomes clear that this is the least square solution now let's look at an equation that has three parameters to estimate we need to estimate a y-intercept a slope that reflects how weight contributes to the prediction of size and a slope that reflects how age contributes to the prediction of size when we have three parameters to estimate then just two data points isn't going to cut it that's because in three dimensions which is what we get when we add another access to our graph for age we have to fit a plane to the data instead of just a line and with only two data points there's no reason why this plane fits the data any better than this plane or this plane but as soon as we have three data points we can solve for these parameters if we have an equation with four parameters then least squares needs at least four data points to estimate all four parameters and if we have an equation with 10,001 parameters then we need at least 10,000 and one data points to estimate all of the parameters an equation with 10,001 parameters might sound bonkers but it's more common than you might expect for example we might use gene expression measurements from 10,000 genes to predict size and that would mean we would need gene expression measurements from ten thousand and one mice unfortunately collecting gene expression measurements from ten thousand and one mice is crazy expensive and time-consuming right now in practice a huge data set might have measurements from 500 mice so what do we do if we have an equation with ten thousand and one parameters in only 500 data points we use Ridge regression it turns out that by adding the ridge regression penalty we can solve for all 10,000 won parameters with only 500 or even fewer samples one way to think about how Ridge regression can solve for parameters when there isn't enough data is to go back to our original size versus weight example only this time there is only one data point in the training set least squares can't find a single optimal solution since any line that goes through the dot will minimize the sum of the squared residuals but Ridge regression can find a solution with cross-validation and the Ridge regression penalty that favors smaller parameter values since this stack quest is already super long we'll save a more thorough discussion of how this works for a future stat quest triple bam in summary when the sample sizes are relatively small then Ridge regression can improve predictions made from new data ie reduce variance by making the predictions less sensitive to the training data this is done by adding the ridge regression penalty to the thing that must be minimized the ridge regression penalty itself is lambda times the sum of all squared parameters except for the y-intercept and lambda is determined using cross-validation lastly even when there isn't enough data to find the least squares parameter estimates Ridge regression can still find a solution using cross-validation and the ridge regression penalty hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to support stack quest well consider buying one or two of my original songs alright until next time quest on"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Ridge and Lasso Regression for Regularization",
        "Video Title":"Ridge vs Lasso Regression, Visualized!!!",
        "URL":"https:\/\/www.youtube.com\/watch?v=Xm2C_gTAl8c",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Xm2C_gTAl8c\/hqdefault.jpg",
        "ID":"Xm2C_gTAl8c",
        "Publish Time":"2020-05-19T04:00:08Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"Ridge regression versus lasso regression which one will survive stat quest hello I'm Josh stormer and welcome to stat quest today we're gonna talk about rage versus lasso regression and the differences are going to be visualised note this stack quest assumes that you are already familiar with rage and lasso regression if not check out the quests to show you the difference between Ridge and lasso regression we're going to use a very simple dataset that consists of weight and height measurements and we'll start by fitting this horizontal line to the data this horizontal line represents a terrible fit and we can measure how bad that fit is by calculating residuals the difference between the observed and predicted values in order to compare the horizontal line to other lines fit to the data we will plot the sum of the squared residuals on a graph the y-axis on this graph is the sum of the squared residuals and the x-axis represents different slope values for the fitted line in this case the slope for the horizontal line is zero now let's increase the slope to zero point two and calculate a new value for the sum of the squared residuals now let's increase the slope to zero point four and calculate a new value for the sum of the squared residuals we can keep plugging in new values for the slope and plotting the sum of the squared residuals or we can just plot the curve for this equation we can see that the best fitting line is at the bottom of the parabola in other words when the slope equals 0.45 we get the lowest sum of the squared residuals in this example we simply calculated the sum of the squared residuals for different slopes now let's add the ridge regression penalty aka the l 2-norm note if you asked me it should be called the squared penalty since that's what it is and for me way easier to remember the thick blue line that we just drew represents lambda equals zero this is because when lambda equals zero the penalty is equal to zero regardless of the slope and we are left with the original sum of squared residuals now let's see what happens when we set lambda equal to 10 and just like before we'll start with a horizontal line only this time the line is orange just like before we can calculate the residuals and we can calculate the sum of the squared residuals plus lambda times the slope squared in this case lambda equals 10 in the slope of the horizontal line is 0 so the penalty is 0 so we plot the sum of the squared residuals here now let's increase the slope to 0.2 note the residuals are smaller than before so the sum of squared residuals is smaller than before but now the penalty is 0.4 and that gives us this point on the graph now let's increase the slope to 0.4 and the residuals are even smaller but now the penalty is 1.6 and that gives us this point on the graph and like we did before we can keep plugging in new values for the slope and plotting the sum of the squared residuals plus the penalty or we can just plot the curve with a thick orange line that represents lambda equals 10 the bottom of the parabola is where the slope gives us the lowest sum of squared residuals plus penalty and that corresponds to this specific line and when we compare that to the optimal slope when lambda equals zero we see that setting lambda equal to ten results in a smaller optimal slope note we can also see that when lambda equals ten the lowest point in the parabola is closer to zero than when lambda equals zero so either way we look at it we see that the larger value for lambda Shrunk the optimal value for the slope likewise the thick green line represents lambda equals 20 we see that the minimum value is closer to zero and the optimal slope has shrunk some more the purple lines represent lambda equals 40 and it shrinks the slope even more in other words as we increase lambda for the ridge regression penalty aka the l2 penalty aka the square penalty the optimal slope gets closer and closer to zero but it does not equal zero BAM now let's see what happens if we use the lasso penalty aka the l1 norm or if you asked me I'd call it the absolute value penalty unfortunately no one asked me again the thick blue line represents lambda equals zero so there is no extra penalty this is because when we plug lambda equals zero into the equation the penalty becomes zero and we are left with the original sum of the squared residuals note just like before we'll keep track of the best fitting line plus penalty in this graph on the left the thick orange line represents lambda equals 10 so now we are turning on the penalty and shrinking the slope note when lambda equals Tim we start to see a kink in the curve where the slope is zero the thick green line represents lambda equals 20 and this kink at zero is becoming more prominent lastly the thick purple line represents lambda equals 40 and now the kink at zero is super obvious now the lowest point in the purple curve aka the optimal slope given the absolute value penalty when lambda equals 40 is zero and that means the slope of the optimal line is zero and that means when lambda equals 40 we ignore weight as a variable when predicting height double bam in summary when we increase the Ridge regression penalty aka the l2 penalty aka the square penalty the optimal slopes shift towards zero but we retain a nice parabola shape and even when we set lambda to something crazy high like 400 we still end up with an optimal value greater than zero in contrast when we increase the lasso penalty aka the l1 penalty aka the absolute value penalty the optimal value shifts towards zero but since we have a kink at zero zero ends up being the optimal slope BAM hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Ridge and Lasso Regression for Regularization",
        "Video Title":"Regularization Part 2: Lasso (L1) Regression",
        "URL":"https:\/\/www.youtube.com\/watch?v=NGf0voTMlcs",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/NGf0voTMlcs\/hqdefault.jpg",
        "ID":"NGf0voTMlcs",
        "Publish Time":"2018-10-01T13:20:40Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"laso and ridge regression of similar but there's a big important difference we'll talk about it stat quest hello I'm Josh stormer and welcome to stat quest today we're gonna do part two of our series on regularization we're gonna talk about lasso regression and it's going to be clearly explained this stat quest follows up on the one on Ridge regression so if you aren't already familiar with that check it out even if you are familiar with Ridge regression you should seriously consider watching or at least skimming that stat quest because the examples in this video are based on the ones in that video last so regression is very very similar to Ridge regression but it has some very very important differences to understand those similarities and differences let's first do a super quick review of Ridge regression in the stat quest on Ridge regression we started out with weight and size measurements from a bunch of mice and we split the data into two sets the red dots were training data and the green dots were testing data then we fit a line to the training data using least squares in other words we minimize the sum of the squared residuals when we did this we saw that even though the line fit the training data really well that is to say it had low bias it did not fit the testing data very well at all that is to say it had high variance then we fit a line to the data using Ridge regression we minimized the sum of the squared residuals plus lambda times the slope squared Ridge regression is just least squares plus the ridge regression penalty the Blue Ridge regression line did not fit the training data as well as the red least squares line in other words Ridge regression had more bias than least squares but in return for that small amount of bias the ridge regression line had a significant drop in variance the main idea was that by starting with a slightly worse fit Richard rushman provided better long-term predictions BAM now let's go back to the equation that Ridge regression minimizes and focus on the ridge regression penalty if instead of squaring the slope we take the absolute value then we have lasso regression note just like with Ridge regression lambda can be any value from 0 to positive infinity and is determined using cross-validation like Ridge regression lasso regression the orange line results in a line with a little bit of bias but less variance than least squares BAM reach regression and lasso regression look very similar and they do similar things in this case they make our predictions of size less sensitive to this tiny training data set both Ridge and lasso regression can be applied in the same context like this situation where we are using two different diets to predict size or in a logistic regression setting where we use weight to predict obesity and both Ridge and lasso regression can be applied to complicated models that combine different types of data in this case we've combined the data from the first two examples weight which is continuous and high fat diet which is discrete just like the ridge regression penalty the lasso regression penalty contains all of the estimated parameters except for the y-intercept it's also worth mentioning that when Ridge and lasso regression shrink parameters they don't have to shrink them all equally for example if these were the training data and these were the testing data then when lambda equals zero we would start with these least squares estimates for the slope and the offset for diet difference but as we increase the value for lambda rage and lasso regression may shrink diet difference a lot more than they shrink the slope okay we've seen how Ridge and lasso regression are similar now let's talk about the big difference between them to see what makes lasso regression different from Ridge regression let's go back to the two-sample training data and let's focus on what happens when we increase the value for lambda when lambda equals zero then the lasso regression line will be the same as the least squares line as lambda increases in value the slope gets smaller until the slope equals zero BAM the big difference between Ridge and lasso regression is that Ridge regression can only shrink a slope asymptotically close to zero while lasso regression can shrink the slope all the way to 0 to appreciate this difference let's look at a big huge crazy equation the goal of this equation is to predict size the terms for weight and high fat diet are both reasonable things to use to predict size but the astrological sign and the airspeed of a swallow African or European are terrible ways to predict size when we apply Richard Russian to this equation we find the minimal sum of the squared residuals plus the ridge regression penalty and the larger we make lambda these parameters might shrink a little bit and these parameters might shrink a lot but they will never be equal to 0 in contrast with lasso regression when we increase the value for lambda then these parameters will shrink a little bit and these parameters will go all the way to 0 and these terms go away and we're left with a way to predict size that only includes weight and diet and excludes all of the silly stuff since lasso regression can exclude useless variables from equations it is a little better than Ridge aggression at reducing the variance and models that contain a lot of useless variables in contrast Ridge regression tends to do a little better when most variables are useful double bam in summary Ridge regression is very similar to lasso regression and the superficial difference is that Ridge regression squares the variables and lasso regression takes the absolute value but the big difference is that Lasser aggression can exclude useless variables from equations this makes the final equation simpler and easier to interpret hooray we've made it to the end of another exciting stack quest if you like this stack quest and want to see more please subscribe and if you want to support stack quest well please consider buying one or two of my original songs alright until next time quest on"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Ridge and Lasso Regression for Regularization",
        "Video Title":"Machine Learning Tutorial Python - 17: L1 and L2 Regularization | Lasso, Ridge Regression",
        "URL":"https:\/\/www.youtube.com\/watch?v=VqKq78PVO9g",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/VqKq78PVO9g\/hqdefault.jpg",
        "ID":"VqKq78PVO9g",
        "Publish Time":"2020-11-26T13:00:09Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"Overfitting is a very common issue in the field of machine learning and L1 and L2 regularizations are some of the techniques that can be used to address the overfitting issue. In this video, we'll go over some theory on what exactly L1 L2 regularization is and then we'll write a python code uh and we'll see how a model which is an overfit can be addressed and the accuracy can be improved when you use L1 and L2 regularization. We will be using a housing price dataset from the city of Melbourne and we'll first build a model using a simple linear regression and we'll see it is overfitting the model and then we'll use L1 and L2 regularization and we'll see how we address that overfitting issue and how it improves the score on our test set. So let's get started! Let's say you're trying to predict number of matches won based on the age. Now usually when the player gets aged any sports person or athlete gets aged the matches won kind of reduces. So you can have this kind of distribution where to build a model you can create a simple linear regression model and the equation might look like this. So matches 1 will be theta 0 plus theta 1 and into h so theta 0 and theta 1 are just a constant. This is a regular uh linear like simple linear equation. But you see that this line is not really accurately describing all the data points. It is trying to find a best fit in terms of a straight line but you see all these data points are kind of going away and then if you have test data points which are lying somewhere here, then this is not a very very accurate representation of our data distribution. Then you can build a distribution which might look like this. So here we are trying to draw a line which kind of exactly passes through all our data points and in that case your equation might look like this. So it's a higher order higher order polynomial equation where you are trying to find out the matches won based on the age of a of a person. But here uh the issue is this equation is really complicated, the line is a zigzag type of line which is just passing through all the data points. And now if you have some general data points at the top here again this is not generalizing the the distribution really well. What might be better is if you have a line like this. So this is this is a a balance between these two cases that we saw previously. So you can have you know only theta 2 into age square the line will look like a curve, and it can generalize your data really well so that tomorrow if new data point comes in this equation will be able to make a better prediction for you. So the first case is called under fitting, the second case is called overfitting, and the third case is balanced fit. So you kind of get an idea on over fitting here where if you try to run training too much and try to fit too much to your training dataset then you will have issues with uh testing dataset. When you try to predict your new data points uh it might not do a better prediction. So you always have to keep a balance between these two extreme cases. Now now if you don't know about all these equations and everything please refer to my linear regression tutorials. I had in the same playlist I had first few videos on linear regression, so you should watch that that's kind of prerequisite. Now how do you reduce overfitting? So here is my overfit line along with the equation and if in this equation I somehow make sure that my theta 0 and theta 4 is almost close to 0, then I will get an equation like this: so just think about it if theta 3 and theta 4 is almost close to 0, then you're kind of ruling out this whole factor and then you can create this type of equation. So the idea here is to shrink your parameters your parameters which is theta 0, theta 3, theta 4, even theta 2, theta 1 if you can reduce this parameter if you can keep these parameters smaller then you can get a better equation for your prediction function. Now how do we do that? We earlier saw in our linear regression video that um we calculate mean squared error so when we run training we pass first sample and then we calculate y predicted on some randomly initialized weights, then we compare it with the truth value and then this is how we call calculate mean square error or MSE. Here y predictor is actually h theta x i where s theta x i could be higher order polynomial equation like this okay, and x 1 x 2 is nothing but it is your feature. So in our case it will be age of a person. If you are thinking about housing price prediction it will be the size of the house. Now just think that in your mean square error function, so by the way this mean square error function we use during training and we want to minimize the value of this this error you know on each iteration. So in this equation what if I add this particular parameter? So what is this? So there is this lambda which is a free parameter you can control it it's like a tuning knob and you are making a square of each of these theta parameters. So now if your theta gets bigger this value will get bigger, the error will be big you know and your model will not converge. So essentially what you're doing is you are penalizing higher values of theta here. So whenever model tries to make the theta value higher you are adding a penalty here. So by adding this penalty you're making sure your theta value doesn't go too high so they will remain very small you can fine tune this thing using this this parameter lambda here, and if you make this bigger the theta value will even get smaller and smaller you know, and if you get this smaller than theta value can be bigger. So this is called L2 regularization it is called L2 because we are using a square and in L1 regularization you are using the absolute value. So that is the only difference between L1 and L2 that in L1 you are using absolute value of theta parameter. Here again if theta is bigger the error overall error is big and it kind of acts as a penalty so that during your training overall the value of theta will remain smaller and again going back to this equation here when these values remain smaller, you come up with a simpler equation you know you don't make it really complicated and simpler equations are the best uh to represent the generic case of your prediction. All right? So let's get into coding now. For the coding I'm using housing price dataset for Melbourne city, I got this dataset from Kaggle and we are going to build a linear regression model. So you can see that there are different features such as room uh the distance, postal code, bathroom, car and so on and in my notebook I am going to as usual first import this dataset and just call it dataset you know. So I imported this into my data frame now and data frame is looking good. I'm going to do some exploration of my data frame now and kind of print out val unique values um in data sets. So you see like there are 351 suburbs these many addresses and so on. Also if you look at the shape of the dataset, there are 3485 237 00:08:51,360 --> 00:08:57,040 7 uh records and 21 columns in total. Now I'm going to discard certain columns. I just did visual observation and discarded certain columns which I don't think are very useful. So for example date right so date is it's not useful, so I will just say okay here are the columns that are useful and I'm just doing copy paste to save the time on recording, and when you uh pass these columns to this dataset you get the filter dataset okay and then again you can run dataset.head so now I have less columns okay? So if you do data set.shape you know I have 15 columns instead of 21. Now I want to check for the NA values and you can do it by calling is NA function on your pandas data frame and you can do dot sum and it will tell you so bedroom 2 has total 8217 NA um values you know NA rows. So now we need to handle these rows. So I'm going to fill some of these columns with value 0 okay and those columns are these: so columns to fill with 0 are this. For example if car is NA which means you know there is no car parking available for that particular property, and when you run this function. So in your data set you're saying all these columns fill NA with zero it will take those NA values and it will fill them with zero. So now after doing this I will do this again and you can see that for example property count it was three so property count now is zero. All the NA values are filled similarly car car parking there were eight thousand seven hundred rows with NA value now I made zero. Now what we're going to do is we'll look at certain other columns such as land size and building area and we will like this through and we will uh calculate the mean and we'll fill those with the mean value. So the way you do that is using this function. So you are doing landsize.filln a with the mean of the same column okay? So this is a safe assumption and after that when you do NA you find that see there are no values with zero. I mean there are some prizes but if you look at your independent features there that's that's basically now curated and there are there are only two columns region name and this council area. So I'm not going to care about it too much. Now I will uh drop those values. So like these three so if you have a couple of random columns with NA value you can drop it you know our dataset is huge and if you drop like these three and three six rows it's not a big deal. So I will just drop them and run the same function again and you see like none of the columns have well uh any values now. So now uh I have some categorical features which I want to convert into dummies you know I want to do one hot encoding. So you guys might know about one hot encoding if you don't. I have a one hot encoding video. So things like suburb for this example right any text column that you have you need to do a dummy encoding and pandas provide a very convenient API called get dummies. So from dataset you get this and you drop first because you want to avoid the dummy variable trap. Again if you don't know about dummy variable trap you can watch my one hot encoding video, uh and this will kind of drop that first column when you do a uh encoding. So now when I look at my data set you see everything there is no text column, and if you look at this council area see council area underscore so it created a separate column for each of the council area. So this is what dummy uh or hot encoding means. Now I want to create x and y. So my x so I can y is basically price and x is you know this one and by the way I had some prices as NA and when I did drop NA the prices those N values got dropped as well so my dataset looks pretty good, and I will do now train test split. So train test split is this. All of these are like standard usual methods we have covered in previous video that's why I'm kind of going over it little fast you are doing a 30% split so 30% test 70% training and you get all these different data sets. Now we will use a regular linear regression okay? So this is how you do regular linear regression, and then uh when you run this it's gonna do training and the model is fit now. Now I will do the score on x test and y test. You realize that my score comes out to be really low 14 percent which is very very low score. But if you do a score on training data set say it is 68 percent so this is clearly overfitting your dataset your dataset was so much over fit that for training samples it gave a good accuracy. But for test samples the data samples that it has not seen before it gave a horrible score. So how do we address this? Sklearn provides a model called LASSO. So LASSO regression is basically L1 regularization. So if you do Sklearn LASSO regression, see this is the LASSO regression and it is L1 regularization. So I am going to use that model, so I created a model imported that and then I created a LASSO regression object with alpha value. I'm initializing this alpha value to be random you can play with these values and see which one gives you a better accuracy, and I initialize few other parameters as well and when I now run my regression it is fitting it with the regularization parameter on. So if you look at our equation earlier. So let me open our equation, in that equation L1 one regularization will add an absolute a theta value in your error and this is the formula that we'll use during the training. Simple linear regression without any regularization will not have this red parameter. So that's the only difference, now let's do a score on your test. So I will do LASSO regression score and you find that the accuracy improved and just to make sure I will also do a score on x test and on training as well. So you see training and test both are giving very good not very good but 67 percent accuracy which is compared to 13 percent it improved to 66 percent. You can see how much of a big difference our regularization can make. There is L2 regularization as well and it's called ridge regression. So if you do sklearn ridge regression this one is an L2 regularization and here I can import that from sklearn library as usual. I create a regression class object which will look like this and call a fit method on this, and then when I do a score on your test dataset it looks six again it's 67 percent. So it's pretty good and let's check on training dataset. Training data set is also you know it's pretty pretty good. So you saw that by using ridge regression and LASSO regression so ridge is L2, LASSO is one your accuracy for your not seen data samples which is your test sample improved a whole lot. If you are trying to learn machine learning you can just in youtube you can just search Code basics machine learning tutorials, I have the complete list of machine learning videos here. You can go in sequence if you're watching regularization video if you want to click get a fundamentals clear initially on linear regression as etc., I would suggest you watch tutorial two and three I hope you like this video if you do please give it a thumbs up, share it with your friends and thank you very much for watching!"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Regularized Regression with Scikit-Learn",
        "Video Title":"Mastering Ridge Regression in Python with scikit-learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=GMF4Td7KtB0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/GMF4Td7KtB0\/hqdefault.jpg",
        "ID":"GMF4Td7KtB0",
        "Publish Time":"2023-10-17T14:45:02Z",
        "Channel":"Ryan & Matt Data Science",
        "Channel ID":"UCKq-lHnyradGRmFClX_ACMw",
        "Transcript":"in today's machine learning video we're going to be covering the ridge regression we're going to be covering a little bit of theory behind it before we end up coding it in a Jupiter notebook with in Python with the help of the S kit learn Library so make sure to stay tuned if you want to learn about this algorithm now few things to call out before we go into the coding this is also called L2 regularization which might sound a little bit familiar to you guys I made another video on the lasto which is going to be considered L1 now with a L1 uh essentially coefficients of features go to zero with an L2 it keeps them small but doesn't go all the way down to zero so a little bit of difference between both of them but the reason why like L1 and L2 uh really exist is that try to help improve models there's an issue out there what is called overfitting and essentially what this does is simplify the models so that way they can perform better with exact test data now with that theory out of the way let's start coding all right so so load up your jupyter notebook let's get started coding so first thing we're going to do is bring in some mock data so this is pretty easy you can just do from SK learn. data sets import make regression and I also know some people mentioned um for me to use some real data sets well this one was prepped a little bit earlier before that so I do apologize I'll work a little bit better on importing them in over here just don't want to have the hassle of downloading data set for you guys like to go grab another link so we'll work around that um so Asen you make regression we're going to fill out a few different parameters we have samples we can have features as well just going to fill out some basic stuff over here so that way we have our data ready to go I'm going to do a random State we're going to use Jackie Robinson number 42 you see that used a lot in examples as well posted online and then we're going to put an effective rank equals to don't worry about the make regression too much um but this is just going to generate your data I'm also going to build a few new cells okay and this also has our X and Y which you seen other videos on like how we split our different data up our X is going to be our features and then our Y is going to be our Target all right so now that we have this in we can do our basic stuff right so first thing we're going to do sklearn model selection try to guess what I'm going to bring in we're going to bring in train test split need that in every one of your machine learning problems okay now we're going to do X oops now we're going to do our X train we're in our X test y train y test equals and we're going to throw our train test split in so train test split then we're going to put XY test size equal 0.2 and this one we'll do a random state of 19 close that off and we now have our data that split between our training and testing data sets great um one other thing that we need to do before we bring in our Ridge is we're going to have to scale our data now one more thing we're going to have to do before we run our Ridge regression is we're going to have to standardize or normalize our data so I'm just going to bring in our standard more God damn all right so one more thing that we need to do before we start running our rid regression we need to standardize our data pretty easy there's something called standard scaler and that's in sklearn pre-processing so from sklearn do preprocessing import standard scaler like that so I don't know if this ran or not it's like a I'm just going to put this down down below this one's kind of blank so we have that now and we'll put scalar equals and either type this out or copy it either way now have your scaler great now once we have this we're going to have to fit our data so we're going to have to fit transform our train and also test for our x side of things which is fun so just do XT train equals and then just put scaler like double tapped the caps lock but we're going to put scaler do fits transform just throw your X train in here again and um we can run this over here shift enter and we're going to do the same thing for our X test this time so just throw your test in here copy paste and your training and testing um features are now going to be scaled which is great so now that we have all this done which isn't too bad right like about eight8 n lines of code we can bring in our Ridge so from SK learn. linear model import Ridge so now our Ridge is going to be in here now if have the caller Ridge so I just say Ridge equal Ridge like this and I know I misspelled it I'll fix that in a second so there we go boom now rid is over here let's fit our data so do fit this time we're going to do our training data set so we have our X train and also our y train great now we have fitted the regression uh for our training data set I know it's a little bit different than this fit transform um we're looking at x's on this side of things our features this one we're fitting right X train and also our y train um okay now what we're going to do is take a look at a few different metrics so all we have to do over here is from sklearn metrics we're going to import in a few others we're going to bring in our mean absolute error and these are just some of the common stuff that you end up seeing with regression problems we can also have our mean squared error and then our R2 score there's other things that you can take a look at as well and you don't even need to import an R2 score because you could just uh run it without this SK learn metrics a different name for it but I prefer just throwing in uh over here just to stay a little bit organized um and I like the nameing convention so one more thing we have to see before we can evaluate this model is you have to do a y prediction so y PR equals ridge. predict and then just put in your X test and this might be a little bit confusing to a few people but essentially we're going to have a y test and we have to compare our y test to the prediction on the only way to do that is by putting our X test data so throw that in over here and let's start finding out how this model runs so all you have to do essentially is just copy these over here then you can just keep it super simple right I just put in your y test and then you just can put in your y prediction right um I guess I could have just typed it out but just going to paste that anyways okay and we get 3.18 on this instance and let's do it a few more times right those our mean absolute so in our mean squared error we have 11.33 and then we'll do it one more time let's do it for our R2 score so grab your R2 score over here grab the end of this over here and 0.927 um so this is a pretty good R2 score that you want to get closer to one if possible now I couldn't tell you how well this mean absolute and mean squared area is because I don't see this underlying data and um we'll see after we do some hyper pret tuning if we can slightly improve this which isn't always the case with Hyper parameter tuning but we should be able to I believe so one thing that you're going to have to know with Ridge regressions is essentially we're going to be taking a look at it value called Alpha now your Alpha value can go in a pretty large range right you can go all the way close to zero I've seen over 100 with this instance and essentially what this is going to allow us to do is change some of these coefficients and our uh model closer to zero our lasso which we talked about last video will take some of these coefficients of zero up but our Ridge will not so a little bit distinction between L1 and also L2 regularization sounds complicated it it is a little bit but just remember lasso zero Ridge no zero for your coefficients and hopefully that helps you out with the data science interview all right so now we're going to put in a pram grid over here very basic stuff right so we're just going to be focusing on our Alpha value on this side of things that's the best bang for our buck and we're going to start off with some very low values so we'll say like 0.001 then 0.01 0.1 then we can have like 1.0 10.0 and then 100 so now we have our PR grid before we do some hyper pamet tuning though one more thing we're going to have to bring in our grid search CV so from SK learn the model selection import grid search CV just like that and now we can do our CV so one thing that I do all the time is I'll just put our model name and then I'll put CV at the end of it just a habit I've generated over time and we'll have our grid search CV in here first thing we have to do is put our model so we defined Ridge a little bit earlier we can throw our Ridge then we have our pram grid so grab your frame grid throw it in there uh we're going to set our CV value I'm just going to do three because we're going to take this three and multiply it by how many values are over here so we have 1 two three four five six right so we're going to do this run 18 times this can balloon up really fast especially of multiple parameters and if they're in this uh because you're going to multiply it out um so I usually do three or five on these instances and then we're going to do n jobs equals ne1 throw that in here boom and that is going to be going um so to actually fit our model we're going to have to do this once again so remember over here ridge. fit copy this or just type it out and I'm just going to put our CV over here probably should take 30 seconds or so or by the time I finish talking okay so we have this Ridge CV over here now let's see how this performs so I'm just going to do y prediction 2 and we're going to that's equal to Ridge cv. predict and then you can throw in your X test over here right and then essentially we can grab these values all over again so I hope these work properly and we get a little bit more of an improvement we'll just throw our y prediction two and we'll copy other two as well okay and lastly our R2 score thr that over here and here's our results so 3.08 11.34 and then 927 so 927 113 so essentially the same exact results between the two and if you want to see specifically you know like what parameter worked the best there's a few different methods to do that uh what's pretty easy you can just go ridge CV and then you can say best estimator like this and it says our Alpha value closer to zero was the best so I'm curious like if we go even lower if we get better results or not um it won't take too long to rerun this so I'm just going to rerun these cells and see and this is the fun part about hyperparameter tuning right you keep messing up you keep messing with your parameter grid and you will get better results over time so this will take a little bit longer which actually loaded really fast um so same exact score there right don't think our estimator is going to change well actually it went closer to zero and um our score isn't really improving on that side of things with the specific Alpha value all right so we can do a few other things too we can take a look at our intercept and also our coefficient now since we did hyper parameter tuning it's going to be like one step further so I show you how to do the spe estimator all we to do is go over here do intercept another way you can do this is make a new model based off of the results from over here and run it again fit it do intercept or you can just do this right um and that doesn't work because I don't have an underscore then so you can see our intercept on this case is going to be 0.15 very very small value then if you want to see the coefficient you can just go over here like this but coefficient like that and then here's our coefficient so we have a 4.3 a 5.08 7.66 and then a 2.89 hope you guys enjoyed this video and you learned something new if you did make sure to subscribe to the channel it's 100% for free but it does allow the channel to grow and it also shares the video with other people on YouTube that want to learn data science right so please subscribe if you have not already now few other things I did use Ridge regression within my final answer for my kaggle housing project now the video isn't out yet but it just shows you how powerful this algorithm is now the vi next video in the series that I recommend you checking out is going to be the elastic net regression it's kind of a mixture of the Ridge and also the lasso or also known as L1 and also L2 and you can check out the video right over here when it's public"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Regularized Regression with Scikit-Learn",
        "Video Title":"Lasso Regression with Scikit-Learn (Beginner Friendly)",
        "URL":"https:\/\/www.youtube.com\/watch?v=LmpBt0tenJE",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/LmpBt0tenJE\/hqdefault.jpg",
        "ID":"LmpBt0tenJE",
        "Publish Time":"2023-10-10T13:52:50Z",
        "Channel":"Ryan & Matt Data Science",
        "Channel ID":"UCKq-lHnyradGRmFClX_ACMw",
        "Transcript":"in today's machine learning video we're going to be taking a look at the lasso regression we're going to go over a little bit of the theory before we end up coding this in Python with the help of the scikit learn library and this is just one video in my full series of scikit learn and also regression models so that out of the way essentially what is a lasso regression well it stands for least absolute shrinkage and selection operate and this is often called L1 regularization now essentially what is regularization well IT addresses overfitting and overfitting is when you have a model that tends to be a little bit too complex and it when we run it it performs well in our training data set but when it comes to our testing data set it performs pretty poorly and what our lasso regression will do is it will get rid of those useless features essentially some of the independent variables it'll turn the coefficient of these to a zero which is known as automatic feature selection and because of that what we're going to have is a little bit more of a simpler model which is a lot less prone to over fitting and with a theory out of the way let's start coding this all right I have a brand new Jupiter notebook let's get started the first thing we're going to do is going to be bringing in our data set and unlike last two times which I did chat GPT this time we're going to be just grabbing our data from sklearn data sets so from sklearn data sets import fetch California Housing now this is kind of like the revised version I think it was Boston Housing that got removed from escaler and data sets and there's a lot of YouTube videos out there I watched in the past um that had it but that data sets depreciated now there's California Housing can't remember the exact reason why um but if you want to learn more about this data set there is this sklearn link over here the California Housing data sets it shows you essentially how you can import this in and then some of the different in information has some medium income house age average rooms bedrooms populations occupany then latitude and also longitude so no missing values this is a very clean data set as you can see over here and um yeah we're just going to use that one because it's super easy to throw in over here that said let's bring this in fully so I'm just going to name this as like ca housing we're going to say that's equal to and you can just copy and paste that over here so I don't want to type that again and then we can set up our X and Y so x equals and we're going to say ca housing again I don't want to type that out we're going to say data and our Y is going to be the target so y equals and then copy that and then just put Target in here and essentially we now have our X and Y you want to see all of our feature names right just go over here the feature names and these are all here again you could just go straight through this documentation and search it but why not right and then if you just want to see the target names we can just go over here and just put Target right and then we get the median house value which again you could just grab that from here but just wanted to show you how that works okay now like every other machine learning problem we're going to have to train test split if you're watching this video on lasso I assume that you already know what that is but I will still explain it in case someone is still kind of new and in the regression Series this is I think one of the First videos um within it so essentially we always are going to bring in train test split we're going to do X train X test y train y test equals we'll throw a train test split over here X Y we're going to say test size equals 0.2 and then random State equals 19 I think that's Joey V's number so that's why we're going to do random State great baseball player that is rumored to retire so you're a baseball fan you know if not don't worry about it all right let me explain how train test split works right we have our X and Y we're kind of transferring those to train and then also test 20% is going to be in our test set 80% is going to be in our train set random St 19 so that way you guys can replicate the same exact results that I'm doing here in this specific video and you always need to split up your data from training and also testing uh you can see some of the different metrics behind that and also it's not good um to train your model on 100% of the day all right so now we're going to bring in a standard scaler essentially if we don't bring in a standard scaler we're going to get some pretty bad results um this is pretty common with both the lasso and also a ridge regression Which Ridge regression will be out shortly after this video I believe um so anyways we're going to just go over here and scaler equals standard scaler and essentially what this is going to do is going to make our mean zero on this side of things again it'll give us more accurate results and I have a full video on the standard scaler and of course this doesn't work because I have to import it right so we're going to say from s learn. preprocessing import standard scaler now this should work over here and this data is pretty easy to scale so you can just do like X train like this and we're going to just say scaler fitore transform and just throw your X train in here okay and we can do the same thing with X test too so throw X test in both of these and now our data is going to be essentially scaled which is really nice now we can bring in our classo regression so what most of you guys watch the video for we can just say from SK learn. linear model import classo okay and I'm not going to put any parameters in this one first we're going to show you some hyper parameter tuning in a second but we're just going to very basic level right so we call our lasso over here now we can fit our data so we can just do fit you throw in your X train you throw in your y I train just like that now lasso is fit to the training data which is fantastic now we can bring in a few different metrics and I've covered these already in the first two regression videos Imports mean absolute error we're going to bring our mean squared error and then we're going to bring in our R2 score see these a ton with regression problems before we run these though you're going to have to do your y prediction so y PR equals lasso do predict like this and then you just throw in X test and I've explained it before also but essentially we have X test we have y test you also need to do a prediction because you want to see how close like these predictions are to your y test it's a little different but it did confuse me at first first so I always try to call that out okay and now we can essentially see how these work so you just throw your mean absolute error over here then we're going to throw our y test and then we're going to put over here your y prediction and then we get our score of 0.91 then we can do the same exact thing for our mean squared and these aren't the best results 1.32 then we can throw in our AR two score and you'll see why yeah this R2 score is horrible this is like zero right to the neg five power model is not very accurate so all right we what we should be doing now is bringing in a parameter grid up is we're going to do some hyper parameter tuning now the value that we're going to be tuning in this model is called an alpha and it's the strength of a regularization penalty essentially with your lasso regression you like I wouldn't say it's a must but you I highly highly recommend to do some parameter tuning especially with this Alpha because your model will get a lot more accurate specifically with it so what I like to do first is just do a parameter grid like this and I will call it there's other parameters as well but Alpha is the most common uh to do and again check the Cy K learn documentation if you want to tune your models specifically however you want again it does take a bit longer with the more parameters that you do throw in here but your the best bang for your buck is going to be through your Alpha okay so we're going to do 0.001 we'll throw in another zero right here 0.001 0.01 0.1 and then one then 10 then 100 just keeping it pretty basic over here so that is in here next for hyperparameter tunning you're always going to need your grid search CV so from sklearn model selection import and you're going to do grid search CV like this and here's our fun part right so now we're going to do our lasso not last but lasso CV we're going to that equal to our grid search CV okay let's throw in our model over here which we're just going to do basic lasso which we called a little bit earlier we have our pram grid and I could have typed that but I'm just going to paste that over here we're going to cross validate three so essentially this is going to run three times how many I have here so have 1 2 3 4 five six 7 so this is can run 21 times if I add another it's going to go 24 so why not right I don't think it'll make I don't think we're going to have an alpha of a th but throw that in there right um essentially that's why this balloons up really fast and then you do n jobs equals ne1 and we have called this but this runs fast because it runs slow once you fit your data into your and you're going to fit your X train and then you're going to also fit your y train throw that in here and this will probably take a second and boom we have a grid search CV right and you see all these different Alpha values that we put over here but this doesn't tell us essentially what our best alow value is and guess what I'm going to show you in a second because what we're going to do is run these all over again so I'm just going to do another one I'm going to say y prediction two essentially is lasso do predict but I need also put in here CV because we just changed that up so we have this in here now and then I can literally just copy all these once again I'll change them all at the very end right and um we'll see how much more accurate this is so we have our mean absolute error we have our mean squared error and then we have our R2 score so now we're going to put y prediction two in each of these and we're going to start comparing our scores so first we have our mean absolute a we have 0.535 over here we have 0.911 and essentially a lower number is better so we've already seen an improvement on this side of things 0.52 right in comparison to 1.32 again really good comparison and then our R2 score is 0.6 now is it a good R2 score no it's not but compared to zero this is a phenomenal upgrade so you might be wondering what was the best Alpha value essentially what worked the best well that's uh pretty easy to do so we can just do lasso we can go CV and then we can just put over here best not Nest but best estimator another over here and 0.001 so this one right here was our best Alpha value for this one and if you want to see essentially even more metrics of this best model there's a few different ways how you can do it I'm just going to create a new model I'm just going to say this is lasso 3 equals lasso again this not the optimal way to do it but just going to show it for you guys over here so because this is essentially how you throw your perimeter in here right your Alpha value just throw that directly in here super easy right again you can just fit the data fit this is assuming that put that in here y train right so now we have that over here and then you can find your intercept so lasso 3 you can put. intercept okay put that here at the end and we can see our intercept on this one is 2.06 okay and lastly we can see all of our coefficients so let's just throw in our coefficient over here so we'll just say lasso 3. coefficient and underscore over here and these are all of our different coefficients so what our lasso model does try to do with some of these coefficients the closer to zero the more it kind of disregards it and like in this format it's probably not the best so what we're going to have to do is I mean what I like to do is put it into a data frame so we're just going to import pandas as PD which I always start the videos with but it's towards the very end this time uh then we're going to just do our feature names and we already have these from earlier so I'm just going to put this over here right and just copy this essentially put this over here for our feature names I don't like the formatting on this so I'm just going to tweak this for a second maybe it's my OCD but I think this looks way better so leave have our feature names over here and then we're just going to name DF equals pd. dataframe like this and essentially you're just going to throw in over here first thing what we're going to do is name our feature names and then we're going to just put dots right and just put a basic comma we'll say coefficients like this right nothing too special and then we'll just grab lasso three coefficient and nothing because we have to put our DF over here and check this out so we have our feature names and then the coefficients that are associated with it so that is the essentially how you build this out so then we take a look very at the very end with this data frame that we just called right and essentially we see the coefficients um with each of these different features that we called from above and these are the final results so that is essentially the lasso model again kind of just a very quick recap of what I did right fetched California Housing I said Cal housing to X and also y right we don't really need the feature Target train test split on your X and Y we scale the data because it's super important for a lasso model and also a ridge model we set our X train X test to that we brought in our lasso model right fit it with XT train and we looked at some metrics like the mean absolute error mean squared error R2 score we have those there then we want to do some hyper parameter tuning the one to take a look at on this one is our Alpha so I just threw in a bunch of different values over here grid searched it got our final value essentially for that over here once we ran these again go 0.001 and I just made another lasso over here just to show you what the parameter looks like if you're going to call it from scratch X train y train we see our intercept over here we also see our coefficients and if you want to see how the coefficients map to each of the different features you're going to have to build out a data frame or you can just go back and forth and look at the feature names like that I'd rather just build data frame and boom we are done now with lasso so hope you guys did learn something new in this video if you did make sure to sub subscribe to the channel it's 100% for free but it does allow YouTube to promote this channel to other people that may be learning data science and I'd really appreciate it now if you're looking for another video to watch I have one over here on Ridge regressions often it's kind of paired with lasso so it's a really good one to start learning from"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Regularized Regression with Scikit-Learn",
        "Video Title":"Lecture 19.05 - Ridge Regression in Scikit Learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=frdGPG10dOA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/frdGPG10dOA\/hqdefault.jpg",
        "ID":"frdGPG10dOA",
        "Publish Time":"2020-04-01T07:19:45Z",
        "Channel":"Joseph E. Gonzalez",
        "Channel ID":"UC2HL-mudeXmqYrBzTB4z66Q",
        "Transcript":"in this part the notebook we walk through how to use the Ridge regression classes in scikit-learn so I'll import the Ridge class from the linear model module the Ridge class implements the Ridge regression loss function that we described earlier in fact if we look at the documentation on this so using the the question mark at the end of our function Jupiter notebooks will show us the documentation one of the neat things about this documentation it has the loss that we described in lecture and we actually work through above however there is one small difference you'll notice that we don't have the one over N so this isn't the average loss but the total loss on all of the data points and this means that the regularization parameter alpha will have to scale with the size of our data and the number of dimensions in our model so I'll press escape to get rid of this so that is the the ridge module so imported again we actually don't need this we get rid of it now we'll insert the Ridge class in our prediction pipeline from before recall that we've defined the Select columns which is going to pull out the appropriate quantitative feature columns then we have the origin encoder which will construct a one hunting of the region of origin for that vehicle and then the text encoder which constructs a bag of words encoding for the name of that vehicle we are using the simple and pewter to impute missing values in the data the simple impute arisia the mean of each column as the value that will be filled in when there is a missing value in that column finally our linear model is no longer the linear regression class but the Ridge class that we just imported earlier we will set the regularization hyper parameter alpha to 0.5 initially and the next part of this notebook will actually walk through the process of cross validating to determine the best value of alpha but we'll start with the 0.5 so you first run this we can then fit the model to our training data and we will save a copy of that model in our dictionary of models without with the alpha value equals to 0.5 and this will allow us to now compare that Ridge regression model with the other models that we defined earlier so a quick recap we've been looking at this this plot through each of the earlier stages of this notebook moving from left to right writing more and more quantitative features the blue column corresponds the training root mean squared error the red column is the cross-validation estimate of the root mean squared error and the hard to see teal column is the test error that we shouldn't be plotting but we're plotting anyways to see how the test stair relates to the training and cross validation errors but again if you make a decision based on this this teal column that will actually invalidate the use of the test data for future testing now if we look at the very far right of this plot this is the model where we combined all the quantitative features the region of origin of the vehicle and the name of the vehicle and we encountered this problem where our training error dropped dramatically so there was a according to our training loss this was a big improvement but the validation error actually went up as did the test air that we're not going to look at but the validation error did in fact go up which would indicate that we had over fit now by taking that exact exact same model set up the same set of features and then introducing the ridge regression penalty so introducing this additional squared penalty on each of the coefficients or parameters in our model we were able to address the overfitting and in fact we sort of went the wrong direction we ended up under fitting again so our training hour went up quite a bit our validation error also went up so we actually made the model even worse by adding this regularization term now there are many explanations for why we might have seen the Ridge regression model perform worse than our original overfitting model the more obvious or standard reason would typically be because we need to adjust the regularization parameter but we actually forgot an important step in the pipeline we forgot to normalize our features so now let's go through the process of normalizing features now I say normalize but I really should say standardize normalize would imply actually constraining the rows of the feature matrix to have unit norm now this is something you might also do to control the magnitudes of the features in fact it's built into some of the regression packages but in this class we're going to focus on standardizing our features because one this is something we've covered already in the past and this is a pretty standard technique pardon the pun for ensuring that our features have all roughly the same order of magnitude so let's fix the notebook so we say standardize all right so to standardize the features we're going to use the standard scalar transformation but actually before we do that I want to look at briefly why the Ridge regression model might have performed so poorly without standardization so let's look at our training data so we look at our training data we notice that our quantitative columns have a pretty wide range of magnitudes from 4-8 cylinders to displacements on the order of hundreds - things like weight on the order of thousands and then we have columns like the origin that would be one hot encoded having values between zero and one and likewise our name column is going to be bag of word encoded and again we'll have values in the orders of 0 and 1 so we have a pretty wide discrepancy in the values that in our feature matrix which means that applying a single regularization parameter to all of these different magnitude features will create a problem for example a very small parameter attached to a very large thing like the weight of a vehicle it could have a big impact in miles per gallon without paying much in terms of the regularization penalty well maybe a really important thing like the number of cylinders might actually need a fairly large parameter to have an impact on the miles per gallon so maybe eight cylinders times or let's say four cylinders times five be a good coefficient estimate miles-per-gallon perhaps and that's a much larger than the maybe if I was going to wait two miles to gallon this would be some small fraction all right so the the features have pretty wide discrepancy in their magnitudes and so we want to standardize these features so to do that we'll use the standard scalar class from scikit-learn and the standard scalar class we'll take the mean of each column subtract off the mean and then divide by the standard deviation now in our pipeline it might be most natural to apply the standard scaler at the very end of the pipeline unfortunately if we just try to apply it directly we'll actually run into a problem because the count vectorizer outputs a sparse matrix so the matrix that's coming in to this stage of the pipeline is actually a large sparse matrix and because it's a large sparse matrix the standard scaler is actually going to raise an error because subtracting the mean from all of the zero entries in the matrix will make them nonzero transforming a sparse matrix to a dense matrix now we can suppress this error by setting the width mean flag to false which will avoid subtracting the mean this would probably still work as a standardization method but we can actually bypass this all together by realizing that the count vectorizer and the one hunters in the order of zero to one anyways so it's not quite as critical that we rescale those in contrast the quantitative features do need rescaling and so we can apply the standard scaler transformation just to those features directly so we put the standard scaler operate the standard scaler class here instead of what was previously here the pass through string which was just passing the values as they are onto the next stage in the pipeline so we'll comment this out and go ahead and run this and now we can train this model with the scanner at standard scalar transformation we're going to call this Ridge n4 Ridge normalized though it probably should have been called Ridge s4 Ridge standardized but we'll stick with n so that the notebook doesn't - dramatically we can fit this and we can examine what the resulting model looks like all right so we've already made a lot of progress we've chose an alpha of 0.5 which was just a guess and then by appropriately standardizing our features we're able to get a training err which is low but more importantly a cross-validation error which is lower than any of the earlier cross validation errors before and this is using the model with a large number of features and just to recall the number of features recall in the earlier video we actually computed the number of features and so this model has 270 features that we're using so it's a lot of features given our training data set has how many entries let's go take a look we lost it here so we look at our training data turns out features to our 98 rows so that is a lot of features for the amount of training data that we have so using regularization we're able to make use of that large number of features without overfitting all right now what about different alpha values so one thing we could try is just a different out alpha value in our earlier setup so here we don't need this extra line we've said alpha instead of 0.5 we'll try a much larger alpha of 10 so let's do that again will train and compare this model so now we've added the alpha of 10 that did as we might have expected so increase in the Alpha value caused our training error to go up it also caused our validation error to go up slightly which would suggest that the model was starting to under fit as we increased the alpha value so we had an opportunity to make the model a little more complex make the alpha value that's smaller move in the direction of overfitting to get better accuracy and in fact that's what we saw here now something you should never do but let's go ahead and do any ways we can look at the test air two it's interesting to note that the test air did appear to actually go down by making the alpha larger this could be just coincidence we shouldn't again make the design decision to choose the larger alpha because our test air went down because we shouldn't be making design decisions based on the test air the only use of the test there is that once we've done making these design decisions to actually look at how our final model performs so based on validation error we would say that probably the smaller alpha is better it's also worth knowing that cross-validation because it does repeated resampling the estimate of the the error is actually pretty robust and so I might actually trust in the design process this validation error more than I would the the hard to see teal bar the test air alright so now let's explore how we can use cross-validation to find the optimal value for our regularization hyper parameter so here's how we'll do that so I'm going to implement the cross-validation search procedure for the optimal value of alpha I will do that by first defining a pipeline so here's my pipeline I've set the initial value of alpha to ten though we'll replace this value immediately when we start to use this pipeline I'm going to try a range of different alpha values between 0.5 and 20 you might play with these values based on how the curves look and I'll talk about that after we've looked at the regularization plot I'm going to keep track of the cross-validation error the training error and again the test error you shouldn't keep track of the test there you shouldn't even be looking at the test error at this point but I want to show you how the test error looks when you when you compare it to something like cross-validation and training so we're going to loop through each of these alpha values in this loop here the very first thing we're going to do is update the alpha parameter of our Ridge regression model and in particular we're going to do that by updating the the final stage of that pipeline because our Ridge model is a pipeline of multiple stages the set params function on scikit-learn models has a neat property where you can pass in the name of the stage of the pipeline in case this case the linear model followed by a pair of underscores and then a parameter for that particular model that you want to update this is actually equivalent to doing something like Ridge model linear model set params alpha equals alpha and in fact I think you can even do alpha equals alpha so these would all be ways to set the parameters I'm going to demonstrate how to use a set parameters function and this neat feature of of constructing the path to that parameter by the the name of the model the stage in the pipeline followed by the parameter for that stage alright so once we've set the parameter for the model we can then call the cross-validation function in scikit-learn which is going to take that model and run five fold cross validation using the RMS e score function we defined earlier to evaluate the cross-validation score root means great error score using the training data and we'll take the mean of all of those so we'll take we'll get five different scores we'll take the mean of that's those scores and append that to our list of cross-validation values well then fit the model on all of the training data and look at the training error here and the test error here so run this so we can now plot each of these different curves so we'll run this plot here we have is the training error as we vary the value of alpha the cross-validation error as we vary the value of alpha and the test air which we should not look at as we vary the value of alpha so first its it so first you notice that the training error increases and we expect this so by increasing the value of alpha we're in a sense causing the optimization problem to favor not fitting the data but instead decreasing the magnitude of all the parameters the cross-validation error which is a better estimate of how the model will perform on new unseen data this initially goes down and then starts to jump back up which suggests that the best value of the regularization is a smaller value of alpha in fact we can zoom in on this curve here this we're trying to optimize so an alpha around 1.8 for four seems like a pretty good alpha for this model and so that would be our best alpha if we're trying to minimize the cross-validation error now one thing that you might notice in this plot is that a lot of these larger alphas are probably not worth examining so we might really want to just focus our search in this space and look at a slightly higher resolution so we could try to do that right now so go back up here and say well really we care about numbers between 0.5 and 3 and maybe we want to take 30 values in that range and then we can run the plot again and so now this red curve here is a little bit flatter because we're focused just on the regions where the model is a good fit potentially so now we could focus just on the cross-validation curve you'll notice it's a bit flatter because we're focused just on the Alpha values that are likely good candidates so we zoom in on this curve here we start to see that it still goes down and then back up and that the alpha value that's optimal is somewhere around 1.5 maybe 1.6 in this this ballpark here all right this is our best alpha and so we can then go back down here if we choose the Arg min of the cross-validation values and take the corresponding alpha we selected the best alpha according to its cross-validation score we can then set our model to use just the best alpha and then we will plot and save this model so here is our model with the best alpha you notice this this now gives the lowest cross-validation error so this is our best model up until this point so this model achieves the best compromise between fitting the data and generalizing to new data in practice implementing this cross-validation search can be a little bit cumbersome you have to write a lot of code scikit-learn actually has a built-in Ridge regression model function that automatically cross validates itself and that this is the orig CV class so we can import this instead of the Ridge class we import the ridge CV class we give it the set of alphas we'd like to automatically cross validate over when defining the instance of that class so do that here and we're building just another pipeline with this new Ridge CV class I can then call the fit function on this model and plot this as well and we expect this will probably perform similarly to our the model we just did a search over so here's our Ridge it actually got just a slightly higher cross-validation error so maybe it's not not quite as good but it's not bad now it's actually worth learning the cross-validation step is actually probably calling the cross-validation search for each of the different cross-validation values so this is you shouldn't use the ridge CV class within a a cross validation loop so this this number might be just slightly off actually let me state that one more time is it's probably a little bit confusing notice that we're using the ridge CV model which is going to do cross validation but in order to compute this red bar are plotting code actually calls this whole pipeline inside of a cross validation loop which means that in each step of the cross validation loop it's going to then run yet another cross validation loop inside of that to choose the best hyper parameter for each of the cross-validation splits and that's probably not what we want to do so instead what we might have wanted to do is pull out the best alpha from the ridge CV and then construct just a simple Ridge model using that alpha and then run the cross validation and I'd expect that to be basically the same as this model here alright so that is the process of doing Ridge regression using cross validation to tune a hyper parameter the remaining video in this notebook will look at the same process focusing mostly on using the the built-in cross-validation functions in the context of lasso and it will also examine the the ability of lasso to do feature selection"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Regularized Regression with Scikit-Learn",
        "Video Title":"Lasso and Ridge Regression Implementation using Scikit-Learn in Python",
        "URL":"https:\/\/www.youtube.com\/watch?v=yXn9b1_T9hY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/yXn9b1_T9hY\/hqdefault.jpg",
        "ID":"yXn9b1_T9hY",
        "Publish Time":"2023-03-30T09:12:36Z",
        "Channel":"Ahmad Dawood",
        "Channel ID":"UC2xj-7qh-VaGbFv_AVS5gxg",
        "Transcript":"and in this session we will see the lasso and easy regression how we can implement it by using SQL Library so in the last session we have seen uh in more detail and in more visual representation how less Source objective function or you can say cost function works how penalty L1 penalty when we add to the cost function it decrease or Shrink our coefficient that we already have or it can also eliminate coefficients by making them zero we also have seen regularization in the case of regionalization we have seen that like here you can see again in the region regularization when we we add L2 penalty in the cost function then our parameter values are you can say shooting but region regularization cannot make them equal to zero okay so today we will see how we can Implement them by sqln so before that in the end of last session I told that we have implemented till now linear regression when we have used the linear regression for a single variable but any example of linear regression multiple regression polynomial regression till we are using linear regression class okay in which I told you that linear regression class implemented by SQL and use closed copy solution okay we have learned two type of algorithms or two type of Concepts how we can train the linear regression model or how we can get our optimal parameters one we have led the closed form solution and second one is was a gradient reset and I told you the lasso regression provided by a scalar we implemented the algorithm by using gradient descent okay lasso regress is regression by a scalar trained by using credity set and regenerabilization has both options okay so before that uh it is important to tell you that there is no restriction that just these two algorithms like closed form and graded descent uh just these are two algorithms to solve or to get parameters okay these are the general two algorithms like you know created incident there are some variations that you can attendee set like page batch gradient reset mini batch created a decent stochastic radiant descent that we have learned was a batch quality set so there are there are multiple variations in the gradient reset and also then uh in the case of closed form solution there are also some variation in the computation okay sometimes uh for some uh core mathematical terms used like a single uh similar value decomposition SVD used okay but uh these are not uh like uh important to go in the data of them but before that uh here why I am telling you about that because SQL implemented them okay there are multiple solvers in the reach we will see that documentation of fridge provided by S killer so in the case of uh lasso okay so why I am telling you because uh in the case of graded descent uh like uh if I will show you help in the case of linear regression we have seen the multiple here like this one okay in the case of gradient descent you have seen that gradient Works in that way first we have to random initialize our parameters F5 and solve it random initialize our parameter values then we have to calculate the partial derivatives of cost function with respect to each parameter then we have to update our parameters in the opposite direction of gradient and the last one is repeat step two and three still we reached maximum iteration or minimum cost function value achieved what exactly this step for is that the process that we are creating taking derivative update our parameters again taking derivative update our parameters how many times we should run that Loop okay this is that how much iterations we should report to optimize or to get our optimal parameters so this is provided as a parameter if I will show you here right here in the case of less so so you can see here there is a parameter Max either okay this is a maximum number of iteration that is by default one thousand times this uh less so in the case of lasso when we use lasso by SQL 1000 times it will repeat that process of updating parameters okay by default we can also control it by passing some value in that parameter maxider okay okay so now come back to what is the reason that I am explaining you here this one the reason is that in the case of if I will make it in the simple way in the case of linear regression when we were using linear regression okay in the case of linear regression in the case of flash so in the case of bridge okay I will make time integration regression um I will solve that so you can see clearly okay in the case of liquidation okay so in the case of linear regression if I can let me use another pen here in the case of linear regression our post function was just P Square that's it okay it means Square in the case of linear regression in the case of less soap we have our course function um this one we have our cost function in which we have added some penalty with the alpha parameter okay some quantity like of L one okay we have added in this in the case of reached regularization we have our course function like this means okay plus Alpha to control the strength of a penalty how much we should paralyze our cost function or V square error here we have L definitely okay actually we have squared and 2. we have L2 penalty we add in the course function of in the case of regionalization or in the case of rigidity regression okay so if I will make here Alpha equal to 0 if I will use this one if I could make it Alpha equal to 0 it will become the same as the linear regression okay and this will also become same as a linear equation okay but when we will do practically by Implement by using these libraries provided by the SQL divided by these classes from the SQL maybe we will not get uh same result that we are getting in the case of linear regression and in the case of lasso regression when we have made Alpha equal to zero okay actually they have when we will make alpha equal to zero the linear regression and lasso regression should have same have same cost function so they should have same type of parameters in the result but we will not have the reason is that linear regression class by SQL is using uh close some solution okay less so used gradient descent approach and reach has both options but again you will not exactly the same result in the case of reach when you have made Alpha equal to zero the parameter values that you have calculated by linear regression okay so they have multiple options one two three so on and we will also study in the case of logistic regression when we will start our supervised machine learning module in the for the classification problems where we will also see in the more detail what are the different swappers like we have studied really decent and closed form solution we will see how some variations of gross form solution the greater distance are available and how we can use them okay so for now uh without wasting time we should move on our part of code so okay before the code uh I want to elaborate here uh in the case of linear regression you have seen that parameters so I will not go in the detail of that parameters that will be have in the case of lesson okay like filter intercept we know that why we should use situated intercept uh that we want intercept value or not okay in this way we can use that fit intercept is equal to fast that we are wondering such about it at uh just have coefficients no any input then normalize we can use to normalize the values before trading the data to our modified training then copy X to copy and geops is not it's fun parallel processing but in our case it's not unnecessary to learn in the case of positive that we are forcing all the coefficients that to be posted okay so these are parameters are in the case of linear regulation we have learned so when we will move on the lasso regression there are some uh extra parameters so first of all uh if I will show you here what course function it is using there is some variation they are using they are not using exactly mean Square atom like this course function we are using uh and plus penalty this is L1 penalty is the same that we have seen Alpha that we are using here the symbol Lambda here they are using uh Alpha okay so this is the cost function which is not exactly the mean square error because in the case of mean square error this is sum of scader like this is our true value so Pi is True Value this is predicted value and then uh it's a summation and the square of them and divided by it should be divided by just n okay like f if we have ADD number of points but they are dividing by uh n into two okay there is one uh extra factor which is 2 in the denominator so but really the purpose of that I don't know but it's really not to address okay the the the parameter values which will give this uh minimum uh [Music] the same parameter will give you the minimum first function at this also okay like I I told you multiple type that uh means if you are using mean square error if you are using sum of squared error your minimum will be at the same parameter values like in the case of reach if I will show you rich I actually use some of scaled error here okay but in the case of so they have some variation I don't know exactly what is the what is actually the reason okay uh if we will study the code of that uh lesser regression we can figure out what can be a reason why they are using a 1 over 2 into uh n samples okay instead of just one one over n samples okay they are using that one as a objective or closed function error okay anyhow moving come back on the uh parameters okay so parameters okay one another concept that uh I am not telling you here is the elastic net okay elastic net is a variation we will study in further modules also hopefully in the next module elastic net is you can say It's a combination of L1 and N2 penalty how much we want L1 penalty how much we want L to penalty there is a variation that we can use in some algorithms uh in some libraries uh like if in the case of logistic reduction if I will show you here yeah in the case of logistic regression there is a variation that we can use L1 penalty and two penalty and elastic land okay we can pretend that we are interested in the combination of both and one and two connected okay anyhow we will not go in the date of the detail of elastic land it is really simple but for now just we should remain on the L1 penalty and L2 penalty for L1 we are using lasso for L2 we are using regionalization or uh as we are solving uh regression problems so lasso regression we will use here in the case of Albert 30 and in the case of reach we are using uh in the case of L2 connective we are using region okay so come back here Alpha Alpha is like we have Lambda in our cost permissions like we have here Lambda or sorry I have also uh today I have mentioned in the form of uh in the form of alpha I have mentioned but usually we have used the term Lambda if I will see there um yeah we have used the term Lambda and here actually foreign the different notations but the concept actually matters uh in fact exactly this is representing here okay this is representing how much uh the value of alpha is represent how much penalty we should uh we are uh we want in our course function how much penalize uh we should do our cost function of our mean square root let's say it shown okay so this is Alpha here uh in which we can pass any value if we will make alpha is equal to zero it will be same as we have cost much of the case of linear regression same parameter that we have in the case of linear model linear regression fit intercept normalizes the same parameter three compute its not necessary here okay it's again the same to copy if we want to copy the our data so for their two parameters I am telling you to see the detail I have explained that Max iteration and tolerance okay the maximum iteration that I have told you in the case of a gradient descent we have uh we can specify how much iterations we want uh how many times we want to update our parameters okay sometimes we uh we don't have a restriction on the uh our iteration that we want as much number of times we should do uh we should iterate our uh our algorithm our parameters until we get our uh this uh you can say until we get our optimal parameters with this 12 range okay like here we have a parameter another tolerance it works in that way my default it is uh raised to power minus 4 you can say 0.001 okay it means if you are updating your parameters okay in each iteration your parameters are updated if the updation okay that like uh that you have updated your parameter the um the difference that the parameters values that you have before now after updating you you have some different parameter values if the difference between the previous and the now is less than 0.0001 then get the algorithm will stop there okay if the difference is higher than 0.001 then it will keep or alternating okay so those is uh it's actually mean how much tolerance we we can bear in our parameter values which are uh away from our exact optimal parameters okay because if when you will see in the neural network we will do in the details a greater descent how because sometimes when we are iterating our uh let's say I can tell you here let's say we have this course function and we are iterating in this way yeah sometimes we are hydrating in this way like in this way we are iterating okay and this is our exact uh minimum of function okay minimum cost option or minimum at this point our function is minimum but sometimes when we are iterating we jump on this side okay and when we see that our cost function is our error is increased then we have to decrease that when we move we will we will again jump over optimal parameters okay so to control that there is another uh parameter that we have learned in the general design learning rate okay we can decrease the learning rate here if in the start is higher then we can decrease our learning rate but anyhow hope we cannot ideally we can reach at this position okay we cannot ideally we should clear some sort of Tolerance in the case of closed form solution we get exact result but in the case of graded decent uh when we have very complex uh functions we should have some toggles okay here we can mention our tolerance by ourselves and by default it is 10 raised to square minus 4. bomb start uh it's uh again not important to discuss here positive is the same that in the linear regression random State uh again for now it's uh um no need to discuss that random State and selection okay for now you can see it it is in the left so that's okay now move on the code and here we have our what okay so first okay this one last one so I am using the same polynomial model so I will zoom in here okay because the the code that we have implemented in the polynomial session main polynomial modeling when we were doing uh the same code I will Implement here okay same function this same function I will use just in the case of linear regression here I will replace this linear regression by less Source okay and one another change I will made here is that I will fix my degree degree is equal to 200 that I will artificially making my model complex okay or over fitted model here making here I am making degree is equal to 200 and you can see in the case of degree equal to 200 what actually the result okay yeah this is our polynomial models with transformation this is our polynomial model which is using linear regression of this polynomial model uh with the transformation using linear regression in that case when you will pass the degree 200 it is giving you this over fitted model URL seeing that very complex only uh very uh you can say it is getting all the variations of training data points but uh very weird behavior on the test data points okay you can see and in the case of accuracy you can see 98 accuracy giving all the training data points and very low on the score or on the test data set it is giving okay so this model exactly this model I will use with the 200 degree I will fix uh in that case in the case of lasso yeah in the case of like here I have mentioned in here I will fix degrees equal to 200 and I will made a function yeah this one lesser regression I will made this function okay this function the same exact diet function just two changes I have made it here okay the same function I am using it first change is that I have I have fixed here the degree is equal to 200 second change is I am using here instead of linear regression I am using here less so that I have imported from the skelef okay third you can say instead of passing that previously that we are passing our degree how much degree in we want here I am passing Lambda value okay how much penalty I want in my mode okay in my cost function how much penalty I want this laptop is Will directly move uh this value directory pass as in to the parameter Alpha in the case of lasso okay if Amazon you can see here this is Lambda will go here in the alpha okay now I can use this function okay this function that I have made some changes in the function of polynomial this function I will use and I will see how lasso regression works okay Okay so one and other important here if I will show you here the last show documentation in the documentation there are some attributes one important attribute is this one's sparse coefficient what exactly means pass coefficient like I told you in the case of laser regression we can when we have added less and one penalty in our post function uh it can make some coefficients equal to zero for the less important features okay or you can say it can make the coefficients 0 for the features uh or maybe the coefficient is equal to zero that are lower lesser in magnitude for that I have lesser value as compared to other coefficients okay or insert the in conceptual we will say that it uh make the coefficient 0 for the less imported features but here okay here I am again uh again with the coefficients I am also Printing Pass pass coefficient sparse coefficient will print all the coefficients that are not lasso has made equal to zero okay that are same as the data remained uh non-zero and the if I will print the all the coefficients then it will print the 0 and no zero both okay now as we run that before that uh okay so I will run this one this function and here I will show you in the case of polynomial model with transformation if I run again this we have this model when we have we have not added any penalty this model which is very complex model you can see here and very weird on the test data point status attached five we can say it is problem first of all I will run lasso regression with this zero uh you can say my Alpha value or you can select the value okay that I am not adding any dependency to my cost function and when I will run here it will give me this function this model now you can say that as we have made it our penalty equal to we have made uh removed our penalty by making Alpha equal to 0 but it is not giving the same model as we are getting in the case of this linear regression this class as I told the reason is that it is using a graded descent approach okay and by default lasso is running of one uh 1000 times here it is running 1000 times so if I will make here Max iteration of Max underscore hydration if I make it one lakh let's say so you know this one here now you can see it will give you some complex okay as before like it now you are getting this sumutra I mean even in the case of 200 degree polynomial but when I will run that okay it is slow back because I have increased the iterations uh previously was 10 000 by default and now I have made the volume lag one lakh times the parameters are updated okay here you can see our model is up become little bit complex okay now I have to again repeat the I have to increase my iterations and I can by increasing the iterations by decreasing the tolerance I can get the same model that I have in that case anyhow the aim is here to get the concept how lasso works okay when we will make uh here you can see there are all the coefficients values are non-zero values hence pass coefficient is also printing all the values here all the 200 coefficients here here you can see all the coefficients are normally but when I will make and here you can see in the case of 0 we have 92 percent score on the uh trading data set but very very less score we have in the form of relative on the uh test data points in the case of mean square error you can see mean square of error on the test data set is uh 10 raised to about 21 which is very high very very high okay now here if I will I will run another function like okay 0.2 now are you passing here the value as 0.2 means I am adding 0.2 penalty in my course option now I will see that most of the coefficients it will make equal to 0 plus regression and it will generalize my model and decreased our fitting we will see here now from these coefficients you have seen that a lot of become equal to zero I have created a sparse coefficients from this past question you you can see the coefficients that are non-zero here total we have one two three four five six seven eight nine ten eleven by Thirteen out of 200 coefficients onto 200 coefficients 200 coefficients due to L1 penalty 187 coefficients become equal to zero now we have just 13 coefficients now in the case of 30 coefficients we have seen here that more importantly you can see here in the case of trading data points we have now 90 percent accuracy and here 0.9 accuracy had in the uh on the test data points we also have 0.89 okay now you have seen how our score is increased by penalize our complex model okay here you can see your model is also uh giving your representing good on the test data points also okay now you could appoint how lasso lasso actually makes the curve shades equal to zero okay now practically you have seen that by implementing it by using SQL and you can do that now here keep in mind we are using polynomial regression 200 degree okay our actually independent variable is just one which was uh in our case like it was made by if I will show you here we were having weighed our okay we were having great already dependent variable we have just one independent variable we have increased our independent variable to polynomial features like we increase by making degree equal to 200 we will have 200 polynomial features here okay so here we are making it now here it is important when we can make our model simple and we can avoid our fitting by making some coefficients equal to zero okay now in some cases lasso cannot work good here we have not or we don't need to make coefficients equal to 0 but in our case in this case uh it is giving us a good result and because here we have need to make some coefficients equal to zero because we have a lot of coefficients that we have made artificially a lot of a lot of complex model okay in the case of lasso now when we will see that each part regression by SQL you will see that it will not give us a good result it will not avoid overfitting instead it will add date to the underfitting case and the reason is that it will here we have need to make some coefficients equal to 0 but Rich cannot make regret or coefficients equal to zero start it tries to make equal to 0 but instead it will decrease coefficients approaches to zero but not exactly big coefficient to zero and you have seen by the by this graph also um where this graph also this lasso when you have added lasso penalty in your post function it increase here you can see it can decrease it can make some coefficient like here it has made Theta 2 equal to zero when it is minimizing but in the case of Ridge it is not making uh some coefficient 0 but it make actually approaches to zero like if you will see here your Theta 2 is not exactly this is if I will show you here yeah this is your Theta 2 it is not equal to 0 here in the case of reach but in the case of in the case of lasso when you have added a lasso connect here your Theta 2 here is on the yeah Theta 2 your Theta 2 is 0 that is okay less we can make coefficients equal to 0 but Rich cannot make and even it decrease that distance it will try to minimize that is approaches to zero but not exactly and make equal to zero sum coefficients okay we will see that practically also 5 S K log okay before going to the uh towards that uh again I will show you the documentation of feature regularization in the region hybridization you can see here it is using sum of squared error now again Alpha parameters to control the penalty ft intercept normalize all most all the parameters and as I told you reach regularization in the case of reach we have multiple options we can use multiple solvers like classical solver is if we are interested is to close from solution of formula cost function if we are interested into use a gradient descent then we can use stochastic average gradient this one we can use and we can also use s a g a for this both sag and Saga we can use what graders okay there is another uh reason for that why in the case of lasso we cannot use a multiple uh course function multiple servers religion can be here that in the case of lasso we have function of the penalty function that we have it is using absolute so if I will show you here yeah in the case of lasso we have this as a parenting and in this we are using absolute values we are using absolute values so absolute function is involved in the our cost function in our overall course function absolute function is involved okay as absolute function is involved then our function become uh it is it will become like uh difficult to get differentiate shape or difficult to uh do differentiation on that function okay to take derivatives okay because it has in the case of absolute function we uh it is not defined at every point okay due to this issue there is not multiple forwards in the case of lasso by SQL but in the case of Rich they have provided multiple solver say okay all the soldiers we will not see here okay but I have told you here here they have provided all the same words multiple soldiers now come back to our code and again in the same way like uh if I will show you our original polynomial regression function yeah this one here just I have replaced my linear regression my digital regression and here instead of passing a degree here the alpha value will pass and it will come directly to here okay and in that case uh I will not use a sparse coefficient because it uh here may be in the rich they have not provided as yeah they also have not provided sparse coefficient the reason is the same uh that because in in the region regression uh there is no chance of making coefficients equal to zero okay so there is not uh such coefficient that are like equal to zero and some sparse coefficient that are not equal to zero all the coefficients will be not equal to zero closer to zero we can say but will not equal to zero now here I will show you read regression function that I have read here this one you can see here rigid regression here I am using this easy regression okay and I have made the solver class key and if we will not use a glass key it's up to us it by default it can use Auto and with the with respect to data it can select by itself what should be the solver here because according to data it can select here like this is Auto by default is auto server and auto choose the server automatically based on the type of data that we are passing in the reach okay I'll remove that but now okay now a small bracket okay if I could run that function now you have seen in the case of polynomial without penalty we have that type of complex function okay when we will make it uh widget regression and I am passing here Alpha value 0 means I am trying to use the same course function without any penalty but here you will get again some different it will be a little bit complex but different um caused a different model here instead of that model the reason is that because of different solvers they are using linear regression using different solver which is here using different forms if I will make here let's say um this one is Rich one okay if I will make here that I am interested in the solver of and this one that if I can use this classic resolver here if I can hear this one Commonwealth okay now again you will not get the exactly the same result because again I am telling you there are some uh variations of the solvers that are actually using okay there can be a variation of exact clothes they are not using exact close form solution linear regression is not using is that close form solution uh because I told you in the part of the session that they are using singular Vector decomposition and pseudo-inverse technique okay okay here okay now if I run that function maybe it will become more from slacks as before we have but it seems really same but anyway here we are again uh having a overfitting problems our model is complex or catching a lot variation in the case of training data points but very weird indication test data points and practically we can see here that the score of standing data points is 94 0.94 but on the test data but it's very less and mean square error is very high in on the test data points then you will run that the uh please regression that's it okay and again see these are the parable coefficients and all the coefficients are non-zero here now in the same way if we will use here 0.2 like we used in the case of lens so we have used 0.2 now alphabet is 0.2 here you can see there is no coefficient here it has made equal to zero but in the case of plus so you have seen in the case of lessons 0.2 yeah indicates of 0.2 a lot of coefficients become equal to zero yeah but in the case of which it is not making here coefficient exactly equal to zero maybe it was making it some closer to zero but not exactly equal to 0. and again it is it has decreased that uh like uh it has increased your test score but not uh somewhere you can say significant increase here that you can see here the previously was placed it is in the form of minus so there is the power 27 is more or lesser value than here um minus value in the form of raised to power type raised to power 7 days so it has decreased your score on test data point but not here is a significant increase and here it also made some smoothness on this uh training data and the model is smoother but again we had on the case of test data points okay like I told you in our case like here we are using uh we are trying to make uh simple but we are trying to make a polynomial 200 polynomial model uh make we are trying to make them to make it simple so here we need to make some coefficients equal to zero but Rich cannot do that so that's why it will not give you a good result even you will like here I have used 200 as Alpha value which is quite High it can make your coffee like here you can see your coefficients very very small like 0.00900 which is very close to zero but it cannot make it it was equal to zero here you can see okay here it it tries to make equal to zero but exactly not we will get here equal to 0. okay in the case of like here I have used very high alpha value then I have penalized my course function upload here you can see instead of increasing it has also decreased my score on test train data points and uh again a lot of improving on the test data points and we are falling into the under fitting model become more flatter here or flatter because we have decreased a lot in the coefficient and I told you coefficients are slopes when we decrease the slopes your model will have uh like we will have lesser sharp slops when we have higher values your your model will have sharp slopes here your model is becomes flattered and again this is in the case of Alto penalty so you are not getting good model on the test data points okay now you have seen in our case where we have implemented our both models uh lasso regression and reason regression lasso works better A lot okay it has reduced the complexity of model by making a lot of parameters because of a lot of coefficients equal to zero and it also uh have overcome the problem of the overfitting but it cannot work here but there are multiple applications where it can work where we don't need to shrink our coefficients we don't need to uh eliminate our coefficients we have to shrink proper coefficients like from the Lesser word less you can work in both ways least absolute shrinkage and selection operator why this selection operator selection operator is for the purpose uh it make the coefficients equal to zero for the less important features okay sometimes it can also be used as a selection operators watch who which features are more important we can select by using lesson again and one another important that in the case of rich and lasso regression we should scale our data okay in the case of like a linear models or in the case of uh linear regression and such type of models we usually normalize our data before fading to model but in the rigid lasso it is really important to do a scaling our you can say features okay before building into the model so I hope this was a technical and more expiration of the last session and a little bit about how we can Implement so I've explained you here and I have explained you that is practically I have shown you here that how lasso can make some coefficients equal to zero but Rich cannot sometimes we have we need less so and sometimes we need reach okay and both um usually used to federalize or you can save to overcome the overfitting problem so I hope you got and learn and you got enough uh for the case of the Regent lasso in the later sessions if we will come across in the case of classification we will again can discuss that and the fun of thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Logistic Regression",
        "Video Title":"StatQuest: Logistic Regression",
        "URL":"https:\/\/www.youtube.com\/watch?v=yIYKR4sgzI8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/yIYKR4sgzI8\/hqdefault.jpg",
        "ID":"yIYKR4sgzI8",
        "Publish Time":"2018-03-05T15:17:39Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"If you can fit a line you can fit a squiggle if you can, make me laugh you can, make me giggle stat quest Hello, i'm josh stormer and welcome to stat quest today we're going to talk about logistic regression This is a technique that can be used for traditional statistics as, well as machine learning so let's get right to it Before we dive into logistic regression let's take. A, step back and review, linear regression in Another stat quest, we talked, about linear regression We had some data Weight and size then, we fit a line to it and With that line, we could do a lot of things First we could calculate r-squared and determine if weight and size are correlated large values imply a large effect, and Second calculate a p-value to determine if the r-squared value is statistically significant and Third, we could use the line, to predict, size given weight if a, new, mouse has this weight Then this, is the size that, we predict, from the weight although We didn't mention it at the time using data to predict something falls under the category of machine learning So plain old linear regression is a form of machine learning We also talked a little bit about multiple regression Now, we are trying to predict, size, using weight and blood volume Alternatively we could, say that, we are trying to model size using weight and blood volume Multiple regression, did the same things that normal regression did we calculated r-squared and we calculated the p-value and We could predict, size, using weight and blood volume and This, makes multiple regression a slightly fancier machine learning method We also talked, about how, we can use discrete measurements like genotype to predict size if you're Not familiar with the term genotype don't freak out it's. No, big deal just know that it refers to different types of mice lastly, we could compare models So on the left side we've got normal regression, using weight to predict size and We can, compare those predictions to the ones, we get from multiple regression, where we're using weight and blood volume to predict size Comparing the simple model to the complicated one tells us if we need to measure weight and blood volume to accurately predict Size or if we can get, away, with just weight Now that we remember all the cool, things, we can, do with linear regression Let's talk, about logistic regression Logistic regression is similar to linear regression except Logistic regression predicts whether something, is true or false instead of predicting something continuous, like, sighs these mice are obese and These mice are not Also instead of fitting a line to the data logistic regression fits an s-shaped logistic function The curve goes from zero to one? And that, means that the curve tells you the probability that a mouse is obese based on its weight If we weighed a very heavy mouse? There is a high probability that the new, mouse is obese? If we weighed an intermediate mouse Then there is only a 50% chance of the mouse is obese? Lastly, there's only a small probability that a light mouse is obese Although, logistic regression, tells the probability that a mouse is obese or not it's usually used for classification For example if the probability of mouse is obese is greater than 50% Then we'll classify it as obese Otherwise we'll classify it as not obese Just like with linear regression, we can, make simple models in this case, we can have obesity predicted, by weight or? more complicated models in this case obesity is predicted by weight and genotype in This, case, obesity is predicted. By weight and genotype and age and Lastly, obesity is predicted by weight genotype, age and Astrological sign in other words just like linear regression logistic Regression can work with continuous data, like weight and age and discrete data like genotype and astrological sign We can, also test to see if each variable is useful for predicting obesity however Unlike normal regression, we can't easily compare the complicated model to the simple model and we'll talk more about, why in a bit Instead we just test to see if a variables affect on the prediction is significantly different from zero If not it, means that the variable is not helping the prediction We use, wald's tests to figure this out we'll talk, about that in another stat quest in This, case, the astrological sign is totes useless That statistical jargon for not helping That, means we can, save time and space in our study. By leaving it out Logistic regressions ability to provide probabilities and classify, new samples using continuous and discrete measurements Makes it a popular machine learning method One big difference between linear regression and logistic regression is how the line is fit to the data With linear regression, we fit the line, using least squares In other words, we find the line that minimizes the sum of the squares of these residuals We also use the residuals to calculate r. Squared and to compare simple models to complicated models Logistic regression doesn't have the same concept of a residual so it can't use least squares and it can't calculate r squared instead it uses something called maximum likelihood There's a whole stack quest on maximum likelihood so see that for details but in a nutshell You, pick a probability scaled. By weight of observing an obese mouse just like this curve and You, use that to calculate the likelihood of observing a, non obese mouse that weighs this much and then you calculate the likelihood of observing, this mouse and you, do that for all of the mice and Lastly, you multiply all of those likelihoods together that's the likelihood of the data given this line then you shift the line and calculate a new, likelihood of the data and then ship the line and calculate the likelihood, again, and again Finally the curve with the maximum value for the likelihood is selected bam in summary logistic regression can be used to classify samples and it can, use different types of data like, size and\/or genotype to do that classification and it can, also be used to assess what variables are useful for classifying samples ie Astrological sign is totes useless Hooray, we've made it to the end of another exciting stat quest do you, like this StackQuest, and want to see more please subscribe if you, have suggestions for future stat quests, well put them in the comments below, until next time quest on"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Logistic Regression",
        "Video Title":"Logistic Regression in 3 Minutes",
        "URL":"https:\/\/www.youtube.com\/watch?v=EKm0spFxFG4",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/EKm0spFxFG4\/hqdefault.jpg",
        "ID":"EKm0spFxFG4",
        "Publish Time":"2022-09-18T19:49:47Z",
        "Channel":"3-Minute Data Science",
        "Channel ID":"UC5fa7_jq9iyN68KZVCtQhIQ",
        "Transcript":"[Music] logistic regression is a statistical technique that models the probability of events given one or more independent variables you'll start with input data that is of any numeric type but the output variable will be binary it'll be zero or one indicating a false or a True Value respectively you then fit this s-shaped curve called the logistic function to this data it can then be used to make predictions of probability whether or not an event will happen such as given so many hours of rain what is the probability of a flood notice how that these points can project themselves onto the logistic function and we can use maximum likelihood estimation to fit the Curve which we'll talk about shortly notice that there are these middle points where there is a mix of true and false cases if these points follow a transitional trend of increasingly showing an event more likely to happen or not happen you will see that that s-shaped curve will climb or decrease respectively we can then leverage it to predict probability between Z and one let's take a case where we expect 6.2 in of rain and this is going to result in a 75 or 75% probability of a flood if we Define a threshold say 0.5 and anything greater than 05 will be true anything less than 0.5 will be false we will then predict there will be a flood we can move this threshold based on our needs for example if we set it to 08 this would actually categorize as false and there won't be a flood we can also reduce it to say 02 and set a much lower barrier to classifying a flood this is the the function that produces that s-shaped curve e is Oiler number which is a special constant that beta 1X Plus beta 0 is a linear function that is actually the log odds function that is something we will talk about in another video what's important to know is that this function will produce that s-shaped curve we need to make predictions we can also extend this to more input variables to create multi-dimensional logistic regressions as shown here let's talk briefly about maximum likelihood estimation the way it works is that we are going to take each of these points multiply their corresponding likelihoods together and that is going to produce our total likelihood we need to find the beta coefficients that will maximize the likelihood of our s-shaped curve producing all of these points this can be done with gradient descent Newton's method and other optimization techniques you might be wondering why did I subtract the false cases from 1.0 this is is because we have to treat the false cases as positive so that they are maximized as well we will cover this on a separate video talking about maximum likelihood estimation in more depth in a separate video we will talk about how to handle false positives and false negatives and we can use a confusion Matrix as a tool to track prediction performance thank you very much for watching I hope you enjoyed this video if you want to support support this Channel please check out my two books getting started with SQL as well as essential math for data science chapter six of essential math for data science actually covers logistic aggression and classification algorithms in depth please like subscribe and share and I will see you next time on three-minute data science"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Logistic Regression",
        "Video Title":"Logistic Regression [Simply explained]",
        "URL":"https:\/\/www.youtube.com\/watch?v=C5268D9t9Ak",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/C5268D9t9Ak\/hqdefault.jpg",
        "ID":"C5268D9t9Ak",
        "Publish Time":"2023-01-08T15:12:20Z",
        "Channel":"DATAtab",
        "Channel ID":"UC3UwrWtAFlAkFl_3Nia756g",
        "Transcript":"what is a logistic regression how is it calculated and most importantly how are the logistic regression results interpreted that's what we will discuss in this video let's start with the first question what is a regression a regression analysis is a method for modeling relationships between variables it makes it possible to inferior or predict a variable based on one or more other variables the variable we want to infer or predict is called the dependent variable or Criterion the variables we use for prediction are called independent variables or predictors but what is the difference between a linear regression and a logistic regression in a linear regression the dependent variable is a metric variable for example salary or electricity consumption in a logistic regression the dependent variable is a dichotomous variable what is a dasharmus variable dichotomous variables are variables with only two values for example whether a person buys or does not buy a particular product or whether a disease is present or not so with the help of a logistic regression we can determine what has an influence on whether a certain disease is present or not for example we could study the influence of age gender and smoking status on that particular disease in this case 0 stands for not deceased and one for deceased and the probability for the occurrence of the characteristic one characteristic present is estimated so our data set might look like this here we have the independent variables age gender smoke status and here the dependent variable with 0 and 1. we could now investigate what influenced the independent variables have on the disease if there is an influence then we can predict How likely a person is to have a certain disease now of course the question arises why do we need logistic regression in this case why can't we just use linear regression a quick recap in linear regression this is our regression equation we have the dependent variable the independent variables and the regression coefficients however we now have a dependent variable that is either 0 or 1. no matter which value we have for the independent variables only 0 or 1 results a linear regression would now simply put a straight line through the points we can now see that in the case of linear regression values between minus and plus infinity can occur however the goal of logistic regression is to estimate the probability of occurrence the value range for the prediction should therefore be between 0 and 1. so we need a function that only takes values between 0 and 1. and that is exactly what the logistic function does no matter where we are on the x-axis between minus and plus infinity only values between 0 and 1 result and that is exactly what we want the equation for the logistic function looks like this we have 1 divided by 1 plus e to the power of minus set logistic regression now uses the logistic function for set the equation of the linear regression is now simply used this gives us this equation thus the probability that the dependent variable is 1 is given by this equation for given values of the independent variables what does this look like for our example in our example the probability of having a certain disease is a function of age gender and smoking status now we need to determine the coefficient so that our model best represents the given data to solve this problem the so-called maximum likelihood method is used for this purpose there are good numerical methods that can solve the problem efficiently as statistics program such as data tab therefore calculates the coefficients B1 B2 to BK and a but how do you interpret the results of a logistic regression let's take a look at this fictitious example for this we first compute a logistic regression for these data on datadab.net if you like you can download the data set for free and follow the steps in parallel you can find a link in the video description we go to datadap.net and copy our own data into this table I just already loaded the data set from the link when we copy our data in here the variables show up down there we want to calculate a logistic regression so we just click on regression depending on how our dependent variable is scaled data tab will calculate either a logistic or a linear regression under the tab regression we choose disease as the dependent variable and age gender and smoking status as the independent variables data that now calculates a logistic regression for us now we can choose for which category we want to build the regression model our data could also look like this that there is not not diseased and deceased but 0 and 1. then we would just need to click here and enter the labels one is deceased and zero is not deceased before we go into detail about the different results a little tip if you don't know how to interpret the results you can also just click on summary in words logistic regression analysis was performed to examine the influence of age male and smoker on variable disease to predict the value deceased logistic regression analysis shows that the model as a whole is significant and then come the different independent variables for example for smokers the coefficient of the variable smoker is b equal to 1.34 which is positive this means that if the value of the variable is smoker the probability of the dependent variable being deceased increases however the p-value of 0.089 indicates that this influence is not statistically significant the odds ratio of 3.81 means that if the value is smoker the probability that the dependent variable is deceased increases by 3.81 times but now in detail to the results we will now go through all the tables slowly and understandably let's start at the top the first thing that is displayed is the results table in the results table you can see that a total of 36 people were examined with the help of the calculated regression model 26 of 36 persons could be correctly assigned that is 72.22 percent then comes the classification table here you can see how often the categories not deceased and deceased were observed and how often they were predicted in total not deceased was observed 16 times of these 16 individuals the regression model correctly scored 11 as not deceased and incorrectly scored five as deceased of the 20 deceased individuals five were incorrectly scored as not deceased and 15 were correctly scored as deceased to be noted for deciding whether a person is deceased or not the threshold of 50 is used if the regression model estimates a value greater than 50 percent this person is assigned deceased otherwise not deceased now comes the chi-square test here we can read whether the model is significant or not two models are compared for this purpose in one model all independent variables are used and in the other model the independent variables are not used now we compare how good the prediction is when the dependent variables are used and how good it is when it dependent variables are not used the chi-square test now tells us if there is a significant difference between these two results the null hypothesis is that both models are the same if the p-value is less than 0.05 this null hypothesis is rejected in our example the p-value is less than 0.05 and we assume that there is a significant difference between the models thus the model as a whole is significant next comes the model summary in this table we see on the one hand the minus two log likelihood value and on the other hand we are given different coefficients of determination R square R square is used to find out how well the regression model explains the dependent variable in a linear regression the R square indicates the proportion of the variance that can be explained by the independent variables the more variance can be explained the better the regression model however in the case of logistic regression the meaning is different and there are different ways to calculate the R square unfortunately there's also no agreement yet on which way is the best way data that gives you the ask where according to Cox and Snell according to Nago kerokee and according to McFadden and now comes the most important table the table with the model coefficients the most important parameters are the coefficient B the p-value and the odds ratio we will now discuss all three columns in the First Column we can read the calculated coefficients from our model we can insert this into the regression equation if we insert the coefficients we get the following regression equation 0.04 0.87 1.34 minus 2.73 with this we can now calculate the probability that the person is deceased an example we want to know how likely a person who is 55 years old female and smoker is to be deceased we insert 55 for the age zero because the person is female and one as the person is a smoker this gives us 0.69 or 69 percent thus it is 69 likely that a 55 year old female smoker is deceased based on this prediction it could now be decided whether to do another extensive investigation as I said the example is purely fictitious in reality there would certainly be many other and different independent variables but now back to the table in this column we can read whether their coefficient is significantly different from zero the following null hypothesis is tested the coefficient is zero in the population so if the value is smaller than 0.05 the respective coefficient has a significant influence in our example we see that none of the coefficients have a significant impact as all p-values are greater than 0.05 in this column we can then read the odds ratio for example the odds ratio of 1.04 means that a one unit increase in a variable age which means that the person is one year older increases the probability that the person is sick by 1.04 times furthermore their receiver operating characteristic Roc curve is very interesting in logistic regression if you want to know what the ROC curve is and how to interpret it watch our video on Roc curve thanks for watching and see you next time"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Logistic Regression",
        "Video Title":"Logistic Regression: An Easy and Clear Beginner\u2019s Guide",
        "URL":"https:\/\/www.youtube.com\/watch?v=Ax5kqLHls-I",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Ax5kqLHls-I\/hqdefault.jpg",
        "ID":"Ax5kqLHls-I",
        "Publish Time":"2025-01-14T07:42:05Z",
        "Channel":"DATAtab",
        "Channel ID":"UC3UwrWtAFlAkFl_3Nia756g",
        "Transcript":"what is a logistic regression how is it calculated and most importantly how are the results interpreted that's what we will discuss in this video this is the last video in our playlist on regression analysis let's start with the first question what is a regression in a regression analysis you want to infer or predict an outcome variable based on one or more other variables the outcome variable is dependent variable and the other variables independent variables okay so what about logistic regression a binary logistic regression is now a type of regression analysis used when the outcome variable is binary meaning it has two possible values like yes or no success or failure let's look at an example let's say we are researchers and we want to know whether a particular medication and a person's age have an influence on whether a person gets a certain disease or not so the outcome we're interested in is whether the patients developed the disease or did not develop it and our independent variables are medication and age of a person now with the help of a logistic regression we want to inere or predict the outcome variable based on the independent variables okay but what is the difference between a linear and a logistic regression in a linear regression the dependent variable is a metric variable EG salary or electricity consumption in a logistic regression the dependent variable is a binary variable so with the help of logistic regression we can determine what has an influence on whether a certain disease is present or not for example we could study the influence of age gender and smoking status on that particular disease in this case one stands for diseased and zero for not diseased we now want to estimate the probability that a person is diseased so our data set might look like this here we have the independent variables and there the dependent variabl with zero and one we could now investigate what influen the independent variables have on the disease if there is an influence we can predict How likely someone is to have a disease okay but why do we need logistic regression in this case why can't we just use linear regression a quick recap in linear regression this is our regression equation we have the dependent variable the independent variables and the regression coefficients however our dependent variable is now binary taking on the value of either zero or one regardless of the values of the independent variables the outcome will always be zero or one a linear regression would now simply put a straight line through the points we can now see that in the case of linear regression values between minus and plus infinity can occur however the goal of logistic regression is to estimate the probability of occurrence the value range for the prediction should therefore be between zero and one so we need a function that only takes values between zero and one and that is exactly what the logistic function does no matter where we are on the x axis between minus and plus infinity only values between zero and one result and that is exactly what we want the equation for the logistic function looks like this logistic regression now uses the logistic function for that the equation of the linear regression is now simply used this gives us that equation this equation gives us the probability of the dependent variable equal one given specific values of the independent variables hm what does this look like for our example now in our example the probability of having a certain disease is a function of age gender and smoking status next we need to determine the coefficients that help our model best fit the given data this is done using the maximum likelihood method for this there are numerical methods that can solve the problem effectively a statistics program such as data tab therefore calculates the coefficients all right let's work through this example on how to calculate a logistic Reg regression and then look at how to interpret the results to calculate the regression we just go to data.net and copy our data into this table if you like you can load the sample data using the link in the video description we want to calculate a logistic regression so we just click on regression we choose diseas as the dependent variable and age gender and smoking status as the independent variables data now calculates a logistic regression for us depending on how our dependent variable is scaled data will calculate either a logistic or a linear regression under the tab regression since we have two categorical variables we can set the reference category we will just use female and nonsmoker as reference now we can choose for for which category we want to build the regression model so we can decide if we want to predict if a person is diseased or not deceased instead of diseased and not diseased we could of course also have one and zero okay before we go into detail about the different results a little tip if you don't know how to interpret the results you can also just click on summary inverts a logistic regression analysis was performed to examine the influence of age gender female and smoking status smoker on the variable disease to predict the value diseased logistic regression analysis showed that the model as a whole was significant and then comes the interpretation of the different independent variables further you can click on AI interpretation at the different tables we will now carefully go through each table step by step to ensure everything is clear to you let's begin at the top first we get the result table here we can see that a total of 36 people were examined with the help of the regression model of these 36 persons 26 could be correctly assigned that is 72.22% next is the classification table this table shows how often the categor is not not deceased and deceased were observed and how frequently they were predicted in total not deceased was observed 16 times among these 16 individuals the regression model correctly classified 11 as not deceased while misclassifying five as deceased of the 20 deceased individuals the regression model misclassified five as not deceased and correct classified 15 as diseased but how do we determine whether a person is classified as diseased or not as mentioned earlier logistic regression provides the probability of a person being deceased so we obtain values ranging from 0 to 100% now we simply set a threshold of 50% if a value exceeds 50% the person is classified as as diseased otherwise they are classified as not diseased of course you can choose a threshold other than 50% to learn more about this check out our video on the r curve so let's have a look at the next table the kai Square test evaluates whether the model as a whole is statistically significant for this two models are compared in one model all independent variables are used and in the other model the independent variables are not used now we can compare how good the prediction is when the independent variables are used and how good it is when the independent variables are not used the kai Square test now tells us if there is a significant difference between these two results the null hypothesis is that both models are the same if the P value is less than 0.05 this null hypothesis is rejected in our example the P value is less than 0.05 and we assume that there is a significant difference between the models thus the model as a whole is significant next comes the model summary in this table we can see on the one hand the minus two log likelihood value and on the other hand we are given different coefficients of the determination r squ R squ is used to find out how well the regression model explains the dependent variable in a linear regression the r squ indicates the proportion of the variance that can be explained by the independent variables the more variance can be explained the better the regression model in a logistic regression however its interpretation differs and multiple methods exist to calculate R squ unfortunately there's no consensus yet on which method is considered the best data gives you the r squar according to Cox and Snell according to Nagle ker and according to mcfaden and now comes the most important table the table with the model coefficients the most important parameters are the coefficient B the P value and the Ys ratio we'll now discuss all three columns in the First Column we can read the calculated coefficients from our model we can insert these into the regression equation so we get the coefficients for age gender smoker and the constant for example for a person which is 55 years old is male and is nonsmoker we get a probability of 36% thus it is 36% likely that a 55-year-old male nonsmoker is diseased in reality there would certainly be many other and different independent variables okay but what about the P value so the P value shows whether the corresponding coefficient is significantly different from zero in other words it tells us if a variable has a real influence or if the result could just be due to chance if the P value is smaller than 0.05 it means the difference is significant in our case all P values are greater than 0.05 indicating that none of the variables have a significant influence and finally the odds ratio but what are odds and what is the odds ratio let's start with the odds let's say we have two possible outcomes of something success and failure for example if a therapy is successful or not let's say that the probability that the therapy is successful is 0.7 so 70% and thus the probability of failure is 1 minus 0.7 so 0.3 okay but what about the odds odds are defined as the ratio of the probability of success and the probability of failure or in other words odds represent the ratio of the probability of event happening to the probability of it not happening if we look at our example the odds are 0.7 ided 0.3 which equals 2.33 this means the event success is 2.33 times more likely to happen than not so odds give us a measure of the likelihood of an event happening versus it not happening in this case we've calculated the odds of success of course we can also calculate the odds of failure all right now that we understand odds let's talk about odds ratios so what are odds ratios let's look at the example from the beginning we're studying a new medication to reduce the risk of a certain disease so we have a group a patients with medication and a group b patients without medication let's say in group a we calculated a probability of 60% or 0.6 of getting diseased so the odds of getting diseased is 0.6 divided by 0.4 which is 1.5 again odds just represent the ratio of the probability of an event happening to the probability of it not happening in our case in group a the likelihood of being diseased is 1.5 times higher than the likelihood of not being diseased let's say in group b where the patients didn't get the medication the probability of getting diseased is 80% or 0.8 so the odds in group b of getting diseased are 0.8 divided by 0.2 so four therefore in group b the likelihood of being diseased is four times higher than the likelihood of not being diseased what about the odds ratio with the odds ratio we can now compare the two groups to do this we can compare the odds of getting the disease in group a relative to the odds of getting the disease in group b so the odds ratio is simply calculated by dividing the odds in group a by the odds in group b this results in an odds ratio of 0.38 the odds ratio of 0.3 38 means that the odds of being deceased in group a are 0.38 times the odds of being deceased in group b of course we can also switch the order then the odds ratio would be the odds in group b divided by the odds in group a in this case the odds ratio of approximately 2.67 means that the odds of being deceased in group b are 2.67 times High higher than the odds of being deceased in group a so an odds ratio is simply a comparison of the odds of an event occurring in two different groups the odds ratio indicates how much more likely the event is to occur in one group compared to the other group if the odds ratio is greater than one the event is more likely to occur in the first group if it is less than one the event is less likely in the first group group okay now let's put it all together and look at how to interpret the odds ratio in logistic regression let's get started first of all to calculate a logistic regression we need data let's say we have data from 50 patients our outcome variable is disease which is coded as zero for not diseased and one for disease and we have two independent variables medication and age now we can use this data to calculate a logistic regression you can find a link to the data set in the video description in the First Column we can see the coefficients that Define our model these coefficients can be entered into the logistic regression formula here we can see the coefficients from the table the constant the coefficients for medication and for AG now we just need to enter a value for medication such as one in indicating the patient received medication and a value for age for example 50 then we can calculate the probability in this case the probability of being diseased is 0.55 or 55% so for a patient who took the medication and is 50 years old the probability of being deceased is 55% of course we can simply use data to calculate this probability to do this just enter one here here and 50 there we will then also get a probability of 0.55 and data further gives us the odds as we know the odds are calculated by the probability that a certain event will happen divided by the probability that the event will not happen we therefore get 0.55 divided by 1 - 0.55 which equals 1.22 okay but we are not interested in the odds alone we are interested in the odds ratio again the odds ratio is simply a comparison of the odds of an event occurring in two different groups the two groups could be persons who took the medication and persons who did not take the medication therefore if we're going back to data tab we just need to compare the odds of a person who took the medication with the odds of a person who did not take the medication so to get the odds ratio we just need to divide the odds of a person who took the medication by the odds of a person who did not take the medication this results in an odds ratio of 0.64 and surprise the calculated value matches the odds ratio listed for the variable medication the odds ratio of 0.64 for medication indicates that for individuals who took the medication the odds of the outcome diseased are 0.64 times the odds of those who did not take the medication all right with medication we have two groups to compare but what about a continuous variable like H in this case we simply look at what happens when we increase AG by one unit for example we might compare the odds of the outcome for someone aged 50 versus someone age 51 this allows us to calculate the odds ratio by comparing the two odds in this case we get an odds ratio of 1.04 so for each one year increase in age the odds of the outcome deceased increase by a factor of 1.04 but there's one thing I haven't told you yet the odds ratio can actually be calculated simply by expon differentiating each coefficient so e to the power of 0.45 is 0.64 which is the odds ratio of medication and e to the power of 0.04 is 1.04 which is the odds ratio for H to sum it up odds are simply the ratio of the probability of an event happening to the probability of it not happening the odds rate ratio is now the ratio of the odds that an event occurs in two different groups great now we have a solid understanding of regression analysis including simple linear regression multiple linear regression and logistic regression as mentioned earlier if you want to learn more about classifying categories using logistic regression please check out our video on the ROC curve thanks for watching I hope you enjoyed the video"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Logistic Regression with Scikit-Learn",
        "Video Title":"Hands-On Machine Learning: Logistic Regression with Python and Scikit-Learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=aL21Y-u0SRs",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/aL21Y-u0SRs\/hqdefault.jpg",
        "ID":"aL21Y-u0SRs",
        "Publish Time":"2023-08-19T13:28:11Z",
        "Channel":"Ryan & Matt Data Science",
        "Channel ID":"UCKq-lHnyradGRmFClX_ACMw",
        "Transcript":"in today's machine learning video we're going to be covering how you can Implement a logistic regression within python with the help of SK learn now if you're not too familiar with what a logistic regression is let's back up towards what a linear regression is something that you might be familiar with algebra your normal equation y equals MX plus b with a slope going either in the positive direction or a negative Direction all right well a logistic regression still deals with the independent or a dependent variable but instead of that slope you're going to see a sigmoid which looks like an S and instead of your normal linear regression where your outcome that continues to go down or upwards you're going to have a binary decision think of like a zero or one now some examples of this would be like if someone's going to buy a product you're going to have some factors that deal with that and then the final outcome yes someone buys a product or no someone does not buy a product another example could be like if someone has a disease right throughout their medical history so either someone does have a disease yes one or no so a binary output now the equation of a logistic regression is somewhat similar to the linear but it is completely different so essentially what we are taking a look at is y equals 1 divided by 1 plus e raised to the negative MX plus b so you still have the MX plus b element but a completely different equation so with that in mind I'm gonna jump on my computer right now and we're going to start coding within Jupiter notebook so load up my kaggle notebook over here let's start cutting the first thing I'm going to do is import pandas as PD and every time I build out a new line I'm just doing shift enter so it runs the cell both and then drops me down below our first thing I'm gonna do is import some data so I made some mock data over here based off of ultra marathon runners and I'll explain it once we go into this data frame how about first and import this uh what I'm going to also say is I'm going to put this code Down Below in the description this right over here so you can copy it you'll have to type it out or grab your own data okay so then what I do is just create a data Frame data equals D which I defined over here and that's going to be DF which is very familiar if you've done pandas quite a lot okay so let me explain how this works this is the results from a 50 mile ultra marathon and over here is like the average amount of miles per week each of the different participants in the marathon did now over here on the right we have either a yes or no like for example this guy over here or girl we don't know I trained 67 miles per week on average completed the race this person trained 88 miles a week on average did not complete the race 37 miles per week no so before we run our logistic regression I would like to plot a few of things um but also at the same time we need to change this over here the completed the 50 mile ultra so we have to change this to zero or one and I've already had two videos on the channel uh both one hot encoder and also ordinal encoder kind of showing you guys how you could do that so make sure to watch those if you have not uh in this instance I'm going to be using an ordinal encoder so I'll show you how to do that so the first thing I'm going to do is from sklearn Dot pre-processing and I completely butchered the spelling right there pre-processing import ordinal encoder great shift enter and first thing I'm going to do is Define our two categories so in here I know that there's only gonna be no or yes so I want to have them in a order and that's the reason why I'm using ordinal encoder over one hot encoder um one hot encoder essentially is categorical data that there's no hierarchy here there is right no would be a zero they did not complete it one is completing it so that's why we're doing that um but we do have to Define it so we're going to say finished underscore race equals and then we'll put over here no and yes so when I run ordinal encoder all the nodes will turn into zeros all the yeses will turn into one okay so shift and enter and if we don't do this early on it's going to be in alphabetical order so I just prefer to call this out although technically even if it was an alphabetic order right no is zero yes is one but for proper coding I'd rather just do that Arc it so now we can call our ordinal encoder so I'm just going to put ENC equals I'm going to copy this ordinal encoder over here and then I'm gonna put categories equal and inside over here the brackets I'm going to put finish Trace right this is the categories that we defined a little bit earlier now normally also with categories if you're dealing with a large data set um you want to usually do dot unique to make sure there's other stuff but I already prepped this data so that there's one knows or yeses okay so we have finished race over here and now we Define an ordinal encoder we have ENC great so now we can overwrite this column over here the completed 50 mile ultra uh so all we're going to do is kind of like how we Define a new column uh within a data frame right so I'm just going to say completed 50ml Ultra and I'm just going to copy that so I don't make any spelling mistakes we're going to say that's equal to ENC dot fit and we're gonna do fit transform should be pretty familiar with that if not no worries and then inside over here we're going to say DF and put two brackets this time and then we're going to copy this well I guess we can just replace the inside brackets with this right completed 50 mile ultra and now this should have changed all these to zeros or ones and the way to check that again you can just run DF super easy right and take a look so completed 50 mile ultra the ones at the beginning that we talked about said no right all zeros the ones at the very end that did complete once and it looks good over here so now we can start charting this data so the first thing I'm going to do is use matplotlib so I'm going to also build out a few new cells over here so import well actually it's from Matt plot lib import Pi plot as PLT and you'll see that all the time has PLT over here and what I'm gonna do is a scatter plot first so we're going to do PLT dot scatter and what I'm going to do over here is have our X and Y so our X is going to be miles per week right so you can just do DF dot miles per week and then our Y is going to be they complete the ultra right so again just copy that we can just put this over here and you can see uh right over here a lot over in the zero especially like the 20 to 40 miles per week and then over here once you start getting to like 60 miles I'd even say 50 miles per week a lot more people finish the race and I mean that would be normal you have to train quite a lot for an ultra marathon and I mean if you're at 20 miles per week and you're completing a 50 mile ultra marathon props you on that although you're still probably walking it but still it's a very tough race right and you can kind of see the S2 right so up over here and over there so about 50 miles per week is that and you do have people over here that did not finish and that could be for a lot of reasons right so not everyone's gonna have a good race and sometimes someone will drop out there could be injuries associated with it a lot of different things that we don't know about um and that's the reason why people don't finish and why a model isn't always going to be 100 correct okay another thing that we should look at is Seaborn uh they have a count plot which I really like so we can just do import Seaborn as SNS and just shout out to Seaborn because I do have a full course on the channel if you guys want to watch it to learn how to use Seaborn I think it's pretty powerful and you can build some plots really really fast I still need to also do a one on matplotlib in the future but I'm right now I'm gonna be focusing on machine learning videos for a while okay so sns.count plots I'm gonna put over here x equals and we're again we're just going to copy this completed 50 mile ultra right so just copy that throw that down below over here and then we're going to say our data equals the f and then you can see right did not complete so we have about a third of the people 30 over here or maybe 33 that did not finish and then about 67 that did I mean we can get those exact numbers but I'm just estimating right now uh some more people completed this race than it did not based off of the training from above okay now I think we can start prepping to run our model we've already gone over here we've changed our categorical data right no's and yeses to zeros and ones we took a look at both of these it looks like it would be something perfect for a logistic regression right we have a bunch over here A bunch over here we have our typical S curve and then we also see over here zeros and once let's start running this so first thing we're going to do is break up our data to X and also y so our X right is gonna be the miles per week or Y is going to be essentially if someone completed that ultra marathon or not so the easiest way that I do this is just through x equals DF Dot ilock and then we're just gonna put a colon and then comma and zero to one and then our Y is just going to be y equals and I'm literally just going to copy this over here and remove the zero and just keep it as one and also that colon and now we have our X and Y to find this specific what we need next thing we need to do is import train test split so before you want to run our model you want to split up your data into training data and also testing data you don't want to train your model on all the data at once that way we can see if our model is accurate or not again I have a full video on that too if you want to check that out so X train X test and don't be intimidated by this I promise you it's actually pretty easy once you do it a few times why train and why test well first actually before I even do that I need to import this because I need an import so from SK learn dot model selection Imports train test split all lowercase on this one okay and now we can actually do our thing so X train X test y train y test equals we'll just copy this train test split over here and then capital x lowercase y our training size I'm just going to keep this at 0.8 I just do that all the time right used to it so equals 0.8 and then we can set up a random state so essentially what this is going to do is randomize your data and then put it in your training and testing sets so if you said a random State you can replicate the exact stuff that I have if I don't right it's going to randomize it and you don't know which random State it's in can I talk about that in that train test split video um but I'm going to say random state equals 11. why not right okay so now we have that over here and just to show you like this has been split up properly we can do over here like xtrain.shape right we have 80 over here we do X test dot shape we should have 20. all right we have 21 okay whatever right either way we have all this built out over here and we are ready to go for our logistic regression I am going to add a lot more cells over here and let's start doing that so the first thing we have to do is import this so just do from SK learn dot linear model import logistic regression like that okay and then we can say something like model equals and then I'm just going to put logistic regression call that over here and now we have to fit our data so we'll do model dot fit and since we split up our data over here right we're not going to do X and Y we're going to do X train and then y train so X train then we have y train now we have our model and you can see the logistic regression is down below over here okay great let's keep going forward with this side of things then we do y prediction right y prep I do that all the time equals model dot predict and then you're going to throw your X test in here um so we're testing this test data set right to figure out why prediction over here okay now that has been run and then we can see a few things so first thing we want to do is our model DOT score see how well our score is and then over here you're gonna put X test and then you're gonna do y test and we have 0.9 which is pretty good overall next thing we're going to do is take a look at our confusion Matrix and also our classification report okay and both these are going to be from sklearnmetrics so I'm just gonna say from sklearn.metrix yep import confusion underscore Matrix we're gonna do this one first no T over there make sure you spell it correctly I always make coding mistakes like that and then what I recommend if you just do this just throw it in your prints is your app to do this anyways just throw in confusion Matrix over here and we're gonna do y test and then y predictions okay and then we have five and fourteen and then one on each of those so on this one we have true positive right predicted correctly on that side of things true negative right and then over here we have false positive and then also false negative um but if we want to even run some of the data off of that it's better just to run a classification report rather than using a calculator or doing it by hand so again from sklearnmetrics import and I'm going to remove confusion Matrix and yes I know I could just call them both over here but I prefer just splitting it up just to teach you guys that so classification reports great and then same exact thing right this time we're going to say classification report so just copy that through that where it says confusion Matrix and we have our classification report so we have Precision recall F1 score now how this specifically works so for precision you look at true positive you divide that by true positive over false positive right so we're taking a look at this side of things right 0.83 now if you look at recall we're going to take a look at true positive over true positive plus false negative right this one the vertical side of things right 0.83 once again now F1 score you take two times your recall times Precision divided by precision and recall that's how you get this over here and let's take a look at our weighted average 0.9 which I do like so just one more time just to run it through this so you guys understand how this works so the first thing I did is import my data over here right and since we have categorical data we have to make sure that this is coded into zeros and one so we can run our specific model I used an ordinal encoder because with this categorical data there is a ranking behind it right no means you failed the race Yes means you completed it so I changed that and that's where we have this in this data frame up next I did two different plots with a scatter plot and then we also have a count plot just to show you how this data works right you can really see the logistic regression on both right over here and on this side of things too write zeros and ones then I separated the data into X and Y X is going to be the miles per week Y is the results from that race I need a train test split so that way we accurately split up our data and we can train our model and then test how accurate it is right and then we did 80 20 essentially with that then I ran my logistic regression I fitted it with the training and data that I did predictions best off of the X test data I ran a few different metrics such as a confusion Matrix and also a classification report hope you guys enjoyed this video if you did make sure to subscribe to the channel as these videos do take a bit of effort to make by the way I mentioned a little bit earlier in the video about ordinal encoding well you should go watch this video right over here I think it's really helpful especially when you run a lot more machine learning models"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Logistic Regression with Scikit-Learn",
        "Video Title":"Logistic Regression Python Sklearn [FROM SCRATCH]",
        "URL":"https:\/\/www.youtube.com\/watch?v=VK6v9Ure8Lk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/VK6v9Ure8Lk\/hqdefault.jpg",
        "ID":"VK6v9Ure8Lk",
        "Publish Time":"2019-07-31T15:29:13Z",
        "Channel":"Python Marathon",
        "Channel ID":"UCub4qT8Sgm7ytZsO-jLL4Ow",
        "Transcript":"let's see if the model says I would survive the Titanic in this video we will do machine learning by implementing logistic regression alright so the first thing we need to do is do some import statements I'm learning no you're not learning yet we haven't even done anything so let's get going from Piatt data sets we'll import data and we'll go ahead and import pandas a speedy and 'let's import numpy as NP also from scikit-learn dot linear model we're going to import our logistic regression and because we want to do a test train split let's do model underscore selection import train test split now let's go ahead and get the data we're going to be using the titanic dataset with the attempt to predict who will live and who will die so let's take a look at a sample of about five random data points in this data set okay so we have the class the age the gender and whether or not they survived so we're going to use class age and gender to determine if they survived but first in order to do this we're going to have to do some feature engineering because not all of these things as you can see up there are numerical so what we're gonna do is use some one hot encoding to classify the categories and help them fit to our model pandas is a built-in way to do one hunt encoding it's called PD get underscore dummies and we'll plug in our Titanic data set and we'll also give it drop first equals true that means we don't have the first column for each one and I'll show you what that means in just a second so let's take another sample of five drop first equals true means that there is no class one because as you can see here class two being zero and class three being zero means that they must have been in class one also for age of child it's either zero or one and so on so survived will either be a one for yes or a zero for no now let's go ahead with the test train split X train X test why train Y test and we'll do the split and we'll plug in everything but the survived yes and we'll plug in we survived yes for our X and our Y alright now it's time to train the model using the training data so we'll do log reg is equal to logistic regression and I'm gonna give it a specific solver you don't have to but I just want it to be this one that it defaults to and we'll do Xtreme y underscore train that we plug in and there is the model now our machine is officially learning I'm learning and now we're gonna predict if a class one child age girl will survive so let's go ahead and plug that in so log reg predict NP dot array will plug it in as an array and it sticks out as one so yes that person is predicting to survive okay let's try predicting if a class three adult age of male would survive so that would be zero in the class to category A one for class three zero for age child and zero for sex woman and let's see if that means they would survive and it comes out as a zero so that means no this model predicts that that person would not survive now let's go ahead and get a score for this model so first off I just want to show you the built-in will do log reg dot score X test y underscore test and this comes out to be point seven seven eight one one five now let's get a better understanding of what exactly the score is doing prediction is equal to our log reg dot predict with the next test plugged in every time that's greater than point five it's going to predict that they did survive and any time is less than or equal to point five it's going to predict that they will not survive and that's exactly what this is doing so we'll if we sum up every time that the prediction is equal to the actual output and divided by the length of that output we should get the same number and indeed we do point seven seven eight one so that's all the scores doing it's predicting that those who have a value of above 0.5 will survive and those who do not it assigns zero and you can test that right there and see that they have the same output so there you have it that is how you can use psychic learns logistic regression in Python please be sure to check out some of my other Python videos and please subscribe for more Python content [Music]"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Logistic Regression with Scikit-Learn",
        "Video Title":"Build a Logistic Regression Model from START to FINISH with Scikit-Learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=tGXpvwdYTtA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/tGXpvwdYTtA\/hqdefault.jpg",
        "ID":"tGXpvwdYTtA",
        "Publish Time":"2023-08-17T05:00:32Z",
        "Channel":"S.M.D.S",
        "Channel ID":"UCfR9kSqx7hYPCc21nGfTqtw",
        "Transcript":"LOGISTIC REGRESSION is a model used to perform binary classification that is categorizing a given input to either 0 or 1 and I will be using this model from scikit-learn to predict if a person has heart disease or not the data set that we will be using on in this video was acquired from kaggle let's start with importing all the modules that we will need so we will first need the logistic regression from scikit-learn so sklearn.linear_model we will have to import logistic regression like that and then I'm also going to import the train test split because we will have to split the data into 80 20 anyways so from sklearn model selection this is where the train to split is so import train to split _test_split and then also from scikit-learn matrix I will import the accuracy score to evaluate the model I will also import standard scaler because I'm just going to scale the features in terms of the input label X because it has wide range of values so from SK learn pre-processing I'm going to use I'm going to use the standard scaler you can use the min max if you wish like that and of course the 3 very important modules pandas as pd, numpy as np and then we will also have to import matplotlib .Pyplot as PLT now let's run our code all right so now we will have to load in the data to our data frame and then make the necessary adjustments and formatting that we will need so our data frame will be pd.read_CSV and then we pass in the file path like this and then let me just first type in DF head to see if that really worked yes we do have the table which contains 16 column and the last column is what we will have to build our logistic regression model onto so what I will do first is I'll just type in isna .sum this will return the number of none values in each and every single column so let's just run this and we see that education column has 105 none values so what I will do first is drop this education column because this column is not actually needed when it comes to predicting if a person has heart disease or not so what I will do is type in df.drop and then passing the columns to be education like that now when you run this code I guess you had to type in Small E on the code I also have to type in in place equal to true to make sure that the changes take place then and there now when you run this code you see that there is no education column so now when you get to the next one which is the cigarettes per day if the person is a current smoker so what we can do for this one is just simply drop all of the records that contains none value in terms of the cigarettes per day so what I will do is our updated DF will be DF of 6 per day the column that we're actually manipulating now per day and just type in notna so this will return all of the records where six per day column is not zero and then we're just updating that to our latest data frame now when you run this we see that six per day is zero what I will do for BP meds is I will just fill that with zeros so what we can do is just type in DF.fillna and then 0, in place equal to true of course I have to mention the column name so what I will do is just type in BP meds like that now when we run this code we see that again BP meds is now zero now for the rest all three I'm just going to drop them by using the notna function so come over here and then actually copy this line paste this three more times paste it let's say three more times and then change it each and every single time it's the first time will be heart rate and then I will also do that for BMI and I will do that for third call now when you run this code we see that glucose alone as 340 non-values so what I will do for this is this I will find the mean and then round it off and then fill it with that value so what I will do is mean will be DF of glucose.mean I will run this to be safe around this value and then I will type in DF of glucose.fillna and then type in the mean comma in place equal to true nice so now we have no Non values let's just print the data set to show you like that this is the top five records let's show you the button file as well and nice so up next we will have to initialize our input label and our output label now when we come to initialization I want you to notice that this 10-year CHD column is what our model has to predict on so what I will do is first create the output label to be this same column DF of 10 year CHD like that and I will convert this to a numpy array because we will have to provide this to our model I'll do that and then I will drop this column off from my main data frame because we will also have to create the input label X which doesn't contain this 10 year CHD so I will mention the columns to be 10 year CHD and then I will type in in place equal to true like that and I will create our X to be numpy array of this DF data set and I will also scale this input label X because I see wide range of values for different features so what I will do is just create the skills variable and then pass in the standard scalar operation standard scalar.fit_transform Pass an X and then see if x is killed let's just print that on a code and we see that we have our scales features like this so what I will do now is split this data into 75% of training set and 25% of test set so X train, X test, y train and similarly y test using the train test split function I will pass in X_scales, y, mentioned the test size to be 0.25 because we want our test set to be 25% of the entire data now let's run our code you will have to run all of the cells above and then run this okay and right nice so now it's time to create the model itself let's create the model and then first I will Define the model to be logistic regression and then I will type in model.fit pass in the training data X train, y train like that now that our model is ready let's just also make predictions off of our test set model.predict pass in the Y test actually the X test and then we can evaluate this by using the accuracy score that we imported from scikit line metrics accuracy score pass in the output label in terms of test set and also the prediction set to see the accuracy score let me just multiply this with 100 to see the % of accuracy let's run our code and we see that our model is 85% accurate now I also want to show you the confusion Matrix as well I will explain what this is but let's first plot this and show you so what we will have to do is import these two statements before actually using it we actually import the confusion Matrix and then we also want to display it so we also import confusion matrix.display so now what I will do is first create a variable confusion Matrix and then Define this to be using the confusion Matrix that we just imported we pass in the Y test just like we did for accuracy score we also pass in y test and then leave it like that now what I will do is create another variable of display and then type in confusion matrix.display passing the confusion Matrix variable that we created some of the display labels to be actual 0 and actual 1 I will explain this in a second so what I will do is type in display labels actually rather than having this to be actual 0 and actual 1 I will have this to be 0 and 1 now we will also have to set the figure size so I will be using matplotlib for this one figure fix size to be 8, 6 like that and then now I will display the plot what I will do is type in disp.Plot and type in cmap to mention the colors plt.cm I'm going to use the red color so just type in Reds you can also use the blue color just type in blues similarly just go on now I will also set the title to be confusion Matrix also type in plt.show and now it's good to run our code as we run our code you see that confusion Matrix is not defined we have to type in t over here now run our code we see that this is the confusion Matrix a confusion Matrix is a 2 by 2 Matrix where the first element that is 1, 1 will contain the total number of times when the true label that is the actual output is 0 and the printed output is also 0 so 867 times the machine has predicted 0 and the actual output is also 0 whereas when we come to 1, 2 we see that the actual output is 0 whereas the predicted output is 1 that is the machine prediction is wrong it predicts as 1 but the actual output is 0 whereas when you come down here the true label that is the actual output is 1 but then the machine predicted 0 so when you come to the 2, 2 the final element you see that the actual output and the predicted output is 1 and so this will be mentioned over here so we see that the major predictions made by the model is 0 that's why we see 153 where the true label is 1 but then the prediction label is 1 5 3 so that's confusion Matrix THAT WAS LOGISTIC REGRESSION NOW IN THIS SMAE SCIKIT-LEARN LINEAR MODEL there is also a linear regression and we have a video on that one as well which is what I recommend you to watch next [Music]"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Logistic Regression with Scikit-Learn",
        "Video Title":"Machine Learning Tutorial 5 - Logistic Regression Python Implementation with Scikit-Learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=ZYToTcsDrL8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ZYToTcsDrL8\/hqdefault.jpg",
        "ID":"ZYToTcsDrL8",
        "Publish Time":"2019-12-02T02:00:04Z",
        "Channel":"ProgrammingKnowledge",
        "Channel ID":"UCs6nmQViDpUw0nuIx9c_WvA",
        "Transcript":"we'll talk about implementing logistic regression my name is Ron acquiesce this video is a collaboration with program acknowledge to watch more videos on machine learning and programming to subscribe to the channel logistic regression is among the most commonly known core machine learning algorithm out there with its cousin linear regression it has many applications in businesses one of which is the pricing optimization in this video we will learn how to code large T regression in Python using the cyclotron library to solve a bit pricing problem let's have some recap long distillation is a predictive linear model that aims to explain the relationship between a dependent binary variable and one or more independent variables the output of floristry regression is a number between 0 & 1 which you can think of as being the probability that a given class is true or not the output is between 0 & 1 because the output is transformed by a function which is usually the sigmoid function let's start implementing logic regression in Python with a very simple example note that the intent of this video is only to implement a very basic law deterioration model using scikit-learn without using a trained rest split on the data set and with minimum data visualization so let's start first we import all the dependencies that are required we need matplotlib for visualization so we need the pipe lot as PLT next is vampire to store our data and finally we need the scalar logistic regression model which we can use to set our data so the next is that we haven't defined a data set let us generate a data set that we'll be using to learn how to apply longitude equation to a pressing problem the bid price is contained in our experiment while the result a binary lost or one category is encoded as 1 or 0 in our Y variable here I've defined my own data set but for complicated or more advanced examples you can also import an intercept from Cagle and use that let's go ahead and visualize this data using MATLAB to gain a better understanding of what we're dealing with let's have a scatter plot of x and y and let's actually give it a title of pricing bins and the X label is going to be price and the Y label is the binary output 1 or loss so status 1 is 1 and 0 is a lost so here each point above represents a bid that we participated in on the x axis you can see the price that was offered and on the y axis you see the result if we won the bid or not our goal is to use dollars to regression to come up with a model that generates the probability of winning or losing a bed at a particular place in python logistic regression is made simple thanks to the scikit-learn module for the task at hand we'll be using the large integration class by the SK learn linear model so log rank let's start let that be the name of the variable and logistic regression class we're regularization strength c is equal to 1.0 and the solver let that be lb FGS which is an optimization just like it in this end and for multi-class we specify OVR because we're using a binary classification problem here so multi class is equal to OVR for binary classification the next step is to Fred the large elevation model by running the fit function of a class and before we do that we transform our X array into a 2d array as is required by the SK learn model this is because we only have one feature which is the price and if we had more than one feature our array would already be 2d so let's reshape our data as 1 comma minus 1 comma 1 and finally we can fit our model so log reg dot fit capital x and y now we have a model and now let's predict some data if we wanted to run the prediction on a specific price you can also do that as shown so let's print a prediction let's say we need to find whether we've lost or 1 if the price is 110 so as you can see on the graph above if it surprises 110 we should be winning so let's try that as you can see when the price is around 110 which is between 100 200 we win the bed and if the price is around 275 we should lose the bet let's try that again which 275 we should lose the bet as you can see we have lost bet this is a very basic implementation of Florida regression using scikit-learn library to understand how the algorithm works on a data set as we have a basic understanding now we can start working with the Cagle data set and also study more about it analytics and data visualization thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Decision Trees and Random Forests",
        "Video Title":"What is Random Forest?",
        "URL":"https:\/\/www.youtube.com\/watch?v=gkXX4h3qYm4",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/gkXX4h3qYm4\/hqdefault.jpg",
        "ID":"gkXX4h3qYm4",
        "Publish Time":"2022-02-07T14:45:51Z",
        "Channel":"IBM Technology",
        "Channel ID":"UCKWaEZ-_VweaEx1j62do_vQ",
        "Transcript":"I just can't decide, should I play a round of golf today? Well, let's use this decision tree to make the decision. So first off, do I have the time? If I don't, well, then that's an easy decision. No golf. But let's say I do. Second decision point, is it sunny today? If there's sun, then I don't care about any other factor. I'm playing golf. If there's no sun, let's go down to the next level. Well, do I have my clubs with me? Do I have them handy? If I do not, then I'm not going to bother playing if it's not sunny. If I do, then I absolutely will. The decision tree here is an example of a classification problem where the class labels are \"golf yes\" and \"golf no\". And, while they're helpful, decision trees they can though be prone to problems. Things like bias and overfitting. But that is where something called \"random forest\" comes in to play. Random forest is a type of machine learning model that uses an ensemble of decision trees to make its predictions. And why do we call it random forest? Well, the reason is because it's actually built by taking a random sample of my data and then building an ongoing series of decision trees on the subsets. So we're essentially creating a whole bunch of decision trees together. And those give us a larger model or group. Look, the chances are that other people have built different and maybe better decision trees to answer the same question. Maybe those trees consider things like the time of day, which I didn't consider, or the difficulty of the course. The more decision trees that I use with different criteria, the better my random forest will perform because it's essentially increasing my prediction accuracy. And if one or two of these smaller decision trees are not relevant on a certain day, well, we just ignore them. One of the primary benefits of random forest is that it can help reduce overfitting. And this occurs when your model starts to memorize the data rather than trying to generalize from making predictions on future data. Essentially, it helps me get around the limitations of my data, which might not be fully representative of all golfers or all the best features in my model. It can also help reduce something else, and that's bias. Bias can occur when there is a certain degree of error introduced into the model. Bias occurs when you're not evenly splitting your instance space during training. So instead of seeing all of the data points, you might see only half because of how you set your model up. Now to set up a random forest, you will set some parameters. We have parameters for node size. We have parameters for number of trees. And we also have parameters for a number of features. And it can be challenging at first because you'll want to use a lot of trees, like as many as you can, to get the best predictive accuracy, but you don't want so many trees that it'll take you a long time to train the model and use a lot of memory space. But once you've set up these parameters, you'll use a random forest model to make predictions on your test data. And you can even segment or slice your results by different criteria. Maybe you want to know how your random forest does on certain types of golf courses or how it performs during different times of day. Random forest is pretty popular among data science professionals and with good reason. It can be extremely helpful in all sorts of classification problems. In finance, for example, it can be used to predict the likelihood of a default. In a medical diagnosis, it can be used to predict prognosis or survival rates depending on treatment options and in economics. It can be used to sort of help understand whether a policy is effective or ineffective. So, what do you think? Should I play golf today? Well, the sum of all my random forest decision trees say yes. I'll see you out on the course. If you have any questions, please drop us a line below, and if you want to see more videos like this in the future, please like and subscribe. Thanks for watching."
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Decision Trees and Random Forests",
        "Video Title":"Random Forest Algorithm Clearly Explained!",
        "URL":"https:\/\/www.youtube.com\/watch?v=v6VJ2RO66Ag",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/v6VJ2RO66Ag\/hqdefault.jpg",
        "ID":"v6VJ2RO66Ag",
        "Publish Time":"2021-04-21T14:00:32Z",
        "Channel":"Normalized Nerd",
        "Channel ID":"UC7Fs-Fdpe0I8GYg3lboEuXw",
        "Transcript":"Hello, people from the future welcome to Normalized Nerd! Today we\u2019ll set up our camp in the Random Forest. First, we\u2019ll see why the random forest is better than our good old decision trees, and then I\u2019ll explain how it works with visualizations. If you wanna see more videos like this, please subscribe to my channel and hit the bell icon because I make videos about machine learning and data science regularly. So without further ado let\u2019s get started. To begin our journey, we need a dataset. Here I\u2019m taking a small dataset with only 6 instances and 5 features. As you can see the target variable y takes 2 values 0 and 1 hence it\u2019s a binary classification problem. First of all, we need to understand why do we even need the random forest when we already have decision trees. Let\u2019s draw the decision tree for this dataset. Now if you don\u2019t know what a decision tree really is or how it is trained then I\u2019d highly recommend you to watch my previous video. In short, a decision tree splits the dataset recursively using the decision nodes unless we are left with pure leaf nodes. And it finds the best split by maximizing the entropy gain. If a data sample satisfies the condition at a decision node then it moves to the left child else it moves to the right and finally reaches a leaf node where a class label is assigned to it. So, what\u2019s the problem with decision trees? Let\u2019s change our training data slightly. Focus on the row with id 1. We are changing the x0 and x1 features. Now if we train our tree on this modified dataset we\u2019ll get a completely different tree. This shows us that decision trees are highly sensitive to the training data which could result in high variance. So our model might fail to generalize. Here comes the random forest algorithm. It is a collection of multiple random decision trees and it\u2019s much less sensitive to the training data. You can guess that we use multiple trees hence the name forest. But why it\u2019s called random? Keep this question in the back of your mind you\u2019ll get the answer by the end of this video. Let me show you the process of creating a random forest. The first step is to build new datasets from our original data. To maintain simplicity we\u2019ll build only 4. We are gonna randomly select rows from the original data to build our new datasets. And every dataset will contain the same number of rows as the original one. Here\u2019s the first dataset. Due to lack of space, I\u2019m writing only the row ids. Notice that, row 2 and 5 came more than once that\u2019s because we are performing random sampling with replacement. That means after selecting a row we are putting it back into the data. And here are the rest of the datasets. The process we just followed to create new data is called Bootstrapping. Now we\u2019ll train a decision tree on each of the bootstrapped datasets independently. But here\u2019s a twist we won\u2019t use every feature for training the trees. We\u2019ll randomly select a subset of features for each tree and use only them for training. For example, in the first case, we\u2019ll only use the features x0, x1. Similarly, here are the subsets used for the remaining trees. Now that we have got the data and the feature subsets let\u2019s build the trees. Just see how different the trees look from each other. And this my friend is the random forest containing 4 trees. But how to make a prediction using this forest? Let\u2019s take a new data point. We\u2019ll pass this data point through each tree one by one and note down the predictions. Now we have to combine all the predictions. As it\u2019s a classification problem we\u2019ll take the majority voting. Clearly, 1 is the winner hence the prediction from our random forest is 1. This process of combining results from multiple models is called aggregation. So in the random forest, we first perform bootstrapping then aggregation and in the jargon, it\u2019s called bagging. Okay so that was how we build a random forest now I should discuss some of the very important points related to this. Why it\u2019s called random forest? Because we have used two random processes, bootstrapping and random feature selection. But what is the reason behind bootstrapping and feature selection? Well, bootstrapping ensures that we are not using the same data for every tree so in a way it helps our model to be less sensitive to the original training data. The random feature selection helps to reduce the correlation between the trees. If you use every feature then most of your trees will have the same decision nodes and will act very similarly. That\u2019ll increase the variance. There\u2019s another benefit of random feature selection. Some of the trees will be trained on less important features so they will give bad predictions but there will also be some trees that give bad predictions in the opposite direction so they will balance out. Next point, what is the ideal size of the feature subset? Well, in our case we took 2 features which is close to the square root of the total number of features i.e. 5. Researchers found that values close to the log and sqrt of the total number of features work well. How to use this for regression? While combining the predictions just take the average and you are all set to use it for regression problems. So that was all about it. I hope now you have a pretty good understanding of the random forest. If you enjoyed this video, please share this and subscribe to my channel. Stay safe and thanks for watching!"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Decision Trees and Random Forests",
        "Video Title":"StatQuest: Random Forests Part 1 - Building, Using and Evaluating",
        "URL":"https:\/\/www.youtube.com\/watch?v=J4Wdy0Wc_xQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/J4Wdy0Wc_xQ\/hqdefault.jpg",
        "ID":"J4Wdy0Wc_xQ",
        "Publish Time":"2018-02-05T15:55:40Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"Wandering around a random forest. I won't get lost because of stat quest Hello, I'm Josh Dharma and welcome to stat quest today We're gonna be starting part one of a series on random forests, and we're going to talk about building and evaluating random forests Note random forests are built from decision trees. So if you don't already know about those check out my stat quest and beef up Decision trees are easy to build easy to use and easy to interpret But in practice they are not that awesome to quote from the elements of statistical learning Aka the Bible of machine learning Trees have one aspect that prevents them from being the ideal tool for predictive learning Namely in accuracy. In other words, they work great with the data used to create them But they are not flexible when it comes to classifying new samples The good news is that random forests combine the simplicity of decision trees with flexibility Resulting in a vast improvement in accuracy So let's make a random forest step 1 create a bootstrap data set imagine that these 4 samples are the entire data set that we are going to build a tree from I Know it's crazy small, but just pretend for now To create a bootstrap data set that is the same size as the original. We just randomly select samples from the original data set The important detail is that we're allowed to pick the same sample more than once This is the first sample that we randomly select So it's the first sample in our bootstrap data set This is the second randomly selected sample from the original data set So it's the second sample in our bootstrap data set Here's the third randomly selected sample So here it is in the bootstrap data set Lastly here's the fourth randomly selected sample note. It's the same as the third and Here it is BAM we've created a bootstrap data set Step2 for creating a random forest is to create a decision tree using the bootstrap dataset But only use a random subset of variables or columns at each step in This example, we will only consider two variables or columns at each step Note, we'll talk more about how to determine the optimal number of variables to consider later Thus instead of considering all four variables to figure out how to split the root node We randomly select two in This case we randomly selected good blood circulation and blocked arteries as candidates for the root node Just for the sake of the example assume that good blood circulation. Did the best job separating the samples? Since we used a good blood circulation, I'm going to gray it out so that we focus on the remaining variables Now we need to figure out how to split samples at this node just like for the route we randomly select two variables as candidates instead of all three remaining columns and We just build the tree as usual, but only considering a random subset of variables at each step double bound we built a tree one using a bootstrap data set and Two only considering a random subset of variables at each step Here's the tree we just made Now go back to step one and repeat Make a new bootstrap data set and build a tree considering a subset of variables at each step Ideally you do this hundreds of times, but we only have space to show six, but you get the idea Using a bootstrap sample and considering only a subset of variables at each step results in a wide variety of trees The variety is what makes random forests more effective than individual decision trees Sweet now that we've created a random forest. How do we use it? Well first we get a new patient We've got all the measurements and now we want to know if they have heart disease or not So we take the data and run it down the first tree that we made Booboo, dooba, dooba, dooba dooba, dooba. Do the first tree says yes, the patient has heart disease and We keep track of that here now we run the data down the second tree that we made the second tree also says yes and We keep track of that here. And then we repeat for all the trees we made After running the data down all of the trees in the random forest. We see which option received more votes in This case yes received the most votes so we will conclude that this patient has heart disease BAM Oh No terminology alert Bootstrapping the data plus using the aggregate to make a decision is called bagging Okay, now we've seen how to create and use a random forest How do we know if it's any good Remember when we created the bootstrapped data set We allow duplicates in trees in the bootstrapped data set as A result. This entry was not included in the bootstrap data set Typically about one third of the original data does not end up in the bootstrap data set Here's the entry that didn't end up in the bootstrapped dataset If the original dataset were larger, we'd have more than just one entry over here This is called the out-of-bag data set If it were up to me I would have named it thee out of boot data set since it's the entries that didn't make it into the bootstrap dataset Unfortunately, it's not up to me Since the out-of-bag data was not used to create this tree We can run it through and see if it correctly classifies the sample as no heart disease In this case the tree correctly labels the out of bag sample. No Then we run this out of bag sample through all of the other trees that were built without it This tree incorrectly labeled the out of bag sample. Yes These trees correctly labeled the out of bag sample know Since the label with the most votes wins is the label that we assign this out of bag sample in This case the out of bag sample is correctly labeled by the random forest We then do the same thing for all of the other out of bag samples for all of the trees This out of bag sample was also correctly labeled This out of bag sample was incorrectly labeled Etc etc, etc Ultimately we can measure how accurate our random forest is by the proportion of out-of-bag samples that were correctly classified by the random forest The proportion of out-of-bag samples that were incorrectly classified is the out of bag error Okay, we now know how to one build a random forest to use a random forest and three estimate the accuracy of a random forest However now that we know how to do this we can talk a little more about how to do this Remember when we built our first tree and we only use two variables columns of data to make a decision at each step Now we can compare the out-of-bag error for a random forest built using only two variables per step to a random forest built using three variables per step and We test a bunch of different settings and choose the most accurate random forest In other words one we build a random forest and then two we estimate the accuracy of a random forest then we change the number of variables used per step and We do this a bunch of times and then choose the one that is the most accurate Typically we start by using the square of the number of variables and then try a few settings above and below that value Triple bail Hooray We've made it to the end of another exciting static quest tune in next week And we'll talk about how to deal with missing data and how to cluster the samples. All right, and tell them quest are armed"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Decision Trees and Random Forests",
        "Video Title":"Decision Trees, Random Forests and Gradient Boosting: What&#39;s the Difference? (Beginner Data Science)",
        "URL":"https:\/\/www.youtube.com\/watch?v=uV2gdNt2MLc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/uV2gdNt2MLc\/hqdefault.jpg",
        "ID":"uV2gdNt2MLc",
        "Publish Time":"2021-09-02T12:00:21Z",
        "Channel":"Leon Lok",
        "Channel ID":"UCqF6b0pT7OJCjCnCn4r3J4Q",
        "Transcript":"What's the difference between decision trees, random forests and gradient boosting? Let's find out. Hey everyone, welcome to the channel, my name's Leon. So decision tree based algorithms are now very popular thanks to their efficiency and also their prediction performance. An example would be XGBoost that's already helped to win a lot of Kaggle competitions and before you can understand how these algorithms work, you need to understand the difference between decision trees, random forest and also gradient boosting. So here are the timestamps in case you want to skip ahead. In this video, I'm going to go over at a higher level what the differences are because even sometimes I forget. Let's get right into it. So decision trees are very, very simple predictors. Basically, they represent a series of conditional steps that you need to take in order to make a decision. As a very basic example, let's say I'm trying to decide whether or not it's worth buying a new phone and I have this very simple decision tree to help me decide. The first and only question is: does my current phone still work? Well yes, it does. So according to this decision tree, that's it, I don't need to buy a new phone. Now would something as simple as this actually be useful in modeling the problem? Probably not, because there's a lot of other factors to take into account than just whether or not my current phone is still working. So instead we can have a more complex decision tree like this one which asks: does my phone still work? Which, yes it does. Do I still have enough memory capacity to record videos and take photos? No, not really. Then we move further down: do I have enough disposable income to buy a new phone? Yeah I do, so then the decision would be to buy a new phone, according to this decision tree. Now in this example, you can see that if we wanted to keep going further and further down the tree, we can easily do that just by adding more questions, or if we wanted to make the tree simpler, then we would just remove some of the questions from the tree. They're very easy to use and they provide a clear visual for making decisions and they're also very easy to build computationally as well. However there's some serious disadvantages to this simplicity. The main one being overfitting, so it performs extremely well on one dataset but if you try to generalize this model on other datasets, then it performs very poorly. You can see in the example I showed, the further down the tree you go, the more specific that tree becomes for that particular situation. Also, depending on the question you start building the decision tree with, the final decision tree can end up looking very different. So in the end, despite them being easy to build, you wouldn't use a decision tree by itself to make generalized predictions with. And this brings us on to random forests. Now, random forests use a concept called collective intelligence. It builds a bunch of decision trees independently, which are simple predictors and aggregates their results into a single result. This collective result should in theory be closer to the true result that we're looking for. As I said earlier, decision trees can look very different depending on the data it uses. Random forests will try to randomise the construction of trees to try and get a variety of different predictions. However, random forests are harder to interpret because they're a group of decision trees, they're not just a single decision tree anymore and they're also slower to build because the algorithm needs to build and evaluate each decision tree independently. Now like random forests, we also have something called gradient boosting, which is also a collection of decision trees. The main difference between random forests and gradient boosting lies in how the trees are created and also how they're aggregated. The trees in gradient boosting are built additively, meaning that each tree is built one after another and each tree is built to improve on the deficiencies of the previous trees. This whole concept is called boosting. The \"gradient\" part of gradient boosting comes from minimizing the gradient of the loss function. If that doesn't make any sense, then don't worry too much about that. The main point is that the trees are built one at a time to improve the overall model instead of being built independently. Another key difference between random forests and gradient boosting is how they aggregate their results. In grading boosting, instead of aggregating the results at the end of the process, it aggregates the results along the way to calculate the final result. Overall, gradient boosting usually performs better than random forests but they're prone to overfitting if you don't tune the parameters very carefully. They're also very sensitive to ourliers so if your data has a lot of noise, then gradient boosting might not do very well. So hopefully this video helps clarify the differences between decision trees, random forests and gradient boosting. Gradient boosting is really popular nowadays so I think it's worth knowing what the differences are. I've got more Beginner Data Science videos up here somewhere that you can check out as well and if you want to stay updated, please like and subscribe, turn on the notification bell and I'll see you in the next one."
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Decision Trees and Random Forests with Scikit-Learn",
        "Video Title":"Decision Trees and Random Forest with sklearn",
        "URL":"https:\/\/www.youtube.com\/watch?v=dlrYlP6TZa4",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/dlrYlP6TZa4\/hqdefault.jpg",
        "ID":"dlrYlP6TZa4",
        "Publish Time":"2021-02-27T01:09:20Z",
        "Channel":"Data Science for Everyone",
        "Channel ID":"UCk13_EXljNVOEQTAbaJo8zA",
        "Transcript":"welcome back everyone to data science for everyone today we're going to be talking about decision trees and random forest with python let's get started so as I mentioned today we're going to be doing decision trees in random Forest uh and this is where we're going to be grabbing our data set from okay it's an uh pretty prevalent R data set that's used uh for statistics particularly when it comes to decision trees uh it's actually from the r part package but we're going to be using it with um uh python today so we import pan p as pandas as PD import nump pi as MP import uh caborn as SNS import map plot lib P plotus PLT and let's make sure and do uh map plot lib in line uh so let's go to do the data so pd. read CSV here we want to grab the kyos or kyphosis uh what make sure I spelled that right kosis CSV here uh let's take a look at the head here uh oh and I want to do a comma in here and I want index column uh is going to be the First Column there we go so that got rid of that unknown column um so first thing that we want to look at then is let's just do a basic exploratory data analysis on here so we want to look at the Eda of our data and uh this is actually quite a small data set so it's not going to be a problem if we do pair plot to take a look look uh and we want the Hue on the uh kyphosis and again I'm sorry if I'm murdering that um but that'll be okay um and again pair plot sometimes does take a little bit of time so let's take a look and see and I I don't want this I don't want the KD's so uh diagonal I want histogram so let's take a look and see what we have here all right so there's a little bit of skew when it comes to the age there's a lot of um probably I'm guessing zero observations in here um everything seems to be pretty stratified there's definitely skew and the start uh position same thing uh here with the number as well um but we'll let's let's just look and see okay what's what's going on with that the next thing um we want to train test split our data so from sklearn model selection import train test split and I'm hoping that by seeing enough of the videos and kind of going over enough of this you guys have kind of understand that things are going are starting to get a little bit repetitive but that is okay so here we want to drw the kosis because that's actually what we're going to want to uh to predict and then our y's uh we want kyphosis oh no kyphosis not Kentucky uh kyphosis okay uh now let's do our train test split and again I'm going to just use their defaults so go down here we grab our train test split and let's start with our basic decision tree example so decision trees are actually really nice because they're very easy to understand so I highly suggest using them at least to kind of get an idea of what's going on and I'm we're going to use this in the uh term of classification we can also use them as regression it's just the same thing but instead of like what we're doing here is from SK learn uh tree import and I'm going to grab decision tree classifier you can also grab a decision tree regressor they work very very well um but actually again decision trees are very basic they're very intuitive but probably random Force are a little bit better when it comes to prediction I've had a very good luck with using both of them in the past a decision trees for the explanation and random Force for the actual prediction metrics so let's go on with the classification so I'm going to call this a DT for de decision tree uh decision treat classifier and you know I may just do DTC so that uh I remember that it's a classifier DTC do fit we want our X train and our y train here and we are going to use the basics again you have they have quite a bit of leaving and sorting and all of this type of stuff and trimming um we're not going to worry about any of those right now we're just going to go over the basics so now let's also make sure we do our prediction and our evaluation so predictions here we'll do uh DTC do predict and we want this on our X test and then we need our uh classification report and our classification Matrix so from sklearn do metric we want to import our classification report and we want our confusion Matrix as well and probably later on I'll probably also show how we can do really nice visualizations with this I have a couple um packages like yellow bread that we can use to really dig into like um the importance of uh each of the variables Etc but for right now let's just do um let's keep with the basics so let's do uh classification report uh y test with our predictions and we see that this is actually not great okay we're we're even even weighted okay we're in the 50s and 60s but it's not terrible but let's uh again it's it's really not that great so let's check the confusion Matrix and see what's going on uh now my guess is partially is because it's actually a quite a small uh sample again because uh we can see here there's uh the classification report did 16 and two okay so again here even even our misclassifications are larger than our correct classification um and part of this Also may be just the way that way that the split happened okay sometimes uh we need to do a stratification split and I'll I'll probably show that in at another time now let's also take a look one of one of the bigger things that we need to do when we're talking about visualization uh visualization about trees is actually doing the tree visualizations themselves um and so let me delete this one let me grab this one and do tree visualization so there are a lot of built-in capabilities for that but uh I don't think that they're the best so let's do from I python. display import image from sklearn whoops image from sklearn dot uh externals do 6 import here and we want the string IO and then uh from SK learn. tree import export graph viz and then we also want import Pi Dot and all of these These are actually uh some network uh data visualization type stuff okay so if you're into Network theory that type of stuff this is also used in that to help uh plot out graphs so let's also grab our features here and I'm going to create a list from our uh data frame columns um and I want one to all because again if if we go back up here the first we want to we don't want this one okay we want all the features so it's age number and start and again we can also explicitly say that but I think this is a little bit easier so let's double check on our features as well so again age number start okay so let's actually start out by creating uh a dot matrix here uh string iio and then we want to export our graph viz so we want our tree data and we want an output file here is going to be uh our DOT data uh and then we want our feature names here is going to be features uh and then we want this to be filled is true uh and I need to clean this up a little bit and we want what let's make them rounded uh and then what else do we want to put in here we need to create the graph so we use Pi dot dot here and we want graph from and we do not want it from an adjacency Matrix we want it from a DOT data file okay so again that's uh those of you that are comfortable with network Theory and those types of things we usually use an adjacency Matrix in order to look at the links between networks so this is another way that we can do that as well um so we can also do fromt data. value um get value sorry get value and then we need to create the image so we want image graph zero we want the first graph and we want to create um a PNG and that should what did I miss output file output data did it not did it not oh whoops that's why all right let me let's double check it one more time what don't you like here output file what what what did it change to let me cut this real quick uh did they change it to oh out file instead of output file okay so out file instead of output file out file okay so this this is a little bit big but okay so what actually is going on here what is this output so it allows us to see kind of the decision process that's been happening with the algorithm so for example we have the start and we have if if the start value is less than uh or equal to 8.5 if that happens to be true then we're going to move on and then the next value here we're going to use is age for our prediction okay and it's going to follow us all the way down to where we have our prediction outcomes down here okay and we'll we'll I'll go over more explanation of these um in a little bit but I want to kind of go through the whole analysis first um so let's also go through and do random Forest now uh so again uh one more thing okay so this is this a lot of times what happens with these um decision trees that we have okay we have we have the whole the whole predictive sequence going on there but sometimes we need to trim the tree so we need to make sure this one's not very long again the data set's not very big so we may need to trim it down a little bit but what we usually want okay is also to make sure that these this these examples okay so for example this start method here and where we have these different values when we're making this Choice okay these are happen usually happen to be random how they are generated with the choice values okay um at least at least the for example start then to age then to number that that those uh not the values themselves are random but the variables that they're using in that order happen to be random so sometimes whenever you need to check this maybe change the random seed sometimes you can even change the order to see if things are a little bit different but they usually don't change a whole lot um again though we saw that it did not have this great predictive accuracy now when we do the random Forest okay the random Forest is basically like taking T many many many decision trees and putting them together and then kind of aggregating what their their output is okay so it should have should have a better predictive accuracy so from sklearn do Ensemble because it is an ensemble method Ensemble means multiple bringing things together so we have uh random force and again here we also have a regressor but we want to use random force classifier today so I'm going to say random Forest classifier ifier and random Forest classifier we're going to instantiate the model now we need to say inside here um how many uh estimators we want so I'm going to actually set it to estimators is equal to 100 okay and you can changing this number will change the accuracy but also take uh will increase the run time the larger it is so be wary of uh when you do that particularly um if you if you're computer isn't very powerful so we do X train y train run this and again we're just going to keep the the same values that we have here again it's going to be do using bootstrapping methods um again we didn't do any Mac samples or anything and we're not worrying about uh impurities at all um and we're doing again our men's samples is one leaf um max sample I don't see the max sample in here Max Leaf nod is be nut so it's just going to keep going until it kind of Peters out in this instance um and again I'm not going to set a random state if we wanted this to be the same every time actually you know what let's let's actually do that set the random uh State uh equal to again I'm just going to do 42 so that it's completely why did I want to do that all right hold on oh whoops oh it's because actually it's not in fit it's here it's up here I want the random State random state is 42 um in this I probably should do this also uh go up and do this also with the decision tree this makes it so that um your results are going to be reproducible every single time um some people have aeny to fuss around with the uh random state in order to better their results I would suggest against that because it's you're kind of packing um meaning that you're trying to uh increase your results by random chance and that's that's not really a good way to go about things um so the next up then is we want to do our predictions uh and let's actually do RFC predictions here uh and so RFC predict and then here we want want our X test uh and then we want to print our classification report on our uh y test with our RFC uh predictions and you know what let me put this on a different line okay and so we see this is doing quite a bit better okay so now we're in this uh the 70s to 80s whenever it comes to our accuracy so that is definitely a significant Improvement off of our original decision tree which was in the low uh the upper 60 uh upper 60s P accuracy let's also go through and double check on our confusion Matrix and you can see here that again it's still not great one one issue here is that we have some imbalanced classes okay there's a lot uh the lot of item going on here for example in the absent versus the present uh but again that's something that you need to we would need to probably deal with u when we're doing our sampling um and working through that as well thank you guys for watching if you like this please comment share subscribe and I'll see you guys next time bye-bye"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Decision Trees and Random Forests with Scikit-Learn",
        "Video Title":"Random Forest Algorithm Explained with Python and scikit-learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=_QuGM_FW9eo",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/_QuGM_FW9eo\/hqdefault.jpg",
        "ID":"_QuGM_FW9eo",
        "Publish Time":"2023-08-31T13:55:36Z",
        "Channel":"Ryan & Matt Data Science",
        "Channel ID":"UCKq-lHnyradGRmFClX_ACMw",
        "Transcript":"today we're going to be covering how you can Implement a random Forest classifier within python with the help of scikit learn now before we do start coding there's a few things that you should know the first is being a decision tree since they do make up a random Force classifier now a decision tree is what you see in programming those trees that are either like a one or a zero yes or no outside of programming it might be like did you run today now I ran this morning super early 5.5 miles so that would be yes or one at the decision that came out of the tree well in a random Force classifier there's going to be multiple decision trees so like imagine you're asking your friends what race should I run next and they all tell you yes you should run this 100 miler in 2024. well that's the outcome of the random Force classifier I'm going to be showing you guys how you can do this at the very basic level and then I'm also going to be taking the difficulty up a little bit and adding in some hyper parameters some of those being n estimators Criterion Min sample split Max step as well as random State all right let's get ready to code all right let's get started so first thing we're going to do is import pandas's PD put that in here shift enter runs the cell builds brand new and down below uh now we're going to build out our data frame so we're going to say DF equals PD dot read underscore CSV and then we're going to put a CSV in here now I'm going to use the same one as my decision Tree video which is 500 hits I do have this under my GitHub so feel free to download it and if you guys want to see more data on my GitHub let me know some of that code that I use in these videos If that is helpful obviously like 50 50 on adding it or not all right also we have to put an encoding line one otherwise this won't work over here and um you don't have to do that all the time for csvs but on this one how I pulled that data initially so just to show you what this looks like we're gonna do a DF head and essentially we have different baseball players 500 to be exact top 500 hits and if they end up making the Baseball Hall of Fame or not um that's the classification that we're gonna be working on right zero not a Hall of Famer one is a Hall of Famer now I am going to drop a few columns because they aren't really practical to use uh so I'm just gonna do DF equals EF dot drop and we'll put over here columns equal and we're gonna put the player name uh because it shouldn't impact our model and then we're also going to put CS which is caught stealing I don't think there's any impact of caught stealing I'm making the Baseball Hall of Fame um stolen bases do count but no one's really looking at the cots dealing and determining a player shouldn't make it or not so I'm going to shift and enter that runs over here and then I'm going to split the data between our X and also why so we're going to say x which always when you're doing models make sure it's capital x just very common to see we're going to df.i law and then over here I'm gonna put a colon and then we're going to say 0 through 13 for the first part right it's going to grab all this information over here minus this player in CS and then we're going to grab our Y which is lowercase again a little confusing at first about DF Dot ilock colon and then 13 over here and then we both have our X and Y which is great right and just to show you what these look like X over here right and then if we put y have that cool cool all right this is working and um first thing we're gonna do is train test split so let's add in a few new cells down below and we have to import this in so from sklearn dot model here's course selection Imports train tests splits now that's imported next thing you're going to do is set up X train and Y train as well as testing so X train test again capital X Y train y tests three then equals train tests Blitz capital x a lowercase y I'm going to say random States of 17 make sure you put this in here either way you can copy my exact results that's what essentially what it does and then test size equals 0.2 that means we have 20 in our testing set we're gonna have 80 in our training and if you're not familiar with train test split just please stop this video and watch the train test split video uh because you should know this before running any specific models it's really critical to be honest with you we're going to shift and enter now this runs over here and now we can start working on our classifier so I'm gonna say from SK learn ensemble import random Forest classifier right and then we have to call our random Force now for the first one I'm not going to put any hyper parameters we're just gonna go everything default so all I'm gonna do on this one is super easy right RF equals and then you can literally just copy random force classifier over here and you just called it right super super easy like every other type of model you have to fit your data next so we're going to do RF dot fit and when you're fitting your model you're going to be using your training data and that's why we set up X train and also y train so we set that up over here okay and then you can see that this populated which is great next thing I would recommend is building out your prediction so we can say why pred like this to see it all the time coded out and we'll say rf.predict and put your X test now I'll explain how this specifically works so we have our X and Y test um but this one essentially if you run in your test over here you're trying to see what that prediction is and then you can take a look at the difference between the Y test and also why predictions uh just to see how your model is working correctly and you can do that through your classification report and confusion Matrix and I'll show you that in a second but regardless that's why we're doing a y prediction over here so remember you have your training you have your testing and then your final prediction when you're running classifiers like this okay hopefully this all makes sense uh we're gonna do a basic score just to see how this model performs and in this one you're going to do X test and also you're going to do y test great and you can see right now is 0.82 now this is a lot more accurate than the decision tree random Force tends to be a little bit of a better model they talked about in the intro but we're also going to tweak this a little bit I also want to show you a classification report real quick so from sklearn.metrix import classification report like this okay and then you can just print this out so print and then we'll just say our classification reports this then you're going to put in your y test and you're going to put your y predictions and you can find a lot more data right your Precision recall F1 score and then how this data was essentially split up if you're 0.82 0.83 0.83 and this takes a look at both zero and one which are classifier as over here right Baseball Hall of Fame or not so overall pretty good but we can definitely get a little bit better next thing we're going to do is take a look at some of the features so we're going to say features equals PD dot data frame and then there's something called future importances so all we're going to do is rf.eacher importances underscore here at the end and then we're going to say our index on this one equals x dot columns and then this should populate if I do a features over here so features dot head 15 I'll show everything right and you can see like what features were the most important with this model so you can see like hits runs at bats all played part over here home runs not as much which is kind of interesting and then you do have RBIs then also batting average uh which batting average actually had the biggest amount right 0.139 and batting average like it's okay right now as a metric in baseball uh people are trying to move away actually from batting average um but a lot of baseball players were kind of inducted in the Hall of Fame based off of batting average it's definitely an older old school metric with that in mind let's take a look at some of the hyper parameters so I'm just going to put hyper parameters over here and we're going to Define this as rf2 and we'll see if I add in some hyper parameters if this is a little bit more accurate so we can just say again random Forest classifier and then we're going to open this up over here and we'll start going so we'll first start off with estimators the meters and set this equal to a thousand another thing is you can set up your criteria so Criterion and then I'm gonna set up entropy but it's not one by default and put that over here and then we can have sample split so Min samples split it's at that equal to 10. another one that we can have is the max depth so max pth missed it this one into 14 and lastly we can have a random State just like what we did with train test split so Random state equals 42 and these are all just common print hyper parameters that I see with random Force classifiers so just put them in over here again super basic in the beginning we just called it with no hyper parameters and we put this over here and something that's really cool with machine learning is you can actually work on these with Hyper parameter tuning but that will be another video in this series so shift and enter and because I've built up that pipe on accident it didn't work but now it does which is good like earlier we're going to fit this data so rf2 dot fits you put over here x train and also y train great now this should fit our model and then you can see everything that we put over here right right the parameter is now populates in comparison earlier which nothing was getting populated and we can just take a look at our score real quick rf2 DOT score put X test y test great and we have 0.849 in comparison to where we had 0.827 which is always good to see okay then you can also do y pred two equals rf2 dot predict and then we'll put over here X test and then I just want to see this classification report so print classification reports and then you'll throw in here your y test then your wide prediction two wipe red two just like this and now you have your classification report which you can see this is definitely a way better model um this kind of scares me a little bit 0.69 and 0.76 uh but just overall better if you guys enjoyed this video if you did make sure to subscribe to the channel for a lot more data science videos and also machine learning uploading over three videos a week right now and these aren't the easiest to make by the way the next video you should watch is this one over here or should we talk about a logistic regression you will enjoy that one as well"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Decision Trees and Random Forests with Scikit-Learn",
        "Video Title":"How to Build Your First Decision Tree in Python (scikit-learn)",
        "URL":"https:\/\/www.youtube.com\/watch?v=YkYpGhsCx4c",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/YkYpGhsCx4c\/hqdefault.jpg",
        "ID":"YkYpGhsCx4c",
        "Publish Time":"2023-08-17T20:43:28Z",
        "Channel":"Ryan & Matt Data Science",
        "Channel ID":"UCKq-lHnyradGRmFClX_ACMw",
        "Transcript":"today we're gonna be covering how you can build out a decision tree machine learning algorithm through sklearn and also python now what this is is a supervised machine learning model that uses pre-labeled data how this works essentially is we split the data based off of different criteria think of like how a flow chart works now how this is broken down at the very top we have a root node followed by decision nodes on each side and when we get to a final outcome that's going to be considered a leaf node kind of a good example of this is if someone is going to complete an ultra marathon we may look at different data points such as like how many miles of training per week does a runner have what's the farthest distance run by the runner has that Runner completed an ultra marathon before and from there we can have the final decision if we can predict if a runner will complete an ultra marathon or not now one caveat before we start programming this is not the most accurate model but since it is fair really simple to code and quick and easy to run it's a great starting point into machine learning all right I'm going to jump on my computer and let's start coding all right we have a blank Jupiter lab notebook over here let's start so import pandas as BD shift and enter that runs that cell and builds out a brand new light now the first thing I'm going to do is import my data in over here I have a CSV called 500 hits I'll explain how this specifically Works um but I'm just going to copy this code over here so DF equals pd.read CSV through the file name over here and then I have to put encoding a lot in one otherwise it will not work now the CSV will be available it's down below in the description and it is on my GitHub page so that way you can download it import it and start running this code so we have that over here I'm just about to head over here just to show you how this works so essentially I I grabbed the top 500 hitters in baseball uh based off of how many hits they've had across their lifetime and over here for the Hall of Fame right uh that's what we're going after one if a player is in the Hall of Fame zero if they're not and I did remove some of the data in here too uh so any active player someone that retired less than five years ago or steroid users um just because they have weird cases for the Hall of Fame if you're an active player you can't make the Hall of Fame five-year minimum uh past retirement to make the Hall of Fame and steroid users they aren't technically banned but there's no steroid members uh that are officially in the Baseball Hall of Fame so I removed those because they had no chance so what we want to do first is clean up this data a little bit before we're in the model uh so I'm gonna drop the player column and I'm also going to drop caught stealing because I don't care about those so just put DF equals DF dot drop and then we'll put columns equal we're gonna put the two columns over here so we have player all caps and then also we have CS not computer science caught stealing and that just drops them from the data frame I'm not going to rerun the data frame just to show you guys how that works but you just gotta believe me on that one and next we need to split up our data between X and Y uh so everything over here minus Scott ceiling and player name is gonna be X our Y is gonna be a Hall of Fame because that's what we're trying to essentially predict uh so all you have to do on the side of things let's put x equals DF dot I lock and then we're going to put a colon over here and then we're going to do 0 through 13. we're going to enter over there and then by the way I put capital x that's just standard across the board when you're running machine learning models then lowercase Y and for this one we're just going to grab this Hall of Fame over here so you can essentially have this code and then what we're going to do is remove this zero over here we just want to have 13 and it's just going to grab the last one and now we have our X and Y our next step is going to be to split up our data between a training set and also a testing Set uh so you do that through train test split so first we have to import this so we're going to do it from SK learn dot model selection Imports train test your score splits now that is imported over here the next thing is we can actually run this so how do you run this is used to put up X train then you have to X test remember capital x is then you have y train y test just like that and then it's equal to we'll put train test split over here but our capital x and also our lowercase y then we need to set up a random state so that way if we run this in the future it can run exactly the same I'm just going to put down 17 and then we have to do our test size I always just put 0.2 feel free to put whatever you would like and that's going to save like 80 data is going to be in trained 20 is in the test right shift and enter and I do have an issue because I misspelled size so shift and enter and now we are good so just to show you the sizes of these just to show that I am not lying to you on that side of things just put xtrain.shape right 372 by 13 right and we do X test.shape great 93 by 13 and just to show you too if we just put over here y train rate 372 and you have to believe me on this one so why test and we have so I know that is working out so now we can look at using our decision tree classifier so let's import that in so from SK learn dot tree which is a little bit different than some other stuff we've used in the past and other videos so import decision tree classifier like this right all capital unlike train test split and then I'm just going to say DTC equals and I'm just going to call this now I'm not going to put any parameters in here I'm going to do a second run of this to just to show you a few parameters and how we can kind of tweak this up but I'm a shift in enter and this now runs now just to show you all the different stuff that you could throw in here all you have to do is DTC dot get underscore params like this and you can see all the different options over here so we're a second run we're going to be using CC Alpha over here because I do think it is kind of important with this data that we are going to be running and then also we'll take a look at Criterion over here so we'll change up both of these in a second but let's run this first version of it so if you're familiar with other machine learning models right you're gonna have to fit your data so the datc we're going to do dot fit and I'll just throw in our training set over here so X train and then also our y train then you'll see this over here and boom and we know that it is working and let's do our prediction right so why prediction equals DTC dot predict and then you're gonna throw in your X test over here just to see how our prediction works so now we have our prediction over here and we can start running some of the different uh metrics to determine how good this model was for the data so the first one I'm going to do is a confusion Matrix which is really popular so from sklearn.metrix import confusion Fusion and then underscore Matrix like this and then I'm just going to run it um so just because it's better if you just throw the statement within print I'm just going to put over here confusion Matrix and then we'll do white tests and why underscore prediction that's and then we have our confusion Matrix over here so just to break this down we have our true positive over here and then we also have our true negative down below and then also on the top right we have false positive then down over here we have a false negative and there's a few other metrics that we can actually get from the confusion Matrix but it's so much easier to run a classification report so essentially I'm just going to copy this again and instead of confusion Matrix here at the end I'm going to put classification ports now that has been imported over here and then essentially I'm going to copy this we'll put classification here instead of fusion Matrix and then we have a lot of data so I'll explain how this all works so first thing we have in here is precision which takes a look at true positive divided by true positive plus false positive so that takes a look at 52 and also 9 right then we have recall which takes a look at true positive over true positive plus false negative so that's your 52 over here and also your 11 so that's how we grab that over here now F1 is a calculation between Precision recall essentially you take two times Precision times recall divided by recall plus precision and that's where we get this over here and then I want to take a look at the weighted average so we have seven eight seven eight seven eight across the board which is okay it's not the best in general um but you can also see like what had the biggest impact on our model and I'll just show you how to do that real quick I'll build up a few more cells down below uh so all we're going to do this is DTC dot feature underscore importances like this and essentially it's going to give us this array which tells you how important each of the different features are and technically you'd have to go back over here and see like okay we have years games at bats but there's a much easier way to represent this data so let's turn this into a data frame uh before that though I just want to show you one quick trick if you put x dot columns in here right it shows you all the specific columns from our original data frame over here but we're going to be using that when we're building our new data frame so I'm just going to say features right equals PD dot data frame it needs to be capital F right and then what we're going to do first is throw in this over here feature importances and then we're gonna do index equals and we're going to grab this x dot columns and that does not work let's see why it is not working reason is I put date frame not data frame okay so now all I have to do over here is features go ahead and I'm just gonna put 15 in here to show everything right but now you can see how important everything is for this model now it makes sense that hits are the most important on this case because we took the top 500 um baseball players with hits all time and determine if they're making the hall of fame or not right so our hits we have 0.403 on this side of things uh it's kind of interesting batting average batting average isn't the best metric in baseball um but you would have a higher batting average the more hits you have so that's pretty high over here walks is actually a lot higher than I would have anticipated home runs is a bit low but there's not many hitters that have like 500 plus home runs that also have a lot of hits it's kind of weird with baseball and then you can see over here uh games years at bats which at bats has more significance than years or games which makes sense right more at bats more opportunities to get hits and different things like that but essentially this shows you how important each of these different features are within our model okay so I hinted at a little bit earlier throwing in a few different parameters I'm gonna do it now so what I'm going to do this time is DTC two and we're gonna run this decision tree classifier again so decision tree classes of fire and this the reason why I'm also running the second time so you can get used to the code but also show you a few other things so we're going to put Criterion equals entropy just another way to kind of run this model so we should get a bit different results on it and then we're going to do CC Alpha so CCP Alpha equals 0.04 now cccp Alpha this one over here it helps if your model is overfitting which I think is the case because we have a ton of different metrics that we're going to be taking a look at and not all of them are that important so uh what I'd recommend if you're running this for the first time is try to keep this close to zero because it does have a pretty big impact so I'm just going to run 0.04 on this side of things right and that is now working I'm gonna build out a few more cells over here um like before I'm also going to fit this data so DTC two dots fit then you throw over here x train and you have y train and then we see um our parameters in here CCP and also Criterion right entropy on this side of things now we can do our predictions so I'm just gonna do y PR d 2 equals DTC 2 dot predict bro X test on this side of things then we can grab our Matrix again so I'm just going to literally copy this over here so do that and while I'm over here too I'm just going to copy this classification report so I don't want to type those out um but that way we can see how this model Works compared to the first one right so 50 11 9 and 23 let's see how this did 52 9 11 and 21. so a little bit different on this side of things and then if we run our classification report so overall weighted average zero seven nine zero seven eight zero seven nine ninety three zero seven eight so it's probably a little bit better overall but didn't really make too much of an impact on that side of things then like before just to see how this significance works over here with the feature importances copy that and we're going to throw that over here right so I'm going to say features two equals P dot data frame DTC two dot feature importance Index right so that's why it's a little bit different and then we'll just put over here the head 15 and then you can see so this CCP removed different parts of the tree so you can see that it only kept in general batting average and then also hits everything else is zero on the side of things now what this is showing is what we did with CCP so essentially what CCP does is it removes parts of the tree uh to stop overfitting so you can see now that years games at bats runs they made them all zero uh they kept batting averaging over here and then also hits hope you guys enjoyed this video if you did make sure to subscribe to the channel as this does take a bit of effort to make now if you want to learn another machine learning algorithm I have one over here on a Canon classification I really recommend you watch that one next"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Decision Trees and Random Forests with Scikit-Learn",
        "Video Title":"How to Implement Decision Trees in Python (Train, Test, Evaluate, Explain)",
        "URL":"https:\/\/www.youtube.com\/watch?v=wxS5P7yDHRA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/wxS5P7yDHRA\/hqdefault.jpg",
        "ID":"wxS5P7yDHRA",
        "Publish Time":"2021-06-11T22:00:31Z",
        "Channel":"M\u0131sra Turp",
        "Channel ID":"UCpNUYWW0kiqyh0j5Qy3aU7w",
        "Transcript":"hey in the last video we learned the theoretical details of decision trees so how they learned how they are trained and in this video i will show you how to implement them on scikit-learn using python we will look into some of the parameters that you can use we will look into some very helpful functions that it has built in and generally we will try to get a better understanding of how you can start using them today on a python on a jupiter notebook so without further ado let's get started we have a bunch of things to look into of course but the first thing that we need to do is import a data set for this one i'm using a built-in data set from scikit-learn you can import it like this basically just if you want other data sets of course go to scikit-learn data sets just google it and then you'll find a list of data sets that are built built-in in scikit-learn that you can easily import this one is called the breast cancer data set it's a classification data set let me show you what it looks like so it's basically these are all the calculations done on a or measurements done on a tumor that was collected or a mass that is collected from a breast uh of course we are not doctors we don't really know what these things mean uh but we don't really have to also you know we're just using this data set to understand how decision trees uh trees can be implemented with scikit-learn so this is just a data set looks like we have 30 columns each of them is a measurement for a tumor and then as a target variable we are learning if a one line just so one data point or one tumor is benign or malignant so that means benign means it's not cancerous or i don't i don't really know the medical term there but i think it's like just not bad for you everything's fine if it's malignant it means that it has cancer and you need to be treated basically uh on a very simple level let's say okay uh before we feed this data set to our um let me make it a little bit closer yeah before we trade give it to our decision tree of course we need to divide it to training and testing data so just to kind of give you a structure at first i'm going to show you how you can train a simple default decision tree and then we are going to go into the details of how you can change it to how you need um okay com coming back to training and testing separation inside scikit-learn there is a default built-in function to separate training and testing data sets so you might remember this from your other training of machine for machine learning x means just all the features that you want to give to determine uh if this data point is benign or malignant basically and this is all the columns that i have in this data set and y is a target value basically and that's going to be either 0 or 1 depending on if it's like good or bad um this built-in data or this built-in function only needs the x and the y basically as i showed here and the test size means how much of it would you like to set aside for testing so for training the training data set means we are giving it all the examples but we are also giving it the answer so the model can train and learn uh whereas for the test one we're only going to be giving the features for the or the columns and then it's going to create the predictions itself so that's how we're going to test to see if our model is performing well or not uh how to do predictions how to train the decision tree is basically very simple you have to import the decision tree classifier or basically the um the model itself from the scikit-learn library and you have to create it once we start putting in parameters this is where we're going to put it in and then all you have to say is classifier fit and you give the training values for the x values and the y values and then it creates you a nice little decision tree model that's all um of course right now we're using the default parameter so if you run this one which is which gives you all the parameters that are being used in this model you're going to see that it's the default values um yeah so these are all the default values max depth is none max features is none we're going to learn more about what these are in a second uh but yeah if you want to use instead the regressor it's very simple then you only need to import regressor and how if you're like oh but i don't know how to import these things when you find the documentation from google so basically by just writing a scikit learn decision tree regression or second learn decision tree classification you're going this is going to be either the first or the second page to pop up so this is the scikit-learn documentation you just need to scroll down and then see these examples and it's going to tell you how to import it here basically so you just need to copy and paste this code to your notebook okay now we trained our data set but of course you want to do some predictions with it there are two ways how you can get predictions the first one is by giving it the x test data set so it's basically my data points where i want to where i haven't told the model the answers to i just want to get the answers that it can it comes up with let's look at what it looks like so it's something like this uh my training my or my whole data set had 569 rows and for the testing we set aside nearly 200. um if i do a prediction it's going to give me the class number that it's predicting so it's going to say for the first one i am predicting that it's malignant for the second one i'm predicting that it's benign second one i'm pretty good at sb9 so on and so forth so this is just a way to see generally how your model is predicting on the new information that it's giving you another way to see predictions is using predict proba then it will give you the probability it has for each of the um classes so let's see so it says for the first instance so the first data point so this one i am predicting that it's going to be um class 0 with 1 out of 1 chance and class 1 with 0 out of 1 chance so there is a reason that our all our probabilities are 0 or 1 because we did not have any uh early stopping criteria so our tree just like grew and grew and grew and grew all the way until there was no other way to split but if we put a very simple stopping criteria like let's say max that can be four so the maximum depth that the tree can have three can have is four so you can see here and let me remove this one and then we do predictions again the predictions also are still going to look the same because it's going to give us uh the prediction so it's going to tell us either zero or one based on which one has the higher prop uh probability but if you look at the probabilities now we're going to see that they're a little bit different so now it's kind of less sure uh which one it should be because we stopped the tree before it was able to grow all the way where the leaves are going to be pure of of one class so if doesn't if this doesn't make sense to you you should go back and watch the first video i think then it's going to make more sense to you what i mean here all right um so we have the probabilities we have the predictions for each of the test instances but how are we going to see if this is good or not we have to compare it to the actual information of course how are we going to do that is by using a performance metrics most of the performance metrics that you're going to need or use are already going to be built-in and scikit-learn so first one is accuracy for example it says this model has an accuracy of 0.92 or 93. if you want to see you can find the confusion matrix uh so this is you know if the class is zero and when the class is zero and it's predicted as zero that happens 69 times when the class is one and predicted as one this happened 105 times and these are the wrongly classified instances uh another one i can see is precision score for example so this is basically precision if you want to see recall so let's see okay um you know i'll just show you how i find these things so i could learn recall and yeah this is the first thing that pops up and then i can go look at the examples it says i need this one to calculate recall and uh my true values are called y test and predictions are called predictions yeah and then i get my recall score too so it's that simple there is also a nice function that they have here it's called classification report and i think then you can basically see precision and recall and f1 score and everything together there also might be a regression report similarly that you can find um of course you see here the mac maker macro average weighted average etc etc so there are some or like precision and recall separately for malignant and benign uh i will not go into details of what these things are in this video because you know this is about decision trees but yeah i can make a separate video about that later but let me know if you would like to learn about it um all right so before we go into future importance and other things i want to show you some of the parameters that decision trees have so let's go back to our model so it was we're only using max steps 4 right now all right uh so let me pull up a list of the stopping criteria so these are all the settings let's say that you can change to stop the tree from growing all the way to its maximum passable length max depth tells me how deep can the tree be so when there is one node when there is one decision that's depth of ones so when you make the decision or when there's only one node then that's a depth of one when you have a decision node there and then you make a split then you tree your tree has a depth of two and then those nodes split and then you get a decision tree of that depth of three so that goes further and further sometimes decision trees can grow to be very long so if you if you wanted something these things are not really things that you can know beforehand you cannot really say oh yeah i want my max step to be three because i know that's going to give me the best results no most of the time what you do is you try uh you try different values for this and then you see which one works best so yeah max step basically gives you the depth of the tree um there are some other ones here as you can see these are all the stopping criteria if you want to learn more about what they are how they change when you stop the tree you can you can find them all here these are all the parameters that the tree has you can go and read about them here they have different ways of stopping the tree tree's growth let's say the training process i think if you have more than one stopping criteria set up it's going to just stop with the first one that it reaches um but yeah so just you know go ahead and learn more about them and then try it and then you'll see the difference that it creates in your performance or the creating the performance of the tree another thing that's important for us other than the stopping criteria are the approach of the decision tree so here are some of the approaches or the some of the settings that you can change for the approach the first one is criterion what is criterion so in the previous video i talked about if you remember that there are two different algorithms that you can use cart algorithm and id3 algorithm and they are using different metrics of which feature they should use to split the data set right so criterion basically depend or determines that one the default default one is ginny but you can also use entropy again if you don't know what these things are and if you're curious go back to the first video and then watch it the video about decision tree theory and you will know what i'm talking about here so this is the parameter that will help you determine how to uh grow the tree or what to use to make the splits the second thing that we can use to determine the approach to growing the decision tree or training the decision tree is splitter um basically you have two options you either choose the best one based on these two criterion or you can just choose it randomly that's also an option you can also say you know what i want to go crazy i want to just choose it in a random way which feature to split on and yeah you can do that that's also a possibility another one that's important to know is max features so in the decision tree we have or let me show you here we have 30 columns right you can say every time you want to make a decision every time you want to make a split only use 20 of them and then the decision tree will decide which 20 randomly and then it will compare their entropy to each other or their information gained to each other and select the best one so maybe the actual best one is outside of that group of 20 but still it will choose only the one the best one in that group of 20. so that's an option that's available to you if you want you can choose the not the max depth but the max number of features to be less than the amount of columns that you have the total amount of columns that you have and then there will be some more randomization involved a random state is basically helping this randomization of um the when you choose less than the amount of columns that you have then there needs to be some randomization of course um if you give an integer for the random state then it's going to be creating the same same results from this randomization every single time but if you don't give anything to the random state it's going to be super random every time but if i add a parameter to be like random state five then i'm going to get the same tree over and over again even when i have some randomization involved um okay i think this was clear uh these are all the things that you can do to change the approach of the tree and there is one other thing that's important to know and that is this one class weight so why did i run this it's not like it's gonna run class weight is this is specific to classification this doesn't exist in regression you know so when you're doing classification you're going to have one two or three or maybe more classes that your model is going to try to predict and sometimes one of those classes might be a little bit more important than the other ones and with class weight you can determine this in the model and you're basically going to say hey it's more important or it's worse when you make an error predicting class one then when you do a mistake trying to predict class two so then it's going to take it into consideration into how it grows or into its genie index or entropy um all right so these are all the parameters that are relevant uh the next thing that we can look at is the future importance so with decision trees as i said in the previous video i think i mentioned it one of the best things is that it's interpret interpretable very hard word to say so you can actually understand how the decision tree is deciding and one of the things that comes with this is you can actually learn or you can see which features are more important than the others so let's go here i'm getting all my features these are all the features that i have and it's very simple you can just say classification future importances well let me show you what this creates anyways in the first place and then it will give me a list corresponding to this list of how important that corresponding feature to determine the result of your prediction so and when i put it into a data frame and everything i can see future importance like this and if you want this is very common to do you can make it into a plot and let's see yeah and then you can basically see it in a plot and then it's easier to kind of understand okay verse perimeter is the one of the best things that uh determines if a tumor is benign or malignant so this is very important to really understand your um model this this is not really possible with most other machine learning algorithms so this is like a really big plus for decision trees another way to see how your data set is or how your model is working is to create a plot of the tree and this is very simple you are basically creating this tree of like an actual tree of how it decides so for example you know when when you get one line one data point uh it says worst perimeter is it bigger than 110.25 or greater than or lower than that one if it's lower than that one go here if it's greater than that one go here and then another decision point and another decision point uh so as far as the depth goes as you can see this is the first step second this is the third level fourth level this whole thing in the fifth level um or now that actually now that i counted it i said next step to be four so probably this starts from zero and then one two three and four uh we can actually see how it changes when we change the parameters so we said max steps should be 4 right so let's change that to like not before let's just have the default tree let's have it grow as much as possible and then let's look at the how the tree looks okay so this is a much busier tree maybe the depth doesn't even go further but probably this is then enough you know for the tree to uh grow but yeah then it's it's kind of like a more dense tree and one other important thing that i nearly forgot to mention is pruning for decision trees right we talked about it in the previous video how it's an important way or how it's a very used way of making sure that the tree is not overfitting to your data set how you do that with scikit-learn is with a parameter actually so this parameter is called ccp alpha um we can go and see the definition here it's basically that there is a algorithm on top of the decision tree algorithm to prune it and it's called minimal cost complexity pruning i'm not going to go into details but if you want to read more about it there here's a link for it uh to describing the details and the general math that is behind it but when you give it a value that's bigger than zero the default value is zero when you give it a value bigger than zero then it's going to prune your tree so let's try so we saw that our tree was a little bit big so if i give it this value i expect for the tree to be a little bit pruned so this is what we have right now and when the tree is a bit pruned then as you can see that value is apparently even already too big so then i have a very nice and simple tree that was pruned um but let's see how the accuracy changed i'm curious i didn't even no i have to do the prediction again um yeah let's see now okay it looks like the accuracy increased so you know that's perfect i guess our tree was uh overfitting maybe to our decision um to our data set i think that's all we have at decision trees so just you know it's very simple as i said just import them import one of the sets that they have built in in their system and yeah just play around with it change some of the settings and then see how it changes your accuracy but yeah i hope this was helpful i hope you learned something at least and yeah thanks for watching i'll see you around"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Naive Bayes Classifiers",
        "Video Title":"Naive Bayes, Clearly Explained!!!",
        "URL":"https:\/\/www.youtube.com\/watch?v=O2L2Uv9pdDA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/O2L2Uv9pdDA\/hqdefault.jpg",
        "ID":"O2L2Uv9pdDA",
        "Publish Time":"2020-06-03T12:35:57Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"I'm at home during lockdown working on my step quest yeah I'm at home during lockdown working on my stack quest yeah stack quest hello I'm Josh starburns welcome to static quest today we're gonna talk about naive Bayes and it's gonna be clearly explained this stack quest is sponsored by jad bio just add data and their automatic machine learning algorithms will do the rest of the work for you for more details follow the link in the pinned comment below note when most people want to learn about naive Bayes they want to learn about the multinomial naive bayes classifier and that's what we talk about in this video however just know that there is another commonly used version of naive Bayes called Gaussian naive Bayes classification and I cover that in a follow-up stat quest so check that one out when you're done with this quest BAM now imagine we received normal messages from friends and family and we also received spam unwanted messages that are usually scams or unsolicited advertisements and we wanted to filter out the spam messages so the first thing we do is make a histogram of all the words that occur in the normal messages from friends and family we can use the histogram to calculate the probabilities of seeing each word given that it was in a normal message for example the probability we see the word dear given that we saw it in a normal message is eight the total number of times deer occurred in normal messages divided by 17 the total number of words in all of the normal messages and that gives us 0.47 so let's put that over the word dear so we don't forget it likewise the probability that we see the word friend given that we saw it in a normal message is 5 the total number of times friend occurred in normal messages divided by 17 the total number of words in all of the normal messages and that gives us zero point two nine so let's put that over the word friend so we don't forget it likewise the probability that we see the word launch given that it is in a normal message is 0.18 and the probability that we see the word money given that it is in a normal message is 0.06 now we make a histogram of all the words that occur in the spam and calculate the probability of seeing the word dear given that we saw it in the spam and that is two the number of times we saw deer in the spam divided by seven the total number of words in the spam and that gives us zero point two nine likewise we calculate the probability of seeing the remaining words given that they were in the spam BAM now because these histograms are taking up a lot of space let's get rid of them but keep the probabilities oh no it's the dreaded terminology alert because we have calculated the probabilities of discreet individual words and not the probability of something continuous like weight or height these probabilities are also called likelihoods I mention this because some tutorials say these are probabilities and others say they are likelihoods in this case the terms are interchangeable so don't sweat it we'll talk more about probabilities versus likelihoods when we talk about Gaussian naive Bayes in the follow-up Quest now imagine we got a new message that said dear friend and we want to decide if it is a normal message or spam we start with an initial guess about the probability that any message regardless of what it says is a normal message this guess can be any probability that we want but a common guess is estimated from the training data for example since 8 of the 12 messages are normal messages our initial guess will be 0.67 so let's put that under the normal messages so we don't forget it oh no it's another dreaded terminology alert the initial guests that we observe a normal message is called a prior probability now we multiply the initial guess by the probability that the word dear occurs in a normal message and the probability that the word friend occurs in a normal message now we just plug in the values that we've worked out earlier and do the math beep-boop beep-boop it and we get 0.09 we can think of 0.09 as the score that dear friend gets if it is a normal message however technically it is proportional to the probability that the message is normal given that it says dear friend so let's put that on top of the normal messages so we don't forget now just like we did before we start with an initial guess about the probability that any message regardless of what it says is spam and just like before the guests can be any probability we want but a common guess is estimated from the training data and since four of the twelve messages are spam our initial guess will be 0.33 so let's put that under the spam so we don't forget it now we multiply that initial guess by the probability that the word dear occurs in spam and the probability that the word friend occurs in spam now we just plugged in the values that we worked out earlier and do the math BIP BIP BIP BIP BIP and we get 0.01 like before we can think of 0.01 as the score the dear friend gets if it is spam however technically it is proportional to the probability that the message is spam given that it says dear friend and because the score we got for normal message 0.09 is greater than the score we got for spam 0.01 we will decide that dear friend is a normal message double BAM now before we move on to a slightly more complex situation let's review what we've done so far we started with histograms of all the words in the normal messages and all of the words in the spam then we calculated the probabilities of seeing each word given that we saw the word in either a normal message or spam then we made an initial guess about the probability of seeing a normal message this guest can be anything between zero and one but we based hours on the classifications in the training data set then we made the same sort of guess about the probability of seeing spam then we multiplied our initial guests that the message was normal by the probabilities of seeing the words dear and friend given that the message was normal then we multiplied our initial guests that the message was spam by the probabilities of seeing the words dear and friend given that the message was spam then we did the math and decided that dear friend was a normal message because 0.09 is greater than 0.01 now that we understand the basics of how naive Bayes classification works let's look at a slightly more complicated example this time let's try to classify this message lunch money money money money note this message contains the word money four times and since the probability of seeing the word money is much higher in spam than in normal messages then it seems reasonable to predict that this message will end up being spam so let's do the math calculating the score for a normal message works just like before we start with the initial guess then we multiply it by the probability we see lunch given that it is in a normal message and the probability we see money four times given that it is in a normal message when we do the math we get this tiny number however when we do the same calculation for spam we get zero this is because the probability we see lunch in spam is zero since it was not in the training data and when we plug in zero for the probability we see lunch given that it was in spam then it doesn't matter what value we picked for the initial guess that the message was spam and it doesn't matter what the probability is that we see money given that the message was spam because anything times zero is zero in other words if a message contains the word lunch it will not be classified as spam and that means we will always classify the messages with lunch in them as normal no matter how many times we see the word money and that's a problem to work around this problem people usually add one count represented by a black box to each word in the histograms note the number of counts we add to each word is typically referred to with the Greek letter alpha in this case alpha equals one but we could have said it to anything anyway now when we calculate the probabilities of observing each word we never get 0 for example the probability of seeing lunch given that it is in spam is 1\/7 the total number of words in spam plus for the extra counts that we added and that gives us 0.09 note adding counts to each word does not change our initial guess that a message is normal or the initial guess that the message is spam because adding a count to each word did not change the number of messages in the training data set that are normal or the number of messages that are spam now when we calculate the scores for this message we still get a small number for the normal message but now when we calculate the value for spam we get a value greater than zero and since the value for spam is greater than the one for a normal message we classify the message as spam spam now let's talk about why naive Bayes is naive the thing that makes naive Bayes so naive is that it treats all word orders the same for example the normal message score for the phrase dear friend is the exact same for the score for friend dear in other words regardless of how the words are ordered we get 0.08 treating all word orders equal is very different from how you and I communicate every language has grammar rules and common phrases but naivebayes ignores all of that stuff instead naivebayes treats language like it is just a bag full of words and each message is a random handful of them naive bayes ignores all the rules because keeping track of every single reasonable phrase in a language would be impossible that said even though naive bayes is naive it tends to perform surprisingly well when separating normal messages from spam in machine learning lingo we'd say that by ignoring relationships among words naivebayes has high bias but because it works well in practice naive Bayes has low variance shameless self-promotion if you are not already familiar with the terms bias and variance check out the quest the link is in the description below triple spam oh no it's one last shameless self-promotion one awesome way to support stack quest is to purchase the naivebayes stack quest study guide it has everything you need to study for an exam or job interview it's eight pages of total awesomeness and while you're there check out the other stack quest study guides there's something for everyone hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to support stack quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Naive Bayes Classifiers",
        "Video Title":"Naive bayes classifier explained for beginners",
        "URL":"https:\/\/www.youtube.com\/watch?v=ny1egBSrsTc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ny1egBSrsTc\/hqdefault.jpg",
        "ID":"ny1egBSrsTc",
        "Publish Time":"2023-04-30T16:00:29Z",
        "Channel":"AI Explored",
        "Channel ID":"UCM35ONKE8_zLtPTHzdpRFXw",
        "Transcript":"welcome to 8i explored in this video we will explore the fundamental concepts behind naive Bayes including Bayes theorem probability and conditional probability and how they all come together to form the naive Bayes algorithm we will also take a look at the different types of my base classifiers and their unique features as well as the various applications in real world scenarios scenarios by the end of this video you'll have a solid understanding of my ebays and its practical uses in the world of data science base theorem comes from 18th century mathematician Thomas Bayes who proposed that the conditional probability of an event based on the occurrence of another event is equal to the likelihood of the second event given the first event multiplied by the probability of the first event the development of my ebays as a machine learning algorithm was greatly greatly influenced by this theorem and was key in the development of the modern naive Bayes to fully understand naive Bayes it's essential to have a solid understanding of probability and conditional probability probability is the likelihood of an event occurring and is represented by a number between zero and one conditional probability is the probability of an event occurring given that another event has already occurred it is represented by P I slash B which means the probability of event I given that event B has occurred naive Bayes relies on the assumption that the variables are conditionally independent which means that the occurrence of one feature does not affect the probability of another feature occurring the algorithm is a simple and effective probabilistic classification algorithm that is based on Bayes theorem the algorithm assumes that the features or attributes of the data are conditionally independent of each other given the class which is where the naive and naive Bayes comes from this means that the algorithm assumes that the presence or absence of one feature has no effect on the presence or absence of another feature the algorithm works by calculating the probability of each class given the observed features of a data point and selecting the class with the highest probability as the predicted clasp this is done by multiplying the prior probability of the class with the conditional probability of each feature given the class and then normalizing the result to obtain a probability distribution over all classes despite its Simplicity in the naive assumption it makes moneybaze can perform surprisingly well on a wide range of classification tasks and is often used as a baseline algorithm for comparison with more complex models there are several types of naive Bayes algorithms each suited for different types of data and classification tasks the first type is gaussian naive Bayes it is used for continuous features and assumes that the data follows a normal distribution this is often used for classification problems where the features are real valued and continuous such as predicting a person's income based on their age and education level the second type is multinomial naive Bayes which is used for discrete count data such as word frequencies and text classification it assumes that the data follows a multinomial distribution and is often used for natural language processing tasks such as span filtering and sentiment analysis the third type is bernaling Mae Bayes which is similar to multinomial naive Bayes but is used for binary or Boolean data where the features can only take on two possible values such as true or false this is often used for text classification tasks where the presence or absence of a certain word in a document is used as a feature the choice of use of my eBay's algorithm depends on the nature of the data and the specific classification task at hand the algorithm has many real world applications in various Fields here are some examples of how it can be applied in real world scenarios number one spam filtering naive Bayes is commonly used for email spam filtering work can distinguish between spam and legitimate emails based on the words and phrases used in the email content number two sentiment analysis my ebays can be used to classify the sentiment of text documents such as social media posts or product reviews as positive negative or neutral this is useful for understanding customer opinions and preferences number three medical diagnosis my eBay's medical diagnosis by predicting the likelihood of a patient having a particular disease based on their symptoms and medical history number four image recognition maibes can be used for image recognition tasks such as classifying images of animals or objects into different categories number five fraud detection phase can be used for fraud detection and financial transactions such as credit card fraud by analyzing patterns and identifying anomalies in the data overall naive Bayes is a powerful and flexible algorithm that can be applied to a wide range of real world problems making it a valuable tool for data scientists and machine learning practitioners if this video has helped you in any way don't forget to like And subscribe comment down below what you'd like to see me do a video on next and click on the screen right now if you want to learn more about AI"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Naive Bayes Classifiers",
        "Video Title":"Na\u00efve Bayes Classifier -  Fun and Easy Machine Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=CPqOCI0ahss",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/CPqOCI0ahss\/hqdefault.jpg",
        "ID":"CPqOCI0ahss",
        "Publish Time":"2017-08-26T22:45:27Z",
        "Channel":"Augmented AI",
        "Channel ID":"UCFJPdVHPZOYhSyxmX_C_Pew",
        "Transcript":"hey guys and welcome to another fun and easy machine learning tutorial on naivebayes ah it's a good day to play golf or is it let's take a look at some of the factors that determine whether or not we play today these features are weather conditions or outlook if it's sunny overcast or rainy there's also temperature humidity and wind please smash the subscribe button and click the doll icon to join our notification squad if you think about it intuitively we are more likely to play when it's not too hot or cold which means overcast and mouth temperatures if it is less humid but minimal wind speeds we are more likely to play if it's too hot or sunny who may become exhausted very quickly let's take a look at our data set so if we recorded our circumstances of 14 days we have our outlook temperature humidity wind speed and our dependent variable which is whether or not we like golf the objective here is to estimate the likelihood of playing golf yes or no given weather condition information so based on this data let's see how we can approach this problem using the naive Bayes algorithm first we determine how many yeses and how many knows we get from our data set so the probability P of C or probability of our classes which are yes in known we can calculate this as follows so the probability of y es is 9 out of 14 we count 1 2 3 all within 9 total yeses from the 14 possible days and similarly we can count 5 nodes which gives us a probability of 5 out of 14 now we also need to calculate the individual probabilities with respect to each features or very conditions in our dataset so for sunny the probability that it is sunny given that it is yes is two out of nine so be sunny given a yes why because from nine yeses it is only sunny twice as we can see over here to calculate the probability that it's sunny given an O is three out of five so from the five no play days only three days were sunny for the overcast outlook we can do the same for both here's in no classes we count four days that we got a s and zero days to do that and no so the probability that it was overcast given a yes is four out of nine and four not play class we got zero out of five respectively now we can compute these probabilities for all the other features you can see how easy this is so soon we need to classify the following new instance with outlook is sunny temperatures cool humidity is high and it's a tad but windy should recall often play some golf or rather stay in those watch a movie firstly we look at the probabilities that we can play the game so we use our lookup tables to get the probability that Africa sunny given place yes is to over nine probability of temperature equals cool given places which is three out of nine and similarly for humidity we win we have three are denied for both of them and then we have probability that play is a yes it's nine out of 14 as we discussed earlier next we consider the fact that we cannot play a game so for elliptical sunny we get three out of five temperature to school under five humidity 4.5 when it goes strong given a no street of five and then play equals no is five out of 14 then using those results you have to multiply the whole lot together so you multiply all the probabilities for lay coziest such as the probability that x given plecos years x play equals yes so that is to develop a 9 by 3 over 9 times 3 over 9 times 3 over 9 times 9 over 14 and this gives us zero point zero five three and this gives us a value that represents the probability of x given a class times probability of a class or in this case we have probability of x given play casillas times probability that play was a yes we also have to do the exact same thing for play was no so the probability of x given that labels no thanks probability the plays know that equals 3\/5 times 1 over 5 is 4 over 5 times 3 or 5 times 5 over 14 and excuse us 0.02 or 6 and finally we have to divide both results by the evidence or probability of X to normalize the evidence for both equations is the same and we can find the values we need within the total columns of the look-up tables therefore probability of x equals probability our local sunny times probability of temperature equals pool times probability that humidity is high as well as windy or strong as we mentioned earlier and let's gives us our probability of x which is zero point zero two one eight six and then dividing the results by this value we get the probability that we plays golf given X and s gives us zero point two four two four as well as probability that we don't play given an X condition where we get zero point nine four to one so given the probabilities can we play the game or not to do this we look at both probabilities and see which one is the highest value and that is our answer and therefore since zero point nine four to one is greater than zero point two four two four the answer is no we cannot play golf today probably you guessed it right it looks like bayes theorem Bayes rule now naive Bayes is based on base here also known as conditional theorem which you can think of as an evidence theorem or trust theorem so basically how much can you trust the evidence that is coming in and it's a formula that describes how much you should believe the evidence that you are presented with an example would be a dog barking in the middle of the night it's a dog barks for no good reason you will become desensitized to it and I've got checked if anything is wrong this is known as a false positive however if the dog barks only went so and enters your premises you'll be more likely to act on the alert and trust or rely the evidence from the dog so Bayes theorem is a mathematical formula for how much you should trust the evidence let's take a deeper look at the formula we can start off with the prior probability which describes the degree to which we believe the model accurately describes reality based on all of the prior information so how probable was our hypothesis before observing the evidence here we have the likelihood which describes how well the model produces the data this term over here is the normalizing constant the constant that makes the posterior density integrate to one like we see over here and finally the output that you want is the posterior probability which represents the degree to which we believe a given model accurately describes the situation given the available data of all our prior information so how probable is our hypothesis given the observed evidence so what's our example above we can view the probability that we play golf given it is sunny equals the probability that we play golf given a yes times the probability of it being sunny divided by the probability of a yes so why not Eve probability theory says if several factors don't depend on each other in any way the probability of seeing them together is just the product of the probabilities so in our example earlier we have the probability that our local sunny given is times the probability of temperature equal school given a yes times the probability that humidity is high given es times probability of the pin equal strong given a yes all looking at another example we can assume that sneezing has the impact on whether you are a bola so the probability of sneezing and being a Boulder given you got the flu equals the probability of sneezing given you that flu times the probability that your bola given the flu so the probability of a sneezing Boulder having flu must depend on the chances of this combination of attributes indicating flu looking at the pros and cons of naive Bayes it is easy and fast to predict a class of it is deficit it also performs well in multi class predictions when the assumption of Independence holds a naive Bayes classifier performs better compared to other models like logistic regression and you need less training data it performs well in the case of categorical input variables compared to numerical variables for numerical variables normal distribution is assumed or a bell curve which is a strong assumption looking at the disadvantages if categorical variable has a category in a test dataset which is not observed in the training dataset then the model will assign a zero probability and will be unable to make a prediction this is often known as zero frequency to solve this we can use the smoothing techniques and one of simplest winning techniques is called the Laplace estimation in some cases like our only example you can just add one to avoid the algorithm dividing by zero on the other side naive Bayes is also known as a bad estimator so the probability outputs from the predicted probabilities are not taken too seriously another limitation of naive Bayes is the assumption of independent predictors in real life this almost impossible that we get a set of predictors which are completely independent naive Bayes can be used for the following applications for credit scoring for e-learning platforms medical data classification through the naive Bayes approach you can be used for real-time prediction so naive Bayes is an eager learning classifier and it is really fast thus it can be used for making predictions in real time so this algorithm is also well known for multi-class prediction features we can predict the probability of multiple classes of targets favor can be used for text classification spam filtering and sentiment analysis so the naive Bayes classifier mostly used in text classification due to better results in multi class problems and independence rule having higher success rate as compared to other algorithms as a result it is widely used in spam filtering to identify spam email and sentiment analysis in social media analysis to identify positive and negative customer sentiments it is also used for recommendation systems so the naive Bayes classifier and collaborative filtering together post a recommendation system that uses machine learning and data mining techniques to foster unforeseen information and predict whether a user would like a given resource or not okay so that is it for me please don't forget to Like subscribe and share click the bell icon if you like to see some more machine learning tutorials and please support us on patreon if you'd like to download the script to this video please click the link down below for free download and stay tuned to the next lectures we will see how we can implement a naive Bayes algorithm in pipe thank you for watching and see you in the next lecture [Music] you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Naive Bayes Classifiers",
        "Video Title":"The Math Behind Bayesian Classifiers Clearly Explained!",
        "URL":"https:\/\/www.youtube.com\/watch?v=lFJbZ6LVxN8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/lFJbZ6LVxN8\/hqdefault.jpg",
        "ID":"lFJbZ6LVxN8",
        "Publish Time":"2020-07-22T14:00:21Z",
        "Channel":"Normalized Nerd",
        "Channel ID":"UC7Fs-Fdpe0I8GYg3lboEuXw",
        "Transcript":"hello people from the future welcome to harmonized nerd in this video I'm gonna talk about Bayesian classifiers and I'm gonna explain how life based classifier works and today I'm very excited because this is the first time I am creating a video using manum yes it's the famous mathematical visualizer library created by one of my favorite youtubers named grant Sanderson so let's get started first of all let's define our classification problem well it says that you will be given a set of features denoted by X which contains the elements X 1 X 2 up to xn that means we have in number of features and given that we need to find the correct label that is represented by Y now to tackle this problem from a probabilistic view we need to consider this Y and X as random variables let's assume that capital Y takes the value lower case Y and capital X takes the value X 1 X 2 up to lower case n now to find the correct label we need to find this expression for all possible values of Y well this expression is actually a conditional probability which simply means the probability of capital Y is equal to lowercase Y given that capital X is equal to this set to be more precise we need to find the particular value of lowercase Y for which this conditional probability becomes maximum why because in that case we will be able to say that okay for this particular value of y the expression becomes maximum so the class label should be this but the problem is it is hard to find the probability of Y given X directly to tackle this problem we use Bayes theorem yes this is the portion where base comes into the play this is the Bayes theorem it says that the probability of Y given X that we want to find is same as the probability of X given Y times probability of Y divided by probability of X now everything that you can see in the right hand side of this equation can be found our dataset let me first tell you how we name this terms the thing in the left-hand side that we want to find is called as posterior and the thing in the right hand side probability of Y is called as prior why this kind of name well prior means the probability corresponding to an event before considering any evidence and posterior means the probability of that event after considering some evidence well the evidence is nothing but the state of features so the probability of X is called as evidence here because X is just a set of features right and the term probability of x given Y is named as likelihood now here's an interesting thing the value of the denominator stays the same regardless of the value of y we put in in the numerator which simply means to compare the value of this conditional probability for different class labels we can just ignore the denominator because the evidence remains the same to really understand the concept we need to go through an example let's consider this small data set you can see that we have two features x1 and x2 and the level variable is y x1 and x2 can take values from 0 1 or 2 that means both x1 and x2 are categorical variables well I will talk about continuous variable later in the video and Y can take two values 0 or 1 so essentially it's just binary classification now imagine that someone is asking us to estimate the value of Y given that X is equal to 0-2 that means somebody has given the value of x1 and x2 that are 0 and 2 and they wants us to find the correct label for that set of feature ok let's compute the value of the conditional probabilities for both the labels that is y is equal to 0 and Y is equal to 1 first of all I'm gonna compute the prior that means probability of Y is equal to 0 and the probability of Y is equal to 1 so the formula is various in the numerator I'm gonna write the frequency of Y is equal to zero and in the denominator I have to write the total number of occurrences now look at the data set well here I have six occurrences of Y is equal to zero and four occurrences of Y is equal to 1 so the value of P Y is equal to zero will be 6 upon 10 now to calculate the probability of Y is equal to 1 we just need to replace the numerator with the frequency of y is equal to 1 and this is going to be 4 upon 10 okay so we have got our priors ready now let's compute the likelihoods first I'm gonna compute the probability of x given y is equal to one we just need to look at all the rules where the value of y is 1 and the feature combination is 0-2 so you can see we have got only one such entry in our data set so the value of this expression will be 1 upon 4 similarly let's calculate the likelihood for y is equal to 0 and you will see that in our data set there is not a single occurrence where the value of y is 0 and the feature combination is 0 comma 2 so the value of this likelihood is 0 now to find the class level that maximizes the posterior probability we just need to compute the numerator that I showed earlier so the way to do is is just by multiplying likelihood with prior after computing we see that for the class level 0 the value of the numerator is 0 and for the class level 1 the value is 1 upon 10 so obviously the class level 1 maximizes our posterior probability ok so we got an answer so we should be fine with that right well no there is a huge problem with this method the problem is it is hard to find that particular combination of x1 and x2 in our data set as you can see that we only find one occurrence of the instance where Y is equal to 1 and X 1 was 0 and X 2 is 2 and we didn't find a single occurrence where Y is equal to 0 and x1 is equal to 0 and x2 is equal to 2 and this is only for two features suppose we have 50 features so just imagine how hard it will be to find that particular combination of 50 features in our data set it is very likely that we will never encounter some particular combination of features right and if we don't find even a single occurrence of multiple search combinations then the probability value will always be 0 and we can't really compare if we have multiple 0 values right so to tackle this problem we naive Bayes classifier well the concept is very simple we just need to consider that x1 and x2 are independent of each other now why this can help well if we just consider that x1 and x2 are independent of each other then we don't need to actually find that particular combination of x1 and x2 in our data set we can just write the probability of X given Y as a product let me show you please understand that we wouldn't be able to write this if the features are not independent so that's the big assumption that knife base makes to be honest in real worlds the features might not be actually independent that's why it's called a naive approach towards the problem but it makes our work easier and it produces really good results so lets us compute the probability of x1 is equal to 0 given Y is equal to 1 well to do this we just need to count the entries where Y is equal to 1 and x1 is equal to 0 so as you can see there are 3 such cases and the number of occurrences where Y is equal to 1 is 4 so the probability will be 3 upon 4 similarly if we calculate the probability of x2 is equal to 2 given Y is equal to 1 we will get 2 upon 4 now let's come to the next line so here first we need to find how many times the value of x1 is equal to 0 where Y is equal to 0 so it turns out there is only one occurrence and the number of cases where Y is equal to 0 is 6 so the probability becomes 1 upon 6 similarly if we calculate the number of cases where x2 is equal to 2 and Y is equal to 0 is 1 so here also I'm gonna have 1 upon 6 now let's compare them so obviously the first probability is greater than a second one so we can clearly see that the class level 1 maximizes our posterior probability so the estimated class level is 1 and voila now you know how naive Bayes works ok so now I'm gonna talk about how you can deal with continuous features so until now we were dealing with categorical variables right x1 and x2 well both categorical and in that case we could actually compute the frequencies but in case of continuous features we can't compute the frequencies right so how to tackle this well there are many ways I'm gonna discuss two ways the first one is discretization suppose there is a continuous variable named age and let's consider that the value of each lies between eleven and fifty now we can actually divide the whole range into several groups such that the continuous variable becomes a categorical one for example let's divide it into four groups and we can interpret these like one corresponds to the age group between eleven and twenty two corresponds the age group between 21 to 30 and so on right and the second method is to fit unknown distribution to our features this distribution can be a normal distribution apasa distribution according to the need of the data but how does this fix the problem well the thing is when we have a known distribution we know it's probability function so to calculate the value of probability of x given Y is equal to some label we can just use the probability density function of the known distribution here F denotes the PDF of a known distribution and the product sign at the beginning tells you that here also I am considering the features to be independent of each other because only then we can multiply the probabilities and that's how we deal with the continuous features so that was all for this video guys I really hope that you enjoyed this video and learned something new about Bayesian classifiers if you like this video please share this video and subscribe this channel and do comment if you like this 3 blue one Brown format video stay safe and thanks for watching [Music]"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Naive Bayes with Scikit-Learn",
        "Video Title":"Naive Bayes Classifier: A Practical Tutorial with Scikit-Learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=T6OxPncD7_w",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/T6OxPncD7_w\/hqdefault.jpg",
        "ID":"T6OxPncD7_w",
        "Publish Time":"2023-09-19T14:54:23Z",
        "Channel":"Ryan & Matt Data Science",
        "Channel ID":"UCKq-lHnyradGRmFClX_ACMw",
        "Transcript":"in today's machine learning video we're going to be covering the naive Bayes algorithm and how we can implement this in Python with the help of the scikit learn library now this is going to be a classification algorithm and it's based off of the Bayes theorem of conditional probability now what this theorem States is that all independent variables are actually completely independent which essentially means that they are going to be unrelated to each other so think of the different columns within a data frame there's no relation between the two now some rural applications where this algorithm is pretty popular in is something like facial recognization and also weather patterns and there's actually three different types of naive Bayes algorithms out there that you can use within scikit learn but in this video I'm going to be focusing on gaussian in fact we're going to be using this gaussian algorithm to predict if Pearl gym a famous rock band out of Seattle sells out a concert or not with that in mind let me jump on my computer right now and let's start coding all right let's get started like other videos Imports and as as PD shift and enter runs a cell and brings in a brand new one now we're going to declare a data frame DF equals and we're going to bring in acsv and we'll explain how the CSV Works in a second but so we're going to put Pearl Jam tour 2 dot CSV and this CSV it doesn't work properly so I have to add in an encoding so encoding equals unit code just called Escape like this and our data firm should work now and all right so we have our data frame over here okay so let me explain how this works a Pearl Jam goes on tour all across the world and we're building out a classification problem it's a binary one right I sold out either zero or one in a city now realistically there's probably a ton of data that goes into if a concert sells out and Pearl Jam pretty much sells out every concert that they play at but and example I'm going to assume that they don't uh so I just have a few other things over here so first off we have the city population I actually generated these through chat GTP I said put different um I think I said like 42 different values for City population and I just threw some over here then we take a look at continents so North America Europe North America South America and Asia pretty much where Pearl gym tours quite a lot then we have our venue capacity so again something that I completely auto-generated anything from 5 000 to pretty much I think like I put 35 or 40 000 in there yeah 40 000 because we have a 39 100 over here and then I asked for it to have two zeros at the end so that's why we have numbers that are like this for the venue capacity day of the week so pretty much same thing right I just asked chat GDP Monday through Friday Saturday Sunday Monday like all generated over here through my CSV uh multiple concerts so this is what I kind of generate myself uh sometimes a band like program will play uh multiple Arenas two days in a row even three four five days in a row sometimes um depending on you know if they're doing a residency in the city so in this instance right like this one since City population is the same right they played multiple days over here the same multiple days over here the same uh which they just did on the recent tour which by the way I hope they visit Florida soon I have not seen Pearl Jam yet I have seen Eddie Vedder so I'm in Vegas and it was awesome ship regardless uh we have sold out over here which most of these are going to be sold out and to be honest with you kind of randomize the ones and zeros on here so we're producting at the most accurate result with this model um but I would hope at least we get over a 50 on this side of things but yeah so maybe I should have engineered sold out a little bit better overall based off a city population and then uh there's multiple concerts or not but why not this is a good example for you guys uh to learn how this specific model works so with that said uh one thing that you should know is pretty much all machine learning models you can't have text right so we got to change North America and also day of the week and I'm just gonna do dummies over here within pandas and there's a few different methods that you could also use I mean uh depending on the categorical information you can use one hot encoder or also ordinal encoder but this version is actually pretty simple and I'm not going to be building out any like real pipelines in this video so we're gonna go to the lazy way so we're gonna do PD dot gets dummies and then over here we're going to call our data frame and then we're going to put continents and then also we're going to put a um actually I think in this one I put day of week like that which make sure you put underscores but I accidentally put spaces in here when I first booked out this CSV but I think that's what we're going to put over here and let's see how this df2 looks like just to show you okay great so this is working properly so essentially how these dummy variables work is if you go back over here right this is continent North America so we're gonna have zero for Asia zero for Euro but then we have a one for North America zero for South America right and wait this is a Monday so we have a one over here zero for everything else I'm just going to exactly uh what I specifically wanted now this is exactly the same thing that one encoder does right uh builds out brand new columns and it's a zero or one right across the board so this is exactly what we want only problem is if you go back to original data frame right we had City population and then also our capacity multiple concerts and sold out and well to be honest with you if we go down over here all that data is missing we can scroll over and uh we just have a bunch of zeros and one so we need to combine these two data frames right because we still have our original data frame over here and we have a df2 oh well you can actually do this pretty easy we're going to concat both of these together so let's do that right now and we'll just say this is df3 so we're going to do a PD talking cats and then over here we'll throw in DF and then also df2 and then we'll have over here axis equals one do apologize for the dogs barking outside if you can hear them on the video um but now we have df3 over here just to show you what you have three looks like right we have all this information over here but now we can drop consonant and then on also day of week so pretty easy thing that we could do I'm just gonna name this as df4 and then just go over here df3 dot drop and then over here we're gonna throw in our continent and then put a comma over here and then we'll put day of week and then we'll put over here axis equals one boom and then we should have df4 like that and check it out so we no longer have any categorical information we have our city population venue capacity and then we have all this that's now either zeros or ones just depending on what was in this original data frame over here so our information is going to be looking really good now what we should do is Define our X and our y before we end up splitting up our data so let's do that right so we're going to have a capital x and then we're going to say x equals and we're going to do our classic df4 dot drop and in this instance right we're going to drop sold out because that's our Target so we're going to say sold out like this and apologize for the spacing so we just have underscores at the end and we're going to see X's equals one now we have our capital x now we can have our lowercase y uh so Y is going to be pretty simple right y equals df4 and then you're going to put over here in sold outs right and now we have our X and Y so now that we have our X and Y defined we can actually throw in here train test split so if you've been following this series super familiar with it if not no worries and I hope you are enjoying your first machine learning algorithm so we're going to just go over here from SK learn the model selection we're going to import in train test split and you'll be using the syndrome models just this used to scare me and originally when I started learning um but honestly I get used to it pretty fast right so X train X test y train y test Vehicles train test split throw in here x and our y test underscore size equals 0.2 and then we're gonna do a random State equals 10 and I hope someone knows the reason why I put that random state of 10. but that'll be a little Easter egg for this video but regardless it's right or X and Y we Define those above we have our test size so 80 is going to be used to train the model 20 for testing that's in the decimal form 0.2 so just multiply that out and random States so that way you guys can replicate the exact results and if you want this original data over here put a link to it in the description I'll just probably throw it on my GitHub and you can download it and I do need to upload all these coding videos to GitHub as well promise you that's going to be a little bit of future project I've been working on uh updating some of my Twitter stuff right now but once that's backlog is cleared I promise I'll throw this up and GitHub you guys can copy this code all right so now we can bring in our algorithm so from SK learn is phase over here we're going to do import and this time we're going to just do a gaussian and then and B like this okay spelled everything correctly that's always a good thing I do make a lot of typos so gnb equals and then we're just going to call this over here all right and now what we have to do just like every other machine learning model is where you are going to fit our data so you always fit with your train set so your X train great and also your y train I don't know why I copy this but we're having fun with that and boom there we go looks so basic right now without a pipeline or any parameters but don't worry we're going to show you parameters in a second on this side of things but no pipelines but we will parameter so here we go on this side of things pretty easy let's do a y prediction he also so why prediction equals g n and B predicts we'll throw in our X test in here okay so now we have also our y prediction and the reason why we want to do that is we're going to do a classification report now there's a lot of metrics over here that you really could be taking a look at I'll just throw this in here really quick and I do need to make a whole video on like confusion Matrix and also classification report but that'll be in the future right okay and then I just always print these out so prints throw in here like that and then throw in here your y test then also your y prediction great and we can see some of our different scores so our Precision over here right overall our aggregated 0.74 recall zero five six or F1 score 0.63 and yeah not a lot of data right so there's that side of things and then we can also see our score so you can just do gnb that score then you can just put over here both your training tests so we'll do train first right by train so pretty horrible score there like I said I just threw this kind of random numbers all across over here so I didn't expect it to be that great and um test is right about the same so not a very accurate model I would not be applying this yeah this model was not the most accurate with how this ended up working on the Pearl Jam's tour but I also want to show you really quick how you can add in a parameter so we'll say add in parameter like this so This is actually pretty easy so the first thing that we're going to do is a param grid Korean grid and I have a full video on hyper parameter Tunes so I'm not going to go in full depth on how that works but we're going to take a look at VAR smoothing so it helps avoid kind of zero probabilities and also smooth out the estimates so let's take a look at that so just do VAR smoothing and then I looked this up also on stack Overflow kind of just some of the different values that people put in over here and honestly when you do like hyper primitive tuning I would recommend that you take a look at some of the documentation scikit learn and also take a look at stack Overflow because guarantee you someone has the same exact question that you've had before and we actually get some really really small values and I'm just going to copy and paste those because I know if I don't I'm going to make an error so yeah very small like this over here and now we can do a grid search but we do have to import this in so we'll just throw that in over here so from SK learn dot model selection import grid search V like dots and let's uh build on our grid search so we're going to say grid search equals grid search TV first thing we're gonna have to do is put in our model it's already called gnb over here so let's put gnb then we have to call in our param grid which made it super easy you have param grid is right there cross validation we're going to say that's going to be equal to five we're going to take a look at accuracy as our scoring so there's a lot of different ways that you could score your model I'm just going to put accuracy that pretty easy and then at the very end I'm going to say n underscore jobs equals negative one okay that has been called now we have to fit our data so just like above we got to fit right this time we have to fit over here so fits we're gonna X train and then also a y squared train like this now we have our grid search over here right a little bit more this time so we can see how everything is built out you can see we just added a little bit level of complexity and then if you want to see essentially what the best parameters are so pretty easy right you can just go over here grid search the best underscore params like this okay so our best was verse moving and it's in scientific notation and lastly we want to see our best score if you can just copy this over here and this is probably going to be pretty awful and it really is a 0.4857 so just to recap kind of what we did in this video is first we threw in some data over here on a Pearl Jam tour 2. kind of very generalized random data that I throw over here and then we changed all the categorical information on this side of things right and because of that we had two different data frames I concatted them together so we have this one over here as a data frame four then I declare X and our y so that way our Y is our Target X is going to be everything else that's why I dropped it our axis equals one then I split the data between X train and L so X test y train and also y test put this over here then I brought in our model right I declared our model over here our fit our data between X train and also y train I made a prediction on this so y prediction equals gnb dot predict just take a look at a quick classification report just to see how overall the model performed again I don't have a lot of data and this data is not very accurate then we took a look at our score we can just throw in here really fast we're going to add in a parameter build up a parameter grid now this version that I use the gaussian NB only has one parameter which is for smoothing just very very tiny values and I found those values from just searching stack overflow then if you want to see which parameter works the best uh the exact metric for it right grid search CV fit our data just like above and then best prams and then also our best score well I hope you learned something new in this video if you did make sure to subscribe to the channel as it does help share the videos to other users on the platform now if you're looking for another classification algorithm to learn I really recommend you take a look at support Vector machines the full video right over here it's part of my scikit learn machine learning playlist"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Naive Bayes with Scikit-Learn",
        "Video Title":"Naive Bayes using Scikit Learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=2ynTNY7UmmM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/2ynTNY7UmmM\/hqdefault.jpg",
        "ID":"2ynTNY7UmmM",
        "Publish Time":"2018-09-23T22:57:40Z",
        "Channel":"Code Heroku",
        "Channel ID":"UCL-_0RrZ3084Ea8Yavtcd9g",
        "Transcript":"all right so let's get started this is a supervised machine learning part two session and before I tell you something about what we are gonna do today I just wanted to ask you what do you remember from the first workshop what can you recollect right so we got the regression line based on the scores of the students what you were trying to do is predict the score of a student based on the number of hours that they might have studied and to do this what we use is supervised machine learning algorithms we were distributing the entire training set into two parts what we said was we'll take the first part and we keep it as a training set the second part we'll keep it as a test data set and today we're gonna do the same the only thing that we are gonna try out today is to try out a different type of a problem if you remember from our very first workshop we said we could generally while solving machine learning problems we come across two types of problems the first one is the regression problem and then the second one was the classification problem wherein we were trying to do the we were trying to see if a user will like a movie or dislike a movie based on the two features that we saw we we saw on the y-axis we had the I&B be ratings of a movie and on the x-axis we had the movie type and based on these two features we were trying to get whether the user will like a movie or dislike it or whether cancer cell is benign or malignant in these cases we are trying to solve a classification problem and so today we are going to do a classification problem and we can see how supervised machine learning can help us solve these kind of problems besides what I already told you we are going to see a few things from our past workshop just to make sure that everything is fresh in our heads and then we are gonna see of another way to categorize machine learning algorithms just like in science when we study you could have taxonomy of things let's say what are the ways you could describe a human a human could be a mammal you could also say that the humans are bipedal because we walk on two legs so there are different ways of describing the same thing or the same class of algorithms in this case you could say whether the algorithm is supervised or unsupervised another form of categorizing these algorithms we are gonna see it this so that we understand things at the higher level as well and not just the details of implementation and then we are gonna build the naive Bayes classifier which is one of the most simplest forms of classifier that we could build and obviously because it's a classifier we'll be solving classification problems using it this is what we saw in our very first video on introduction to machine learning where we said you could solve your problems in two ways you could have either unsupervised machine learning or supervised machine learning for supervised machine learning algorithms what he said is we are given a set of data points and in this case we saw that to predict the price of a house we were given these features or these data points and based on these three data points we were asked to predict what are the prices gonna be in these two cities in Mumbai and in Pune in those cases what we would want to do is create our model based on these training data points and then once we have that model you could give it new data points and ask what's the price is gonna be in Mumbai or what the price is gonna be impudent we said this is our training data and again I try to keep the terminologies minimal at first but two words like no going of the third lecture today you should be quite familiar with these terms when someone asks you what are the features features are the parameters based on which we are building a model these are our training data points the thing that we are trying to predict over here is the test data and these are the features and the price over here is also called as a target variable this is the target that we are trying to come up with the price in this case is the target variable these are the features these are the training data points and these are the test data points and in the last workshop what we saw was we were trying to do the linear regression model which is a one of the simpler ways to implement supervised machine learning in that case what we said was let's say someone gave us these data points for number of hours a student studied versus the score that they have received in their exam and what he said was let's make a assumption it's almost a naive assumption to make that let's say the number of our studied and the score that the student guess has a linear relationship and we could see it from the graph as well but there are some curves to it if you had to make an exact prediction there should be a little bit of curve to it but over here you were seeing that let's say that for the sake of simplicity let's say that to create a model we use a straight line we said that the number of hours the student studied and the scores has a linear relationship and how do you represent a line in a computer's memory or how are you gonna do it what's the general form of the model that we said is which works for any line if we have to represent this 9 yes so Y is MX plus C where m is the slope of this line and C is the y-intercept we said that you know what I can be present any line in my memory using just these two coefficients the M and the C if you give me this I can plot any line of the chart or on the graph we said okay the only thing that we need to do is just try out different values of M and C the way that the actual model is gonna get built is the your computer is gonna literally try out several different values of M NC till it reaches to a point or another distance between these points and the predictions is minimal y MX plus B and then what we said was let's say if someone gives us a point on x-axis in this case we said that let's say someone gives us a student has studied 10 hours you could easily plot the point on the line and then say that based on what I what we know about the trend it looks like if a student studies 10 al he's gonna score 60 marks but then the person tells you you know what that wasn't correct when a student studied tennis he actually got our prediction was 60 but the student actually got more than 90 marks you were like okay so let me learn from this mistake but the distance between what he predicted and what the actual result was it's gonna be the error in the model if someone gives you 10 data points you just add up all the errors that you got and now you can get a sense of what the average or the mean error in your model is this is what we did in the last workshop we were trying to generate the linear model of the world based on the training data points that we had we said that there are different ways of classifying a same thing you could describe a human as someone who is a mammal or you could also say a human is someone who walks on two legs there are different ways of describing the same object same thing and in this case we are trying to classify our know classify or categorize our machine learning algorithms so one way of saying classifying them was let's say whether the machine learning algorithm is supervised or unsupervised another way of looking at it is whether the machine learning algorithm is something called as discriminative or generative and what do we mean by that let me show you using an example of how a discriminative algorithm will go about solving a machine learning classification problem whereas how a generative algorithm will go about solving the same problem this is the same example that we saw in the first video wherein we said on the x-axis we have the movie time and on the y-axis we have the IMDB ratings of that movie and being on these two features we are saying that let's say we plot all these points on the graph and then we find that all the green points are the one that the user likes those movies and the red points are the ones that the user doesn't like those movies let's see how a discriminative algorithm will go about solving it let's say something like a linear regression model it is gonna try to find a trend in this data in this case it might not succeed properly because there wasn't that much of linear relationship between these points and it might get confused I know it might draw a line like this if we try some other supervised machine learning algorithms which are also discriminative another example of that is a gradient descent algorithm and what its current try gonna do is in the first iteration it might come with a line like this but slowly it's gonna realize that this is not the best way of classifying the data and eventually it's going to come up with a line like this which is best separating these two classes or these two labels this is how a discriminative algorithm will go about trying to find the best classification in this case it will try out the few things and eventually reach out up to a point where it has best created these two classes let's see what happens in the generative case in the generative case let's say we have this the same example again let's not look at the green points first let's just look at these red points so your algorithm is not even going to look at all the green points it is gonna look at only all the examples which have the labels as red because at this point we we are training the model and the the model already knows what the red points are we the last call of the target values we have all the Reds Reds reds or the dislikes dislikes dislikes the model knows which are the red points and which are the green points and what we are telling is let's say for our model we don't even look at green points first what we are gonna do is just look at all the red points and based on all these red points we are gonna say let's create our understanding of this world in this case we said that all these red points they seem to line this area of the graph the model is going to create this cluster or just like you could represent the line you could also represent an oval or a region or a curve right an enclosed area through an equation we are gonna create an equation in our memory and say that you know what all these points they seem to lie in this region of the graph we are gonna create the circle in our memory and you say that all these red points anything that lies over here it's gonna something that we think that the user is gonna dislike then we look at all the green points and say okay based on what we know it seems like all the green points lie in this region of the graph and now what you could do is based on the understanding of the world that you have created even if you take away the training example now if someone gives you this new point on the graph let's say in this case someone gave you this blue point in this region of the graph what do you think it's the most likely result whether the user is going to dislike a movie or whether he's gonna like it just like it right based on the knowledge of the world that we created we know that all the points which lie in this region of the graph the user is going to dislike them this is how generative algorithm will go about solving the same problem it's not gonna look at all the points at once it's gonna look through them one label or one category at a time an example of these two algorithm the first one is the linear regression for discriminative algorithms and for generative algorithms example is a naive Bayes classifier which we are gonna see in our workshop today as well for the points which are gonna lie outside we are gonna have to make the best guess based on the regalton that we are using we could say calculate the distance between that point and the circle and whichever is the closest you could do that or in your model you could say that your accurate our probability of prediction our confidence in the prediction is not good so you are just gonna select the one randomly you could either say either red or green based on how you want to implement it let's see what do we mean when we talk about a naive Bayes classifier the naive Bayes algorithm or a naive Bayes classifier obviously comes from something called as Bayes rule and you might have heard of it not only heard but we have studied it in our 12th class and also most of us have studied in our engineering courses as well so I'm not gonna go into doing those equations and how do we reach to how do we do that the base algorithm but what I'm gonna do is try to give you an intuition on how it's working behind the scenes and based on that maybe no you will have a much better insight into how Bayes rule applies to supervised machine learning rather than me trying to go over some formulas here I have an example and let me say what I am trying to show you over here we have a training example or from a Titanic data set which is a popular data set that we are going to work on today as well what we are seeing over here is we have two classes over here the first one is the survivors and the second one is not surviving on a titanic accident and we are saying that if you travelled from first class your chances of surviving were point six and if you were traveling from first class of chances of not surviving is point four if you were a male your chances of survival was 0.5 but if you were a female your chances of survival were little less so it was only 0.4 and so on for the north survival as well and over here the bigger greens which are we are saying is overall your chances of survival were 50\/50 the prior probabilities in this case which is probability of survival and probability of not survival overall is 0.5 0.5 but this 50-50 chance that you you survive or you don't survive but now just looking at the numbers can you find out on this intuitively guess will a male traveling in first-class survive look at these numbers look at some probabilities these probabilities and try to guess if intuitively that whether a male who's gonna travel in first-class is he gonna survive or not the chances of him surviving are more or chances of him not surviving a more and again don't try to multiply those things and just try to come up with an intuitive answer the chances of them surviving are more and the reason is we can look at this data point over here and say let's see first thing that we know about it is that it is a male over here we went and we said that male is 0.5 but over here the male is 0.5 as well so it is still 50\/50 right now but then when we went in the first class we saw that the chances of person surviving in first class is 0.6 and over here in not surviving this point four definitely this is always going to be greater than this calculation that we did in our head and to formalize this let me show you how the actual likelihoods are gonna come out what you could do is multiply these two things over here the way you would do the math is you multiply 0.6 by 0.5 and you would multiply this by overall probability of surviving in this case is 0.5 you would get this answer as 0.15 and then what you could do is you multiply 0.4 by 0.5 and then you will multiply this overall probability of not surviving in this case was 0.5 and if you multiply all these three you get 0.1 and the cost point 1 Phi is greater than 0.1 our classifier or a machine learning algorithm is gonna say that you know what based on what I know chances of survival of this guy is more than chances of him not surviving and oh here these are not the probabilities make sure that you understand that these are not probabilities but these are the likelihood this is the likelihood of that person surviving versus the likelihood of them not surviving let's do one more example and I let you answered on this one we have another example over here and the naive Bayes algorithm is very popular especially for text-based problems in problems where you have too many features so for example for natural language problems or things like spam detection this is an example of spam detection wherein what we are saying is let's say you get an email and now your email provider has to determine whether this is a spam email or not a spam email right now let's make a very naive assumption that there are only three words in this email and there are three words money lottery and Cory Rocco we are just looking at these three words in the email and we know from our past experience that whenever the word money has come it is 0.6 percent chance that this is most likely a spam if the world lottery has come we know that its point 8 percent chance so there is significantly large chance that this email is a spam and for the word code hero 2 it's most likely that this is not a spam and that's why the probability of it being a spam is very less it this is just 0.01 and similarly for not spam as well based on what you know can you find out if you saw an email which had the word kool hero Combe money in it oh you're the overall probability of spam emails is more this is 0.6 and this is point four in a previous example this was 50\/50 but over here we are saying that overall the chances of any email being a spam is more likely that the spam then not a spam can you look at this and try to either do those multiplications or even you could intuitively guess if you see both of these work or hiroko and money in your email whether it's gonna be a spam or it's gonna label it as not a spam we are gonna do 0.6 times 0.6 times point zero one they are that's correct on the first and on the second one obviously we are gonna do the same thing you're gonna select money from your so here the select point 3 times 0.9 5 times 0.4 and whichever is greater we are gonna say that that is the label that we can assign if you do this math you're gonna get point 0 3 6 over here and on the second one we're gonna get point 1 1 4 and point 1 1 4 is gonna be greater than 0.03 6 what you're gonna say in this case is that it's not a spam does that make sense I guess you get an intention behind what we are trying to do in the naive Bayes algorithm but can you think of a major pitfall or something that we might be ignoring when we are doing the naive Bayes algorithm over here look at this example in the email for the email and think about what are some assumptions that we are making over here that's not an assumption that we are making because we are saying that if there are some spam emails use the word code here okay we are counting that probability over here we know that with 0.01 percent chance the word Kotaku will occur in a spam email so we are taking that into the account and that's the reason of let me show you what are some assumptions that we are making over here the biggest assumption that we are making over here is we don't care where these words occur in the email even if the world money or Lottery occurs in the first place or rather it occurs in the last paragraph or the last line we are gonna count the frequency of these words it doesn't matter whether you have the word code here oku as the first word or the last word we are gonna count it the same way and the other thing that you are gonna see is that the sequence of these words let's say sometimes we are also assuming over your that the word money and lottery are independent of each other these are the two features of the email and we are assuming that whenever the word money comes or whenever the word lottery comes those two are independent events and they are not necessarily dependent on each other but if you think about it even logically there like if you take a sample of let's say 1 million emails it is most likely that you are gonna see there is some correlation between the emails who have the word money in them and the word Lord tree in them both of these variables these features are gonna be present in those emails and in that case what we are gonna do is count the same thing twice we are gonna add those probabilities twice even though we are looking at the same thing or there is some correlation between these two things and we are assuming that the word money and Lord tree are independent of each other we are giving an extra weight over in that case and we are saying that these two features are independent of each other that's why we are gonna count their individual probabilities as well and that's the reason this naive Bayes algorithm is called naive as well that what we do is we are saying that these features that we are going to look they are independent of each other we are making an assumption and also the second assumption that we are making is that the order in which these features occur they don't really matter we are just kind of down their frequencies was I clear on the why the naivebayes algorithm is naive I hope that gives you a sense of how naive Bill's algorithm works behind the scenes and again I didn't want to go through those equations but it is more important that you get a sense of and intuitively you get how it works behind the scenes what are the advantages of using the naive base it is very simple and easy to implement not just because we are gonna use those libraries but even behind the scenes the implementing goes as long as you have those probabilities once you calculate those probabilities it becomes pretty very easy to just it's a match just the matter of multiplying those probabilities together and seeing which class has more likelihood of being there and we just select that class so it is very simple and easy to implement and what that does is because it is so simple and quick to implement it also executes very quickly in cases where you want to do real-time detection or you want to work with a massive data set now what you could do is know write this very simple algorithm and it's still gonna do a fairly good job it might not be best algorithm if you are looking for a great accuracy but if you want to do something really quick if you want to try out what the classification looks like at a high level you can first try it out using naive Bayes and then try out some other classifiers that we are study in a future lectures it's a very good way of trying to get a rough idea of where the classes are and then once you have those sort of that rough idea or you could also do is use in cases where real time aspect of it is very important let's say if you want to execute a model which is gonna complete in few milliseconds or which is gonna run twice or thrice in one second in those cases it is gonna be difficult to run something which is gonna take a lot more time and if you are trying to do a video processing or those kind of stuff you don't want to be in a situation where you are using a machine learning algorithm which takes a lot of time to execute given that I have a quiz over here let me go ahead and shoot the quiz let me go ahead and share the result as well the naive Bayes algorithm is more suitable for cases where the speed of algorithm is critical we just saw that if you are looking for a great accuracy this might not be the best choice but if snoodles very critical to you then you would want to use something such as naive bayes to make sure that you get that speed in your comparisons or well you're doing the testing on your data let's go ahead and do the hands-on part for our today's workshop the only thing that I would want to make sure is you have all these libraries installed the other thing that I want to do is let's download the zip file for today's workshop Marron which has the starter code and the test data that we are going to work on so I'm going to copy and paste this in our chat window go ahead and download it extract it in the folder that where you want to work in and then let me know in the chat that you have extracted it let's see what is there inside our Titanic dot CSV file again this is the same data set that I spoke about before wherein we are trying to look at let me just open it over here as you see this is the same data set that we spoke about a few minutes back and oh you know what we are saying is these are the features that we have and our job is to predict whether the passenger is going to survive or he is not gonna survive and in this case the zero means he did not survive and one means that they survived our job is to predict whether the passenger is gonna survive on Titanic or not based on these features that someone can give us so you could have either sex their age their whether they have any sibling or spouses abroad or whether they have any parents or children abroad and the fare of the ticket that they have paid well year again just one more thing that whenever you see so for males we have a 1 and for females we have a zero that's another thing that I wanted to make sure that you understand besides that everything else should be straightforward this is the name of the passenger and this is the affair I don't know what the units are what the currency is but this is the fare that they have given us and this is a real data set that they have from the Titanic list that they might have that's for the data set and let's open the naive Bayes dot py file which is the starter code that we are going to use for our today's workshop and this is pretty much the same thing that we had in our last workshop as well for the supervised machine learning part one where we were doing the regression line and over here as well we have the same starter kit and the only difference is in this case we are in importing the naive based Gaussian model instead of importing the linear regression model from the sty Kettler knife library let's open this file and let me know once you are ready make sure you have looked at the CSV file and let's work on this one again the first step in over here is to load the data from our CSV file so given that we have already done this in our last workshop why don't you go ahead and load the data from the CSV file similar to what we did in the last one and tell me what is the line of the code that you're going to use over here we are gonna use the pandas library to import our data and then we are gonna use PD dot read underscore CSV let me just go ahead and do that you're gonna say data frame because you're gonna load the data into our data frame so we're gonna say data frame is equal to PD dot read underscore CSV and then we are gonna say Titanic not CSV then let's make sure that we are getting the data set correctly so we are going to say data frame not head what head method does is it prints out the first few lines so we let me shall we gonna say print over here print beta friend not head and this should give us the first few lines of this data set let's make sure that we run this and we are getting this correct I'm just gonna wait a few seconds for this to load all the libraries the first time is it's generally little bit slower but then it's faster we have the data set over here and over here as you see we are printing the first few things and the first few columns over your we have the data set in our program and the next thing that we want to want to do is let's also just try out data frame dot describe this is gonna give us the statistical properties of the data the our library is telling us that there are 87 points in this data frame this data set and all of this 887 data said the mean over your for the survival class is 0.38 so it looks like the average over your is looks like middle around 0.38 and again this is the distribution in the different classes and we don't know exactly how the distribution for it's difficult for us to tell using does these numbers know how these numbers are distributed what we're gonna do next is try to visualize this data that we have similar to what we did last time what we're gonna do is try to get the x axis the y axis in our program and then try to plot it using the math lot library let's try to get a few things from our data set and over here what we are going to say is we have a few classes over there right let me make sure that I open the text file over here again I'm going to open this text file and let's take this first line from your these are all the columns that we have in the data side I just pasted over here as a comment so that we don't have to look at the CSV file again and again and what we are saying over here is these are the different features and to visualize it what we are going to do is to select a few features and based on what features we select we are gonna try to visualize the data because remember it's gonna be difficult for us to visualize these many features in one go so we'll have to select two features at a time so that we can plot a graph out of it and now let me ask you my question is they don't know what you know about Titanic accident what do you think are the most likely features which is going to drive whether a person is going to survive in their accident or not the siblings and spouses abroad it might affect it definitely there is some relation between if they have someone abroad they might be wealthy as well but think about what happened in the Titanic case was the richer people they were able to get out of the ship they could bribe the guards on the ship and they were able to get on the life boats before then the people who were poor and who were in the second class the fare of the ticket is going to have a significant impact on their chances of survival right because if they have paid more it is most likely that they were able to bribe the guards and never be able to get on the lifeboats what we are going to do is let's try to visualize these two features and you're gonna visualize the fare and their age and we're gonna see how did it affect their chances of survival let's extract two columns from this data frame and I'm gonna ask you how are you gonna do that let's go to the step two over your and we want to do the plot data for the plot data we are going to extract two features from the data frame let's say ages is equal to we're going to say data frame off something and then dot values what is the name of the key that I should give over your four ages and what is the name of the key that I should give for phase there is data frame of something dot values for ages we held the edge and for fairs we have the faith okay seems like we have that and to convert this data frame object to numpy array we are gonna use the dot values over here also let's extract the survivals so we are say survived is equal to data frame of survived dot values last time we used the math dot light dot dot plot today we are gonna use something called as a scatter plot instead of the line plot we are gonna use the scatter plot we are gonna say plot dot scatter and this is gonna do the same thing instead the using the O parameter over your for the line style you're gonna use the plot dot scatter so that we can also have some different colors over here let's say edges comma pairs and then let's make the size of this plot or each point as a little bit bigger so that we can visualize it correctly we're gonna say s is fifty so that the size of each dot over there is a little bit bigger so that we can see it properly and then we can say plot not show and then save it and run it and see what do you see and see what do you get we made an error over here we don't need this it's not an method it's just attribute over there so we need to say it not values and then run this and we see a plot like this from this are you able to make out whether the person survived or not or is there something that we are missing yeah we haven't user survived features what do you think what is the best way to show that survived feature on this graph we already have used the two accesses we already have something on the y-axis over here which is the fares and on the x-axis we already have the edge which is between 0 to 80 over your you already have those two access is over there what might be a good way to get that intuitively to have that third feature over there what we could do is instead of adding another axis or something like that we could point the colors on these points so that if it plays a red we can say that the person has not survived and if it's a green we can say that the person has survived so that might give us a good way to get the insights into the data what we are gonna do is go over here and let's add some colors instead of this why would being like this you can also say colors you can create an array of colors bailed on the survivals and let me show you how you could do that in one line or like let me just do it as a symbol for loop so I'm gonna say colors as an empty list over here and then I'm gonna say for every item in survived we want to say if item is greater than zero that means that that point is 1 if item is greater than 0 or even let's just say if item is equal to equal to 0 which means that they have not survived so I'm gonna say colors not append red else colors not append green if it is a 1 then we are gonna append our green over here and if it is a 0 we are going to obtain the red over here save it and instead of this plot dot scatter we are going to say colors is equal to color is equal to colors let's see if this works and we made a mistake it's a type over there and we get a good sense of what is happening now by looking at the graph over here can you tell me what is the area or where is the region in the graph where you think that the it is most likely that you will survive yep as the fair is nearing zero over your most of these people have not survived and as the fare is increasing as you see there are more green points and these two points where they have paid a lot of money they have definitely survived and for the H part of it seems to be pretty uniform but oh you're nearing the zeros it looks like the kids were able to survive more then towards the end over your evenin in the second class or third class the kids were able to survive more than the older people over here that's the insight that we got from the data and based on what we can see intuitively and what we as humans understand we also want our computer to understand that let's see if we can make give that same thinking capability of just what we did visually with our brains can we teach this to our computer as well can we teach this to our machine learning algorithm as well let's go ahead and do that let's close this I'm gonna comment out these two plots so that we don't get confused I'm gonna comment these both plots we were able to now get a good sense of what is happening in the data and now once we know that let's go ahead and prepare that training and test data sets as well let's go ahead and go over here and say building a model part what we're gonna say is we are gonna create two sets over here the first one will be our features and the second one will be our target values based on what we know from a supervised machine learning introduction class we have two types of columns the first type of column we have is a feature column or you could have a target value that we are trying to predict let's do our features over here so we're gonna say features is equal to you are gonna say for the features we want all the columns in our training data set except for our target value what is our target value over here what is the target column what are we trying to predict or what is the class that we are trying to come up with survive right so that's the target column or the target we are trying to come up with for the features we don't want that target we can say dataframe dot drop do you want to remove that and this is how you do it you want to remove survival and then you're gonna say axis as one what this does is it's just saying it as remove it from the first axis over there and then we're gonna say values just like we did before and then we're gonna say target is equal to data frame of survived we already had it before over here as well the survived that same thing but let's just name that as targets so that we know what we are referring to and then what you want to do is let's divide this entire data set into two parts the training data set and the test data set just like what we did before and over here we know that there are about 887 data points and if you do the math and you take 80% of that you will get a number around seven 110 what you want to do is let's take the first 710 points as our training data I'm gonna say features train comma and I said target train is equal to features of 0 to 7 1 0 comma target's of 0 to 7 1 0 and when you're ready we're gonna say features of test comma targets of test is equal to features of 7 1 0 till the end and targets of 7 1 0 till the end let's just print these out so that we know we are getting everything correctly I'm just gonna put into one of those things so I'm gonna say feature start praying let's see what do we get over there I'm gonna run this let me make sure that I get the capitalization is correct over here so I'm gonna say train and let me make sure that I don't use the values over here right now let's think movie values for now and the first thing that we see is we are getting until zero to 709 for the features test we should get from 7 1 0 to 886 you're getting all our data points in there let's add the values over here again so that we know that we would need the numpy array instead of the data frame object over there what we saw was the indexing was not working correctly I'm not sure if that was the case with the CSV file whether we were missing we had an extra line character over there there was something off over there definitely and for this one when I was trying to inspect over here we were getting the even if we do the normal indexing just like we did like we do in our Python array or Python list it was working fine I'm definitely not sure if now if you could figure out and tell me what was going wrong in the last session now that will help because I looked at the CSV file and I think it might had an extra line character in there because of which we were missing it out all right we were able to separate out both our training data and test it up the next thing that we are going to do is build the Gaussian model and try to fit the training data inside to our naive Bayes Gaussian model I already have that example in the comments in the top over here why don't you try that out and tell me what are the two lines of the code that you're going to use to generate the model and then try to fit this training set that we have inside our model what are the two lines of the code that we need to write the first one that you need to use though is first you need to build the model as well you're gonna say model is Gaussian naive Bayes and once we have this model what we could do is fit it so we can say fit the data inside the model we're going to say model not fit feature strain and see how easy it was because if we name our features correctly and the training and test data sets correctly it becomes very intuitive to when we are using it in our program as well when we are saying model dot fit we are training the model so it is obvious that we want to give the features and the target values and we are gonna give the both the training points we are going to say target trained it becomes very simple for us when we are trying to do those operations in feature if we name these things correctly over here and once we have this thing over here let's see how can we predict it what we could do is say model let's print this thing print model dot predict what is the list that I should give for my predictions we need to give the tray features which are in our test data points we're going to say tested based on the features in test data points save it and then let's try to print this out and see what do we get we are seeing that our model is complaining that it could not convert the name of this guy to a string what do you think we should do now first of all do you think that the name of the person makes any sense for us in trying to predict whether he survived or not is it important to preserve this feature it doesn't make sense to keep this what we are gonna do is go on the top over here just like we drop the survived features in the feature set what we're gonna do is right at the point where we are reading this thing we are going to remove from the data frame these columns which we don't want we are going to go over here and instead of having this over here just right after the read CSV we are going to say data frame is equal to data frame not drop we don't want these things and what do we don't want we don't want this column so the name of the column is name we don't want this feature over there so we are gonna go back to the top and remove this name from our data frame completely so that it doesn't affect our model save it and then run it again and it is complaining I can't find the word name over there we want to make sure that we also give axis a zero just like we did on the first one so you're gonna say remove it from the first axis over there and now we have something we are getting the predictions over your for the survivals that we we are trying to see but it is not that helpful right now we just see zeros and one let me ask you what will you print out so that you can compare if the model is performing accurately or not right now you are just printing out the predicted values what can we print out along with these predicted values so side by side so that we can see both of them and see if our model is working correctly when of course you could do is print those mean square errors but even to generate those mean square errors you will need to compare this predicted values with something what do you compare the predicted values with the test points right the target tests what we are going to do is let's print the target test and the predicted values side-by-side so that we can see if it is performing correctly what we are gonna do is come over here and let me write this thing over here even before we do the error estimation we just gonna print out both of these things side by side so instead of just saying bring this thing out we are gonna say these are the predicted values predicted values is equal to this thing and then over here we are gonna say for X n zip we want to show two things side by side and that's why we are using zip over here let's say though actual is the test target so we are going to say target test comma addicted values you're gonna print both of these side-by-side and then we're gonna say print actual was X of zero if you don't want to use X and you maybe we could use an item over here item of zero and the predicted was item off one what we are saying over here is print both of these values side by side and to print both of them side by side we use the zip over here which will combine both of these in one tuple and we're gonna print both of these things side by side so that we can see if we are doing if our model proof is performing good save it and then run it again as you see over here now we could see both of these things side by side actually was zero predicted was zero well you're it seems that we are doing good one was one and in some cases we are making an error for example in this case the actual was zero but we predicted one it means that the actual was like the person did not survive but we predicted that the person survived it still gives us a sense that we are doing good because lot of these cases you are saying that majority of these cases we are performing quite good over there let's just get a number out of it instead of just trying to visually do it let's get the error rate by comparing these two we know that we can get the error rate and you could do it in a for loop as well but what we are going to use is there is a method that scikit-learn models provide us to evaluate our models directly what you could write is instead of trying to compare each you could compare both of these elements and then count how many times you did not perform correctly or how many times you were off and then you can get an average or a simpler way is to print model dot score we are trying to get the score of this model and we are gonna use these two things the same things that we had before in our target over there let me just make sure that I get these two things directly and I'm gonna say more a lot score target tests let me just look at the documentation as well and make sure that I'm getting this correctly I'm gonna say sky kid learn model score we want to quantify our model and we are gonna use the model at score method over here well your you would see is they are using the core method so I'm trying to find an example where I know they have used the score method so that we want to make sure that the parameters that we are giving are the right matrices over there or the right lists over there they have an example on this one so let's see well your what they are saying is if we need to score it we are gonna give it the X and the y which are the test data points what we want to do is know give the test data points over here let's try to give a taste test data points both of our test points we are gonna give it over here what were our two test points we had the features test and the target tests what it is gonna do is in behind-the-scenes it is gonna run the predictions and get these features test and the predicted values so we don't have to give the predicted values over here what we need to give it is both are the test data features and the test targets and it in behind the scenes it is gonna run the predictions and then try to evaluate if a correct or not let's go ahead and run this and let's see what we get and as you see we get an accuracy of about 81 point nine percent which is not bad at all for a model that we could quickly build let me just also make sure we print this out accuracy is we printed this thing out over here and we saw that the accuracy of this model that we have a zero point eight were nine or eighty one point nine percent eighty one point nine percent of time we were able to predict if the portion is surviving or he is not gonna survive days on the features that we gave do you have any questions on why did we do something and because in the last part I had to look at the documentation over there just to make sure that we were giving the right lists over there but was that clear on why did we give the test features and the test targets over here because it is gonna call the predictions behind the scenes the moral of score is gonna call that and we just have to give the creatures tests and the target tests so let me know if you have any questions on this one yes the modalert score is doing exactly what you said it is gonna create the predictions out of the feature test and based on the predictions that the model is going to make it is gonna then compare it with target test just like me were printing it out over here and then if there was an error it is gonna increment the count in its memory we could do it ourselves as well in this for loop but it was it is easier way to just do it using more a lot score in a future programs as well that's it for our today's workshop hopefully you got something out of it and you want to make sure that everyone in our workshops walk away with the sense of being able to create something and I hope I was able to deliver that our main objective of year is to try to reach as much of people as we can if you know some of your friends who might be interested as well get them along as well next time and if you haven't already reviewed us make sure you go to our Facebook page and review us and so that we can get more audience in our workshops also how we are doing in our workshops thank you bye"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Naive Bayes with Scikit-Learn",
        "Video Title":"Naive Bayes Python Implementation | Naive Bayes | Scikit Learn | Practical Example | Classification",
        "URL":"https:\/\/www.youtube.com\/watch?v=609alr7_B9w",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/609alr7_B9w\/hqdefault.jpg",
        "ID":"609alr7_B9w",
        "Publish Time":"2020-04-01T15:23:09Z",
        "Channel":"TechCore Easy",
        "Channel ID":"UCLQKs7hgJ97jA27W78OXeNA",
        "Transcript":""
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Naive Bayes with Scikit-Learn",
        "Video Title":"Machine Learning Tutorial Python - 14: Naive Bayes Classifier Algorithm Part 1",
        "URL":"https:\/\/www.youtube.com\/watch?v=PPeaRc-r1OI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/PPeaRc-r1OI\/hqdefault.jpg",
        "ID":"PPeaRc-r1OI",
        "Publish Time":"2019-11-16T23:57:51Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"in this two-part tutorial we are going to learn about naive Bayes in the first part will cover some theory and then predict Titanic's survival rate using naive Bayes and the second one will build email spam detector and here is the list of topics along with the time line so if you want to skip to a specific topic you can do that let's start with some basic probability theory you all know that when you flip a coin the probability of getting head or tail is 1 by 2 because there are two possible outcomes and the chance of getting head or tail is 50% similarly when you pick a random card the probability of that being a queen is how much just think about it it's very simple there are total 4 queens 52 cards hence the probability of getting Queen is 4 by 52 which is 1 by 13 if you know that the card that you have picked is a diamond then what's the probability of getting a queen well this is called a conditional probability where you know that even a is occurred and then now you are trying to predict the probability of B so total diamonds are 13 Queen is 1 we all know by just simple intuition that probably here is 1 by 13 but this is how the conditional probability is represented where you say P of Queens less diamond where you know that event diamond has already occurred and the probability of getting a queen is something you're calculating so that's called a conditional priority and the way that is represented is P of a slash B where event B has occurred already which is that you know that the card is diamond and you're trying to find the probability of event a which is whether the car is Queen or not Thomas Bayes gave this famous equation of finding a conditional probability where you can find probability of a given the event B has occurred by knowing some parameters which is the individual probabilities of a and B and knowing the probability of B given that a has curr so let's look at it in the context of our queen and diamond problem so here this is the same equation I have represented for our specific use case and you know these individual probabilities and if you put them into our equation you can easily find this conditional probability now this is a very powerful theorem where you know probability of certain events but you don't know the probability of some other events and using those certain events you can find other probabilities you all know about Titanic craps there was a movie that was made and it was a super hit but that event actually occurred and it was said that so many people died we have a data set of this Titanic crash well there are name of the people along with certain features which is your fare ticket cabinet cetera based on that we are trying to find out the survival rate and here we can use Bayes theorem where we are trying to find the probability of survival based on the features such as if the person was male the class age cabin fare and so on the reason is called naive Bayes is because we make a naive assumption that the features which is male class age cabins and so on are independent of each other in reality some of these features might be dependent such as the fair is fair and cabin are kind of related but we assume for simplicity purpose that these are not related hence it is called naive Bayes and it is a simple assumption which reduces our calculation and makes the algorithm simple yet effective if you want to go in math a little bit you can watch this video this video is by luis serrano and he has really explained it well i have a link of this video in the video description below so go watch it it's gonna be very useful if you want to know the details now you base is used in email spam detection Henry character recognization the weather prediction face detection and news article categorization now let's go straight into the coding from the Cagle website I have downloaded the Titanic data set and it's available locally here in form of CSV you can see all the passenger names their features and whether they survived or not I have downloaded this files locally I will load it the same CSV file into a panda's data frame in my Jupiter notebook now we are going to do some data exploration first we can see that some of the features are probably not relevant so I'm just going to assume that they don't have an impact on my target variable for example name write like name doesn't matter like what was the name it doesn't have an impact on the survival rate and so I'm gonna drop those variables and make my data frame little simpler so I dropped all these variables and now I have this data frame now one thing I noticed was there is this target variable which I want to separate out into a different series so I will say target is equal to DF dot survived and inputs which is my dependent variables for that variable I will drop survived column and just put a rest of it so this way I have my independent variables and dependent variables into two separate entities we can see that the sex column is our text and we want to convert it into a dummy columns because we all know that the machine learning models can't handle text so we have to convert them into numbers and I have one hot encoding or the dummy's tutorial and there I've explained why it's needed but here that's what I'm going to do so the dummies will basically convert the sex column into two different columns and it will just put 0 and 1 values we are going to append these dummies columns our inputs data frame and the way you do that is by using the pandas concat function where you can get these to our data frames on columns and then you check the data frame which came as a result of Concord operation you will find that now I have these two columns okay now I need to also drop the original sex column because now I have dummy column so I don't need that particular text column so now my data frame looks like this it is much simpler with all the numbers there is no text I also want to find out whether there are any any numbers in any of the columns and the way you do that is by using this when you run it it tells me that each column has some na values and I'm just curious to find out what those values are so I'm just gonna print like the first ten values and I find that some of these values are any N now I had tutorial on how to handle and n values in pandas so you can refer to that but the popular approach here is you can take a mean of the entire column and then feel those na values with the mean value alright and the way you do that is by doing this you can say inputs dot age dot fill any with the mean and when you do that let's print first five the fifth one had I guess the any value and you will see that it filled it with the mean value I think fifth or the sixth ok the sixth value had it so you see that this was the mean value I know 0.62 nine probably doesn't make much sense so you can just average it out you can just make it like a whole number but yeah now we are going to use a scale on strain test split math to split our data set into training and taste sample this is what we usually do before training our model so that the model is not biased when we are trying to calculate the score and this is how you train the split this is something we have done in all the tutorials so this is pretty simple there is no rocket science to it I'm going to divide my taste in training sample into twenty eighty percent ratio all right so when I do control enter it's gonna divide it into training taste samples I can just quickly check the length of X train and x-3x taste to see the distribution so you can see that and the total is length of input so the total was 891 and eighty percent of that is seven one to twenty percent is 179 you can also print these in usually if you are interested for example if you want to check what is your extreme you can print like that all right now it's the time to create the naive Bayes model now there are a couple of sniper's classes we are going to use Gaussian naive Bayes the question is is something that you use when the your data distribution in not is normal or it is like a Gaussian distribution this is a concept in statistics you can learn about it it is also called a bell curve okay so we can use that model here okay so I have created that model and now it is time to play in the model you all know that you have to call fit method whenever you want to train the model it and you train it on X train and Y train a next rain and white rain is something that we caught here so when you execute this it will train it now the training here is very fast because our samples are very less when you have so many samples it might take time and you have to use parallelization GPUs and so on the first thing I do after my model is train is always measure the score to find accuracy and I found that it was 77% I know it's not very high but it actually depends on the the samples as well so if I run this again the school is going to vary so you can see it is now 78% now let's look at our ex taste samples so ex tastes I will check the first ten samples and I will see this is what it is I will also check my whitest sample to see what are their values so white taste is this oh this is probably not a good taste that okay let me just run it once more so when I do control enter it again creates some random samples and the score is little better this time you can see there are a couple of ones here and you can now use model dot product for our fifteen ten samples and you can now compare that to why taste so you can see that it was 0 1 here is 0 1 0 0 0 0 0 this sample it didn't get right you see it says it's 0 so our score is like 81% so it is expected it will make mistake some time and that's fine you can also use predict probability function to figure out what are the probabilities of each class like in our case each class is whether a person survived or not survived when you run this this sample says that 97% probability is the person didn't survive and this was probably probably that actually this much probability that person survived and this much probably that he didn't survive right that's why it makes a decision that it is zero in the second one the person survived it is so that's why this probability is like 93 percent that's all I had for part 1 in part 2 we will cover email spam detection using nabis and we'll also have exercise at the end of that part 2 tutorial thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Support Vector Machines (SVM)",
        "Video Title":"Support Vector Machine (SVM) in 2 minutes",
        "URL":"https:\/\/www.youtube.com\/watch?v=_YPScrckx28",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/_YPScrckx28\/hqdefault.jpg",
        "ID":"_YPScrckx28",
        "Publish Time":"2021-09-09T12:30:07Z",
        "Channel":"Visually Explained",
        "Channel ID":"UCoTo2gtN527CXhe7jbP6hUg",
        "Transcript":"in machine learning one of the most fundamental tasks is when you have a bunch of objects that you want to classify into two categories or more is this picture of a dog or cat is this stock going up or down svms or support vector machines are some of the simplest and arguably the most elegant methods for classification each object you want to classify is represented as a point in an n-dimensional space and the coordinates of this point are usually called features svms perform the classification test by drawing a hyperplane that is a line in 2d or a plane in 3d in such a way that all points of one category are on one side of the hyperplane and all points of the other category are on the other side and while there could be multiple such hyperplanes svm tries to find the one that best separates the two categories in the sense that it maximizes the distance to points in either category this distance is called the margin and the points that fall exactly on the margin are called the supporting vectors to find this hyperplane in the first place svm requires a training set or a set of points that are already labeled with the correct category this is why svm is said to be a supervised learning algorithm in the background svm solves a convex optimization problem that maximizes this margin and where the constraints say that points of each category should be on the correct side of the hyperplane in practice you don't have to worry about the implementation details of this optimization problem using svm can be as simple as loading a python library preparing your training data feeding it to the fit function and calling predict to assign the correct category to a new object the biggest pros of svms is that they are easy to understand implement use and interpret furthermore they are effective when the size of the training data is small the simplicity of svms can also be a problem in many applications the points cannot be separated by hyperplane a common workaround in this case is 2a augmented data with some nonlinear features that are computed from the existing ones b find the separating hyperplane in this higher dimensional space and c project back to the original space a clever technique known as the kernel track allows us to perform all of these steps in a very efficient manner and now that you know about svms you can use them for face detection spam fill train and text recognition this was svm in about 2 minutes like and subscribe if you liked the video and see you next time"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Support Vector Machines (SVM)",
        "Video Title":"Support Vector Machines Part 1 (of 3): Main Ideas!!!",
        "URL":"https:\/\/www.youtube.com\/watch?v=efR1C6CvhmE",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/efR1C6CvhmE\/hqdefault.jpg",
        "ID":"efR1C6CvhmE",
        "Publish Time":"2019-09-30T18:45:50Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"support vector machines have a lot of terminology associated with them brace yourself hello I'm Josh stormer and welcome to stat quest today we're going to talk about support vector machines and they're gonna be clearly explained note this stack quest assumes that you are already familiar with the trade-off that plagues all of machine learning the bias-variance tradeoff you should also be familiar with cross-validation if not check out the quests the links are in the description below let's start by imagining we measured the mass of a bunch of mice the red dots represent mice that are not obese and the green dots represent mice that are obese based on these observations we can pick a threshold and when we get a new observation that has less mass than the threshold we can classify it as not obese and when we get a new observation with more mass than the threshold we can classify it as obese however what if we get a new observation here because this observation has more mass than the threshold we classify it as obese but that doesn't make sense because it is much closer to the observations that are not obese so this threshold is pretty lame can we do better yes going back to the original training data set we can focus on the observations on the edges of each cluster and use the midpoint between them as the threshold now when a new observation falls on the left side of the threshold it will be closer to the observations that are not obese than it is to the obese observations so it makes sense to classify this new observation as not obese BAM no it's a terminology alert the shortest distance between the observations and the threshold is called the margin since we put the threshold halfway between these two observations the distances between the observations and the threshold are the same and both reflect the margin when the threshold is halfway between the two observations the margin is as large as it can be for example if we move the threshold to the left a little bit then the distance between the threshold and the observation that is not obese would be smaller and thus the margin would be smaller than it was before and if we move the threshold to the right a little bit then the distance between the obese observation and the threshold would get smaller and again the margin would be smaller when we use the threshold that gives us the largest margin to make classifications heads up terminology alert we are using a maximal margin classifier BAM no no BAM maximal margin classifiers seem pretty cool but what if our training data looked like this and we had an outlier observation that was classified as not obese but was much closer to the obese observations in this case the maximum margin classifier would be super close to the obese observations and really far from the majority of the observations that are not obese now if we got this new observation we would classify it as not obese even though most of the not obese observations are much further away than the obese observations so maximum margin classifiers are super sensitive to outliers in the training data and that makes them pretty lame can we do better yes to make a threshold that is not so sensitive to outliers we must allow misclassifications for example if we put the threshold halfway between these two observations then we will miss classify this observation however now when we get a new observation here we will classify it as obese make sense because it is closer to most of the obese observations choosing a threshold that allows misclassifications is an example of the bias-variance tradeoff that plagues all of machine learning in other words before we allowed misclassifications we picked a threshold that was very sensitive to the training data it had low bias and it performed poorly when we got new data it had high variance contrast when we picked a threshold that was less sensitive to the training data and allowed misclassifications so it had higher bias it performed better when we got new data so it had low variance small BAM oh no it's another terminology alert when we allow misclassifications the distance between the observations and the threshold is called a soft margin so the question is how do we know that this soft margin is better than this soft margin the answer is simple we use cross-validation to determine how many misclassifications and observations to allow inside of the soft margin to get the best classification for example if cross-validation determined that this was the best soft margin then we would allow one miss classification and two observations that are correctly classified to be within the soft margin BAM when we use a soft margin to determine the location of a threshold brace yourself we have another terminology alert then we are using a soft margin classifier aka a support vector classifier to classify observations the names support vector classifier comes from the fact that the observations on the edge and within the soft margin are called support factors super-small bam note if each observation had a mass measurement and a height measurement then the data would be two-dimensional when the data are two-dimensional a support vector classifier is a line and in this case the soft margin is measured from these two points the blue parallel lines give us a sense of where all of the other points are in relation to the soft margin these observations are outside of the soft margin and this observation is inside the soft margin and misclassified just like before we used cross-validation to determine that allowing this miss classification results in better classification in the long run BAM now if each observation has a mass a height and an age then the data would be three-dimensional note the axis that age is on is supposed to represent depth and these circles are larger in order to appear closer and thus younger and these circles are smaller in order to look further away and thus older when the data are three-dimensional the support vector classifier forms a plane instead of a line and we classify new observations by determining which side of the plane they are on for example if this were a new observation we would classify it as not obese since it is above the support vector classifier note if we measured mass height age and blood pressure then the data would be in four dimensions and I don't know how to draw four dimensional graph wah-wah but we know that when the data are one-dimensional the support vector classifier is a single point on a one-dimensional number line just in mathematical jargon a point is a flat affin zero dimensional subspace and when the data are in two dimensions the support vector classifier is a one-dimensional line in a two dimensional space in mathematical jargon a line is a flat affin one-dimensional subspace and when the data are three-dimensional the support vector classifier is a two-dimensional plane in a three-dimensional space in mathematical jargon a plane is a flat affin two dimensional subspace and when the data are in four or more dimensions the support vector classifier is a hyperplane in mathematical jargon a hyperplane is a flat Athene subspace note technically speaking all flat Athene subspaces are called hyperplanes so technically speaking this one-dimensional line is a hyperplane but we generally only use the term when we can't draw it on paper small BAM because this is just more terminology ugh support vector classifier seemed pretty cool because they can handle outliers and because they can allow misclassifications they can handle overlapping classifications but what if this was our training data and we had tons of overlap in this new example with tons of overlap we are now looking at drug dosages and the red dots represent patients that were not cured and the green dots represent patients that were cured in other words the drug doesn't work if the dosage is too small or too large it only works when the dosage is just right now no matter where we put the classifier we will make a lot of misclassifications so support vector classifier czar only semi-cool since they don't perform well with this type of data can we do better than maximal margin classifiers and support vector classifier x' yes since maximal margin classifiers and support vector classifier x' can't handle this data it's high time we talked about support vector machines so let's start by getting an intuitive sense of the main ideas behind support vector machines we start by adding a y-axis so we can draw a graph the x axis coordinates in this graph will be the dosages that we have already observed and the y axis coordinates will be the square of the dosages so for this observation with dosage equals 0.5 on the x axis the y axis value equals dosage squared which equals 0.5 squared which equals 0.25 now we use dosage squared for this y-axis coordinate and then we use dosage squared for the y-axis coordinates for the remaining observations since each observation has X and y-axis coordinates the data are now two-dimensional and now that the data are two-dimensional we can draw a support vector classifier that separates the people who were cured from the people who were not cured and the support vector classifier can be used to classify new observations for example if a new observation had this dosage then we could calculate the y-axis coordinate by squaring the dosage and classify the observation as not cured because it ended up on this side of the support vector classifier on the other hand if we got a new observation with this dosage then we would square the dosage and get a y-axis coordinate and classify this observation as cured because it falls on the other side of the support vector classifier BAM the main ideas behind support vector machines are one start with data in a relatively low dimension in this example the data started in one dimension to move the data into a higher dimension in this example we move the data from one dimension to two dimensions three find a support vector classifier that separates the higher dimensional data into two groups that's all there is to it double BAM going back to the original one dimensional data you may be wondering why we decided to create y-axis coordinates with dosage squared why not dosage cubed or pi divided by four times the square root of dosage in other words how do we decide how to transform the data in order to make the mathematics possible support vector machines use something called kernel functions to systematically find support vector classifier in higher dimensions so let me show you how a kernel function systematically finds support vector classifier in higher dimensions for this example I use the polynomial kernel which has a parameter D which stands for the degree of the polynomial when D equals 1 the polynomial kernel computes the relationships between each pair of observations in one dimension chips are used to find a support vector classifier when D equals two we get a second dimension based on dosages squared and the polynomial kernel computes the two-dimensional relationships between each pair of observations and those relationships are used to find a support vector classifier and when we set D equal three then we would get a third dimension based on dosages cubed and the polynomial kernel computes the three-dimensional relationships between each pair of observations and those relationships are used to find a support vector classifier and when D equals four or more then we get even more dimensions to find a support vector classifier in summary the polynomial kernel systematically increases dimensions by setting D the degree of the polynomial and the relationships between each pair of observations are used to find a support vector classifier last but not least we can find a good value for D with cross validation double bam another very commonly used kernel is the radial kernel also known as the radial basis function kernel unfortunately the radio Colonel finds support vector classifier x' in infinite dimensions so I can't give you an example of what it does exactly however when using it on a new observation like this the radio kernel behaves like a weighted nearest neighbor model in other words the closest observations aka the nearest neighbors have a lot of influence on how we classify the new observation and observations that are further away have relatively little influence on the classification so since these observations are the closest to the new observation the radio kernel uses their classification for the new observation BAM now for the sake of completeness let me mention one last detail about kernels although the examples I've given show the data being transformed from a relatively low dimension to a relatively high dimension kernel functions only calculate the relationships between every pair of points as if they are in the higher dimensions they don't actually do the transformation this trick calculating the high-dimensional relationships without actually transforming the data to the higher dimension is called the kernel trick the kernel trick reduces the amount of computation required for support vector machines by avoiding the math that transforms the data from low to high dimensions and it makes calculating the relationships in the infinite dimensions used by the radial kernel possible however regardless of how the relationships are calculated the concepts are the same when we have two categories but no obvious linear classifier that separates them in a nice way support vector machines work by moving the data into a relatively high dimensional space and finding a relatively high dimensional support vector classifier that can effectively classify the observations triple bam hooray we've made it to the end of another exciting stat quest if you liked this stack quest and want to see more please subscribe and if you want to supports that quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or hoodie or just donate the links are in the description below alright until next time quest on"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Support Vector Machines (SVM)",
        "Video Title":"Support Vector Machine (SVM) in 7 minutes - Fun Machine Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=Y6RRHw9uN9o",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Y6RRHw9uN9o\/hqdefault.jpg",
        "ID":"Y6RRHw9uN9o",
        "Publish Time":"2017-08-15T19:30:29Z",
        "Channel":"Augmented AI",
        "Channel ID":"UCFJPdVHPZOYhSyxmX_C_Pew",
        "Transcript":"hey guys and welcome to another fun and easy machine learning video on support vector machines so the other day I was walking through the park where I saw a lot of people with their pets dogs as well as cats and then I came across this strange creature just really challenging for me to tell whether it was a dog or get but I eventually figured it out that it was a cat groom like a dog now if it was challenging for me to figure out imagine how difficult and challenging it would be for a computer to precisely classify between a dog and a cat a really great algorithm for these types of applications is the support vector machine algorithm or SVM it looks at the extremes of the data sets and draws a decision boundary also known as a hyperplane near the extreme points in the data set so essentially the support vector machine algorithm is a frontier which best segregates the two classes so how does it work to understand SVM Zapatera let's first take a look at why they called support vector machines so say we got some sample data over here of features that classify whether an observed picture is a dog or cat so we can for example look at the snout length or the ear geometry if we assume that dogs generally have longer snouts and gets have much more pointy ear shapes so how would we decide where to draw our decision boundary well we can draw it over here or here or like this any of these would be fine but what would be the best if you do not have the optimal decision boundary we could incorrectly classify a dog with a cat so if we draw an arbitrary separation line and we use intuition to draw it somewhere between this data point for the dhoklas and this point for the cat glass these points are also known as support vectors which are defined as data points that the margin pushes up against all points that are close to the opposing glass so the algorithm basically implies that only support vectors are important whereas training examples are ignore an example of this is so that if we have in our case of a dog that loser cat or get that is chrome like a dog we want our classifier to look at the extremes and set our margins based on these support vectors so we have D plus which is the shortest distance to the closest positive point and D minus which is the shortest distance to the closest negative point and then we have the margin of a separating hyperplane which is the positive plus the negative the line or decision boundary that segregates the two classes is commonly referred to as a hyperplane because s VMs can be used in multi dimensional data sets and the data points are referred to as vectors as they have coordinates within the space of data so what we discussed so far is also known as linear support vector machines or L SVM because the class are linearly separable but what happens if we have a dataset that is not linear separable so say we are presented with data that looks like this where it looks almost impossible to use a single line to separate the two classes we can use a function to transform our data into high dimensional space so you can see over here we go from one dimensional to two dimensional space we can apply a simple polynomial function to get a parabola and now you can easily see how we can draw our hyperplane we can do the same for this data set where it's easy to draw the hyperplane or line but for a machine will use a function to transform our data from two-dimensional to three dimensional feature space now the only problem with transformation into higher dimensional feature space is that it's computationally expensive we can use a kernel trick to reduce the computational costs a function that takes as inputs vectors in the original space and returns the dot product of the vectors in the feature space is called a kernel function also referred to as kernel trick using a kernel function we can apply the dot product between two vectors so that every point is mapped into a high dimensional space via some transformation so essentially we use it to transform a non linear space into a linear space if you look at some popular kernel types here are some popular kernel types that you can use transform our data into high dimensional feature space there are polynomial kernel radial basis function RPF or RBF kernel sigmoid kernel amongst others unfortunately choosing the correct kernel is a non-trivial task and may depend on specific task at hand no matter which kernel you choose you need to tune the kernel parameters to get good performance from a classifier a popular parameter tuning technique includes k-fold cross-validation you'll deal some of these parameters in our Python labs so the advantages of support vector machines are that they are effective in high dimensional spaces they are so effective in cases where a number of dimensions is greater than the number of samples they use a subset of training points in the decision function or support vectors so it's also memory efficient suppose factors are first down so different kernels can be specified for the decision function common kernels are provided but it's also possible to specify custom kernels we can add kernel functions together to achieve even more complex hyperplanes the disadvantages however of sports vector machines include if the number of features is greater than the number of samples the method is likely to give poor performance support vector machines do not directly provide probability estimates these are calculated using an expensive five fold cross validation if you take a look at the applications of support vector machines the support vector machine algorithm has numerous applications and can be a quite popular alternative to artificial neural networks or a ends here are some applications from published journal papers so we can use support vector machines in medical imaging there's one application for SVM based regression models to study the Equality in urban areas in the city of Oviedo in Spain support vector machines is also used for image interpolation as well as medical classification tasks in the financial industry support vectors are used for time series predictions as well as financial and there's one paper on the application of neural networks mixed with support vector machines in coding theory and practice there's also for pattern recognition for machine volts diagnosis which also uses support vector machines as well as page ranking algorithm and Dex an object correct mission okay so that is it for me please don't forget to Like subscribe and share look at a lifeline if you'd like to see more machine learning tutorials and please don't forget to support us on patreon if you'd like to download the script to this video please click the link down below and download for free so stay tuned to the next lecture where we will implement a support vector machine in Python thank you for watching and see you in the next election [Music] you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Support Vector Machines (SVM)",
        "Video Title":"Support Vector Machines: All you need to know!",
        "URL":"https:\/\/www.youtube.com\/watch?v=ny1iZ5A8ilA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ny1iZ5A8ilA\/hqdefault.jpg",
        "ID":"ny1iZ5A8ilA",
        "Publish Time":"2020-06-23T05:44:10Z",
        "Channel":"Intuitive Machine Learning",
        "Channel ID":"UCiuhuf2Xq0d05_4sHG0xmQA",
        "Transcript":"support vector machine is one of the past and nonlinear supervised - machine learning models given a set of labeled training data SVM will help us to find the optimum hyperplane which categorized new examples in 1 dimensional space the hyperplane is a point in two dimensional space the hyperplane is a land and in three-dimensional space the hyperplane is a surface that divides a space into two parts where each class lies on either side in the first section of this video we are going to go through the detailed process of finding the optimum hyperplane let's first start with the two dimensional linear separable case figure Y ensures that the data are separated by a line in this case the data are linearly separable figure two is an example of non linearly separable data which means that we can now define a line to separate the data therefore we say the data are linearly separable if we can find a line to separate the original data let's have a closer look at figure one in general there are lots of possible solutions for drawing a hyperplane to separate the positive samples from the negative samples but which one is the best SVM can help us choose the best hyperplane which maximizes the margin or sometimes we call it a street around the hyperplane to separate as a positive ones from the negative ones that's the reason why sometimes we also call SVM the maximum margin classifier now let's introduce another important concept support vectors support vectors are the data points that lie closest to the hyperplane they are the data points most difficult to classify which also decide which hyperplane we should choose unlike some other Michelini models like linear regression on your network which all data points influence the final optimization for SVM only the difficult points which is the support vectors has an impact on the final decision boundary moving a support vectors move the decision boundary moving the other vectors has no impact just like any other muscle in your model during the inference time as VM will predict the output Y with input X the input is just a set of features X for each sample SVM will help us to find a set of weights W for each feature then through the linear combination of input features and weights we can predict the output Y so far it is just like the neural net however there is a major difference SVM will optimize the weights in such a way that the only support vectors determines weights and the decision boundary now that starts the fun part of this section we're going to go through the underlying mathematical process finds VM to find the hyperplane and maximum the margin this part only requires that you have some basic background of linear algebra and optimization Theory still for this 2d example we have three important lines the middle line of the street h0 which is the hyperplane two gutters of the straight h1 and h2 now let's assume there is a vector W perpendicular to these three lines for all the points on h0 they should have the same projection distance on w direction which is w dotted x over the magnitude of W is equal to say let's multiply the magnitude of W on each set of the equation we can obtain W dot 8x is equal to say times the magnitude of W now let's substitute se times the magnitude of W with negative B we can get the function of the land h0 as W dotted ax plus B is equal to zero which is the equation for H zero here we only have two dimensions so this can be expanded as a following function in fact this expression works for any number of dimensions once we have the expression of hyperplane we can then use it to make predictions for this example if the expression for any unknown sample is equal or greater than zero will predict it as a positive sample otherwise we predict all know as negative similarly we can write the expression for garter h1 and h2 here K is just another constant value and we can rescale W and B to make K is equal to 1 for mathematical convenience as we have introduced before SVM is a maximum margin classifier to find a way to express a margin or street weights here let's introduce two vectors 1 X negative 2 an active sample on the garda h2 another 1 x+ to an appositive sample on the gara each one now let's take the difference of these two vectors and dot product is with the unit vector perpendicular to h0 we can get an expression of the ways of the street from previous draft function for gather h1 and h2 we can get w dotted x+ it's equal to 1 minus b for the positive sample and w dotted x- is equal to negative 1 minus p for negative sample let's substitute them back to the straight weights equation and derive 8 as 2 over the magnitude of W in order to maximize this ways with us need to minimize the magnitude of W for mathematical convenience it is equivalent to minimize 1\/2 times the magnitude of W squared which also equivalent to 1\/2 times w dot e 2w however we cannot make this straight with arbitrary large we can only expand the street weights under the condition that there are no data points between each one and to mathematically this means for all the positive samples they should obey the constraint the WX plus B is greater or equal to 1 and for all the negative samples the constraint is doublet ax plus B is less or equal than negative 1 now let's introduce another variable Y to transform these two equations to make it more convenient from mathematical perspective here we assume Y to be 1 for all positive samples and negative 1 for all negative samples so the previous two equations can be rewritten as a single equation for all the samples let's revisit the problems we're trying to solve here we want to optimize the street weights 1\/2 times W dotted W subjected to the constraint equation for all the samples it is a constrained optimization problems and we have a powerful technique lagrangian multiplier for this if you are going to find an extreme of a function with constraint then we are going to have to use Lagrangian multipliers this will give us some new expressions using which we can find the optimization of 8 without thinking about the constraints anymore for this problem the new expression can be written as one half times the WTTW manners the summation of all the constraints and each of the constraints is going to have a multiplier alpha sub I what we are going to do next is to find an extremum of this new expression well for this we just need to find all the derivatives of this expression and set them to 0 the first equation is a partial derivative of L the Lagrangian with respect to the vector tableau after setting it to 0 we can get to the W equals to the summation of alpha sub I times y sub I times X sub I over I for all the samples this is very interesting it tells us the vector W can be obtained with a linear sum of the sample of actors actually at the end of the optimization a lot of alpha-i is going to be 0 and the non 0 alpha I respond to the samples in the gutter which is the support vectors now we should differentiate the Lagrangian with respect to B as well which is going to be equal to the sum of alpha sub I times y sub I also at the extremum condition it should be equal to 0 now we are going to plug the derived expression W back in the Lagrangian equation and the magic happens here we can merge the first two term and the second term and rewrite the equation as a following one what we've discovered is that the optimization depends only on the dot product of pairs of samples now we can also substitute u2w back to the decision rule which is going to be sum of alpha sub I times y sub I times X sub I taught it to the unknown vector u plus B if that is greater or equal to 0 then the prediction is a positive sample now we discovered that the decision rule also depends only on the dot product of those samples vectors with the arno vector our previous discussion is based on the assumption that the samples are linearly separable and there is no outliers in the data and above SVM formulation is called hot margin SVM what if there are some noise and outlier in the data set then the heart margin SVM simply cannot tolerate them and the field to find the optimization let's revisit our inequality constraint here for the optimization problem to be solved all the constraints have to be satisfied in this part we will talk about how to deal with this limitation using soft margin SVM basically the trick is very simple let's add a slack variable Zeta to the constraint of the optimization problem it now becomes this equation with theta by adding these slack variables we're minimizing the objective function it is not possible to satisfy the constraint even if some outliers do not meet the original constraint the problem is we can always choose a large enough value of data so that all examples will satisfy the constraints one technique to handle this problem is to use regular addition for example we could use l1 regulation to penalize the large value of theta now the regularized optimization problem becomes the following expression notice that the Zeta here is always a positive number and the regularization parameter C determines how important data should be which means how much we want to avoid misclassifying each training example a smaller C emphasized the importance of Zeta and a larger Z diminishes the importance of data if we set say to positive infinite we will get the same optimization result at the heart margin SVM so small values of C will result in a wider margin at the cost of some misclassifications large value of C will result in the smaller margin at a less tolerant to outliers now the soft margin SVM can handle the non linearly separable data with some outliers and noise what if the non linearly separable data is not caused by the noise what if the data are characteristically non linearly separable like the following case in this part we will talk about a technique called kernel trick to deal with this kind of problem for this example it looks like there is no easy way to draw a line to separate the positive samples from the negative samples however if we transform the data from one space to another space we would be able to find a hyperplane to separate the data now let's revisit our Lagrangian we find that the optimization only depends on the dot product of the sample AK sub I and the sub J so we could use any transformation function that can be expressed at the dot product of X sub I and X sub J we call this function kernel function so the kernel trick is to define a kernel function k and use it to replace the X sub I dotted X sub J in the Lagrangian this is a small but powerful trick for these problems we could try to the kernel as X 1 sub I square plus X 2 sub I squared if we plot these samples in the new space a clear separation is visible and the line can be drawn to separate the positive samples from the negative samples here I will also show two popular kernels polynomial kernel and RBF kernel the polynomial kernel is defined as the following expression this kernel contains two parameters a constant C and the degree of freedom D a larger value of T will make the decision boundary more complex and Matt resort in overfitting the RBF kernel is defined as the following expression the RBF kernel is also called Gaussian kernel it will result in a more complex Gaussian boundary it has one parameter gamma here are some examples for SVM with RBF kernel of different gamma values the simulation is now with second learn a small value of gamma will make the model behave like a linear SVM without kernel a large value of gamma will make the model heavily impacted by the support factors and may result in overfitting I hope you like this video don't forget to subscribe to us and the rain the notification bell and stay tuned"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing SVM with Scikit-Learn",
        "Video Title":"Mastering Support Vector Machines with Python and Scikit-Learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=kPkwf1x7zpU",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/kPkwf1x7zpU\/hqdefault.jpg",
        "ID":"kPkwf1x7zpU",
        "Publish Time":"2023-08-30T13:07:28Z",
        "Channel":"Ryan & Matt Data Science",
        "Channel ID":"UCKq-lHnyradGRmFClX_ACMw",
        "Transcript":"today we're gonna be covering how you can Implement a support Vector machine within python with the help of scikit learn in fact this is one of the easiest machine learning algorithms and you can get this set up within just a few lines of code so first off why are we going to be using a support Vector well essentially this is really good for classification problems and here's how it works essentially imagine that you have a scatter plot and then you can draw a line through that scatter plot and you can classify different data points based around that so that's actually like the easiest level how this works uh work gets a little bit more complicated though is it doesn't just have to be a 2d scatter plot you can go to 3D or you can also go into hyperplanes also you can add on different types of parameters so you have different things like gamma regularization and also kernels and I do want to call it regularization because regularization deals with the specific line that splits up the data and when most people think of lines you just think of a straight line however regularization allows you to have a curvy line so with that being said I'm going to jump on my computer right now and let's start coding okay so the first thing we're going to do is import a few different libraries in so import pandas as PD or import numpy as NP and then import matte plot lib dot Pi plot as PLT ship and enter this runs a sill and then builds a brand new one down below now I'm going to import in some code that I actually generated from chat GTP but I'll explain how this works and this is going to create our data frame with random numbers now it's not specifically random because I did make a few different choices on here but I'll explain through the code but let me shift and enter this and just show you the data frame um so with three different columns here we have miles per week we have the farthest run and then qualified for Boston Marathon so as NG where we're doing on this data frame is taking a look at Runners how many miles they run for each week pretty much on average the farthest run they did before uh the race and then if they qualified for the Boston Marathon which is a very tough race to qualify for if you're a younger male you have to run under a three hour marathon and that just gets you qualified that you still have to go through essentially like a lottery you get into the race so it's very tough and that is something I want to do eventually one day as a runner but uh I could talk about that another day now how this code is generated over here let me explain uh my choices so the first thing is miles per week now I limited this the 30 and 120 I set a mean at 55 a standard deviation of 10 and then the number of samples for this data set I put at 500 now 500 is slow um but again this is just a tutorial so that's why I chose 500 and also 30 to 120 are the people that run less than 30 miles per week that could technically run a marathon yes there is are there people that run over 120 miles per week absolutely some delete Runners definitely do that but for the sample I made the choice to do between 30 and 120. now for the second column the farthest run I chose numbers between 12 and 26 I know 26.2 is a marathon but I just cut it off at 26 and a few things one are there Ultra Runners that go and try to run for the Boston Marathon yes I 100 know that but I just put air 26 for this example I put our mean over here at 18. because a lot of people when they run a marathon the farthest distance they run usually it's about like 18 20 or 22 miles so 18 is pretty good and it's the standard deviation of three so that's why we're between 12 and 26 over here and lastly qualified for Boston Marathon I did have a filter over here where I essentially wanted to call number one numbers to be greater than mean one um and it's going to show more often for qualifying for the marathon and the reason why I I did this is essentially the more miles you run per week hey you're a better Runner which means you're faster and you're like more dedicated towards the training towards that Marathon so more likely you're going to be able to qualify for the Boston Marathon plus here are more seasoned Runner if you're running 80 miles per week like this example over here right probably been running a few different years if not longer and you know what you're doing and this is probably not your first marathon with that being said right there is going to be some bias based off the numbers I put because it's not truly random but for this example that is why and I'm gonna put all this code down below so that way you guys can copy it and it's a lot easier than just importing in a CSV by any means like that just okay so up next I'm gonna actually just plot this for you guys real quick again I'm just going to copy over some code because this is a not a matplotlib tutorial but just to show you how this looks over here right so our support Vector we want to pretty much have a line that's going to cut this over here between the people that qualify for Boston Marathon and people that didn't again you can see miles per week people that run more more typical to qualify for the Boston Marathon than people that don't obviously there are some outliers like over here this person only runs over 30 miles per week uh but maybe they're a really good 10K Runner and decided to get into uh the marathon so that's why someone like this might be able to qualify um but our machine learning model is going to be taking a look at this and doing a lot of different calculations behind the scene so with that being said let's start coding and the first thing that you want to do like every other machine learning model is split up your X and Y so I'm just going to do x equals DF dot ilock then we're going to put over here a colon and then we're gonna go zero through two just like that and our X is defined now if you want to see what our X looks like we go over here miles per week and then almost to the farthest run and also capital x make sure you guys put capital x because it's the proper way to do it now we're going to do y so y equals DF dot I lock right and then pretty much essentially the same thing except this time we just put 2 here at the end and just to show you why as we just have these results right one or zero let me go back over here right qualify for Boston Marathon and that's the only thing that we're going to show in our y column now the first thing you want to do too is also uh train test split so that way we have a training set and then also a testing set we can take a look at at the very end so to do that first I'm going to add in a few new cells over here already from sklearn.model selection [Music] import train test split just like this and then we're gonna do X train X test y train y test equals train test split capital x lowercase y we'll say random date equals 33 and then test size equals 0.2 we're essentially wrapping 20 of our data that is going to be in the testing set which will be about a hundred right so we have that over here and I have a full video train test split I'll link it down below in the description if you were not too familiar with it in that regards but you should be before you start running any machine learning models okay now let's Import in our model so we're going to say from SK learn dot svm right that's our thing over here we're going to import SVC because it is a classifier well this year should be this should be capitalized not lowercase and we have that working okay make sure you do all capitalize on there and then we're gonna say model equals SVC and I'm not going to put any parameters for this first instance just to show you how this works okay so we have that over here and like everything else we're gonna have to do a fit so model dot fit putting our training data so X train and also y train just like this right our model is now down below and it's good we have fit the model and now we're gonna do a score so model DOT score and throw in your X test and then also throw in your y tests right and we have 0.75 so that is our first case now I want to show you guys a few of the different parameters that you can put in here that are pretty common now first we're going to do regularization so essentially if you have a higher number on regularization you're going to have a more curvy of a line over here to try to split up the data low regularization you're going to have a straight line so I'll show you a few different instances of this so I'm just going to do like model we're going to say like rag zero equals and then SVC and then inside over here our regularization is a c which let me put a comment over here too so I can split these up like the alertization right and we're going to say C equals 0.1 like this right and then again we're gonna have to fit our data so all we're going to do down below is put this this dot fit X train y train and we have this over here and then lastly we can do our score so then you just copy this we'll say score and then test and test right and then you can see we have a 0.74 which a little bit less when we regularized it but there's a few different values you want to check and if you want to get the exact value you're going to do hyper parameter tuning but I have not done that video on the channel yet be aware of that in the future so I'll show you a few other examples I'm just going to copy all these in one cell so it makes it pretty easy right and then what I'm going to take a look at this time is just look at one so we'll put this over here as just one change this up okay 0.75 so one performed a little bit better and this time we're going to also take a look at one thousand okay and we'll just say this is like two okay and then we have 0.77 so the higher regularization on this actually worked a little bit better which is nice so now we're going to be taking a look at Gamma and essentially if you have a high gamma uh all these points are closer to the line if you have a low gamma they're farther away so we can actually pretty much use the same exact example from above right we can use 0.11 and a thousand so I'm just going to copy this and we're gonna fill these out so first I'm going to do over here we'll say gamma zero I'm just gonna copy that over here and this time you're going to just write out gamma so gamma equals 0.1 and let's see how our gamma works right 0.77 which I think that was better than our first model right 0.75 okay now let's look at one 0.81 so we're performing better now that we have a gamma over here and actually I should have put this as a one let me just fix that real quick for you guys that will annoy me we don't fix that great 0.81 and lastly we're going to do a two which is going to be a thousand so let's see how that performs two three four and zero point eight so one performed better on this side of things and lastly I just want to show you how some kernels work another parameter that you can use kernel and we're going to use first a linear kernel so I'm just going to say a model linear equals SVC and then you're going to put kernel equals linear like this and then I'm just going to copy the same thing from here right we're going to fit and then find the score but I'm just gonna put this like this okay and we can see with 0.74 so it actually performed worse and lastly let's take a look at what RBF would look like so just put RBF like this and we have 0.75 I appreciate you checking out this video and following through with the tutorial if you did enjoy it and you learned something new make sure to subscribe to the channel as it does help it grow and these videos take a ton of effort to make by the way I have a full machine learning playlist right over here and I highly recommend you watch more videos from the series"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing SVM with Scikit-Learn",
        "Video Title":"Machine Learning Tutorial Python - 10  Support Vector Machine (SVM)",
        "URL":"https:\/\/www.youtube.com\/watch?v=FB5EdxAGxQg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/FB5EdxAGxQg\/hqdefault.jpg",
        "ID":"FB5EdxAGxQg",
        "Publish Time":"2018-12-19T12:41:14Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"support vector machine is a very popular classification algorithm and that's what we are going to cover today we'll start with some theory first and then we will solve a classification problem for iris flowers using svm in the end we'll have an interesting exercise for you to solve so it's going to be a lot of fun today so please stay till the end in the picture you are seeing iris flower which has four features petal width and height sample width and height based on these four features you can determine the species of that iris flower there are like three different species this data set is available in sk learn uh data set module so you can easily import it on this scatter plot i have two features pattern length and petal width just to make things simple and based on that you can determine whether the species is setosa or versi color now when you draw a classification boundary to separate these two groups you will notice that there are many possible ways of drawing this boundary all these three are valid boundaries so how do you decide which boundary is the best for my classification problem one way of looking at it is you can take nearby data points and you can measure the distance from that line to the data point so here you can see the distance is smaller here the distance is higher so this distance is called margin so which line is better the one with a lower margin or the one with a higher margin if you think carefully you will realize that the line with a higher margin is better because it classifies these two groups in a better way for example here if you have a data point in between these two lines then this line will probably misclassify it versus this line will classify it better and that's what support vector machine tries to do so it will try to maximize this margin here between the nearby data points and the line itself and these nearby data points are called support vectors hence the name support vector machine so in case of a 2d space where you have two features the boundary is a line in case of 3d the boundary is a plane what will it be if you have n dimensional space usually you have n number of features right so just pause this video for a moment try to visualize what that boundary will look like and you will realize that it's kind of impossible to visualize it but theoretically or mathematically still possible and that boundary is called a hyperplane so hyperplane is a plane in n-dimensional and dimension that tries to separate out different classification groups and that's what support vector machine algorithms tries to do so we need to familiarize ourselves with certain technical terms such as gamma and regular regularization and we'll go over these terms so on this graph you can see that this decision boundary is only considering the data points which are very near to it okay so this is one way of drawing a decision boundary you can see that i have excluded these data points and these far away data points uh in making my decision for the decision boundary the other approach of looking at same problem could be that you consider the far away data points as well so on the left hand side i have a high gamma and right hand side i have a low gamma and both the approaches are valid it's just that with low gamma sometimes you might get you might get uh acura problem with accuracy but that might be okay you know it might be computationally more efficient so both the approaches are right it depends on your individual situation the other example here is this is a separate data set where i try to draw my boundary very carefully to avoid any classification error you can see that this is almost over fitting the model so if you have a very complex data set this line might be might be very zigzag and wiggly and it might try to overfit the model on the other hand i can take some errors so here you can see this there is this classification error which might be okay and my line might look more smoother so on the left hand side what i have is a high regularization on the right hand side i have a low regularization with low regularization you might get some error but that might be okay that might be even better for your model when we will use sklearn library to train our model you'll see a parameter called c and c means regularization basically you might have a complex data set like this so what do you do like here it's not very easy to draw the boundary one approach is create a third dimension so i have x and y here so what if you create z dimension and the way you do it by doing x square plus y squared so you are doing some transformation on your basic features and creating this new feature here and with that you will be able to draw this decision boundary so the y plane is perpendicular to your monitor right now so that's why you are not able to see it okay so just try to visualize the situation it's not very hard and once you have the boundary you can superimpose it on your y plane y and x the plane formed by x and y axis and you will get a boundary like this so the z here is called a kernel by kernel what we mean is we are creating a transformation on your existing features uh so that we can draw the decision boundary easily as usual i have my jupyter notebook open here and i have imported uh iris data set from sklearn library and when you look at the iris data set the basic features you will see it has these values right and when i do feature names i'll see the four features that i shown you showed you in the first picture all right so what i'm going to do is create a data frame out of this because it's easier to visualize uh this data set with a data frame so i will do something like df is equal to pd dot data frame and my data is in this iris dot data and my columns is actually the feature names all right and when you do df.head you will see that i have a nice data frame like this ready i also have target variable in this target parameter so i will append one more column called target in my data frame so now i have my target all right so the target is the possible values of target is 0 1 and 2. so what does 0 1 2 means so for that if you do iris dot target names so 0 index means setosa 1 means versicolor three means virginica so we have three type of iris flowers and the type is determined by these four features sample width and height and length uh the petal um width and height okay all right so now i want to i want to do some exploration and see how many of my data points have one in it so what i'm going to do is i will say df target equal to one i want to see how many rows in my data frame has uh all right i think i had a spelling mistake so you can see from row number 50 onwards my target value is one which means it is versicolor similarly if i do two you will notice that it starts from 100 and my total data points is 150. so 0 to 50 is setosa 5200 is versicolor and 100 to 150 is virginia let me add one more column called flower name so that is clear so flower name is equal to df dot target so from one column you are trying to generate another column and the way you do that in pandas is by using apply function and here lambda is just a small function or a transformation that you can apply on target column and you can generate this new column called flower name okay so iris dot target names maybe so for each value in my target column it will so that each value is x and it will return the index of the the value in target names so for example if it is 2 so in this array 0 1 2 is virginica so virginica will be placed in flower name column and if you want to see it you can see that now i have a new column called flower name if you export this to csv file it will be even more clear on how this data set looks so you can visualize it better basically now let's do some data visualization too and you know that in order to do that you have to uh use matplotlib you can use some other libraries such as bokeh or there are a couple of libraries available for visualization all right so from mat plot lib import py plot splt and this is inline magic this is a concept specific to jupiter notebook all right now let's first create three data frames so i want to separate these three species into three separate data frames okay so how do you do that df dot target is equal to zero is first data frame the second one is this the third one is this so now i have uh three different data frames right and if you want to look at those data frames you realize that the first one is satosa second one is versicolor the third one is virginica now let's draw a scatter plot so you can do plt dot scatter and specify your x and y here okay so what is your x and y so i'm going to plot the first two flowers so df 0 and it's a 2d plot so i will only use these two features okay so let's do this you can specify color and marker okay so let's say my marker is this so my scatter plot looks like this okay now i will do the same thing actually i made a mistake i should have done df zero actually so df0 is the sample width and df1 and let's say this color is blue so you can see that there is a clear classification so if i use my sv svm algorithm here it will probably draw a boundary like this all right you can also just to make this plot clear you can specify x label and y label also so what is my x label x label is sample length and y label is my width you know when you're looking at the chart now it's very clear all right let's plot uh pattern length and petal width also okay so i'll just copy paste this guy here all you need to do is replace apple with petal and this is just plotting two more features and you can see that this has even clear distinction so looks like our svm is gonna perform really well because it will be able to easily draw very nice and clear boundary between these data points now here i have plotted only two features each on on these scatter plots actually when we train our algorithm we are going to use all four features and the classification will also be between all three species now let's train our model using sklearn right so the first step is as usual use train test split to split your data set into training and test data set remember you don't want to test your model on the same data set that you have performed training on because it might be biased okay so let us do this our df data frame has target column also so i want to remove those first all right and the way you remove them is you want to say i want to drop certain columns from my data frame and which are the which are the columns that you want to drop okay so let's look at our data frame our data frame has all these okay out of this first four columns are your features that you want to use for training your model second two columns are your target columns okay so i want to drop target and flower name perfect and my y will be just df.target okay so if you look at y it's gonna be your usual numbers between 0 1 and 2. okay so now let's use train this split x and y your test size let's say i want to use 20 of my samples as a test and 80 percent as a training all right so x train the output of this train test split method is x strain x test y train y test and if you look at the length of your ex train and next test you will notice that uh this is the 20 of your 150 samples right so it looks good now now let's from sk learn dot svm import svc so the classifier is svc basically and this is how you create a svm classifier and as usual you can call fit method to train your model okay i'm going to use extreme y chain so my model is trained you will notice some parameters here such as c if you remember from the presentation earlier this is a regularization parameter you have gamma also and you have kernel okay all right now let's look at the accuracy of our model you can call score method and now this time i'm going to use x taste and y taste so my model is trained and i'm performing the accuracy of this model by supplying this x taste and y test what the score method is gonna do is it will take x test it will predict the values so it will be called like y predicted and it will compare y predicted with y test to measure how accurate the model is and you can see that it is 0.96 if you execute this again your x train and x test changes you know the sample changes so i want to now train it on a different set of samples so again it's like 0.96 you can now use uh the separate parameters for example here by default c is 1.0 so what if i modify the c parameter let's say you do c is equal to 10 and on the same data set if you train it see increasing regularization is actually decreasing my score so you can use these parameters to tune your model and you can do this on like a couple of data sets using cross validation techniques and you can figure out what parameters are best suited for your given problem all right you can use even gamma also so gamma is let's see if i use gamma is equal to 1 what happens okay i get the same score gamma is equal to 10 again a same score so looks like this is not making much difference oh so gamma 100 is going to make the model performance worse all right you can also use kernel so by default the kernel was rbf let's use linear now how do you know which kernel to pass here right so if you use shift tab it will show the help from the sqlearn api and you can say these are the possible kernels i have available to use all right so let's do linear and with linear also you are getting a very high accuracy score we have realized that it never goes beyond 0.96 so 0.96 looks like is the optimum score that you can achieve all right let's move on to the most interesting part which is the exercise remember uh learning programming is more about practicing it it's sort of like taking swimming lessons right if you see someone giving instructions for swimming on video you're not probably going to learn it you have to jump into the pool all right so open your jupiter notebook and let's start doing this exercise for this you are going to use uh sk learns uh digits data set so the digits data set looks like this it's like handwritten digits and you have to classify it into one of the numbers from zero to eight all right and you're going to use different kernels different um regularization and gamma parameters to tune the model and you have to tell me which parameters gives you the best accuracy for this classification also use 80 of the samples for training if you look at the video description below i have a jupiter notebook used in this tutorial available on my github repository look at that jupyter notebook go towards the end and you will find the exercise section so i have mentioned all the criterias for the exercise so do this and post your answers in the comment section below uh one last thing is i didn't cover the mathematics behind this sv model and different kernels because it is gonna make it will make probably this tutorial very long so that's something that will uh cover in some of the future videos all right thank you very much for watching bye"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing SVM with Scikit-Learn",
        "Video Title":"Support Vector Machine (SVM) in 2 minutes",
        "URL":"https:\/\/www.youtube.com\/watch?v=_YPScrckx28",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/_YPScrckx28\/hqdefault.jpg",
        "ID":"_YPScrckx28",
        "Publish Time":"2021-09-09T12:30:07Z",
        "Channel":"Visually Explained",
        "Channel ID":"UCoTo2gtN527CXhe7jbP6hUg",
        "Transcript":"in machine learning one of the most fundamental tasks is when you have a bunch of objects that you want to classify into two categories or more is this picture of a dog or cat is this stock going up or down svms or support vector machines are some of the simplest and arguably the most elegant methods for classification each object you want to classify is represented as a point in an n-dimensional space and the coordinates of this point are usually called features svms perform the classification test by drawing a hyperplane that is a line in 2d or a plane in 3d in such a way that all points of one category are on one side of the hyperplane and all points of the other category are on the other side and while there could be multiple such hyperplanes svm tries to find the one that best separates the two categories in the sense that it maximizes the distance to points in either category this distance is called the margin and the points that fall exactly on the margin are called the supporting vectors to find this hyperplane in the first place svm requires a training set or a set of points that are already labeled with the correct category this is why svm is said to be a supervised learning algorithm in the background svm solves a convex optimization problem that maximizes this margin and where the constraints say that points of each category should be on the correct side of the hyperplane in practice you don't have to worry about the implementation details of this optimization problem using svm can be as simple as loading a python library preparing your training data feeding it to the fit function and calling predict to assign the correct category to a new object the biggest pros of svms is that they are easy to understand implement use and interpret furthermore they are effective when the size of the training data is small the simplicity of svms can also be a problem in many applications the points cannot be separated by hyperplane a common workaround in this case is 2a augmented data with some nonlinear features that are computed from the existing ones b find the separating hyperplane in this higher dimensional space and c project back to the original space a clever technique known as the kernel track allows us to perform all of these steps in a very efficient manner and now that you know about svms you can use them for face detection spam fill train and text recognition this was svm in about 2 minutes like and subscribe if you liked the video and see you next time"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing SVM with Scikit-Learn",
        "Video Title":"How To Implement Support Vector Machine (SVM) Classifier in Scikit Learn: Example By Stanford PhD",
        "URL":"https:\/\/www.youtube.com\/watch?v=ST3aro0BJMI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ST3aro0BJMI\/hqdefault.jpg",
        "ID":"ST3aro0BJMI",
        "Publish Time":"2021-11-08T17:40:23Z",
        "Channel":"MLPro",
        "Channel ID":"UCxeOOGRh1B4k2dAbUqRknOQ",
        "Transcript":"on this ml pro problem we're supposed to train a support vector machine classifier for the given x train y train data and then make a prediction on the x test data and return the predictions the solution to this problem is pretty similar for all scikit-learn based model tasks in that we are going to call a dot fit to train the model and a dot predict to predict with the model so let's go ahead and do that the only thing that really changes here is which classifier we use we're going to use a support vector classifier it comes with scikit-learn we can call classifier on the x train and y train and then finally return the result of predicting on the test data so let's see how this does and we pass all test cases thanks for watching"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Ensemble Learning: Bagging, Boosting, and Stacking",
        "Video Title":"Ensemble (Boosting, Bagging, and Stacking) in Machine Learning: Easy Explanation for Data Scientists",
        "URL":"https:\/\/www.youtube.com\/watch?v=sN5ZcJLDMaE",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/sN5ZcJLDMaE\/hqdefault.jpg",
        "ID":"sN5ZcJLDMaE",
        "Publish Time":"2022-12-26T17:50:00Z",
        "Channel":"Emma Ding",
        "Channel ID":"UCAWsBMQY4KSuOuGODki-l7A",
        "Transcript":"hey guys in this video let's talk about Ensemble learning methods we'll cover three common types of Ensemble begging boosting and stacking here are some interview questions what is Ensemble learning what are the examples of Ensemble learning what are boosting and bagging what are the advantages of backing and boosting what are the differences between back-end boosting why boosting models are good explain stacking as you can tell being able to answer these interview questions requires you to have a clear understanding of different Ensemble learning methods so in this video we'll go over all of them and let's start with understanding what is Ensemble the idea behind Ensemble method is pretty simple is that a group of weak Learners can come together to form a strong learner and the strong learner will have better predictive performance than could be obtained from any of the base Learners alone if one base learner is erroneous it can be auto corrected by others so the final model is typically less prone to overfeeding and more robust it's unlikely to be influenced by small changes in the training data here's a diagram showing the concept of Ensemble for classification tasks we start with the training set then we build classification models based on this training set we have m models in total C1 C2 to CM and then each model has its own predictions and then we look at all these predictions and we use voting to Output the final prediction voting means that we select the most frequent results across all these predictions now we know the general idea of Ensemble let's look at one Ensemble learning method begging bagging is short for bootstrap aggregation it builds several instances of an estimator on bootstrap samples of the original training data and then aggregate their individual predictions to form a final prediction here's a diagram showing how it works the first step is to create bootstrap samples simply with replacement from the training data here we have amp bootstrap samples from T1 to TM the reason we create bootstrap samples is that we want to ensure each sample is independent from others as it does not depend on previous student samples when creating bootstrap samples then we use a single learning algorithm to build a model using each bootstrap sample these models can be built in parallel as you can see we have C1 C2 to CM M classification models in total then we use mobile models to make predictions and the predictions are combined using voting or averaging this diagram shows backing for classification tasks so it's using the voting method voting means that the most frequent result will be the final prediction and averaging for regression tasks means that we want to take the average of all these predictions to be the final prediction of the model an example algorithm leveraging the back investor is random Forest here's how it works random Forest combine a set of decision trees to make predictions the symmetries are good candidates for bagging because they can capture complicated interactions in the data and it grows sufficiently deep they have relatively no bias because trees can be noisy that benefit greatly from the voting or average method to get the final prediction from a bias versus three dollar point of view random virus starts with low bias and high variance because its tree is fully grown and decision trees tend to overfit and then the random forests work towards reducing variance by taking majority vote or averaging across trees all right now let's move forward to boosting boosting improves the prediction Power by training weak Learners sequentially each compensating the weaknesses of its predecessors let me explain what this means both things start with a weak learner gradually turned it in a strong learner by letting future weak Learners focus on correcting mistakes made by previous learners for example in classification tasks misclassified examples get a higher weight than examples that are classified correctly so future Learners focus more on examples that previous Learners misclassified so essentially we are making the weak learner more complicated and this process will help reduce the bias of the weak learner here's a diagram showing how it works both things start with a weak learner better than random gas you can see that in the first iteration some examples are classified correctly and some are not in the next iteration the weight of the data are readjusted so that they can be corrected boosting assigns higher weights to those that were classified incorrectly and the lower weight to those that were classified correctly this sequential process of giving higher weights to misclassified predictions continues until a stopping Criterion is reached and the final prediction is the weighted result of all weak Learners one probably example of boost investors is gradient boosted trees gradient boosted trees trendy series sequentially it optimizes the residual loss of the tree by adding another tree it appears to art form backing on lots of problems and become the Preferred Choice from a biased variance Trader point of view gradient boosted trees start with high bias and no balance because the first three is shallow it's slightly better than random gas and then they work towards reducing bias by making the tree more complicated now let's summarize the differences between these two Ensemble learning methods backing and boosting in bagging individual Learners are trained independently they can be built in parallel versus in boosting individual Learners are dependent on each other other remember that a new learner is created to correct mistakes of the previous learner so the process is sequential from a bias variance perspective begging reduces variance of individual Learners while boosting reduces bias by making individual Learners more complicated because of this packing Masters work best with strong and complex models for example fully developed decision trees while boosting methods usually work best with weak models for example shallow design trees a good example of bagging is random forest and example of boosting is a gradient boosted trees finally let's look at stacking another Ensemble learning method which is the last frequently appears in interviews then back-end boosting the idea of stacking is to build a metal model that takes output of Base Learners as inputs it combines estimators to reduce their biases it can be applied to classification and regression tasks similar to backend boosting here's a diagram of stacking and we can consider it as two level Ensemble method in the first level individual estimators are created based on the training data here's a diagram showing how it works we can consider stacking as two level Ensemble learning in the first level individual estimators are created using the training data and then a combiner estimator of meta learner is created to fit to the level 1 estimator predictions to make the final prediction for example a static model for classification tasks can look like this the individual classifiers are random forest and support Vector machines and then we have a meta learner or stacking classifier which is logistic regression to make the final prediction based on the output of individual classifiers the stack method has some pros and cons in practice a stacking predictor predicts as good as the best predictor of the base layer and sometimes outperforms it by combining the different strengths of these predictors but training a stacking predictor is computationally expensive"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Ensemble Learning: Bagging, Boosting, and Stacking",
        "Video Title":"Ensemble Learning - Bagging, Boosting, and Stacking explained in 4 minutes!",
        "URL":"https:\/\/www.youtube.com\/watch?v=eLt4a8-316E",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/eLt4a8-316E\/hqdefault.jpg",
        "ID":"eLt4a8-316E",
        "Publish Time":"2021-03-29T01:29:28Z",
        "Channel":"Melissa Van Bussel (ggnot2)",
        "Channel ID":"UCWPCd6tPtoLJYzQQ681pe5Q",
        "Transcript":"ensemble methods take multiple models which we'll call weak learners and combine them together to form a more powerful model overall ensemble models are often at the top of the leaderboard for data science competitions such as those found on kaggle there are several different ensembling techniques and today we'll talk about bagging boosting and stacking bagging or bootstrap aggregating combines homogeneous weak learners when i say homogeneous what i mean here is that the weak learners are all the same type of model for example perhaps they're all decision trees with bagging we train each of the weak learners independently which means we can parallelize the model training across multiple cores or computers very easily to obtain predictions on unseen data we then aggregate the predictions from each of the weak learners and perform some type of averaging so why is it called bootstrap aggregating well after we've split our data into a training set and a test set we create b different bootstrap samples and train a weak learner on each of these b bootstrap samples and train a weak learner on each one to make predictions using the bagged model we pass an observation through each of the b weak learners and then we aggregate the predictions if it's classification this usually takes the form of a vote if we have a continuous response variable this is typically performed as averaging so what's the difference between bagging decision trees and a random forest a random forest is a bagging algorithm but there's one main difference between bagging decision trees in general and the random forest algorithm with a random forest we not only use a different subset of observations to train each of the individual decision trees but we also take a different subset of the variables to train each of the individual decision trees boosting is an ensembling method that trains models sequentially or iteratively therefore the weak learners are not independent the training of the model at the current step depends on previous models one example of a boosting algorithm is adaboost or adaptive boost for binary classification the adaboost algorithm works in the following way each model in the sequence is trained where higher importance is given to the observations that were most difficult to predict in the previous step specifically the weights of the previously misclassified observations are increased at the end in order to form the final strong learner from the weak learners a weighted sum of the weak learners is taken the weights for each of the weak learners is dependent on the performance of the weak learner the weights for each of the weak learners is dependent on the performance of the weak learner in other words the better the weak learner performed the higher its weight will be another example of a boosting algorithm is gradient boosting with gradient boosting the final model is also a weighted sum of the weak learners however gradient descent is used to determine how to improve at each step in the sequence gradient boosting is a generalization of boosting where optimization can be based on any arbitrary differentiable loss function gradient boosting is often used for decision trees and gradient boosted trees often outperform random forests stacking is an ensembling method that combines heterogeneous weak learners for example you might combine neural networks with decision trees with glms and so on it's also important to note that it's common to have bagged and boosted models as weak learners in a stacked ensemble for this reason stacked models can be very very difficult to interpret meaningfully with that being said they are usually top performing models so if ensemble methods are not interpretable why may we want to use them well often our priority is not interpretability there are situations where we want to have the model with the highest possible accuracy in these cases ensemble methods are highly desirable"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Ensemble Learning: Bagging, Boosting, and Stacking",
        "Video Title":"Bagging vs Boosting - Ensemble Learning In Machine Learning Explained",
        "URL":"https:\/\/www.youtube.com\/watch?v=tjy0yL1rRRU",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/tjy0yL1rRRU\/hqdefault.jpg",
        "ID":"tjy0yL1rRRU",
        "Publish Time":"2023-01-14T13:36:14Z",
        "Channel":"DataMListic",
        "Channel ID":"UCRM1urw2ECVHH7ojJw8MXiQ",
        "Transcript":"hi everyone this is yml and today you are going to talk about two of the most used methods in assemble learning begging and boosting we'll see how each algorithm works and what are the similarities and the difference between the two so let's not waste any more time and let's dig in to start with let's suppose that we have a data set what begging does with this data set is quite simple you create and sub data set of equal size by sampling with the replacement from the original data set a technique known as bootstrapping and train a classifier on each sub data set then at the end we use all these models to make a prediction in an assembler classifier so we know nutshell begging consists of two steps one is bootstrapping the data set and two is aggregating the results henceforth its name begging when used boosting on the other hand the way you create this models changes quite a bit so let's suppose that you have that same data set the first thing you do is to train a classifier on this data set and see which samples were correctly classified by the model and which ones were incorrectly classified then you use this information to weight up the samples which were incorrectly classified by the model so that when you train the next model it pays more attention to those samples and hopefully learns to correctly classify them then you look again at the incorrectly classified samples weigh them up drain a new model look at its predictions weigh up the misclassified samples and so on and so on until you get the desired number of models at the end as in the beginning case you use all these models in an assemble to make predictions on new data now let's see what are the similarities and the differences between the two by firstly looking at how they work so at a high level both Metals Builds an example of models but begging builds them in parallel and boosting boost them sequentially knowing this may help us in choosing which one to use depending on the Computing resources and development time we have at our disposal if we have a lot of computing then due to its parallel nature begin may be a more suitable algorithm since it may take a lot less time to train the models while we might get no significant Improvement in training for boosting due to its sequential nature another 13 that we might consider when looking at these two Ensemble learning methods is the data set on which they train the classifiers so both models builds a separate data for each model but they do it differently begging uses a subset of the original data set that is generated by same thing with replacement while boosting uses the same samples as in the original data set also another important distinction is that in back in the samples are unweighted while in boosting they are weighted in regards to the predictions given by the previous classifier how each Ensemble makes the predictions is yet another important Dimension to analyze so both methods make predictions by taking the average of the models butting back in the classifiers are equally weighted while in boosting the models are weighted in The Ensemble based on their training performance as in any machine learning problem the bias and variance of the system plays an important role in the final performance in our case because they are an enzyme pod backing and boosting are good at reducing the variance however begging has closely zero bias reduction the reason being that because we don't change the voiding of our data when sampling the bias of the individual model is transferred to the assemble this doesn't happen in the boosting case since the samples are waiting from one model to another but unfortunately this makes boosting more prone to overheating in comparison with pegging I know that I may have lesser questions unanswered in this video and things like why do we sample with replacement in begging or why is boosting prone to overheating more exactly may have popped in your mind well I've done that on purpose mostly because you may notice a thing that this question have in common the why which is the main theme on this channel so I intend to make videos about these subjects in the future this was the video for today I hope you enjoyed it please leave a like if you did share with me your thoughts in the comment section subscribe to be up to date with the new content and until next time I wish you a wonderful time bye bye"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Ensemble Learning: Bagging, Boosting, and Stacking",
        "Video Title":"Tutorial 42 - Ensemble: What is Bagging (Bootstrap Aggregation)?",
        "URL":"https:\/\/www.youtube.com\/watch?v=KIOeZ5cFZ50",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/KIOeZ5cFZ50\/hqdefault.jpg",
        "ID":"KIOeZ5cFZ50",
        "Publish Time":"2019-08-23T04:57:52Z",
        "Channel":"Krish Naik",
        "Channel ID":"UCNU_lfiiWBdtULKOw6X0Dig",
        "Transcript":""
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Ensemble Methods with Scikit-Learn",
        "Video Title":"Machine Learning Tutorial Python - 21: Ensemble Learning - Bagging",
        "URL":"https:\/\/www.youtube.com\/watch?v=RtrBtAKwcxQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/RtrBtAKwcxQ\/hqdefault.jpg",
        "ID":"RtrBtAKwcxQ",
        "Publish Time":"2021-10-22T13:30:08Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"Once I wanted to buy nest thermostat and I wasn't sure if I should be buying that or not. I then called four of my friends who already have that device and then asked for their opinion. Three of them told me I should buy it. One guy told me no I shouldn't buy it, and I just took a majority vote and I went ahead and installed nest thermostat in my home. We use ensemble learning in our real life, where to make a decision we take opinion from different people. Similarly in machine learning sometimes what happens is if you have just one model and if you train that model using all your data set, that model might get over fit or it might suffer from a high variance problem. If you don't know about bias and variance I have another video, which you must check before continuing on this video but to tackle this high variance problem we can use ensemble learning. In the case of my nest thermostat why didn't I call just one of my friend, because that if I call only one person that person might be biased. So I wanted to make a good decision. Hence I talked to multiple people and took a majority vote. In ensemble learning we train multiple models on a same data set and then we when we do prediction we do prediction on multiple models and then whatever output, we get we combine that result somehow to get our final output. Bagging and Boosting are the two main techniques used in ensemble.. ensemble learning and in this video we are going to talk about Bagging. We'll also write some python code. Let's get started Let's say I have this data set of 100 samples and when I train a machine learning model one of the problem I might encounter is overfitting and overfitting happens due to the nature of the data set, your machine learning, methodology, etc., and usually overfitting model has a problem of high variance. To tackle this problem one of the things you can do is out of 100 samples, create a small data set of 70 samples. Let's say I'm just giving an example and the way you create this subset is by using resampling with replacement. Now what is that exactly? Let's first understand that. Let's say I have this 10 samples, out of which I want to create smaller data set with four samples. In resampling with replacement, we randomly pick any data point; let's say four and then when we go and pick second data point we again randomly pick any data point from 1 to 10 with equal probability. We don't look at what we already have in our subset. So second time I will let's say get 8. Third time also we randomly pick any data point from 1 to 10 and this time I might get same sample again. So this is a resampling with replacement where in my subset I can get same data sample multiple times. So here, from my original data set I created a subset of 70 sample. I might create n number of such smaller data sets from my original data set, using resampling with replacement and then on individual data set I can train my machine learning model. Let's say I'm trying to classify if a person should buy insurance or not and I'm using logistic regression. So I will use logistic regression model. So here M1 M2 M3 they all are logistic regression but they are trained on a different data set. And when they're trained and now I have to perform the prediction or inference I will perform that prediction on all three models in parallel individually and whatever result I get I just take a majority vote. So here M1 and M3 is saying person should buy insurance M2 is saying they should not. Majority vote is clearly one and that is my final outcome. The benefit you get here is these individual models are weak learners. Weak learners means they are trained on a subset of data set and hence they they will not overfit. You know it is likely that they will generalize better, they will not over fit and these individual weak learners when you combine the result you get overall a good quality result. This was a case of classification, same thing applies for regression. Let's say you're doing housing price prediction. Here, you take an average of whatever is the prediction by individual model. Now this technique is also called Bootstrap Aggregation because when you are creating this small subset of data set using resampling with replacement that procedure is called bootstrap, and when you combine their results using either an average or majority vote that is called aggregation. So hence bagging is also called bootstrap aggregation. Many many times you hear all these terms and jargons and you get worried what it is, but really these concepts are very very easy. You know you already understood what bootstrap aggregation means. Now Random Forest is one of the bagging technique with only one distinction, which is here we not only sample your your data rows but you also samples your features. So basically you sample your rows as well as your columns. Let's look at our classical housing price prediction example where town, area, bedroom, etc are features and pricing which is a green column is your target variable. Here you will sample rows and columns both. So here you can see I don't have a bedroom column or a plot column. I randomly resample out of see one two three four five six out of seven columns, I got only four columns. In the second time again I randomly sample this column. So I did not get uh for example here bedrooms in this particular data set. And in the third one for example here I did not get school rating. Okay. So you are resampling you're randomly picking rows as well as columns then on individual data set you train a decision tree model, and then you aggregate the results. Here I have decision tree regression you can use it for classification problem as well. But the point is very simple random tree is basically a bagging technique, but here we do one additional thing which is we randomly pick features as well. And the difference between these terms bagging and bagged trees is that, in bagging individual model can be svm knn logistic regression pretty much any model. Whereas in bagged trees, so random forest is a bag tree example. Here the every model that you're training is a tree. Alright so I hope the theory is clear. Let's move on to python coding using SKlearn. I will be using this data set for the coding today. The data set is about Pima Indian Diabetes where based on certain features you have to predict whether the person has a diabetes or not. By clicking on this link I have downloaded this csv file, which looks something like this. Here the features are pregnancies, glucose, blood pressure; these are all contributing factors for diabetes. Based on these you are deciding whether the person has diabetes or not. I have loaded this csv file in pandas data frame as you can see here and as soon as I load data in my data frame, I like to do some basic exploration. So let's first start and find out if there is any column which has null values. So the way you find it out is you will do df dot is null dot sum. This will tell you in this column, let's say if the number is 5 which means it has 5 null values. But we are lucky here there are no null values, so no need to worry about it. Second thing that I do is df.describe. This tells me the basic statistics for each of the columns. For pregnancies look at min and max. You know max, I understand 17 is little high but it's not unrealistic. When I examine min max values in all these columns they look normal and I don't feel like we need to do any outlier detection or outlier removal, etc. So I will just go ahead and assume that there are no outliers. The second thing I check is whether there is any imbalance in the data set. Because see there's this outcome right. So here if you do value counts what you will find is there are 500 samples which which says no diabetes, 268 samples which says yes person has a diabetes, and if you look at the ratio it's 0.53. So it looks like some imbalance but it is not a major imbalance. Major imbalance would be like you know 10 to 1 or 100 to 1 ratio. This is more like 2 to 1 ratio, you know. So I would say that this there is slight imbalance, but it's it's not something that you should worry about. So I would go ahead and move on to the next stage, which is creating x and y. So my x will be df dot drop, because outcome is my target column. I need to drop that and the way you do that is by using drop function in pandas and in the axis you will say columns. And y would be df dot outcome. Okay so my x and y is ready. Now I will do scaling, because the values are on a different scale, here. You can see this particular number you know 0 to 17 versus 0 to 199. I mean it's not a huge difference in the scale but still just to be on a safe side I will use standard scaling. You can use min max scalar as well. Let's create our scaler object, scaler and if you have followed my previous videos you know how to use scalar object you can just say fit transform x and what you get in return is your x scale. This will be NumPy array. Hence I will just print like first three rows out of it. You can see the values are scaled. If you use min max scalar, you'll get different set of values but both works okay. Now this should be in your muscle memory. When you have x scale and y ready you will do train test split and for that I will import this method in train test split what do you supply x and y, right x and y. What is our x? x scale right we want to use the scale value and then in the output, you know the standard output that I get is x train x test y train and y test. Now here, I will use stratify argument because there is slight imbalance. So I want to make sure the test and train data set e has equal number of samples like equal proportion basically and I will say y I mean it won't be equal but at least the this ratio should be maintained and random state I will set to 10 or maybe 20 let's go 10. This is just a random number by the way and it allows you the reproducibility. Every time you run this method you will get same x train, y train. So if you do the shape okay 576 samples in your train data set and this data set has 192 and if you look at this. Oh no actually you know what I should be looking at y train dot value counts and if you look at this around same kind of ratio that you saw here, okay. Now, we will use decision tree classifier to train a standalone model. You can use svm kanye west neighbor any classification model. I am using decision tree to demonstrate that decision tree is relatively imbalanced classifier. It can overfit and it will you know it might generate an um high variance model. So let's train decision tree first and I will use cross validation score here, to just try it out on a different type of data set rather than just x train and you know x test. So cross validation, if you don't know about cross-validation score I have done separate video on k-fold cross-validation. You should watch that if you are not aware. Otherwise you know this thing will bounce off your head. So this isn't cross validation score expects a classifier which is my decision tree classifier then x and y and then cv is equal to 5 10; I'll give 5. So what this will do is uh if I have if I have um 768 samples. It will divide them into five folds and it will try different set of x test and you know y test to x train and x test to try the model. You should watch my video on k4 cross validation. You will get a good understanding of it. And this will return this will do like five time training and all those training results would be inside the scores. So see the score that you got by running 5 iteration of training is this and if you take a this is a NumPy array, so just take a mean of it. You'll find your model is giving you 71 percent accuracy, which is okay. But now let me use bagging classifier and first thing you can do is ask your friend sklearn bagging classifier. Your friend is Google and Google will tell you which api you need to use. So here see, I will use the most important tool for any programmer which is copy paste and I create a bagging classifier. And backing classifier you can read all the arguments but I'm just going to use couple of arguments. First of all okay which estimator you are using. Well I am using decision tree. How many estimators like how many sub groups of data set? 100, trial and error, okay. You try 10 20 figure out, which one is giving you best performance and this 100 is nothing but this. See in my presentation I said 3 model, I am doing 100 models and 100 subset of data sets and I will be training them in parallel and how many samples. See here we used 70 out of 100. So for sampling use 80%, use 80% of my samples. There is another thing called oob score. So oob score is equal to.. now what is oob score? Well oob means out of bag. When you are sampling, because you are doing random sampling by the law of probability you are not going to cover all 100 samples in this subset. Let's say in this subset, all this subset number 29 did not appear at all. Okay, so number 29, let's say number 29 is here right 1 to 100 number, that 29 number sample did not appear in any of this subset. So now all these models which are trained they have not seen that data data point 29. So you can use that 29 to test the accuracy of these models. So you are kind of treating that 29 sample as a test data set. Ideally what you do is you take your data set, you split it into train and test. So this diagram that I have shown here this block is actually your x train. So your x test is separate already, which you will be using to test the performance of your final model before before deploying that into a wild. But even within x train because of our sampling strategy you might miss some samples. Let's say you might you have 20 data samples, which which which has not appeared in any of this subset and those 20 samples after these models are trained. You can use those 20 samples for prediction take the majority vote and figure out what was the accuracy, and that accuracy is your oob score. So you realize that okay okay let me first do random state here. Again random state is for predictability and I will call this a bag model and when I have a bag model I will do oob score. Oob score. oob [Music] Bagging classifier. Actually you know what I have to fit dot fit. So I am doing x you know x and y train fit and then I get this. You realize I did not try even x test and y test, on training data set when I did 80 samples. When I train 100 classifier, it might have missed some of the samples from my training data set and on them I ran my model prediction and the accuracy I got is stored in this oob score. now I can do regular scoring x test y test and you see improvement right 77 percent versus standalone model giving you 71 percent. Now I agree you will tell me here you use cross validation. Here, I did not use cross validation. So let me use cross validation then. So I'm going to do some copy paste magic and create the same bagging model and then use cross validation scroll, okay. In cross validation score what do you supply? You supply first your model then x then y and how many folds well five folds, okay. You get scores back and those scores you can just take a mean. You will find that the base model gives you seventy one percent accuracy bagged model gives you seventy five percent accuracy. So for unstable classifier like decision tree, bagging helps. If you have a classifier you know sometimes, you have unstable classifier like decision tree and sometimes your data set is such that there are so many null values, you know your columns are such that your resulting model has high variance. And whenever you have this high variance it makes sense to use bagging classifier. Now, we talked about random forest. So let's let me try random forest as well on this particular data set. So I will try random forest here and I will they say okay random forest classifier x y cv equal to 5, I get scores and pretty straightforward x mean. random forest classifier gives me one little better performance. Inside like underneath it will use bagging. It will not only sample your data rows but it will sample your feature columns as well as we saw in the presentation. Now comes the most important part of this video, which is an exercise. Learning coding or data science is like learning swimming. By watching the swimming video you are not going to learn swimming obviously. Similarly you need to work on this exercise, otherwise it will be hard to grasp the concepts which I just taught you. Here I'm giving a csv file, which I took from kaggle by the way and it is about heart disease prediction. You have to load the data set apply some outlier removal. I have given all the information here. So work on this exercise and once you have put your sensor effort then only click on solution link, because I have an AI technology built into this video where if you click on this link without trying out on your own your laptop or computer will get a fever and it will not recover for next 10 days! Okay, so you will miss all the fun. So better you try first on your own and then click on the solution link. I hope this you like this video. If you did give it a thumbs up!! at least and share it with your friends. I wish you all the best. If you have any question, there is a comment section below. Thank you!"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Ensemble Methods with Scikit-Learn",
        "Video Title":"Ensemble Methods in Scikit Learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=NqdyfMbVo1Q",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/NqdyfMbVo1Q\/hqdefault.jpg",
        "ID":"NqdyfMbVo1Q",
        "Publish Time":"2017-06-11T20:11:15Z",
        "Channel":"Data Talks",
        "Channel ID":"UCQTQ0AbOupKNxKKY-_x46OQ",
        "Transcript":"Nathaniel here welcome again to a bit of data science and scikit-learn where we do learn a little bit of data science and a lot of scikit-learn again we're working from the ground up literally the only thing you should know right now is how to get data sets in scikit-learn and then how to use the estimators in scikit-learn hopefully understand the slight difference between classification and regression estimators and that one will allow you to predict the probability and the other just can't do that okay so we're moving on to like ironically the hardest-hitting algorithms and stuffin and so if you if you want to stop after this one you've had the like heavy hitters but you'll be missing you'll be missing a lot of the the more subtle things that are very important for doing data science that we'll be doing a little bit later on these heavy hitters are called ensemble methods generally speaking I've written some stuff up here the goal of ensemble methods is to combine the predictions of a couple of different estimators so there are two families there's the averaging methods so this is the driving principle to build several estimators independently and then average their predictions and then bagging and force or the common ones and then you have the boosting methods which estimators are sequentially built enabled to or in order to compensate for each other's weaknesses so bagging is like a a lot of people compare it to like a democracy you get less variance you get tons of independent voters to independent algorithms that all vote on what the right answer should be and hopefully because they're all independent of each other and they'll have different facts and knowledge you'll come together with a with an appropriate answer then you have boosting you know it's that's I can't really the approach here is that you start off with a weak algorithm right but then you make new algorithms to compensate for their weakness and you continue to compensate for those previous algorithms weakness until they're all together as a whole purse um so let's let's go through this we'll start off with the bagging let's let's go ahead and load our data set we're using ogress again so we like flowers are you doing lots of flowers so I'm gonna be introducing you to Mehta estimators it's the first thing and then I'm going to be introducing you to a couple of pre-canned ensemble methods a Mehta estimator is something that takes in other estimators to build a more powerful estimator on top so the bagging Mehta estimator will take lots of small estimators right so in this case we're going to be using K nearest neighbors estimators in order to build a more powerful estimator on top let me show you what the signature looks like so a bagging classifier so this is a classifier which means we're going to be predicting classes not real number values we will have a predict method will we have a predict probability method yes we will have a predictive probability method so what this basically has this so we're again we're looking at the bagging classifier what we need to do is when you specify a base estimator so is the thing that we're gonna be using a lot of and then the number of estimators so a base estimator this is gonna be can your neighbors this can be logistic regression this can be elastic net if you're doing a regression the number of estimators so we're gonna be having lots of these estimators remember we're in a democracy we've got thousands of people or in this case thousands of estimators and they all vote and figure out what the best thing is and then there's a couple of other things here one of the important things that I'd like to suggest is the out of bag score I'll go over then just in just a second when I show you these sorts of things yeah okay okay so I'm gonna go ahead and I'll make a key nearest neighbors classifier as short what a K nearest neighbors classifier he does it looks at so I'm a point I will look at what my nearest neighbors are so in this case you might see a flower that's really close to you I'll say you're my neighbor I'll see flower that's pretty close you're my neighbor and the third closest flowers because also your neighbor you look at these three neighbors and you'll say hey what class are you guys and if they all happen to be citizens you're like I'm probably a Satoshi - that's what I came to miss neighbor classifier does and now let's go ahead we do we make our bagging classifier our bagging classifier takes a k-nearest neighbors classifier as one of its arguments so in this case it's a it's a Mehta estimator it's a little bit composable which is pretty cool I think you can actually take banging classifiers as arguments to bagging classifiers so you can just keep stacking them see how that does so some things that you'll want to suggest max samples so we're going to and we're bagging we bootstrap our data so we sample from our data lots of times so we take out about 50% of the data we only look at two features so in our iris dataset we have four features we only we're only going to look at two of them for each estimator I'm going to do number of jobs equals to 2 so we'll paralyze our stuff and this is the important thing the out-of-bag score so I'll go ahead and I'll run this we can fit it takes just a little bit of time there will be things that will take even more time later on but you know this is fitting a lot of stuff it's fitting it's fitting 10 estimators so it's fitting 10 King in its neighbors let's check out the out of back score there's this kind of like magic that happens when you're doing bagging so remember previously we were talking about taking a test and if you were given the answer key to the test and you were then to take the test it would kind of be unfair you wouldn't get a really good judge of how well you were doing and so we introduced cross-validation which is a way and I'll explain much more about this in the future but it's a way to give you practice tests right and then test you want something that's you've never seen before and a better assessment of of how well you've done when you're bagging something you get this out of bag score which is a really good assessment of how well you'll do and what the out of bank score is is remember we're sampling 50% of the samples each time we just we just look at our samples so we look at all of our data points we take 50% of them and we use those to train and then we use the rest to test so so I mean it's it's kind of cool in that sense the the outer bag score is a pretty decent proxy for how well you're going to be doing on on the test data set which we'll be talking about a little bit so out of X score is pretty reliable so we can go ahead and predict so we predict the first one incorrect we'll go ahead and create probability how do we predict probability here well we're taking about we're taking all ten of these guys and they're all voting if nine of them but one way and one of them both the other way then we've got a 90% probability and 10% probability so it looked like everyone voted for the first guy here and then we can go ahead and score so how well are we doing we're we're doing 97.9 so basically 98% of our predictions were correct on the training data so we're kind of cheating when we go ahead and slow it down here okay so that's what a meta estimated those I'm at an estimator has the exact same stuff it looks the exact same it's got the predict that's got the predicted probability it's got the score he's gonna fit it's normal it's got hyper parameters whoo who cares the one thing that it has it's a little bit different is this out of back score and when you're instantiated when you're making this model in the first part it also takes in another model so it's sort of composable there are some pre-canned estimators as well so these pre-canned estimators are random force random forests are incredibly powerful so you can even check out this here so a random forest is a meta estimator that fits lots of decision trees and so decision trees basically at each step you sort of ask yourself hey does my data point have weeds as leaves then it's probably a plant if it doesn't it's probably a mammal or something and so this is a super powerful estimator the random force classifier has always has out of that score and also can specify the number of estimator so this is the number of decision trees inside of that so we can specify all that Wow a fit a lot of a predict a lot of score and yeah you noticed like random four it's really powerful there are two super powerful estimators one of them is random forests and one of them will get to right down here is gradient tree boosting these things are incredibly powerful okay so the next thing here we have a to boost so this is kind of like the the basic boosting meta estimator you can check out the what it does here so it's going to take a base estimator it's going to take a number of estimators it's gonna take a loading rate and it's gonna take some algorithm don't don't worry about the algorithm okay if you don't specify an estimator I think it's just gonna be using trees we can check this out here base estimator it defaults to a decision tree so you don't you don't even need to specify one but if you want to specify one you can specify K nearest neighbors or something like that the only thing I guess you not sure you specify canoes names the only thing that you need to specify is you need to make sure that these estimators can take weights so the logistic regression would be fine but they need to take class weights and if they're able to take class weights and you can specify them so you could Newton you can check this out here as well based estimator it must support for sample weighting is required as well as the proper classes in main classes attribute so it needs to be a classifier and used to support waiting so and yeah this is you know the what I'm trying to do here is I'm trying to teach you the tools to learn in this case I to go over each of these estimators in depth and if you'd like to please leave comments and I would be happy to do so if I seen enough desire otherwise you know what I'm trying to like teach you to do is you know hey this is a meta estimate I know about estimators do they take in other estimators and you look very similar um but there are some things about like what it takes in what type of estimator it might take and I don't know and so I'll need to go ahead and put a question mark at the end and sort of read like oh let's see what type of estimator it takes in it takes in a regresar or it takes in an ensemble method or all this sort of stuff okay and then of course you can fit and you can even score so we got a score of 97 the fit of blah blah so again very similar after after you're done with all of this like at the end of the day these guys are just estimators and of course we have the out of bags or a fine I didn't I didn't said that's true anyways in boosting you don't even have an outer bag score so okay so adaboost so this is like your classic I want to do boosting and what boosting is series of of estimators each one compensating for the others weakness it's like two ways to do it a sort of like a high level what what bagging is you'll start off with a lot of really low bias estimators and you'll combine them all together in order to get low variance what a Debus does or what boosting does you'll start off with low variance estimators and you'll continue to improve on those until you get low bias so okay final one gradient tree boosting I'm not going to go into this a whole ton but this guy this guy is a powerhouse if you if you want a powerhouse algorithm use random for it so use gradient boosting out-of-the-box these guys are incredibly powerful there's a ton of stuff that you can specify here it works on trees as well these decision trees they're super super powerful you fit you score you get a ninety nine point three percent accuracy incredibly powerful um one of the things that not specified before is all these algorithms have this thing called warm start every estimator and save the tool now because we haven't really needed a every estimate adds a warm start I don't know if you noticed this but I can keep fitting my guy fit fit fit it doesn't do anything if I refit my guy on some more training data so I have an estimator I get more training data I'm like oh let's let's improve us if I fit it again it's not gonna do it's just gonna ignore the old training data it's been fiddling new stuff unless I specify one start equals true if I specify one star equals to true I will go ahead and will fit complete or will fit on on the new data but retain all the information that had in the old data in addition I can go ahead and I can add new params so in this case I was previously using ten estimators now I'm going to be using 20 estimators I'll start put warm start equals true all fit again now score so again I hit you know the 100% support really easily because I mean these guys in gradient boosting and decision trees are extremely powerful both of them also have a future importance which shows you which features were the most important and it looks like for iris the the last two features were the most important features okay final things I'm going to show you here is a boding classifier the idea behind this is to combine like very different machine learning classifiers into a majority vote and then average them so you can check it out here do you need estimators they need to invoke the fit method if it clones these original estimators and these stores process to boots and so you just need these guys we can go ahead and take logistic regression a Gaussian naive Bayes and ran to forest we can all stick them into one voting estimator we can pick we can score the difference between a boding classifying a boosting a method here is that each of these estimators is different they're just very different and each of these estimators is trained on the entire data set so okay so that's ensembl these these are the powerful methods that you need to or you'll want to use when you're trying to you know crush a machine learning problem you pop in it but JA I'll make sure it doesn't get back up there's some important things to note about them that song are meta estimators they take estimators as an argument in their constructor so they're composable voting classifier ada boost and bagging are all meta estimators then the two sort of powerhouse trucks coming out of the bag from the ensemble package would be random force and gradient boosting okay I hope I hope you've enjoyed this please do remember the meta estimators please do remember I had a bag and please do remember one start and please do remember the future employees okay this will hopefully be the longest one of these that will do I hope you guys enjoyed it hope you guys have learned your tongue and it's always it's my pleasure so to mekin"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Ensemble Methods with Scikit-Learn",
        "Video Title":"scikit-learn Skills: Employing Ensemble Methods with scikit-learn Course Preview",
        "URL":"https:\/\/www.youtube.com\/watch?v=SHC5cwbrn40",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/SHC5cwbrn40\/hqdefault.jpg",
        "ID":"SHC5cwbrn40",
        "Publish Time":"2019-08-13T21:00:23Z",
        "Channel":"Pluralsight",
        "Channel ID":"UCbrXRQHV4TOU4Pqzw325Z1A",
        "Transcript":"[Music] hi my name is Jana niryabi and welcome to this course on employing ensemble methods with scikit-learn a little about myself I have a master's degree in electrical engineering from Stanford and Haverford completes it has Microsoft Google and Flipkart at Google I was one of the first engineers working on a real-time collaborative editing in Google Docs and I hold for patterns for its underlying technologies I currently work on my own start-up lunacorn a studio for high quality video content even as the number of machine learning frameworks and libraries increases on a daily basis scikit-learn is retaining its popularity with ease in particular scikit-learn features extremely comprehensive support for ensemble learning an important technique to mitigate overfitting in this course you will gain the ability to construct several important types of ensemble learning models first you will learn decision trees and a random forests are ideal building blocks for ensemble learning and how hard voting and soft voting can be used in an ensemble model you will understand the differences between averaging techniques and boosting techniques to build an ensemble next you will discover how bagging and pasting can be used to control the manner in which individual learners in the ensemble are trained you will also construct boosting models using both adaptive boosting as well as gradient boosting techniques finally you will round out your knowledge by utilizing model stacking to combine the output of individual learners when you're finished with this course you will have the skills and knowledge to design and implement sophisticated ensemble learning techniques using the support provided by the scikit-learn framework"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Ensemble Methods with Scikit-Learn",
        "Video Title":"#85: Scikit-learn 82:Supervised Learning 60: Ensemble methods",
        "URL":"https:\/\/www.youtube.com\/watch?v=jdxg2bSdEhA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/jdxg2bSdEhA\/hqdefault.jpg",
        "ID":"jdxg2bSdEhA",
        "Publish Time":"2021-05-16T18:35:48Z",
        "Channel":"learndataa",
        "Channel ID":"UCzUpHQAfj3ulmJkTAfBtXLw",
        "Transcript":"hello and welcome to learn data it's great to have you on this channel i'm nilesh and in this video we'll look at the actual implementation of ensembl methods and before we do that we'll do a quick recap on what we learned in the previous video so in previous video we looked at the four different categories averaging boosting voting and then stack generalization and we looked at the intuition for each of these methods that we have listed on this particular flowchart so in this video we'll write the code to run each of these on the same data set one for classifier and one for regressor and see how the output varies so this would be a toy data set uh we'll work on the real world data set soon with all of these methods so let's get into jupyter notebook and see how we can create the models for these here in jupiter notebook i have imported several libraries uh including an tree and ensemble and also these additional library which is scalan dot experimental import enable underscore hist underscore gradient underscore boosting and we'll use that to run the hist gradient boosting method now we also have some additional libraries for other classifiers so what we'll do is we can combine any type of classifier and see how the results turn out they have listed them here we may or may not use all of them so let's see how we can work with this and i have already uh these are the uh versions installed on this machine that i'm using now first we'll create a two data sets which would be simulated data sets and these would be for classification and regression so for classification uh let's create one here so for classification we have class c fire i will have x c and then y c i'm adding the c because uh just to differentiate it from regression uh data sets dot make underscore classification and this is a really easy way to create toy data sets by using the make underscore classification we'll have a thousand rows and 50 columns so samples is thousand features is 50 and number of classes will set it to just two so that's our data set and we can go ahead and split these data sets into train and test sets so x x c underscore test and y c underscore train y c underscore test is equal to train underscore underscore test underscore split and here we have x c y c then stratify is equal to y c and shuffle is equal to true [Music] so with this we can also print the shapes so print xc why don't we copy this so what i'll do is copy this just it here and then just add the word shape in front of all of them so xc test and then yc train yc test and [Music] let's see okay so that's a mistake and so this is the data we have okay once we have an error classification that's again my typo oh there is another error keyword argument number of samples should there there should be an s there okay finally so this is the data set we have and now we can create a similar dataset for regression so instead of classifier i'll just use regressor and instead of make classification here we can write regression and again the number of samples and features stays the same we do not have number of classes but we do have targets so we'll set just one target which is y and instead of having c in the letter c uh because this is a classifier i will change that to r so that okay so that's r and change all of these to r so that uh when we use this this would be for the regression and we do not need the stratified so i'll delete that and again this would be xr train y r xr test fire train and wire test so let's check if everything is okay and now run this and that is for regression so we can look at x train so that's how the train looks like and then y r train so those are numbers and if we change that and see why our test those are ones and zeros and if you look at exit trend those are the numbers we have okay so now we have the data let's begin with classification so classification and here we'll use our usual method for running different classifiers is by creating a list of all the models we are going to use and that just makes it easy so we'll create a list model underscore list is equal to uh in the tuples ensembl dot i'll copy this word because we'll be typing it over and over uh the first one is bagging classifier and here we need to specify the base estimator so the base estimator will set it as linear underscore we'll use the ridge classifier so ridge classifier and the number of estimators we can set it to n underscore st meter so how many trees we want to build let's set it to 20 and then we can specify the name here for name this would be bagging classifier and the next one after this is random forest so we have ensemble random forest classifier and we'll use the default values the random forest classifier and after this we'll use extra trees classifier extra trees classifier and again name that extra trees classifier and next we have adaboost so ada b double ost add a boost classifier and again name that here boost classifier and after this we have the gradient boosting classifier so we add that a gradient boosting with ing classifier and this is again gradient boosting classifier and then we have east gradient boosting classifiers so histogram based test gradient boosting classifier and again the name for that hist gradient boosting classifier so these are the classifiers we have and let's run that so that looks okay now we'll create the function to fit each of these classifiers so we'll name it fit underscore model and that takes in the classifier we have in the list above and then just performs the fit on the xc train and yc train and then we get the predicted value yc spread is equal to tlf tlf dot predict and this would be on the test set xc underscore test and finally we can get the accuracy score so matrix dot accuracy underscore score yc underscore test ic underscore print and then we can print the accuracy accuracy is equal to acu now we have the function now let's run through the model list and call this function so for clf name in model underscore list to make this another so we'll we are going to create a similar list for regression so i'm going to rename that as model list clf just to make sure that it's a different name then we can add some lines here for printing so just so that we can visualize easily name and then again print uh add a affinity line there times 50 and this should be in the outside of the quotes or delete those now we can fit call the model fit underscore model clf and we can print a new line to separate the output so let's run this here is the output for different types of classifiers we have backing classifier which has accuracy 0.87 random forest 0.9 these are extra trees it's 0.9 add a boost is 0.88 and then we have gradient boosting 0.89 this gradient boosting is 0.90 so they are pretty close and when we apply this to another data set these can be the parameters for each of these can be tuned using cross validation so so as to improve the accuracy of these models so that was classification now let's look at regression and for that i'm going to copy this entire list and paste it here so we have regression and for regression uh the list is going to be reg and we'll use the sim will use the same sequence so the first one instead of classifier now we have regressor so i'm going to change those words regressor and base estimator is instead of reach classifier this would be just rich we could also use any other regressor or doesn't matter then for random forest classifier this would be regressor again this would be regressor extra trees or regressor and then for adoboost again this is regressor so having this in such a nice standard naming format helps a lot so you can simply change the classifier to regressor and now you have you now you can use that to perform regression tasks so this is the list for regression and as before we can now copy the rest of the code which is the function and then we have the loop which is the for loop now for function this is going to be different so let's add reg there i'm going to add reg there as well and this is going to take x r and y r and similarly for the test it's xr and predicted is wire and instead of accuracy we are going to calculate mean squared error so mean underscore squared underscore error and here we have yr test and we have ir thread and this would be mse again this would be mse so that's the function we have here so we have the updated function now all we need to do is change the some things in this loop and we are ready so we have reg there and here we can change this to reg now we also need to change these so this would be reg instead of clf again this would be our eg and then we just change the name of the model to reg so change that now let's try to run this and see how it works so we are seeing the msc score and at the very first uh look we can see that the mse for the bagging regressor is much lower as compared to all of the other tree regressors we won't go into the details of why that is in this video but that's something to note so uh using these forest regressors maybe they can be tuned to bring down the msc significantly uh right now we are running them based on just the default values that are there in each of those regressors now let's look at stack generalization tag generalization and what stacking helps us do is we can stack the output of previous ensembles and put them as an input to a new ensemble or new classifier and here we'll first look at just the stack one stack and then we'll look at stack of stacks or two stacks together so this is we'll create a list of uh class regressors in this case so estimators is equal to uh we have we'll use the ridge and linear underscore model or ridge then we have linear i'll use linear regressions linear underscore model dot linear regression and after that we have knr so [Music] neighbors dot k neighbor's regressor and we'll uh start with these three and then the first we specify the estimators for the stack and then once these are evaluated the output of this would go into the final estimator which is this one ensemble dot gradient boosting regressor and then to fit we can specify it in this way reg is equal to on ensembl ensemble dot stacking regressor and here we specify estimators is equal to estimators and then the final estimator estimator is equal to final estimator and with this we can then use the reg to fit reg.fit and this would be on the xr underscore train yr underscore train and then we have y r underscore thread is equal to reg dot predict xr underscore test and then we have mse is equal to matrix dot mean mean underscore squared underscore error and then we have wire underscore test ir underscore thread and then we can get the mvc so when we run this linear model a linear model is not defined so let me go back up and run these again whether to restart the computer so we have this now this should work when we run this stacking regressor of course it needs to be a small either final estimator that's again my mistake okay so we get an msc of 10.88 and so the way this works is we have this initial estimators and the output of that estimator is then uh used as an input to the gradient boosting s regressor and to make all this work we input that in this ensemble dot stacking regressor so here are the previous estimators and here is the final estimator dax that takes the output of previous estimators as the input and the run is pretty standard fit and then predict now let's look at an example of how we can use stack on stack of stack and for this we are going to use the again the same data set and we will continue using the regressor so what we'll do is we'll create a new set of final estimators so these final estimators will create three of these and then use them as input to another final estimator the final underscore estimator underscore one is equal to ons combo dot gradient boosting regressor and then i'm going to copy that 1 2 3 and then run this to this three and for this part then we can use random forest so i'm going to change that to random forest regressor and then change this to ada boost regressor and we can put all of these in a list so estimators 2 is equal to the first one is call it gbr that's because it's the gradient boost so uh final underscore estimator underscore one and then copy this and paste it two times so we have two names coming up and for this one this is going to be rfg random forest rfg and then this one would be abr and let's rename this to 2 and rename this to 3. so with that now we can create a final estimator so final estimator is equal to on symbol dot stacking regressor estimator is equal to estimators with s emitters underscore 2 because that's the list we are using in here as an input and then final estimator is equal to linear so we'll just use a linear linear model this linear underscore model dot ridge so that's our final estimator and now this is the new items that we are going to stack so the previous items would would have been these uh estimators so what i am going to do is simply copy all of this copy and paste it here so we have estimators raise linear knr then we have this up there which is a new one and then we have ensemble stacking regressor and estimators is equal to estimators final estimator is that one which has ridge in it and it takes in those values so overall we have one two three uh then four five six and then seven so we have seven different layers uh on this now when we run all of this we should get an output so we get an output where the msc is 20.65 which is higher than before but this is uh just to show an uh example of how the stacking generalization works now moving on let's look at how voting classifier works voting classifier and for this we'll use three different types of classifiers so clf one is equal to ensemble dot bagging classifier and here we'll use a base estimator which is rich classifier linear underscore model dot rage classifier we could use any other classifier as well here then the second one i'm using naive bayes knife underscore base dot gaussian knife base and then the third one is ensemble dot hist gradient boosting classifier and for s then we can create a list of estimators estimators is equal to uh here we have br up for the first one so lf1 and then copy all of this and put it down below sorry so this should be copy paste it here paste it here so now this is lr and this is hgb so this is lf2 this is clf 3 and we have uh the final ensemble eclf is equal to ensemble dot porting classifier with a capital v and c estimators is equal to estimators and then eclf dot pit y c so x c train y c train and then we have y c thread is equal to eclf dot predict xc x c underscore test and then we have the accuracy acu is equal to matrix dot accuracy underscore score and we have yc underscore test yc underscore thread and then acu now when we run this uh we should get the output so this is accuracy of 0.86 which has three different classifiers and the way voting class file works is takes the output of each of these for each sample and takes the majority vote to assign that sample to a particular class now next let's copy this is voting regressor and for voting regressor again we'll use the same format so here except of this we'll change this to reg 1 this will be reg 2 and we have reg 3 r our eg3 and instead of classifier we'll just use ridge so that would work for regressor and then instead of naive bayes i'm going to use linear underscore model dot linear regression and for a hist gradient boosting this would be regressor and finally this would be lrgb up this shouldn't matter if we have those names i'm going to change these to reg and now we can again rename these as well and this would be r and this would be r again this would be voting regressor and we have those estimators there and e c e r e g and again this would be y our thread then finally for uh matrix we have matrix dot mean underscore squared underscore error here we have y test yr test and then we have yr underscore thread and we calculate the msc when we run this we have an error so let's see what the error is estimator a bagging classifier so that's my mistake again we've seen that regressor and so now we have output where the msc is 383 this could be tuned further to get a lower msc or these regressors could be swapped and depending on the type of problem and so i hope in this video you got a hands-on experience on using different types of ensemble methods such as bagging forests gradient boosting erdo boost voting classifiers stack stacking methods to either classify or perform regression on a data set and in the in the previous video we looked at the intuition about this uh all these methods i hope you'll be able to apply these to your projects or in real world data sets as well we'll use these in one of the future video projects so you get an hands-on experience on that as well please like share and subscribe i hope to see you all in the next video until then thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Model Evaluation Metrics",
        "Video Title":"How to evaluate ML models | Evaluation metrics for machine learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=LbX4X71-TFI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/LbX4X71-TFI\/hqdefault.jpg",
        "ID":"LbX4X71-TFI",
        "Publish Time":"2022-01-24T22:37:18Z",
        "Channel":"AssemblyAI",
        "Channel ID":"UCtatfZMf-8EkIwASXM4ts0A",
        "Transcript":"training your machine learning model with the data that you have is not enough you also have to evaluate it to understand if it needs to be improved or if it's going to perform well in the real world or not and to do that we use evaluation metrics depending on the type of problem that you have the kind of evaluation metric you're going to use is going to be different so in this video we will learn about evaluation metrics what they mean and in which cases to use them this video is brought to you by assembly ai assembly ai is a company that is making a state-of-the-art speech-to-text api if you want to try it out for free go and grab your free api token using the link in the description all right let's get started so let's first take a look at evaluation metrics that we use for classification tasks the most popular one is accuracy and the reason that is being used so much is because it's a very simple metric it's very easy to understand and it's also very easy to compare different models with each other because it's just one number that you need to look at accuracy is basically how many of the instances that you got right divided by the total number of instances that you have even though it's simple and very popular at the same time it might not always be the best thing for you to use because accuracy simplifies things a little bit too much and that's why you might need to look into things that are a little bit more detailed like precision and recall so precision and recall are classification metrics but they are mainly defined on true positives true negatives false positives and false negatives but this implies that the problem needs to be a binary classification of tagging the instances either with zero or one or true or false but you can actually use precision and recall for some other classification tasks too you don't only have to have two or false or only two different classes so we will look into what we would do if we are not dealing with a binary classification task but let's first define precision and recall thinking that we're doing a binary classification task precision is the percentage of correctly labeled positive instances out of all the instances that were labeled as positive whereas recall is the percentage of correctly labeled positive instances out of all the instances that are actually positive so basically precision tells us out of everything that i labeled as positive or as one of the classes how many of them were actually belonging to that class whereas recalls tells us out of everything that belonged to that class or you can say out of everything that was positive how many of them i was able to capture because they are slightly different they give us different perspectives to the same model and that's why most of the time they are used together and by looking at the precision and recall values it's easier to understand what's going wrong in the model and to also improve it as i mentioned sometimes your classification problem might have more than one class that you want to classify things into at that point you might want to use different approaches to your either accuracy precision or recall whatever you're calculating there are a bunch of different ways to do that one of the ways to do is to just calculate all of these values for all of the different classes and then take their average you can also weigh one of the classes or the others based on how important they are to get correctly and then again take the average it is important to decide on the correct way to calculate accuracy precision and recall based on the kind of problem that you have so before you go forward with those ones make sure that you check the documentation of the type of framework that you're using or the type of library that you're using and to choose the correct one for your problem another thing that you can use is called the f1 score it is a combination of precision and recall and it gives you only one number so sometimes people prefer to look at f1 score f1 score is basically a harmonic mean of precision and recall and this is how it's calculated but remember that f1 score is most of the time best used in combination with other metrics so for example the pr curve or the roc curve so let's talk about them now pr curve is called precision and recall curve it's basically a comparison of what the recoil is when precision is has this value and also vice versa so in this example graph for example we would want the curve to be on the top right corner because that's the point where we have a high precision value and also a high recoil value roc curve on the other hand is again very similar but this time you are comparing the true positive rate to the false positive rate by looking at these two different graphs you can understand how your model is performing better and sometimes you would hear people use the word auc area under the curve and sometimes they calculate this using the pr curve and sometimes they calculate this using the roc curve but either way you want this auc number to be as high as possible because then you know your model is performing the best and lastly for classification tasks we have cross entropy cross entropy basically calculates the difference or the distance between two probability distributions depending on the kind of problem that you have you might need to use binary cross entropy categorical cross entropy or sparse categorical cross entropy but these are different implementations that you can find easily for example in the keras library for example let's say you have a one hot encoded classification for your model and that goes zero one zero so your model belongs your instance belongs to the second class and if your outcome from your model is let's say 0.5 0.95 and then 0.0 that means that your model has classified it with really high confidence to be the second class but not fully so that's why calculating the distance or the difference between these two distributions is important to understand how accurate your model is so let's talk about regression evaluation metrics the first and the simplest one is called mean absolute error and as you can understand from the name also mean absolute error is basically the sum of all the errors but their absolute values because if you do not take their absolute values the minuses the negative values might just cancel out the positive values and you might get an error or the mean error that actually looks very small that's why first you need to take all the absolute values of your errors and then sum them up together and take their mean another way to calculate the error for regression problems is called mean squared error and the way you calculate it is basically taking the square of all the errors and then taking their mean by doing that you are letting the greater error values to impact this mean squared error value more by exaggerating their importance by getting their square but this value sometimes the mean squared error value sometimes can be a bit hard to understand that's why instead we use root mean squared error and that is calculated by again taking the square of all the errors getting their mean and then getting the square root of this final value so it's basically the square root of mean squared error by doing that you're still exaggerating the importance of the greater error values but still you're bringing it down to the scale of mean absolute error metric so that it's easier for you to compare these two metrics and understand what might be going wrong in your model r squared or coefficient of determination gives us a good measure of how well your model fits this data so it's basically the metric of how much the real values vary from the curve that your model came out with so if you look at this example let's say this line is our the the is the curve that our model came up with and the dots are the values of the real values of the instances that we have in our data so if all the values lie perfectly on the curve that our model came out with our r squared value is going to be one whereas if our model does not fit the data at all our r squared value is going to be zero and for anything in between where our model is fitting a little bit better or a little bit worse we're going to get a value between zero and one so for r squared the higher the value the better between zero and one and lastly we have cosine similarity cosine similarity is very similar to cross entropy metric which is for which was for classification problems but cosine similarity is for regression problems because it can deal with real values cosine similarity tells us how similar two different vectors are to each other and in this way we are able to compare the predictions to the real values these are of course not at all all of the evaluation metrics that you can use in your machine learning problems if you go visit some of the documentations of some libraries like scikit-learn or keras or tensorflow you will see that they have many more different evaluation metrics or different implementations of the evaluation metrics that we talked about here or different types of evaluation metrics that are based on the ones that we talked about here so it's always a good idea before you start your project to do some research to understand what evaluation metric you're going to work towards so i hope this video was helpful to give you a quick review of the possible evaluation metrics that are out there and what you can consider using on your next project if you like this video don't forget to give us a like and maybe even subscribe or leave a comment with your questions or comments or any ideas of what kind of videos that we can make next we would love to hear from you and before you leave don't forget to go grab your free api token for assembly ai's speech to text api using the link in the description thanks for watching and i will see you in the next video"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Model Evaluation Metrics",
        "Video Title":"How to Evaluate Your ML Models Effectively? | Evaluation Metrics in Machine Learning!",
        "URL":"https:\/\/www.youtube.com\/watch?v=FXokoJUhsLQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/FXokoJUhsLQ\/hqdefault.jpg",
        "ID":"FXokoJUhsLQ",
        "Publish Time":"2024-09-03T03:34:12Z",
        "Channel":"AI For Beginners",
        "Channel ID":"UCj_ftUuFQwBOiKJicsE_72A",
        "Transcript":"how do you evaluate your ml models evaluation is a critical step in the model development process it ensures that our model is good enough to perform well on unseen Data before diving into evaluation metrics remember that the data should be divided into train test and often validation sets more on this here let's call one class positive and the other one negative we can arrange the predictions in four possible ways either we predict the positive class correctly or we predict the negative class incorrectly and either we predict correctly the negative class or incorrectly the positive one this is called the confusion Matrix a popular metric used in classification tasks from the confusion Matrix we can extract other metrics like accuracy accuracy measures how often a model predicts correctly out of all the predictions it made if we translate that to a formula we will need to divide the total number of correct predictions by the total number of predictions let's say if a person has a certain flu we predict positive if not then negative the flu is very rare and of 100,000 people only 100 have it our model learned to classify all observations to the negative class meaning none of them has the flu when we calculate the accuracy score we get a very high score however our model is useless imagine telling a person that he has no flu when he actually has it is generally better in such cases to classify the person having the flu when he doesn't than to miss a case where someone does have the flu but the model says they don't two more formulas appear here recall and precision their difference is in the denominator in our problem terms Precision has false positives in the denominator meaning a high Precision value will mean the model does not predict flu when the person is healthy recall on the other hand targets false negatives meaning if we have very high recall the model identifies everyone who has the flu and does not miss anyone who is sick there is another measure that tries to maximize both recall and precision called F1 score it takes the harmonic mean of precision and recall meaning that you can get a high F1 score in the case you have high precision and recall thus we use recall precision and F1 score for imbalanced data sets while accuracy for balanced ones for multiclass scenarios the method is slightly different we will refer to it later there are also other important metrics such as Au and Roc curves for regression and unsupervised tasks the metrics are different they are more complex ones and we will talk about all those in the upcoming videos so stay with us if you want to learn more about artificial intelligence subscribe to our channel to be aware of the new videos press the like button and let's discuss AI in the comments section"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Model Evaluation Metrics",
        "Video Title":"Machine Learning Model Evaluation Metrics",
        "URL":"https:\/\/www.youtube.com\/watch?v=wpQiEHYkBys",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/wpQiEHYkBys\/hqdefault.jpg",
        "ID":"wpQiEHYkBys",
        "Publish Time":"2019-05-08T16:47:44Z",
        "Channel":"Anaconda, Inc.",
        "Channel ID":"UCND4vKhJssAtK8p1Blfj14Q",
        "Transcript":"as has been mentioned I basically self educated myself in machine learning and data science and even though I have a I have graduated from Applied informatics program at my university it was a long time ago and most of the practical knowledge comes from courses pet projects competition that sort of thing and one of the things that was confusing for me was evaluation metrics why there are so many what's the point what's the difference what what do they mean and today I'm going to try to save some of the confusion to those who are relatively new to machine learning so what's an evaluation metric just a quick recap it's a way to quantify performance of machine learning model and it's basically a number that tells you if it's any good and you can use this number to compare different models it is not the same as loss function although it can be the same thing but it doesn't have to be so basically the difference is that evaluation metric is something I mean what's function is something that you used while you're training your model while you're optimizing while evaluation metric is is used on an already trained much machine learning model to see the result as if it's any good and today I'm only going to focus on supervised learning metrics when you have only have 40 minutes I'm not going to cover them all there's a lot of them but these I think are the main ones to know to understand them and I'm gonna give you an idea of what they are and how they differ we're gonna start with classification metrics and to make things a bit simpler I'm gonna start with a binary classification problem in the first metric that you encounter on your machine learning journey if you're doing classifications of course accuracy and it is a number of correct predictions out of total number of predictions it's super easy to understand it ranges from zero to 100% or zero to one and it's very intuitive you can easily get it in scikit-learn for any classification with a score method so the score method for estimators in scikit-learn gives you an evaluation metric and for classification it is accuracy so I build some logistic regression on some test they some some sample data and I get almost 96% well this looks amazing but that it may not be that good we don't really know because we don't know the context and I'm gonna show you why this is not necessarily a good thing in this case I'm gonna use the same data set to build dummy classifier and a diamond classifier and scikit-learn is something that doesn't learn anything from the data it follows a simple strategy you can generate predictions uniformly at random or it can in my case just pretty the most frequent value it or the most frequent class it has seen in the training data and in this case I'm gonna get 94% accuracy and this is because the data the data is highly class in balance I made this artificial data set with the 10,000 samples and here you can see that only 95% of the examples are positive and the 5% are negative so simply saying that everything is positive we're going to get 95% accuracy in this case 94 is probably because of the way that the data gods played so 96% accuracy is good we don't know is it a good model is it a bad model in if we don't know what our data looks like we cannot say if it's a good number or not and even if it was balanced data we and we wanted to improve from just this number we cannot know what the errors the model is making and what to do with and how to improve but luckily there is a lot of other classification metrics that we can use and diagnostic tools and we're gonna start with confusion matrix confusion matrix is table basically matrix that gives you numbers of how many samples your model classified correctly for what they are and how many it means on mistook for something else it's technically not a metric it's a more of a diagnostic tool but it has to get insight into the types of errors your models making and it also helps to understand other metrics that are derived from it so that's why we're going to look at that here's another example I built a basic random force classifier and got eighty-three percent accuracy on it so to understand what errors my model is making I'm gonna build a confusion matrix and to do that you can simply import confusion matrix from scikit-learn dot matrix and a convention is that first you pass their actual values to a metric and then you pass their predictions and you get this beautiful array which if you haven't seen the confusion matrix before can leave you confused and to understand this better typically confusion matrix are also represented as tables just like this one for example so in by convention on the rows to get the actual values and in the columns to get the predicted values this is the convention that is used in scikit-learn and for example in tensorflow but be careful because other tools may be using a different convention and it's gonna be the other way around so even for example a wikipedia page has a the other way around so sometimes tutorials or articles on this cannabis lady and you can you need to pay attention to what's where so we can learn a lot from this matrix for example in diagonal we have the true negatives and true positives true negatives are the values that were predicted negative and they were negative and well true positives is when it was predicted positive and it was positive we also yeah false negatives false pas and by summing up the true negatives and true positives and dividing by the total number of predictions we can get the accuracy but I wouldn't be showing the confusion matrix just to show another way to get the accuracy there are other metrics that we can get from from it let's say we're building a spam filter or a recommendation system something for a user where we if we say that this is something that you want and they don't really want it they're gonna be super annoying if we see something or if we put some important email in the spam folder and it's not as spam again users will be really pissed so we don't want that so we care about false positives we don't want to say that this is the thing that you want when it's not and in this case one metric that it was going to be helpful for us it's precision precision is calculated by dividing through positives by the sum of true positives and false positives so if we don't have any false positives this gonna be one and if we had some false positives and then we're gonna manage to reduce the number we're gonna have we're gonna improve the metric and it's gonna be a bit closer to one now another case is for example if we do something with medical diagnosis and we don't want to send people home saying that they don't don't have a disease but they do actually in this case we are carrying more about false negatives and in this case we need recall and it's very similar to precision in key but in this case we care about false negatives and about having as little of them as possible and there is another way of summarizing the confusion matrix in one number taking into account both precision and recall and it's f1 score which is just a harmonic mean of the two so depending on your business problem you might want to care not not so much about accuracy accuracy but not or not only about accuracy but also about precision if you want to minimize false positives or recall if is false negatives and these are actually unlike confusion matrix these are actually numbers which you can compare and you can get them easily from scikit-learn dot matrix the same ways we got the confusion matrix by passing the actuals and the predictions and we can use them for example in grid search if we want to choose a model that gives us better recall we're gonna put a scoring parameter recall and in this case it's going to get us the estimator that's going to give us the best recall among the possible versions of hyper parameters another way to summarize the confusion matrix is called Matthews correlation coefficient and what's important to notice about this formula is that it takes into account all four cells of the confusion matrix and this is important because it gives it it makes it different from f1 score and has some nice properties I'm going to show you with examples so let's say we have this data we have a hundreds of samples 95 of them are positive 5 or negative and we're just going to use them in classifier so we get 95% accuracy on this now we're gonna calculate f1 score and it's going to be even better than accuracy is going to be 90 0.97 and then MCC it's gonna be ante fide scikit-learn is going to return a 0 and give you a huge red warning because you were dividing by 0 and this is good because this is the only metric so far that gave you any red flag any indication that something's fishy is going on with your model and it it is because it's a dummy classifier there's nothing there's nothing good about it now another example let's say we have the same data but now we managed to classify one negative so it's a little bit better we get a f1 score of 0.9.5 2 we get MCC of 0.135 but what we're gonna do we're going to say okay now what we call positive we're gonna call negative the data is the same is just a way where you say like this is a positive class this is a negative class we're gonna use the same model and we basically just flipping the confusion matrix everything is the same but everyone's chord changes for this model and MCC doesn't and this is precisely because f1 score takes into account true positives false positives false negatives and does not care about true negatives so f1 score is very sensitive to what you call a positive class and what you call a negative class while MCC is not so far and I feel that if you want to summarize a confusion matrix a number for a binary problem MCC gives you a better feeling of what's going on but unfortunately there's of course the downside it doesn't really extend the well into a multi-class problem and so far we've looked only on the metrics that take into account whether prediction was correct or not but a lot of classifiers they generate probabilities of a cloud of an example belonging to one class or the other and there of course metrics that take that into account for example one of the popular matrix is ROC curve which stands for a receiver operating characteristic and to be honest this is one of those cases where knowing what the abbreviation stands for it doesn't really have to a normal person to understand what it is and this is basically a plot where you have false positive rate on the eggs and true positive rate on the y-axis and why is it a line was it a curve why is it not a dog and this is where the probability thresholds come to place to play when we have a probability generated of an example belonging to one class or the other it's by if all this 50% there's a decision threshold where we decide okay if it's if the probability is larger than 50% then we're gonna say it's a positive class and you can actually change this threshold you can move it around you can say okay we are only going to say that it's a positive class if the probability is higher than 60 70 80 and if you move you're gonna this threshold number of true positives and true negatives is going to change because if something was identify a class a' fide as positive at with 60% probability and it was if you move the threshold to 80 percent is going to be misclassified as false negative so what that is exactly what's going on here we're going to be moving the threshold and we're gonna be for each at each threshold calculating the true positive rate for the model and the false positive rate I'm gonna plot it as a dot I'm gonna connect them with a curve and the next question is if it's a good curve or a bad curve and to me was helpful to think of best case scenario the idea unicorn sort of model that can perfectly split the two classes without making any mistakes and in this case the true positive rate for such a model will be one and the false positive rate will be zero because we're not going to have any false negatives and in this case we're not going to have any false positives so and when we move Thresh or one way we can be ly really close to this upper border of this graph and then changing the threshold and the other way we're going to be moving this way so it's going to be really really hugging that corner and what we want our model to have is the curve really really tightly closed to that corner button again it's a plot it's difficult to compare automatically without looking models based on the plot that's why there is a metric called area under the curve which basically just calculates the percentage of this plot that's under this curve when you understand what the curve means and what it does it's a lot easier to understand this metric but this is not the only curve that you can use there is also a precision recall curve which is follows exactly the same principle where you move the threshold and then you plot precision and recall in this case and then you get a curve and you can calculate the area under it again why there is one curve where it is another curve in this case again it makes sense to look at the data and see if you have a class imbalance they just said I'm going to use I'll be using exactly the same example you get the probabilities and plot the curves and as you can see for again for a class imbalance that I said ROC gives 92 and for the precision recall is only 57 almost 58 so again this a lot depends on what kind of data you are dealing with there is also log loss is another way of assessing the performance of a machine learning model that generates prediction sorry probabilities of as well as predictions and it also often used as a loss function of course and it takes into account uncertainty of model predictions here you have yy4 in binary case is going to be for the true labels if it's a zero one and the logarithm of probability the minus sign before the formula is to make it a positive number because logarithm of something smaller than one is going to be negative so it's going to be difficult to compare models with with negative numbers and intuition for log loss is easier to plot than to explain with some words so if you have a prediction that is if you have a to label of one and predictive probability of class belonging to one you're going to have a very small log loss but the more wrong your predictions are the more confidence your model is in wrong predictions the log loss is gonna skyrocket so you would care about log loss when you not only care about the accuracy of your predictions but also about how confident your model is in the prediction predictions it make makes so we've talked about some metrics for binary problems and let's see how they can be extended to multi-class classification problems and you can plot the confusion matrix for multi-class problem in this case it's the digits data set so numbers 0 to 9 actually the handwritten classes numbers and the classes that they represent you can have the same story with a true label on in the rows predicted in the columns the diagonal is going to be representing the correctly classified examples and but then you can actually diagnose and see like that there's eight is mistaken for one or there is two that's mistaken for three for some reason and you can make some adjustments based on what you know about the data in some cases something will be expected in some cases you can see indication of something is really going wrong with the model you can also get precision recall and f1 score but for multi-class problem the notions of true positive to negative and so on don't really apply directly but this matrix can actually be extended to a multi-class problem by calculating them per label and an average in them and the three there are modern more than three ways of averaging but I find that the more commonly used are the macro average in micro averaging and weighted averaging I'm going to show an example of how they're calculated for precision for instance because I used the same way of the same principle how their average is calculated so we're gonna take this tour example and we're gonna make it confusion matrix and then we're going to calculate each metric for each label sorry for each class I'm going to say we're gonna treat them as one versus all problem me and that if it's for example for a bird a bird that we're gonna calculate the true positives for a bird it's gonna be this cell this is where we said it was a bird and it was a bird that false positives are going to be where we said it was a bird so we predicted it was a bird so this column right under but it was not we're given some of these up and this will be false false negatives where it was actually a bird but we said there wasn't so we're gonna do this the same thing for all classes and we're gonna calculate the totals as well now to get micro-macro and weighted average which is going to add the number of samples here as a column and micro precision is going to be calculated by just using the total numbers in this manner every sample equally contributes to the average and the equally represent each sample is equally represented in the average and then for a macro precision we're going to calculate the precision per class just for example for a bird it's gonna be a 1 divided by one plus one it's one for cat 4 divided by 4 plus one is 0.8 and we're just gonna take the average of that column and this way every class regarding regardless of its size is going to contribute equally to a macro precision and weighted precision is similar we're gonna again precision calculated per class but then we're gonna weight it by the number of samples about how they are represented in the data so just micro average like I said before it allows you to make sure that all samples equally contribute to the average and micro average allows all classes to equally contribute to the average while weighted average weights each causes contribution to the average and when would you want what the well that largely depends on your data and again a bit a bit on a business problem if you have a class imbalance data and you have one class that is underrepresented but you really want to get this one right you wanna you maybe want to use macro average to make sure this this this classes contribution is amplified and it's on the same level as for other classes scikit-learn documentation recommends using micro average for a multi labeled problem I personally haven't done much with multi level problems but I'll trust them on this and a multi class log loss is actually a more general case of a binary log loss and the intuition is exactly the same and the formula looks a little bit different it's a general case it's a general case but it's essentially the sum of we're gonna go through every sample and through every possible label and for every every time we're going to be taking Y which is whether this label is correct for this sample a binary indicator and we're gonna multiply it by the logarithm of probability of this label being right for this sample to me it makes it easy to understand a metric like this to just try to try to code it and in this in my case I just used for loops to show like to make it intuitive but of course in psychic learn it's done with vectorized operation it looks it's a little more optimized what I think so but for me it helps to understand it this way now I'm gonna switch to regression matrix and I find that regression matrix are a little bit easier because you're not dealing with probabilities and you only have a continuous value and your prediction and continuous value subtract one from the other you get residuals and now the question is how to evaluate a model based on all those individual residuals and if you remember for all classifiers scikit-learn gives you a default evaluation metric of which is accuracy and four regressors this would be R squared also called our coefficient of determination you the same way as you would get accuracy with square math that you can get R squared with a square metal four regressors r squared shows you how well model predictions are practicing my approximately the true values and is going to be one for a perfect perfect fit and it's going to be zero for a dummy regress or prediction average and why is that so this actually stems from the formula and at the numerator we in the numerator here we have the sum of squared residuals so the difference between the actual and the predictive value and then the denominator we're going to we have the squared distance from the actual values and the mean and if you think of it if you have a model that just predicts to mean that at the top and the bottom parts are going to be exactly the same so this will be 1 minus 1 is going to be 0 and if your model is doing even slightly better than predicting the average then you're gonna have the shorter distance between the actual values and the prediction values compared to the distance between the actuals and the mean and this will be somewhat closer to the whole matter who is going to be somewhat closer to one but you can of course go negative if you rent if you predict infinity for everything everything is going to be one minus infinity and it technically it can be negative but that means that something is very very wrong with the model and the good thing about this matter is that is that it has an intuitive scale it's kind of like accuracies you get the percentage value and doesn't depend on your target units which can be a good thing but it also doesn't give you any information about the prediction error how far actually your predictions are from the actuals there's a lot of metrics which give you exactly that because oftentimes you care about the error that a model is making and the most intuitive one is the mean absolute error it's basically the average of absolute value of the residuals and you can get it again as scikit-learn we from dot matrix there's another way of summarizing this getting the average of the residuals using instead of absolute value squared values but then you lose the the metric is now not in the units of the target value base instead in the squared units of the target value that's why more commonly you would see our MSC being used which is basically just a root root of the mean squared error and the the M mean absolute error and root mean squared error have a lot in common they range from 0 to infinity and they have the same units as Y values so you can see the error that your model is making in the units of the target values they don't care about the direction of the errors because of their the values of residuals are u squared or taken or have the absolute value and you want to this metric to be as low as possible but they're different of course and our MSE gives very relatively high way to large errors because the residuals are squared before contributing to the average and this makes absolute error more robust to outliers because they're gonna because they're not squared aramis e is often used as a loss function because it's differentiable but for an evaluation metric it doesn't matter this much and there's a lot of debate and there's a lot you're gonna see a lot of tutorials saying no you should use mean absolute error be of not our MSE for evaluating new metric because our MSE isn't appropriate and it's me misinterpret the air and these articles are mainly based on the paper that was published in 2005 by Wilma and Matt Sorum however a bit later in 2009 there was another paper by chai and Draxler that argued that actually it's not the case and then our MSE is perfectly fine metric and in fact sometimes can be better than the absolute error especially if you expect the error distribution to be Gaussian which is often the case and it's important to know that neither of this metric is going to be good enough on a small test set when you have less than 100 examples but in practice most of the time you're going to have more examples in there so that's okay and in practice I find that our MSE is completely fine metric to use and if you really really want to downplay the outliers you can use absolute mean absolute error to as a second metric to and but for most cases our MSE seems to be doing well and there is one version of it which also is quite often used as a root mean square logarithmic error which is very similar to our MC except instead of wise you get a logarithm of Y and there's a small technicalities Y there is plus one and the good thing about this matrix metric is that it shows a relative error and this is important in cases where your targets have an exponential growth for example if you have if you're particularly prices and they have a wide range if you want to error of final five dollars for 50 dollars be quite a large error where else you wanna if it's a five dollar error on a fifteen thousand dollar price it's not a big deal so logarithm helps you to level these things and has an interesting side effect it also penalizes under predicted estimates more than other predicted which is also sometimes useful so I've rushed through a lot of metrics both for classification problems and for logarithmic problems of sorry regression problems and I don't expect you to we understand them all but I what I want you to take away from this talk is that there is a reason for this variety and there is no metric that's gonna feed all the cases you need to get to know your data you need to understand what how many outliers do you have if it's a miss class imbalance data or it's not without understanding your data your metric may not make sense and more importantly you have to know your business problem and understand what your model should care more about because in some cases one metric is going to help you more to achieve your final goal in some cases is going to be another so you always need to start with knowing your data and knowing your problem thank you I've put some possibly helpful links here [Applause]"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Model Evaluation Metrics",
        "Video Title":"Evaluation Metrics for Machine Learning Models | Full Course",
        "URL":"https:\/\/www.youtube.com\/watch?v=lt1YxJ_8Jzs",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/lt1YxJ_8Jzs\/hqdefault.jpg",
        "ID":"lt1YxJ_8Jzs",
        "Publish Time":"2023-02-15T12:30:25Z",
        "Channel":"Analytics Vidhya",
        "Channel ID":"UCH6gDteHtH4hg3o2343iObA",
        "Transcript":"in this video we'll learn about evaluation metrics used for classification problems before we dive into that it will be helpful to define a formal way of representing the values predicted by a classification model and the actual values in the data set this will add a lot more clarity to our evaluation process for this purpose we use a confusion Matrix the word confusion might be there in the name but don't worry it's easy to grasp when you understand the intuition behind a confusion Matrix so let's begin a confusion Matrix is a n by n Matrix where n represent the number of classes in the variable in the Titanic data set our dependent variable had only two classes one and zero representing survived and not survived respectively therefore in this case the value of n will be 2 and the confusion Matrix formed will be a 2 by 2 Matrix note that there can be multiple classes in the dependent variable but for this video we are only going to use a 2 by 2 Matrix or a binary Matrix for easier understanding of the concept now each row in this Matrix represent the actual class values and the columns represent the predicted classes each row represents a class under the actual value and each column represents the class under the predictive value each cell holds some value and each of these cell represents some meaning that also decides the name of a particular cell let's discuss this in a bit more detail let's take a step back and first focus on the values predicted by our model as this is a binary classification problem the model can only predict two values positive and negative or as you can say 1 and 0. if we look at this column on the left this column represents the values predicted positive by the model therefore let us write p on those cells so here it is similarly the column on the right represent the values predicted as negative wire model once that's done the next step is to check the predicted values against the actual values The observed or actual value in our data set can either be positive or negative right since this is a binary classification problem now consider that the classification model predicts the value as positive which implies that we are in the left column so we need to check whether the value is predicted by our model are actually positive let's say the value predicted by my model is 1 and the actual value is also 1. so in this case the predicted value is correct or true so in the cell positive positive we can write T now if the value predicted as positive by the classification model but actually the value is negative in this case the prediction is incorrect or we can say false so in this cell we can write false since the model has predicted positive but actually the value is negative so here are the names of these two cells similarly the question comes are the values we predicted as negative actually negative if yes then we can say that the model predicted negative the actual value is negative so the prediction is correct or true on the other hand if the model predicted negative but the actual value is positive I can say the prediction is incorrect or false so the total number of actual positive values is given by true positive and false negative while the total number of actual negative values are given by false positive and true negative now that we have understood the notation what they mean and how to write them let us solve an example to solidify our understanding I'll pull up the Titanic data set again since we are quite familiar with it by now here we have a few examples from the actual data set and we know that whether the passenger on the Titanic has survived or not the values are given in the actual column now suppose we build a model and it produces these given predicted values as in this column here comes the notations we learned in the video in order to compute the confusion Matrix we'll first denote each observation so for the first observation the predicted value is 0 which means n but the actual value is 1. so this prediction is incorrect or false similarly for the second observation the predicted value is 1 so positive and the actual value is also 1. so this prediction is correct or I can say true similarly I can fill for all the observations and here are the notations for all the observations so what's next count the number of true positives false positives true negatives and false negatives we'll start by counting the true positives so here I have three true positives so I can write three in this cell now let's count the number of false negatives so I have two false negatives here so in this cell I will write 2 and the next let's look at false positive so I have only one false positive here so in this cell I'll write one and now let us count the number of true negatives so I have one and two true negatives here so in this cell I can write 2 and there we are congratulations on building your first confusion Matrix let's quickly recap what we have learned in this video we saw what a confusion Matrix is and how we can represent the predicted values against the actual values we also covered what each of these notation means in the confusion Matrix so if we have the actual values as positive and the model correctly predicts it as positive then I can say the prediction is correct and it will be long it will fall in true positive similarly a correctly predicted negative class is labeled as true negative the false positives represent those values which were mistakenly predicted as positive and similarly for the false negatives these are the values which were actually positive but mistakenly predicted as negative in this video we also learned how to denote each observation on the predicted label and the actual label and how to fill the confusion Matrix using these notations now let's turn our Focus to the first and most popular evaluation metric used for classification tasks accuracy to put a formal definition to accuracy it is the ratio of the total number of correct predictions over the total predicted values if we look at the confusion Matrix that we studied previously the true positives and the true negatives here are the correct predictions which means were predicted positive by the model and are actually positive or the values predicted negative by the model and were actually negative so the accuracy can be denoted as the true positives plus true negatives the corrected predictions over total predictions so the total predictions would include all these four values right so I can write accuracy as true positive plus true negative upon true positive false negative false positive plus two negative pretty straightforward right let's take up an example to understand how accuracy works suppose there's a group of 500 people who undergo a cancer diagnostic test six patients are tested positive that is it's detected that six of these people have cancer whereas the other 494 were cancer-free intuitively we can say that the patient suffering with cancer are very Less in number as compared to the people who do not have cancer so this is a classic example of imbalanced data now our job as data scientists is to build a predictive model that can predict whether a patient has cancer or not this model would take inputs such as age gender previous medical history of the patient the tumor size the the tumor color and a lot other things so let's assume we built a model and have a confusion Matrix ready as shown here take a moment to pass through this what's your first reaction there are some big values and some small values in the confusion Matrix our first intuition would be to calculate the accuracy of our model just plug in the values to this framework we defined previously so I'll have here true positive as 4 and true negative as 486 so 4 plus 486 upon the total predictions so that would be 4 plus 8 plus 2 plus 486 right now on calculating the accuracy comes out to be 98 percent looking at the model accuracy we can easily conclude that the model we made is performing extremely well right well not exactly in order to understand why accuracy is not sufficient for this type of classification problem let us consider another example an example of a dumb model so we again consider the same 500 people but this time we build a dumb model which predicts negative for every patient every time if we look at the confusion Matrix for the dumb model it will look something like this we can see that the model has predicted all the values as negative now if we use the accuracy formula and input the values from the confusion Matrix we'll have 0 plus 494 upon all the values so I'll have 0 plus 6 Plus 494 again now if we calculate the accuracy comes out to be 98.8 percent what just happened did a dumb model manage to Trump a predictive model we simply used the way of Brute Force to predict every patient as negative or maybe the predictive model we trained is worse than the dumb model that is not possible Right in reality using accuracy as an evaluation metric on unbalanced data is not a great idea as we saw in the example here in order to evaluate the classification model properly we need to look closely at the confusion Matrix to find some better alternatives in this section we'll take a closer look at the confusion Matrix in order to find the alternatives to accuracy metric we'll start with the true positive rate so the true positive rate is defined as the value is predicted as positive by the model which were actually positive versus the total actual positives so looking at the confusion Matrix the true positive rate can be written as the true positive over t p plus FN the total actual positives as you can see this metric emphasizes on the true positives predicted by the model so it can be used in case where the business objective is to focus on identifying the actual positives like out of the total positives in the data what was the percentage of positives your model was able to identify correctly since higher value would mean more number of positives correctly identified this metric indicates that higher the value better the model and the range of this metric varies from 0 to 1. now the next metric which we'll look at is the false negative rate it is defined as the ratio of values predicted as negatives which were actually positive so the false negative over the total actual positives so looking at the confusion Matrix again we can formulate the false negative rate as here the false negative upon true positive plus false negative now this metric emphasizes on the false negative values which means that where were we not correct in terms of predicting the positives in the data so lower the fnr represents a better model next one is the true negative rate now it is very similar to the true positive rate but the focus here is on the negative classes so it is the ratio of the predicted negative values which were actually negative so the true negatives over the total actual negatives so we can formulate this as true negative upon true negative plus false negative now here the focus is on the true negatives so the total number of negative classes in the data which were correctly captured by the model are the true negatives and the business objective of this metric would be to focus on identifying the actual negatives so what percentage of negative values your model was able to correctly identify out of the total negative values in the data this Matrix again ranges from 0 to 1. the next we will see is the false positive rate it's the ratio of values predicted as positive but were actually negative so the false positives here divided by the total number of negative values so here the focus is on false positive now this metric also says that our model has wrongly classified the actual negative classes to positive classes so lower the value of fpr better is the model Again The Matrix ranges from 0 to 1. now we see that we have derived four metrics from the confusion Matrix let us discuss about each of them in brief in any given model the true positive rate is always desirable so higher the value of true positive rate better is the model since we are focusing on the correctly predicted positive values also lower the value of false negative rate better is our model since false negative rate signifies the error rate of the model in case of true negative rate the higher value of true negative rate is desired since the true negative signifies the correctly predicted negative values on the other hand false positive rate is often desired to be tending towards zero as it is an error on behalf of the model so the lower the value of false positive rate better is the model let us now revisit the dumb model again where the model predicts every person as negative that is nobody has cancer we remember that the accuracy of this model came out to be 98.8 percent now let us check how the model performs with their new defined evaluation metrics let us calculate the tpr so using the values from this confusion Matrix here I can Define tpr the true positive as 0 upon 0 plus false negative which is 6. so this comes out to be 0. the true positive rate of the dumb model comes out to be zero this means that the model is not performing well since it has not correctly identified any of the positive values this model doesn't look good anymore does it similarly when we calculate the other metric we can find that the false negative rate is 1 which means all the cancer patients were classified as negative which is the worst possible model the false positive rate is zero this implies that the patients who do not have cancer were not misclassified but again this is not relevant to the context since our focus is on identifying the patients who actually have cancer and the true negative rate comes out to be one so all the patients who were not having cancer were correctly identified but we just studied about four metrics and one could easily ask do I need to check all of them long answer short we can check all of the above metrics but it's not necessary to do so using a combination of two metrics often works best for instance aiming for the true positive rate and the true negative rate is enough these two metrics removes the need of checking any other metrics the other metrics can be used in a combination so as to validate the results and not relying on any single metric but choosing the right metric for a model totally depends on the use case different domains require optimization for different metrics for example the the cancer patient problem that we talked about the focus was on identifying the positives correctly so we should use the true positive rate as our evaluation metric one should understand the business requirement talk to the relevant people about what's important for business and then select the right metric it's now time to see some of the most common use cases in data science and the metrics we use in these scenarios first up is the concept of precision it is defined as the ratio of the values predicted as positives that were actually positives so as we already know these are the true positives over total predicted positives so using the confusion metrics we can write this as the true positives over all the predicted positives so it includes true positive and the false positive so here we have the formula for precision true positive upon true positive plus false positive we'll take up an example to get an intuition of this Precision concept so let's use the cancer detection problem since we are already familiar with the problem statement we'll reconsider the predictive model we built earlier previously the accuracy we obtained for this model came out to be 98 and we discussed that accuracy is not the best metric in this case let us now calculate the Precision metric for the same model so using the formula we studied previously and the values from the confusion metric that we have here I can write precision as 4 upon 4 plus 8. which comes out to be 33.33 well the model doesn't look so precise now does it now that we have a basic idea about the evaluation metric precision let's look at a case where we will need to use precision as the evaluation metric in this example you are a detective tasked with catching a group of criminals and you know that those criminals are attending a party full of very important people as you can imagine declaring these important people as criminals can have dire consequences for your agency so as a detective you can only arrest the criminals to be perfectly clear avoiding the wrongful arrest of these VIPs takes precedence over catching these criminals in order to do that we must minimize the VIP we declare as criminals so take a moment to ponder this avoiding arresting a VIP as a criminal is analogous to minimizing the false positives but in order to minimize this false positive it is essential to understand that there is a very high chance of a criminal escaping and not being caught this means that the false negative value may increase in a nutshell Precision is used when we cannot afford to have false positives or avoiding false positives is more important than encountering false negatives so I hope you now understand what is precision and what is the use case where Precision can be used as a valuation metric let us turn to the second evaluation metric we'll learn in this video called recall this is also fairly commonly used in the industry recall can be defined as out of total actual positive values how many did the model predict as positive let us use the confusion Matrix to understand this better so we have the total actual positive values which is the true positive against the total positives so that is true positive and false negative so here is the formula for recall again we'll take up an example to explore how recall works so we'll evaluate the cancer detection model again this time we'll use recall as a metric in mind let us input the values from the confusion Matrix into the formula here so I have four true positives plus two false negatives so this comes out to be 66 percent again the accuracy metric turned out to be misleading like we did for precision I am going to take up a use case to show how recall works in the real world scenario consider that you are at the airport and passing through the metal detector the aim of the security is to catch all the people who are carrying weapons in order to catch every person who is carrying a weapon the security guard will check any person for whom the metal detector Rings irrespective of whether he or she is innocent or not in other words it is important to catch the weapon carrier then check your innocent person for weapons if we look at the definition for recall trying to catch every weapon carrier is analogous to minimizing the value of false negatives that is no weapon carrier should get by the price of trying to catch all the weapon carriers leads to innocent people being checked by the security which is analogous to high false positive rate so in a nutshell recall is used when we cannot afford to have false positives or avoiding false negative is prioritized over encountering false positives so we have learned both precision and Recall now is there any direct relation between them think about it for a moment as you can see here this happens to be the trade-off between the Precision and recall note that this trade-off is often very small for instance 70 to 90. so if we aim for very high Precision that is trying to strictly predict the true positives and avoiding false positives there's a good chance that the false negatives will go higher therefore the recall will drop to a lower value similarly if we aim for a high recall that is trying to predict all the actual positives as true positives and avoiding false negatives there's a good chance that the false positive will go higher therefore the Precision will drop to a lower value makes sense right I just want to reiterate that precision and recoil are used in diverse cases and prioritizing one over the other depends upon the use case you might come across scenarios where prioritizing Precision over recall or vice versa is not clear in this case they both can be combined in order to obtain a good model that's right the combined evaluation metric is known as F1 score F1 score is the harmonic mean of precision and recall since we took the mean it reaches its maximum value when Precision is equal to recall a point to note is that F1 score lacks interpretability and therefore it is often used in combination with other evaluation metrics so far we have covered the evaluation metrics which considered only deterministic outcomes like 1 and 0. but there are some classification predictive models which predict probabilities of classes rather than the class itself these cannot be handled by the evaluation metrics we learned so far we're quite familiar with this confusion Matrix we have seen how it works how to interpret it and we also derived various evaluation metrics from this like the true positive rate the false positive rate true negative rate and the false negative rate let's quickly review these metrics so the true positive rate here is the ratio of correctly predicted positive values over the total actual positives similar to True positive rate we have the true negative rate which takes into account the correctly predicted negative values over the total actual negatives another metric that we saw was the false positive rate which was the incorrectly predicted negative classes over the total actual negatives and lastly we have the false negative rate which is the ratio of incorrectly predicted negative values over the total actual positives let us look at an example remember this table we used before these are the sample observations taken from the Titanic data set here we have the actual values and we also have the predicted values for our model now we can use these predicted values to determine the true positives false positives for each of the observation so let's do that I have one here which is a positive and the actual value is also 1 so it is a true positive similarly for this it will be positive and the actual value is 0 so false positive and so on and so forth we can do it for all the rows so as I've done here now we can count the total number of true positives false positives true negatives and false negatives once we have these numbers we can use it to calculate the true positive rate and false positive rate let's quickly do that so here I have three true positives so I can write three here and then for false negatives I have two false negatives so it comes out to be three by five which is 0.6 again if I want to calculate the false positive rate I'll count the total number of false positives which comes out to be 2 and the true negatives comes out to be one so I'll have this again as 0.66 so you can see how we can calculate true positive rate and false negative rate similarly we can do it for the true negative rate and the false negative rate I encourage you to pause the video and calculate these values at your end before moving ahead so far we use the predicted classes which were 0 and 1. now what if I have a model that is predicting probabilities of classes and not the classes themselves and suppose I want to use these probability values to understand how my model performs we know that we can't really compare these probabilities with the actual classes can we so is there any way to generate actual classes based on these probabilities for the classes think about it let's say I set a cutoff to 0.5 so the probability values which are greater than 0.5 or equal to 0.5 can be labeled as 1 and the other probabilities which are less than 0.5 can be labeled as 0. so if I do this for this problem I'll have 0.9 as 1 0.51 is again greater than 0.5 so it will be 1 then similarly this will be 0 and this will be 0 and so on so once I do it for all the predicted probabilities I will actually have the predicted classes after obtaining the predicted classes we know the steps right we can calculate which one of them is true positive which is false positive and so on and using the count of the true positive false positive true negative and false negative we can generate the true negative rates false negative rates Etc you know the formulas to calculate these right in case you face any difficulty I encourage you to watch the previous videos again so moving forward I can see that my model has predicted 4 out of eight values incorrectly for example in the second ID the predicted value is 1 but the actual value is 0. similarly here we have the predicted value is 0 and the actual as one and so forth for the id4 and id8 so what can we do to improve the model's predictions the first obvious solution would be to retrain the model which we usually do we can build a new model calculate the new predictions and then go ahead and calculate the tpr fpr rates but is there any way we can use the same word model can we play around with the threshold value remember we set it to 0.5 currently now let's try and change the threshold value to 0.4 and calculate these actual values so here I've done that we can now see that I have only three incorrect predictions once we have these actual values we can calculate the TPT and FP and FN and if I have the total number of true positives and false positives I can finally calculate the true positive rate false positive rate Etc using the formula we saw previously so in our case the true positive rate goes to 0.8 previously we had it 0.6 so I can see that the true positive rate has increased which means I have predicted more number of positives correctly now let's play around with the threshold value a bit more so let's say instead of 0.4 I set it to 0.6 this time in this case we have new actual values and I can again calculate the tptn FP and FN for these and go ahead and calculate the true positive of rate false positive rate Etc so we see that for each different threshold I get a different value of tpr fpr Etc what I can see here is that instead of changing the model I can try altering the threshold value and see if the model performance changes this is called thresholding we are familiar with how we can predict classes using predicted probabilities now the question is how do we evaluate our model based on that that's what we'll find out in this video let me introduce you to one of my favorite evaluation metrics the AUC Roc metric AUC stands for area under the curve let's look at some simple examples to give you an intuition about what area under the curve is given this curve as shown on this Slide the area under the curve would be this region now suppose you have a curve as shown here can you guess what would be the area under this curve it'll be this part right similarly if you have a curve like this the area under the curve would be this region so when we talk about AUC Roc it is basically the area under the ROC curve and what is this Roc Roc stands for receiver operating characteristic this name comes from signal detection theory that was originally used for distinguishing noise from not noise and now it's widely used in machine learning as an evaluation metric for binary classification this metric gives us the trade-off between the true positives and the false positives let us look at an example of what Roc curve looks like so here is an example of The Roc curve as mentioned previously The ROC curve gives the trade-off between the true positives and the false positives this is represented by the true positive rate and the false positive rate so on the x-axis of Roc curve we have the false positive rates and on the y-axis we have the true positive rates we know that false positive rate and true positive rate range from 0 to 1. so here on the axis we have 0 to 1. so you can imagine this as a box of size 1 therefore its area will be one unit this is pretty important to keep in mind now moving forward if we make completely random predictions the ROC would look something like this line here so the area under this curve would be 0.5 right since the total area is 1. so as we understand that the area under the line would be 0.5 now for a model better than this random model something like the example shown here the area under the curve would obviously be greater than 0.5 so that's the intuition behind the AUC Roc metric in simple words the more the area under the curve the better is the model now the question comes how is this curve plotted looking at this curve we can clearly say that we need multiple fpr and tpr values to plot and in the last video we saw how changing this threshold can give us different sets of tpr and fpr values so recall this example we used previously here we have the actual values 1 and 0 and the predicted probabilities as shown here we saw that changing the threshold generated different tpr and fpr values now the question is should we try every value in the rain 0 and 1 as a threshold that's not really practical so instead we can set each of these probabilities as thresholds and calculate the tpr and fpr values so first we will need to arrange these predicted probabilities in descending order then we need to take the first value as threshold and calculate the actual values as shown in this table once this is done we can calculate the true positive rate and the false positive rate using the formulas we studied previously so these three steps will give us one set of tpr and fpr values we can repeat these steps for the next value so we can predict the classes by taking 0.9 as threshold and these will be the classes then we can take 0.78 as threshold and here are the classes similarly we can take it for 0.56 Generate the predicted classes and move forward to calculate the tpr and fpr values once we have multiple tpr and fpr values we'll be able to generate the ROC curve now suppose for a particular example I got the ROC curves something like this I can calculate the area under this curve and the area under the curve would be an indication as to how good the performance of my model is pretty awesome right I have a question for you now can you use this metric to compare two models think about it it considers the predicted probabilities for determining our model's performance but this one issue with AUC Roc it takes into account only the order of probabilities and because of this we cannot use it to compare two models and that can often be a big drawback let us see why this is the case using a simple example so here I have the prediction probabilities of two different models already arranged in descending order now when we pick the first probability as threshold we'll have only this value here as 1 and rest all the values will be 0. in this case when we calculate the true positives and the false positives the number of true positive false positive to negative and false negative will be same and so will be the tpr fpr values again for the second scenario when we take the second value as threshold and identify the predicted classes these two values will be 1 and all the values will be zero on comparing these predicted classes with the actual classes to generate the number of TPT and FP and FN will see that they are again same in this case so the ROC curve for both these models will be exactly the same and if we calculate the area under the curve it would look like both the models are equally good but do you think these models perform equally well certainly not for example the first model is predicting this class as one with 0.94 probability while the second model is predicting with 0.74 probability similarly for the second scenario here the first model is predicting with 0.9 probability while the second model has only 0.69 so we can clearly see that model 1 is able to predict better probabilities for the actual classes so AUC Roc does not take into account the value of probability but it only considers the order of the probability and hence it's not a good metric to compare the different models so when we are faced with this scenario we lean on another evaluation metric called The Log loss that solves this problem to put a formal definition to log loss it is defined as the negative average of the log of corrected predicted probabilities for each instance now what do we mean by corrected predicted probabilities let us go back to our example in this table we have the actual classes 1 and 0 and the predicted probabilities if you remember this was the Titanic data set so this column says what is the probability that a person has survived or in other words what is the probability that this ID belongs to class 1. for instance here the probability of the class being 1 for ID 6 is 0.94 similarly for the second it is 0.9 and if you look at the last row the probability of id5 belonging to class 1 is 0.1 which means the probability of id5 belonging to class 0 will be 1 minus 0.1 that's 0.9 so this 0.9 will be the correct probability for id5 in order to obtain the corrected probabilities we consider each observation with the actual value 0 and subtract it so obtain probabilities from 1. so here I've done this for all the rows so you can see that these values are the corrected probabilities now that we have the corrected probabilities we need to calculate the log of these values and here is the formula for doing that so when we apply this to our data the last column shows the calculated log of corrected probabilities as you can see these values are negative to deal with this negative sign we take the negative average of these values so when we do that log loss comes out to be 0.21 in this case if I had to summarize all of these steps then instead of calculating the corrected probabilities I can change the formula and write it as shown here don't be afraid of this formula it's actually pretty intuitive to understand let's look at each component to simplify the matters here y i is the actual class and p i is the probability of 1. these I refer the I throw in the data now when we calculate 1 minus P of I this basically gives us the probability of zero and that is what we were doing when we were calculating the character probabilities so now considering this formula when the actual class is 1 or I can say when y i is 1 this part of the formula is used while this part vanishes since 1 minus y i becomes 0. on the other hand when y i is 0 or the actual class is 0 this whole part becomes 0 and only this part of the formula is activated where we correct the probabilities or Calculate 1 minus p i which gives us probability of 0. so log loss is pretty robust metric for comparing two models something which AUC Roc could not do evaluating a classification model where the target variable is discrete like 0 and 1 is still an intuitive process but what happens when we need to predict a continuous value how do we evaluate that model that's what we'll cover in this video we'll focus on evaluation metrics for regression problems where the target variable is continuous to give you an overview we'll be covering these metrics the mean absolute error the mean squared error root mean squared error root means squared log error r squared and adjusted r squared we'll cover the first four metrics in this video and the remaining two in the next one but before all that it will be helpful to Define what we mean by the term error in this context I'll pick up an example to explore this take a look at this plot the green line here is the regression line and these red dots are the actual values now this point here is some distance away from the regression line so is this point and so is this so this is the error for our model error basically determines how far our predicted values are from the actual observed values consider these observed and predicted values so to calculate the error I can subtract the predicted from the actual which means I am calculating this distance so here 28 Minus 19 would give me 9 then 33 minus 37 would be minus 4 and so on that's how the error is calculated this is for each observation now how do I calculate the overall model error one of the ways to do this is by taking the average of all of these individual errors so how should I do that we'll first add up all these errors and then divide by the total number of values but if you notice in this case we have some negative errors and some positive values so this will effectively cancel each other out and if you take a sum it will come out to be zero that didn't help right so what else can we do to calculate the aggregation as you know that one of the problems is that we have some positive and some negative values these positive and negative signs denote the direction so for example for this error it will be positive sign then for this it will be negative here it'll be negative and here it will be positive and so on so we can remove these directions and take only magnitude into consideration one way of doing this is by taking the sum of absolute values and then calculating the average this is called the mean absolute error so here we are taking the average of the absolute values so the negative sign disappears for a previous example the absolute values of error come out to be as given here now when we calculate the mean absolute error it comes out to be 5.6 as second method to remove the negative signs could be taking a square of this difference while taking the square all of these differences will be positive and so the sum won't turn out to be zero this is called the mean squared error or MSC and here is the formula but one problem with mean squared error is that since we are taking the square of the errors it changes the unit let's go back to our previous example suppose I have these values in meters so let's say the actual value is 19 meters 37 meters and so on now when I calculate the error and take a square the unit goes to meter Square so if I calculate the mean squared error in this case it comes out to be 34.4 meter Square this doesn't precisely represent our error right and this happened because we just took the square of the errors so to compensate this we can take a root of this error and we'll be back to our same unit as the target variable this new metric is called the root mean squared error all the rmse as it's commonly known so for the same example when I calculate the rmse it comes out to be 5.86 meters rmsc is one of the most commonly used evaluation metrics for regression problems you'll encounter this quite often in data science hackathons as well but note here even rmsc might not be the best metric for regression problems wondering why let me illustrate this using a simple example so suppose I have these two cases in the first case my actual value is 1 and predicted is four zero one something has clearly gone wrong in our model in the second case the actual value is ten thousand one and the predicted is 10 401 that's a pretty decent performance by the model now if I calculate the rmsc for both cases it comes out to be 282 that's right we have the rmsc for both cases equal this is because the rmse considers only the difference between the observed and the predicted values and the difference in both the cases as you can see is the same 400 so if we can't use rmse in these problems what can we do now instead of directly taking the average of square errors we can take a log of the values and then take the average of square of the errors using log basically scales down the larger values this metric is called the root mean squared log error or the rmsle so for the same example we saw in the last Slide the RM SLE in the first case is 2.65 whereas in the SEC second case it's point zero zero one four it's a far better metric right since we now know that this model is performing decently well while this is very bad that's a much more practical explanation of the errors we learned what the rmsc and MSC values are we also saw if the rmse decreases or the MSC decreases the model's performance will improve but these values alone are not intuitive why let's take a small example in case of classification problem if the accuracy is 0.8 we know how good our model is against a random model the accuracy of which is 0.5 so the random model here can be treated as a benchmark but when we talk about MSC or rmse metrics we do not have a benchmark to compare so is there any metric for regression where we can compare our model against a benchmark model yes there is recall the mean squared error concept we discussed in the last video here we simply take the average of sum of square errors now instead of using the MSC value alone we can look at it with respect to the Baseline model which means we should divide the MSC of model with the MSC of Baseline model now what is a baseline model it is basically the MSC when the predicted values are simply replaced by the mean of all values as you can see the 1 by n in numerator and denominator cancel each other out so we are left with the sum of squared errors which is called the relative squared error a few important things to note here is that when the MSC of model and MSC of Baseline model is equal in that case this value is 1. which means our model is as good as predicting the average for all the values if this value is more than 1 we can say that our model is worse than the Baseline model as you might have guessed ideally this value should lie between 0 to 1. now when MSE decreases this whole value decreases and we can say that the model performance has improved so lower this value better our model which is quite contradictory to the general idea that higher values are better is there any solution to this well here's the trick simply subtract the relative squared error which is this term from 1. this new metric is called the r squared so when the MSE decreases this whole value decreases which means we are subtracting a smaller value from 1 and in turn the R square will increase so better the model higher will be the R square value but there's a big problem with R square when you add more features to the data the value of R square either increases or does not change it never decreases regardless of how any feature impact the model so we are not penalized for any random or unimportant features that we add it's a very inefficient way of building models to deal with this issue we have a better version of r squared and it's called the adjusted R square here's the formula for adjusted R square and here is the number of samples and K is the number of features as you can see this metric takes the number of features into account so now when we add more features this term in the denominator would increase which means the whole fraction increases now if adjusted R square increases significantly the change in this fraction can be compensated but if this term increases and R square does not change in that case we are subtracting a larger value from 1 and so the adjusted R square would decrease so now we have a metric for regression that compares the model with a benchmark and provides a score between 0 and 1. that was all about R square and adjusted R Square thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Model Evaluation with Scikit-Learn",
        "Video Title":"Model Evaluation in Scikit Learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=yz0egHfZEeY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/yz0egHfZEeY\/hqdefault.jpg",
        "ID":"yz0egHfZEeY",
        "Publish Time":"2017-07-31T17:09:57Z",
        "Channel":"Data Talks",
        "Channel ID":"UCQTQ0AbOupKNxKKY-_x46OQ",
        "Transcript":"well here welcome to another episode of a bit of data science and scikit-learn we learned a little bit of data science and a whole lot of sideline we're talking about model evaluation and so please if you've not watched the previous episode on grid search please do watch that as well as cross validation these these are both very very important topics to talk about when we're discussing model evaluation now model evaluation is a bag a big bag of tricks and they're all used in basically the same way so let's sort of check this out when you're trying to evaluate a model there's tons of ways you can evaluate a model it's right here listed a ton of ways accuracy average precision f1f1 micro macro weighted samples negative flow of loss precision recall these are all for classification regression you've also got a ton of stuff a second learn hasn't made it so that being bigger is better so for regression you're looking at negative mean absolute error so these are all ways to score a model and why do you want a score model what do you need to score a model if you're trying to do hyper parameter selection is is one example if you're trying to do feature selection there's another example especially with recursive feature elimination so you can use any of these strings to to go ahead and use this metric so for example I'll do cross-validation scoring and I'll use negative log loss on an SVC so support vector classifier right and it prints out this negative log loss which is really nice if I use a wrong choice you'll throw an error as it as it does right down here nothing too crazy here you can do other things as well you can make your own type of metric the way that you make this is you use the make score and generally speaking you're gonna go ahead and have some sort of function that does scoring for you so for example I've got an F beta score it requires to have a sort of like a hyper parameter of being a beta you specify the beta being to use the make score and now you're able to have this new F to score so it makes force it's somewhat useful you can do this all the time you can even make your own custom so you can make score using this sort of like custom loss function I've never done this I can certainly imagine cases where this would be very useful especially if you're trying to do research but generally speaking there's a ton of scoring functions up here that are very very useful so I'd recommend reading through all these first before you decide to make your own okay so that's that's pretty much it in this scoring library there are a lot of other kind of cool functions that just want to show you there is the confusion matrix so given a set of truthful targets and a set of predicted targets you can make a confusion matrix the I always the truth is on the on the left and the predictive is on the bottom I believe predicted yes the predicted is on the bottom and the truth is across the top so this is saying this is what I predicted and this is the this is the truth so anything along those diagonals of qi+ anything off the diagonals is false it's not correct so for example here I I guess I predicted that this was a 3 and it was actually just 0 or I guess I predicted it was a 1 or a 0 and it was actually a 3 confusion matrix useful you can even extract the true positives and false positive rates from the confusion matrix using the stop rabble this will only work if you have a 2 by 2 confusion matrix classification poor report this is also awesome you you need white true and you need Y predict and you also some target names would be helpful in order to just have like a nicer you just gives you these class labels right over here this will sort of tell you what your what your f one's for precision recall are I love this I use this each time and the final thing that I want to show you this in this package is a dummy estimator these are like yeah if you ever want to compare to the the dumb baseline where the dumb baseline is like let's predict the average what's like whoa you don't even look at the the data just do that this this is basically what this package is here for so you got its most frequents maximizes the class prior uniform at random all sorts of different strategies we change s but the IRS will compare a linear support vector classifier versus versus a dummy classifier most frequent Wow it's not too different right and at this point we just got to be like hey you know we we this this can't be like they're right the right strategy and so we because you to look for the model and you find something that's much better so if you're ever looking for like a dumb baseline to compare your model against the dummy classifier is a good thing too so that's right there there's also dummy regressors okay oh this was quick easy and fun um always tune in next time and I'll see you again"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Model Evaluation with Scikit-Learn",
        "Video Title":"110 Evaluating A Model With Scikit learn Functions | Scikit-learn Creating Machine Learning Models",
        "URL":"https:\/\/www.youtube.com\/watch?v=gi848-bzMu8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/gi848-bzMu8\/hqdefault.jpg",
        "ID":"gi848-bzMu8",
        "Publish Time":"2021-03-20T04:30:13Z",
        "Channel":"Machine Learning",
        "Channel ID":"UC4zi1AT9TlP9Sy4TpahKpLg",
        "Transcript":"alrighty let's finish up this section on evaluating machine learning models so we can do so by tackling the third and final way to evaluate a machine learning model metric functions so in essence we've kind of already covered this because all the metrics we've previously seen have their own function in scikit-learn let's see what i mean by this so using different evaluation metrics as scikit learn functions 4.3 wonderful so to do so for classification we had accuracy we had precision we had recall we had f1 and then for regression we had r squared i mean absolute error and mean squared error all right so let's do what we always do and see the code first and then we'll talk through it so from sk learn import metrics i'm going to do a full example as we always do right because that's what we like to do we like to be complete with what we're working on precision score recall score you might be able to figure out what the one is for f1 score oh typed it a bit too quick so from sklearn we'll also import our model because what we might do is create a section here we might go classification evaluation functions now again this section here is just another way to do what we've done before so ensemble but it's good to practice it's always good to practice random forest classifier and then we want sklearn.model selection import train test split mp.random seed 42 lovely x equals heart disease dot drop we can almost write this in our sleep now y equals heart disease dot target and if you can't that is more than okay right then the reason i can write this sort of out is because i've had a fair bit of practice with it and you'll be the same too if you're starting out now you might be looking at all these functions all this code and going holy goodness there is so much to remember but the beautiful thing is is that it's here it's available for you you can run it and jupiter notebook and you can practice as much as you like so really your only roadblock is just putting in the work and practicing learning but don't forget learning something new especially machine learning takes time and it's not going away so you've got plenty of time so what we're doing here we've seen this code before importing some metrics specifically accuracy score precision score recall score f1 score and we're importing a model and we're importing train test split we're splitting our data into x and y we're splitting it into train and test sets we're instantiating a random forest classifier and fitting it to the training data beautiful writing machine learning code we'll make some predictions and then we'll go why preds because remember what is an evaluation metric doing if you said comparing our model's predictions to the truth labels to the actual labels you would be correct evaluate the classifier so now what we're going to do is we're going to take advantage of these inbuilt functions here right we could use score we could use the scoring parameter but we've already covered those this is using scikit-learn functions so evaluate the classifier what we'll do is we'll print out something nice maybe classifier metrics on the test set wonderful and then we're going to print out we'll do an f string accuracy is going to be we use the accuracy score function on y test and y threads wonderful then we're going to times that by 100 so it comes out in a nice neat percentage because i prefer that or we all prefer that than the decimals maybe you don't then we got here we're going to go precision now again we could function eye something like this we probably will in a future video but just for example's sake we'll type it out we'll practice typing it out that can stay like that oh it needs the end of a string there we go now we're going to do recall how would you do this one if i start you off with the f string that's all right we'll keep going so recall score y test remember just comparing our predictions to the test labels to the truth labels and then finally we're going to go f1 so this is something you might do like if you're reporting to your colleague or to your boss or to your manager or something like that or to the greater public like how your model is doing you might give them all these different evaluation metrics so they can start to understand okay the accuracy is a certain thing but the precision is this so they have an idea of how many false positives there are and the recall is this so they have an idea of how many false negatives there are and the f1 is kind of a combination between the precision and recall so we'll hit shift and enter voila now we've taken advantage of the third method of evaluating models and that's by directly using functions such as accuracy score precision score recall score and f1 score if we come up here the documentation this is a metric function right classification metrics here we go there's a whole bunch more there if you want to check them out but these are some of the main ones that we've covered and the principle is still the exact same for the rest of them and so if we come in here we're going to do regression evaluation functions turn that into markdown so same thing again you could almost do this yourself i reckon and if not type it out but how would you go about it if we look at our classification evaluation functions what you might do is from sk learn.metrics import some regression functions then import the regression model then import this train test split create the data split it into training and test instantiate your regression model make some predictions and then evaluate them but this time instead of evaluating a classifier you're evaluating the regression model using regression metrics but just for completeness let's type it out again metrics import r2 score wonderful mean absolute error now we've seen these before mean absolute error mean squared error bea youtube learn dot ensemble import random forest regressor so this is the kind of workflow you might do for your own problems right if you're working on a regression problem you might have some sort of import statement at the top of your notebook like this import train test split in our case we've already got our data in a data frame you may have some more lines of code getting your data to a proper data frame oh we almost forgot mp random seed so that out you actually don't need a random seed i just like to have one and you'll see them all over the place just so if you run the results the same as what someone else was getting target axis equals one one equals boston df target beautiful so and another reason the random seed right so if i ran these cells and then you took this notebook as the resource for the course and then you wanted to compare your results to mine without the random seed they'd probably be different because all of the randomness in this notebook such as train tests split randomly splitting our data into training and test sets would use different samples for each and so we'd get different numbers and that would cause confusion which is not what we're about right if we're all about communicating what we're finding so we're instantiating a model here so random forest regressor wonderful i'm going to go model.fit we're going to x train y train beautiful make predictions using our regression model model dot predict x test yes yes yes and now evaluate the regression model so we're going to go here print regression model matrix on the test set again you could functionalize this to pass it your regression model as well as some metrics but we're just going to write it out here just for just for good practice r2 score y test y preds wonderful then we just need to end the string we can do the same for mean absolute error so m a e equals mean we'll tab complete that one of course we will wipe reds wonderful then we go print f mse means squared error comparing it predictions to the actual labels finishing off with the string and boom oh we got an error of course we did and this is going to give us a warning because our n estimators is not equal to 100 and what is our other error found input variables with inconsistent number of samples 102 what has happened here 102 why test y prince versus 61. well you know what what it was happening it's 61 is the number of ah here we go there we go you know how i knew that is because if we go up here and if our classification problem if we go len y preds before we instantiate our regression problem it's 61. so because we didn't set y preds here right previously this was just this it was using y threads from above that's where i got called out from using the same variable names throughout the notebook right ideally we'd have different variable names for our classification and regression problems but just to illustrate purposes this is usually called ypres or something of the like and there we go regression model metrics on the test set now we've seen similar metrics before what can we do now well we've covered a whole bunch right and the reason being is because evaluating a machine learning model is paramount right it's one thing to train one but then again there's nothing worse than training a machine learning model and optimizing it for the wrong evaluation metric so keep the metrics and evaluation methods we've gone through when training your future models so make sure you keep them in mind go through them and have a little read here this is probably the most important section that you'll read of the entire scikit-learn documentation but after you've done that you'll naturally probably start to ask is that how do we improve these numbers how do we make them better they're kind of stagnant we've been using a random seed and we've been seeing just the same numbers for accuracy precision recall and f1 over and over and same with r squared mae and mse so the next section that's what we're going to cover so if we look back our list what we're covering number five we're up to improving a model all right so take a little break have a look at the scikit-learn documentation for metrics and scorings quantifying the quality of predictions you can find it by just going to this url here or searching sk learn evaluator model but otherwise get ready for the next section we're going to see how to improve our models"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Model Evaluation with Scikit-Learn",
        "Video Title":"Model Evaluation and Selection Using scikit-learn Course Preview",
        "URL":"https:\/\/www.youtube.com\/watch?v=IvDmIYs0tiQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/IvDmIYs0tiQ\/hqdefault.jpg",
        "ID":"IvDmIYs0tiQ",
        "Publish Time":"2020-01-08T20:16:01Z",
        "Channel":"Pluralsight",
        "Channel ID":"UCbrXRQHV4TOU4Pqzw325Z1A",
        "Transcript":"[Music] hi everyone my name is Jason Prabhu and welcome to my course model evaluation and selection using scikit-learn I am a data science practitioner and leader with over 10 years of data science experience in industry in this course we're going to learn about techniques and methods that you can use to evaluate how well your machine learning model is performing and also how to select the best model from a set of models that you've built some of the major topics that we'll cover include why model evaluation of selection is important and useful metrics and formulas used to measure the performance of classification regression models separately methodologies to perform model selection avoid overfitting and picking the model that will perform best in the future finally you will learn how you can implement all of these ideas in scikit-learn which is a major machine learning library in python by the end of this course you'll be able to incorporate modern evaluation and selection techniques within your machine learning workflow all powered through scikit-learn but before beginning this course you should familiarize yourself with how classification regression machine learning models are built I will assume you're comfortable with what a machine learning model is and how they are created I hope you'll join me on this journey to learn more about improving your machine learning workflow with the model selection and evaluation in scikit-learn course at Pluralsight"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Model Evaluation with Scikit-Learn",
        "Video Title":"Train Test Split with Python Machine Learning (Scikit-Learn)",
        "URL":"https:\/\/www.youtube.com\/watch?v=SjOfbbfI2qY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/SjOfbbfI2qY\/hqdefault.jpg",
        "ID":"SjOfbbfI2qY",
        "Publish Time":"2023-08-07T14:58:09Z",
        "Channel":"Ryan & Matt Data Science",
        "Channel ID":"UCKq-lHnyradGRmFClX_ACMw",
        "Transcript":"today we're going to be taking a look at how you can Implement train and test split within SK learn in Python now what train test split allows you to do is split up your data that you're going to be using for a machine learning model in fact what's most common is 80 for the training data and then a 20 for testing the reason why is with your machine learning algorithm you don't want to give it all the information in fact you want to have a test sample at the very end with unseen data this is actually pretty easy to implement within Python and I'm going to be coding it with you right now all right so I have a blank Jupiter notebook over here first thing I would recommend is importing pandas as PD import pandas as the shift and enter this will create a brand new cell and also run the one from above next we're going to import train to split so from SK learn the model selection Imports train test lit just like that next we're gonna build out our data frame so I'm going to say DF equals PD dot read CSV now the file I'm going to use is 500 hits I have this under my GitHub so link is down below in the description so we'll just say 500 hits dot CSV and then the other thing we're gonna have to do in this data set is just put encoding equals and we're going to put Latin one if we don't do it uh the this ends up breaking so just make sure that you copy this encoding I know you don't see it all the time within either read CSV call okay so shift and enter and just to show you guys what this data looks like we're just gonna do a DF dot head over here and then you can see specifically how this ends up working if player years games bats and all the way through Hall of Fame so what we're going to do now is we're going to change this to X and Y so essentially we're splitting up this data looking at these different stats over here in determining if a player is in the Hall of Fame so Hall of Fame is going to be our why X is going to be all this we also need to drop this player over here because we don't want to have that uh so when we're assigning our X we're gonna have to drop two different columns so we're going to drop player and also Hall of Fame and then why we're just going to equal to the DF on this head so uh what's pretty common for a train test split and I should just say machine learning in general is with X you're going to have a capital like this so we're going to say x equals and then we're going to go over here DF that drop and we're going to say columns equal and then we're going to put our two columns over here so we're going to say player right and then we're going to put a comma over here and then we're going to say now that's going to be our X and then y we're going to say is equal to DF and then we'll just put over here Hall of Fame just like that and then you can see specifically how these work so if I just put over here x dot head great now you have all the specific data that we need and then if you want to see how large this is just to show you that too x dot shape and you can see 465 with 14 over here and we could do the same thing with Y right so if I just want to do y dot shape rate 465 and the reason why we don't have anything over here we just do wally.head right we just show specifically if someone is in the Hall of Fame but now that we split up our data between X and Y it is time to build up the training test so how you do this is first you're going to do X train then you're gonna do X test then you're gonna have y train then y test and then we're going to equal this to train test Blitz again capital x and lowercase y on this side of things now we're going to add in a random state so let me explain what this does and we're just going to say random state of 11. what this allows you to do is keep the randomization uniform every time that you run it so let's say for example I want to run this train test split again five D's from now well if I have a random state of 11 if I code this in here it's going to be the exact same randomization that time in fact if you guys are running this code exactly like I am and you put random State 11 you're going to get the same exact output so that's what random state allows you to do and you can change it to whatever number you want for example if I put 23 over here that is going to give you a different randomization than 11 or 15 or 28 or 55. so I just for this example I'm just going to put 11 over here feel free to change whatever you would like and lastly we're going to do our test size so I'm going to say test size equals the 0.2 which is essentially 20 now it's kind of common you see either like 20 or 30 it just really kind of depends on how much data that you have for this example I'm just going to keep it at 20 so again how this works it's a little complicated you have your X train you have your X test right we're splitting up the X variable then you have your y train and you also have your y test we're splitting up the Y variable right training data testing data training data testing data we call this train test split capital x lowercase Y and we have a random state of 11 and then we also have our test size so we're doing an 80 20 split so shift and enter on this one and now our data is changed so if I go over here for example and I go instead of X shape we're going to do X train dot shape right and now you can see it is 372 compared to 465. if I go over here to X test right now you're going to see 93. I can do the same exact thing with Y right so we're going to do y train dot shape right 372 and then also y test and you want to take a look at some of the heads too so I can just go over here and do X train the head and you can see all the different data over here right and if I wanted to do for example some more information on here I can do X train dot describe and you can see all these averages on this side of things right so count 372 means 17 games 2046 at bats so this gives you all this and I I can also round it just to make it look a little cleaner we'll save just rounding of three you can see how this looks now if I just do an X train to describe or not X train but let's say X test dot describe you can see how this is a little bit different right so first off our count is 93 compared to 372 which we talked about earlier but now if you look at our mean right 17.011 17.204 our standard deviation is actually quite larger on this X test we have 3.154 in comparison to 2.662 our minimum is 11 minimum 11 for here for the years and our Max is 25 with a Max of 26. so a little bit different between both our training and testing and you can even look at other things like home runs right so count again same 372 and then 93 like before we talked about but mean on this one is 202 the mean one and this one is 194. so in your Zone we're building out our model right this first section over here is what we're going to be training it on and then the second section over here this is what we're going to be testing on to make sure the model is working correctly hope you guys enjoyed this video and you are now familiar with train test split within python by the way after you accomplish train test split on your data set you should also look at scaling your data in fact I have a whole video on here comparing a standard scaler with min max both are very useful to use before you run your machine learning model"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Cross-Validation Techniques for Model Evaluation",
        "Video Title":"Machine Learning Fundamentals: Cross Validation",
        "URL":"https:\/\/www.youtube.com\/watch?v=fSytzGwwBVw",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/fSytzGwwBVw\/hqdefault.jpg",
        "ID":"fSytzGwwBVw",
        "Publish Time":"2018-04-24T18:37:54Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"StatQuest Check it out talking about Machine-learning. Yeah StatQuest Check it out Talking about cross-validation StatQuest Hello, I'm Josh stormer and welcome to StatQuest today we're going to talk about cross validation and it's gonna be clearly explained Okay, let's start with some data We want to use the variables chest pain good blood circulation Etc To predict if someone has heart disease Then when a new patient shows up we can measure these variables and Predict if they have heart disease or not However, first we have to decide which machine learning method would be best we could use logistic regression or K nearest neighbors Or support vector machines and Many more machine learning methods. How do we decide which one to use? Cross-validation allows us to compare different machine learning methods and get a sense of how well they will work in practice Imagine that this blue column represented all of the data that we have collected about people with and without heart disease We need to do two things with this data One we need to estimate the parameters for the machine learning methods in In other words to use logistic regression we have to use some of the data to estimate the shape of this curve in machine learning lingo Estimating parameters is called training the algorithm The second thing we need to do with this data is evaluate how well the machine learning methods work in? Other words we need to find out if this curve will do a good job categorizing new data in In machine learning lingo Evaluating a method is called testing the algorithm Thus using machine learning lingo we need the data to one train the machine learning methods and to test the machine learning methods a A terrible approach would be to use all the data to estimate the parameters ie to train the algorithm Because then we wouldn't have any data left to test the method Reusing the same data for both training and Testing is a bad idea because we need to know how the method will work on data. It wasn't trained on a Slightly better idea would be to use the first seventy-five percent of the data for training and the last 25% of the data for testing  We could then compare methods by seeing how well each one categorized the test data But how do we know that using the first? Seventy-five percent of the data for training in the last 25% of the data for testing is the best way to divide up the data What if we use the first 25% of the data for testing Or what about one of these middle blocks? Rather than worry too much about which block would be best for testing cross-validation uses them all one at a time and summarizes the results at the end   For example cross-validation would start by using the first three blocks to train the method and then use the last block to test the method and Then it keeps track of how well the method did with the test data then it uses this combination of blocks to train the method and this block is used for testing and Then it keeps track of how well the method did with the test data, etc Etc, etc  in the end every block of data is used for testing and we can compare methods by seeing how well they performed in This case since the support vector machine did the best job classifying the test data sets. We'll use it BAM!!! Note: in this example, we divided the data into 4 blocks. This is called four-fold cross validation However, the number of blocks is arbitrary In an extreme case we could call each individual patient (or sample) a block This is called \"Leave One Out Cross Validation\" Each sample is tested individually That said in practice it is very common to divide the data into ten blocks. This is called 10-fold cross-validation Double BAM!!! One last note before we're done Say like we wanted to use a method that involved a tuning parameter a parameter that isn't estimated but is just sort of guessed For example Ridge regression has a tuning parameter Then we could use 10-fold cross validation to help find the best value for that tuning parameter Tiny Bam! Hooray we've made it to the end of another exciting StatQuest if you like this StatQuest and want to see more please subscribe And if you want to support StatQuest well Please click the like button down below and consider buying one of my original songs Alright until next time quest on"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Cross-Validation Techniques for Model Evaluation",
        "Video Title":"K-Fold Cross Validation - Intro to Machine Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=TIgfjmp-4BA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/TIgfjmp-4BA\/hqdefault.jpg",
        "ID":"TIgfjmp-4BA",
        "Publish Time":"2015-02-23T19:14:34Z",
        "Channel":"Udacity",
        "Channel ID":"UCBVCi5JbYmfG3q5MEuoWdOw",
        "Transcript":"So Katie, you told everybody about training and test sets, and I hope people exercise it quite a bit. Is that correct? >> Yes, that's right. >> So now I'm going to talk about something that slightly generalizes this called cross validation. And to get into cross validation, let's first talk about problems with splitting a data set into training and testing data. Suppose this is your data. By doing what Katie told you, you now have to say what fraction of data is testing and what is training. And the dilemma you're running into is you like to maximize both of the sets. You want to have as many data points in the training sets to get the best learning results, and you want the maximum number of data items in your test set to get the best validation. But obviously, there's an inherent trade-off here, which is every data point you take out of the training set into the test is lost for the training set. So we had to reset this trade-off. And this is where cross validation comes into the picture. The basic idea is that you partition the data set into k bins of equal size. So example, if you have 200 data points. And you have ten bins. Very quickly. What's the number of data points per bin? Quite obviously, it's 20. So you will have 20 data points in each of the 10 bins. So here's the picture. Whereas in the work that Katie showed you, you just pick one of those bins as a testing bin and the other then as a training bin. In k-fold cross validation, you run k separate learning experiments. In each of those, you pick one of those k subsets as your testing set. The remaining k minus one bins are put together into the training set, then you train your machine learning algorithm and just like before, you'll test the performance on the testing set. The key thing in cross validation is you run this multiple times. In this case ten times, and then you average the ten different testing set performances for the ten different hold out sets, so you average the test results from those k experiments. So obviously, this takes more compute time because you now have to run k separate learning experiments, but the assessment of the learning algorithm will be more accurate. And in a way, you've kind of used all your data for training and all your data for testing, which is kind of cool. Say we just ask one question. Suppose you have a choice to do the static train test methodology that Katie told you about, or you do say 10-fold cross validation, C.V., and you really care about minimizing training time. Minimize run time after training using your machine learning algorithm to output past the training time and maximize accuracy. In each of these three situations, you might pick either train\/test or 10-fold cross validation. Give me your best guess. Which one would you pick? So for each minimum training time, pick one of the two over here on the right side."
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Cross-Validation Techniques for Model Evaluation",
        "Video Title":"Lecture 19: Cross-Validation Techniques for Model Evaluation",
        "URL":"https:\/\/www.youtube.com\/watch?v=pw5-mc27lo0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/pw5-mc27lo0\/hqdefault.jpg",
        "ID":"pw5-mc27lo0",
        "Publish Time":"2025-01-18T17:28:42Z",
        "Channel":"Across the globe(ATG)",
        "Channel ID":"UCnfAIt_m6Smsv2tQYsim97g",
        "Transcript":"welcome to Across The Globe today we will learn about cross validation techniques cross validation is a technique utilized to evaluate and generalize the predictive capabilities of a model by assessing its performance on unseen data this is crucial in ensuring the model's Effectiveness in making accurate predictions for new data beyond the training set one prevalent approach to cross validation is kfold cross validation where the data set is divided into subsets the model is trained on K1 subsets and validated on the remaining subset in each iteration this process helps to gauge the model's consistency in predicting diverse data sets for instance in a scenario where a machine learning model is developed to predict housing prices kfold cross validation ensures that the model can make make reliable predictions for different neighborhoods and housing types kfold cross validation is a technique used to assess the performance of a machine learning model the data is partitioned into K subsets or folds the model is trained on K1 folds and tested on the remaining fold this process is repeated K times with each fold acting as the test set once the final performance metric is calculated by averaging the metric obtained in each iteration for example in a five-fold cross validation the data is divided into five equal parts the model is trained on four parts and tested on the remaining one repeated five times finally the average performance metric is computed to evaluate the model's Effectiveness to evaluate the performance of a logistic regression model using cross validation you can follow these steps in Python One Import necessary libraries scar. model selection. kfold for creating cross validation folds scar. model selection cross valal score for calculating cross validation scores scar. linear model. logistic regression for creating a logistic regression model two Define the number of folds for cross validation using kfold EG KF equals kfold n splits equals 5 for fivefold cross validation three create an instance of the logistic regression model model equals logistic regression four calculate cross validated accuracy scores using cross Val score by passing the model training data X Trin y train cross validation iterator cval KF five print the cross validated accuracy scores to evaluate the model's performance example python from scar. model selection import k. model selection import cross Val score from escar linear model import logistic regression KF equals case full and splits equals 5 full and splits equal 5 model equals logistic regression scores equals cross Val score and Y train CV equals KF print cross validated accuracy scores scores cross validation is a technique used to estimate model performance more accurately than a single train test split by dividing the data into multiple subsets or folds this helps reduce the risk of overfitting by using different test sets for validation and provides a more robust evaluation of the model's performance across various data subsets for example in a five-fold cross validation the data is divided into five equal parts and the model is trained and evaluated five times each time using a different part as the test set this process allows for a more comprehensive assessment of how well the model generalizes to unseen data cross validation is essential for evaluating the performance and generalizability of predictive models it involves dividing the data into K subsets and iteratively using each subset as a validation set while the remaining subsets are used for training by repeating this process for each subset and averaging the results we obtain more reliable performance estimates compared to a single train test split using kfold cross validation helps in obtaining a more robust estimation of the model's performance and ensures that the model is not overfitting to a specific train test split this technique is particularly useful when the data set is limited or imbalanced incorporating cross validation into the model evaluation process is crucial for obtaining accurate and unbiased performance metrics ultimately helping in selecting the best model for deployment an example scenario could be evaluating different machine learning algorithms on a data set to determine which one performs best in terms of predictive accuracy and generalizability cross validation is like practicing a presentation multiple times before the big event to ensure it goes smoothly just like how trying out different driving routes helps you understand the best way to reach your destination cross validation helps test different models to understand which one performs the best it's like dividing a test preparation booklet into sections and completing each section to make sure you are ready for the entire exam similarly in programming cross validation involves splitting the data set into multiple subsets for testing the model's performance on each subset to ensure its accuracy and reliability hey code conquerors ready to level up with tech learn atg showcase your skills with official course certificate just visit our site and start need that extra boost get tailored learning 247 support and expert guidance coding smarter growing faster and making learning fun go to our website across the globe ww.w the presented python code exemplifies the utilization of cross validation techniques in evaluating a machine learning model using the renowned saren Library initially essential libraries such as kfold logistic regression load Iris and accuracy SC score are imported to facilitate the cross validation process the iris data set is loaded consisting of feature data X and Target labels y by initializing a kfold cross validation splitter KF with five splits the data set is partitioned for training and testing subsets a logistic regression model is instantiated to be trained and tested during the cross validation process subsequently the cross validation process begins iterating over each fold generated by kold the model is trained on the training data within the fold and then predictions are made on the test data the accuracy scores for each fold are computed contributing to an accuracy score list ACT scores following the completion of all folds the average accuracy score is determined by calculating the total sum of accuracy scores and dividing it by the number of folds finally the average accuracy score is printed out for evaluation the concept behind this code implement in machine learning is crucial for model evaluation and performance optimization cross validation helps in validating a model's generalization capability as it assesses the model across multiple data subsets this enables a more robust understanding of the model's performance and helps in detecting issues like overfitting an example of a real world application for cross validation techniques is in medical diagnosis by utilizing cross validation a machine learning model can be systematically evaluated on different patient data sets to ensure its accuracy and reliability in predicting medical conditions one real world application of cross validation techniques can be found in the field of credit scoring when building a credit scoring model to assess the credit worthiness of individuals applying for loans it is crucial to ensure that the model's performance is robust and generalizes well to unseen data by utilizing cross Val validation techniques such as kfold cross validation data scientists can evaluate the model's accuracy precision and recall across multiple folds thus providing a more comprehensive assessment of the model's performance another industrial application of cross validation techniques is in algorithmic trading When developing trading strategies based on machine learning models it is essential to assess the model's predictive power and stability cross validation can help in evalu valuating the trading strategy's performance across different time periods or market conditions thus reducing the risk of overfitting and ensuring that the model performs well in real world scenarios across the globe Industries leverage cross validation techniques to enhance the robustness and reliability of machine learning models leading to more accurate predictions and informed decision-making keep learning stay curious and I'll catch you in the next video"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Cross-Validation Techniques for Model Evaluation",
        "Video Title":"Cross Validation",
        "URL":"https:\/\/www.youtube.com\/watch?v=hoNpvry0370",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/hoNpvry0370\/hqdefault.jpg",
        "ID":"hoNpvry0370",
        "Publish Time":"2020-05-13T02:35:20Z",
        "Channel":"TheDataPost",
        "Channel ID":"UCo8qbws1-to6LDgMNECu2dw",
        "Transcript":"cross-validation is a commonly used technique where we can leverage all of our data and still generate a fair model evaluation the first block represents our total data set every observation we have is represented in this orange block now in a traditional data split we have two sets a training test and a testing set with cross-validation we split our data into partitions known as folds these folds aren't immediately designated as training or testing we want to pick how many folds will split our data into common sizes or five or ten but that's to your discretion you can see in this example I chose to create five folds each fold will contain the same percentage of the total data you will not see the same observation in two different folds a given observation can only show up in one fold what we do is create K models where K is the number of folds so if I created five folds as I have here I will create five different models in each model I will test on a different fold using the remaining folds as training data so for example if we look at model one you can see I will train my model on two folds two three four and five and test my model unfold one for model two I will train on fold one three four and five and test unfold two so hopefully you get the idea after building each model I can evaluate the results on each models test set here you can see I have returned the accuracy score of each model because I have built five models or returned five accuracy scores the significance of cross-validation is I can average the scores to get a more accurate evaluation in this case a return a cross validation accuracy of 80.8% the most common reason we use cross-validation is in scenarios where we have a limited amount of data consider we have a hundred observations in our data set we want to use as many training observations as possible considering we already have such a small data set so let's say I choose a 90\/10 data split as I'm concerned with the model seeing enough data from that standpoint it makes sense however it would only be left with ten observations to test our model with such a small number it is not reasonable to gauge the effectiveness of our model with one group of ten test observations our model may look very strong if we got nine correct which represents a 90% accurate however if we use a different ten observations maybe our model would have only gotten six right and our model would seem pretty weak returning an accuracy of 60% there just isn't a big enough test set to make a fair assessment that is why cross-validation can be so useful in doing so we can get a fair prediction for all 100 observations to improve the reliability of our results so you may be thinking okay so now I have five models I only want to use one model to make my predictions how does that work well we're not going to use one of the cross-validation models for predictions cross-validation is only for model evaluation not for model building once we have a fair evaluation we can go back to our full data set and train our model on a hundred percent of the data assuming the model doesn't require early stopping if early stopping is required you can train your final model with a traditional training and testing set however if you are performing early stopping you are likely using a neural network and if you're using a neural network you likely have a lot of data and thus it may not be necessary to use cross-validation regardless cross-validation is a widely used technique and something you should understand when and how to implement"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Cross-Validation with Scikit-Learn",
        "Video Title":"A Comprehensive Guide to Cross-Validation with Scikit-Learn and Python",
        "URL":"https:\/\/www.youtube.com\/watch?v=glLNo1ZnmPA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/glLNo1ZnmPA\/hqdefault.jpg",
        "ID":"glLNo1ZnmPA",
        "Publish Time":"2023-08-28T19:45:22Z",
        "Channel":"Ryan & Matt Data Science",
        "Channel ID":"UCKq-lHnyradGRmFClX_ACMw",
        "Transcript":"in today's video we're going to be covering cross-validation within python with the help of Sci kit learn so first off what is a cross validation well cross-validation allows you to split up your data set into different subsets or also folds and when you run your model through it you're going to be able to get multiple scores at the very end now why this is important essentially in machine learning we've talked about train tests split and how you can split up your data between a training set and then also a testing set well at the very end of train test split and once we run our model we're going to only get one score and the problem is with only getting one score is it can sometimes be bias and I'm going to be showing you an example within this video some other benefits of using cross validation is it can help you select certain features a specific model to run and also help with parameter tuning as a bonus in this video I'm also going to be showing you stratified k-fold I'm also going to be talking about how you can call k-fold before running your cross valve score and then at the very end we're going to do an example with a pipeline so we're going to take this from like a very beginner entry level version of cross validation all the way towards intermediate hope you guys will enjoy this video and let's start coding okay so let's start off with importing a few different libraries we're going to import pandas as PD shift enter this runs the cell and then builds out a brand new line so she made this to a lot let's import numpy as NP shift key and enter as well uh next I'm going to randomize some data so that way I don't have to create brand new data for this cross validation so I'm gonna show you how to do that real quick uh no worries if you don't understand how this code works because uh you can do your cross validation without it but just copy it so that you have the same data I do so we're just doing a random seed over here and then when I'm gonna take a look at see fastball speed equals NP dot random dot brand hint I'm not randomized ends between 90 and 106 and then we're going to say the size is 500. okay and then I'm gonna look at Tommy John which is a surgery that pitchers get their arms were out and then I'm just gonna put over here oh speed is greater than 96. because pictures of through faster they tend to get more Tommy John surgeries so I'm going to say this is NP dot random choice between zero and one right you get a surgery or you do not get a surgery then I'm going to say size equals 500 just like above and then we're going to put our probabilities in here so I'll just put that between zero three and then 0.7 essentially I'm taking a look at pictures that throw over 96 they're more likely to get Tommy John surgery and that's what I want this data to show and then not zero so now we have all that built out over here now we can start assign this to a data frame so I'm just going to say d equals over here and we'll say fastball speed said feel free to skip ahead and just copy this code but this is just me adding in some data over here rather than a CSV that way you guys can just put this in pretty easily and then throw it in Tommy John this again that is the surgery name and uh you have that and then one last step we're going to say DF equals PD dot beta frame and I'll just put D in here or data equals d and then DF boom there we go okay so you can see 500 rows two columns you have a fastball speed and we have picture has Tommy John over here so if you guys copy exactly what I have with the seed and also the code here you should have the same exact data frame that I do so with that in mind let's start coding so I'm just gonna go over here and we're gonna set up our X and Y so the first thing that we're going to do is say X equals DF and we'll throw in here fast ball speed rate that's over here and then our Y is going to be if someone gets Tommy John's surgery or not so we're going to say DF this Tommy J okay and the first thing we're going to do and we're going to show you an example with this first is just do our normal train test split in the beginning so I'm just gonna put here normal train test split and if you've been watching the series you're super familiar with it if not we're splitting our data in essence for a training set and also a testing set so you have to write out this fun code X train X test why train and also why test equals train test split X and also y are tested size so it tells you how much is going to be in our test data set and we're going to see 0.2 which is 20 and we'll do random states of 11. now before I run this I did forget one thing so I'm just going to go above over here and I'm going to import in train test split which is from SK learn model selection so from SK learn that model selection import train test split so that's run right and then we should be able to have this go see you see is line 11 and now we split up our data right so okay we're looking pretty good now and for this one over here we have a binary output right zero one if someone gets a Tommy John surgery I'm just gonna do something basic and do a logistic regression you're familiar with those so right you have either result that's a zero or a one and then you also have your horizontal axis which is pretty good right because you have fastball speed over here and how I coded this out is pictures that throw faster more likely to get I mean John which is true but just wanted to throw that in here if again so oh pretty easy right I'm gonna say LR equals logistic regression [Music] have that over here and also because I'm dumb I did not import this so make sure that you import this also from SK learn dot linear model Imports logistic regression like this right now you can call this make sure it works over here and we have our logistic regression now we're going to fit our data so we're going to say LR dot fit we're going to throw in here extreme and then we're gonna throw in here y train and then we're gonna do just stop values dot Ravel make sure you put this otherwise it will not work properly we have our logistic regression over here and you can just see our normal score right so or DOT score X test and also our white test okay and our score is 0.71 so the reason why I wanted to show you this in the beginning is train tests split and running your score over here isn't always the most accurate right so our first example is 0.71 we have our test size 0.2 and we chose a random State 11. well if we choose a different random State this score over here could be higher or it can be lower right it just depends how the data is split up and when the model is run like is it going to get accurate results or not so let's do another example of it right so let's test let's test again so I'm gonna just go over here and use x train again but this time we're going to name this number two right so what I'm going to say is X2 train X2 test Y2 train Y2 test we're going to still keep 0.2 but this time we're going to do a random state of 25 right so let's shift and enter again I'm going to do the same thing over here we're gonna keep this logistic regression the same uh but this time we're going to put X2 train and Y2 train values over here so we have our logistic regression and now let's do our LR score so over here right we fitted our data and then we put X2 test and why two tests and take a look we have 0.8 that's a big difference between the two right uh that's only from changing this random State over here we have 0.71 when it's 11 and when we have 0 0.8 when it's 25. so let's take a look at like a basic cross validation score essentially and how that would specifically work so like above we'll say basic cross vowel over here and we'll do a little bit more bin stuff in a minute but we'll start off right so make sure you import this so from SK learn dot model selection import cross vowel score just like that okay next thing I'm going to do is call this out so I'm going to say CVS well CV score but CVS over here I'm going to say cross vowel score and then the first thing you have to do is put your specific model now I already called our logistic regression above LR so I'm just going to put this over here you can literally put this code on logistic regression in it but again I'm just going to throw o r next thing I'm going to do is our X and Y values so we're going to put X over here we're going to put y values dot rival great and then we're going to put a CV of 10. in CV of 10 is probably the most common that you're going to see so we're going to get 10 results out of this one so again CBS score cross Val or X and our y value and we'll Ravel this over here in our cb10 shift and enter and now just to show you all the different scores right we have 0.68 0.78 zero seven two seven two seven eight eight two seven four seven four seven six and also 0.82 so they're all over the place right and there's two things that you should take a look at with this at least as like a very Elementary level uh first thing that we want to do is see our average on this so we can just put NP dot average and throw in our score over here and what this is going to do is just average these out and again we have 0.756 which is kind of funny right um we had 0.71 and then 0.8 which is kind of in the middle between those two scores uh but just to show you like how big of a range this is over here between just running it twice with two different random States okay uh next thing we want to do is a standard deviation so we can just do np.std and then we'll throw in our CV score over here instead this time and we have a standard deviation of 0.04 so like if you think about it we add 0.04 here we're at 0.79 and really it's almost 0.78 or 0.8 just because of what's over here and then if we subtract it also we're pretty close to 0.71 and yeah it did take me a little bit of time to make sure we got a zero seven one and a 0.8 over here did that on purpose uh but just to show you like our cross valve score gives us a lot more accurate results and we can change this over here this value of 10. so I'm Gonna Change 5 15 20 and I'll show you some other examples over there um but essentially pretty powerful now I want to show you an example where we Define our k-fold beforehand um and usually you'll see this in other examples so I'm just going to say defining Define standard capable and we're going to import this in again right so from sklearn dot model selection import K capital K by the way import K fold and uh I do have an issue so what did I put over here needs to be a capital f as well and now we have that in okay now let's call our K4 so I'm just going to call it KF right K with a capital fold and now we can talk about how many splits that we want to have so in this instance let's do 15 splits we did 10 above right so let's do 15 this time so n splits equals 15. there's another option where you can shuffle your data I'm going to say that's equal to true and then lastly our random States so we can let's replicate this result again I think earlier what did I use 42 so I'm going to use 42 again down over here um and then we should have our k-fold that is now called okay and just to do our scoring system over here so I'm just gonna put KF score equals and then cross bow score now again write our logistic regression which is LR then X we can do y dot values dot Ravel and then we're going to say our cross validation CV equals KF which we defined above over here so again just more data on this side things you can be lazy and just put over here like what I did where is that uh 10 right uh but in this instance we already defined this above and we're just going to throw this over here now there is a little bit more that you can do to this you can actually Define what scoring method you would like to use here we go just take a look at the scikit learn documentation there's a ton of great resources on the website as well uh but you can see all the different scoring metrics so you have accuracy balanced accuracy top K F1 F1 micro macro you have clustering you have regression so again feel free all you want right you can see over here how you essentially do it so essentially you have a bunch of different ways that you could add in scoring so I just want to show you that does impact it so uh the first example over here I'm just going to say F1 so scoring equals F1 and that should be lowercase F1 we'll run that and then I'm just gonna do a calf score two and we're going to throw in accuracy all right C feel free to read up on all the different scoring types and determine what methods you would like to use and feel free to tweak your models but for this way I'm going to do that so first thing I'm going to do is just show you what these look like so we're going to start with chaop score right so we have our array over here and already this looks a lot less accurate than what I used above but that's okay so now we can go over here and say NP the average throw in our KF score 0.68 which is a lot worse right and then let's take a look at the standard deviation right STD and .05 so that definitely did not work with this F1 score and just to show you what number two looks like accidentally remove that we'll throw it down below right another array so we go all the way down to 0.63 it looks like which is pretty large right and then 0.85 over here um might have a higher average bigger standard deviation but we will find out shortly right so throw this in over here okay score 0.75 which is a big difference with our accuracy score and then TD and then .06 so a larger standard deviation right I'll show you a stratified K folder it's a little bit different uh this time you're taking a look at percentages for the different samples in each class so let's throw that in here is stratified okay fold and again we're gonna have to import this in so from SK learn dot model selection import stratified k old right and that is in here now I'm just going to build out a few more cells okay so first thing I'm going to do is kf3 because we already have KF one and two essentially so we're going to say kf3 equals stratified k-fold we'll say our splits this time it n splits equals 10 or again Shuffle equals true you like that whole equals true and then random State we'll say is I don't know what eleven so throw that in here and then we'll say KF score three equals and then we'll throw in Cross vowel score or logistic regression x y y values dot rival and then CV equals kf3 and I don't care about the scoring this time right and why do I have an issue and because I put y here twice okay so now that is implemented so now if we want to see our calf score three right there's our array once again and then we can just store NP dot average and our npstd.s t t and then throw this in here so this time we're taking a look at 0.756 and our standard deviation on this one is 0.03 which is a little bit lower than I think everything else we've had so far which is pretty nice and I also wanted to show you guys a pipeline example because the previous video was on pipelines when I upload on the channel and also on the series too so we'll just save pipeline example so we're going to build out our pipeline now I do want to throw in an additional stuff in our pipeline so we're just going to do like a standard scaler so pretty easy stuff and I do have a video on standard scaler uh that is a part of pre-processing import standard scaler and I'll put in the description too if you want to learn a little bit more about it it's not too important for this video in itself but we're going to throw this into our pipeline so we're going to see scalar equals standard scalar just copy and paste here right we call not skills Skiller okay now let's import our pipeline so I like using make pipeline because it's easier at least in my opinion and you don't make as many typos so pipeline Imports make Pipeline and I do make a lot of Errors so that's why I like make pipeline instead all right so now we're going to say pipe one equals make pipeline now the first thing I want to throw in here is our scaler right so we just have our scalar and then we're going to run a logistic regression right after again super super basic pipeline full video in the description about pipelines and go much more in depth but I mean I talked about that in well over 30 minutes that video so essentially we're calling our pipeline which first scales our data and then it's going to throw it into a logistic regression right but one of the benefits of pipelines is you can also use cross-validation with it so okay we have our pipe one over here now we're going to fit our data so let me just say Pipe One Dot fit or X train in here then y train dot values Ravel there's that in here too right so again basic pipeline or standard scalar and logistic regression okay and now we can actually take a look at our scores so let's just say our scores this time scores pipe equals cross vowel score then throw our pipe in here right uh because we're going to be using our pipeline this time instead of our just basic logistic regression which is pretty nice right X Y dot values dot Ravel or that in here and then CV equals 10. okay now we have that in here and if we want to see our scores again just throw scores pipe you can see our array that's been built out and then I'll just do entity. average and then np.std and P dot st e throw the scores pipe into both of these and let's see our final results and we'll do an overview again right 0.756 and 0.42 okay so let's just talk about what happened overall right and the reason why we're using cross validation so first we have our data over here that we imported again this does not matter too much is how I built out this data frame uh but when we use train test split over here the random state is going to give you different data samples each time for your training test sets so in this first example when I use random State 11 we had a final score of 0.71 when I did a random step 25 we have 0.8 so it's not the best method overall to use uh to essentially see how accurate your model can be so by using a cross validation score we're going to get multiple scores over here right in this example we get an array of 0.68078072 all the way through 0.82 which we have over here twice then what's the most common thing to do is take a look at the average and also the standard deviation on it so we can see our NP average is 0.756 and then .0427 for this first instance another thing you can do if you want to add even more additional information to it is import a k-fold before you do cross evaluation score right and then we just have our CV equal to KF which we Define above which we can put in our splits our Shuffle and also random state so that way you can duplicate of the exact same results another really nice thing with cross valve score is how many different options for scoring and you can take a look at the scikit documentation to see all the different types of scoring methods I just throw two ones in here pretty quick F1 and also accuracy and again you can build out your array again average and also your standard deviation you also have a stratified k-fold which you can use it's pretty popular and lastly you can apply this to your pipelines which I highly recommend so just build out your pipeline over here fit your pipeline and then take a look at your cross valve score and then when you run your crossbow score in this instance you don't have to put in your model you just put your pipeline in over here and then you can do your CV and get your same exact results that you were expecting hey we made it this far you've watched this entire video I really appreciate you're one step closer to learning even more information about machine learning all these videos do take a while to make if you want to help support the channel you can subscribe it's absolutely for free and does help grow the channel too now I do have a bunch of other videos on machine learning so make sure to check out this playlist right over here"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Cross-Validation with Scikit-Learn",
        "Video Title":"Complete Guide to Cross Validation",
        "URL":"https:\/\/www.youtube.com\/watch?v=-8s9KuNo5SA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/-8s9KuNo5SA\/hqdefault.jpg",
        "ID":"-8s9KuNo5SA",
        "Publish Time":"2022-04-12T11:30:01Z",
        "Channel":"Rob Mulla",
        "Channel ID":"UCxladMszXan-jfgzyeIMyvw",
        "Transcript":"machine learning it's incredibly powerful and as a data scientist you'll use it all the time you know what they say with great power comes great responsibility with all the open source python packages out there it's really easy to train a machine learning model but it's also really easy to mess things up before you really get started as a data scientist i'm surprised how often i come across projects that are set up for failure just because they haven't set up cross validation correctly and that's why in today's video we're going to be talking all about cross validation and how important it is and trust me as i say this as someone who's trained thousands of machine learning models competed in kaggle competitions and creative machine learning models in production in major companies cross-validation is extremely important i'd even go as far as to say that cross-validation is even more important than the model that you use itself hi my name is rob i'm a data scientist and i make videos about coding in python and machine learning today's video we're going to be talking all about cross validation we're going to show an example that explains exactly why cross cross-validation is needed then we're going to walk through a few of the different cross-validation techniques that are used and when to use them and finally we're going to apply the correct cross-validation to the first model that we showed and show how we can actually evaluate it in a proper manner if you enjoy this video please consider liking and subscribing you can also check me out on twitch where i stream all right let's get into it all right so here we are in a kaggle notebook now the nice thing about this notebook is i'll link it in the description below and you'll be able to click that link and come into the same data set and go through the code later if you want to so before we get too far here i want to show you i've added a data set called the stroke prediction data set we're going to be using this as an example for showing why cross validation is important and basically this is our plan for this video we're going to show an example where cross-validation is important step through all the different cross-validation techniques which i commonly use and then run our example with a proper cross-validation to show how that actually works in practice we're going to do a handful of imports here so these imports we import most of the time so import pandas import numpy import map map plot lib now for our model we're going to import light gbm and you don't need to know that much about that model other than the fact that it's a machine learning model and without proper cross-validation we're going to see that it'll over fit to our data but this could really be any type of machine learning model that as and we're just using this as our example now we're going to import some metrics from sklearn sk learn we're going to import accuracy and the area under the curve score and now we're also going to import here a bunch of model selection techniques from sk learn and let's also import train test split but these are basically an sklearn which is a very powerful machine learning library they've already built in a lot of these cross-validation techniques for us so all we have to do is use the predefined objects and apply it to our data set to get the cross validation working and we're going to import all those at the start here make sure i do a comma okay so i already said we imported a data set but a little bit of information about this data set it's the stroke prediction data set it's using information about patients to predict how likely they are to have a stroke and you can imagine this being important for medical fields we're also adding in a column that we're making that's just fictitious that'll represent the doctor that gathered the data and the reason why we want to add that to the data set is because often when you're doing cross-validation it's important to group your different uh your data sets by anything that's common amongst them and we'll show that later in in this example it'll be something like the doctor now i've created this this function beforehand all this really does is reads in our data with pandas it sets the format of the data and then it does a simple split where we take some of the data and we make it our holdout set so imagine this being like data in the future that we plan to run our model on and then we have our training set which is a bunch of late label data that we have for this data set that we'll want to train our machine learning model on so if we run this function you can see that i have this train data set with a bunch of different features about each patient and then you can see here on the right side the stroke column is either a one or a zero and that's what we're trying to predict for the data set if the patient had a stroke or not or was high likelihood of having a stroke or not now of all these train columns i can show you here that most of them will be our features and then we will want stroke to be our target so i'm just going to make variables called features with each of these features that we have in our data set we're also going to name something called groups which is the doctor that's the different group we're in and then target equals stroke that's the thing that we're trying to predict out of this data set now very commonly in machine learning you'll have an x and a y variable x is just the features in your data set and y will represent the target that you're trying to predict so let's take this training data frame that we have and let's make an x and y out of it and we'll do that by getting the features as our x and then so let's call this x and y will be our target our target column and then our groups will just be this group column which will represent which doctor pull gathered the data for that given patient let's go ahead and wrap this in a function so we can pull this with our training data set whenever we need it let's define a function called get x y and we'll give it our training data and it will return x y and groups now if we run this function we now have our x y in groups data and we're gonna go ahead here be super excited about this data set want to make the best machine learning model we can and let's just create a classifier for this using light gbm so this is just that um machine learning library that's a gradient boosted tree it's a very common very powerful machine learning model we're gonna set one parameter here that's the number of estimators and that's our model then we're going to call fit on our training data with first the features and then the target and lookie here we've trained our model it's a trained model and now we actually want to predict on our training set just to see what the predictions look like so we can do that pretty simply with a classifier we can do predict which will give us just a one or a zero and that we'll call that pred and then we'll call pred prob which is with a classifier you can do predict prabha which will predict the probabilities instead of just a binary 100 value predraba actually gives us an array with both the positive and negative class predictions and we just want the predictions for the positive class so we'll just we'll add this to the end which will just pull out the second predictions in each of these all right we're doing pretty good so we've trained a model all we have to do is uh score how well it did so let's get the accuracy score and this is a function from sklearn where we just give the true values which is y and then our predictions which is pred and let's go ahead and get the auc score uh auc or area under the curve is just another way to evaluate binary predictions but you actually feed it in the probabilities so this is a better way than accuracy that we'll see here in a second why it's better to use in this case so but we'll also feed this in but we're gonna feed in the pred prop let's go ahead and print these out so we're gonna print the score on the training set is accuracy of act score and auc of auc score and we're just gonna do it to the fourth digit oh my goodness our model has an accuracy of 0.99 or 99 an auc of 0.999 this is a really good model all right let's publish a paper uh model can predict with 100 wait no 99 accuracy but let's get this published it's awesome that's not that easy nope this is the problem that beginners often have with machine learning is you get so excited about jumping into applying the model that you're not actually sure what's going on and you can overstate the predictive power of your model since we did keep some of the data to the side as a holdout set and we're pretending like that will be future data that we'll use our model to predict on the holdout set we can actually score on that to see if this score is actually 99 actress so let's make a x holdout using the hold out data features and a y hold out hold out target and then we'll do all the same things we did above for predicting using our classifier which we've trained and scoring both the accuracy and the area under the curve score okay now we're back to earth here actually if we had put this model into production and thought that we were 99 accurate turns out it's only about 93 accurate and the area under the curve is 0.78 now a perfect area under the curve would be one and a random guess would be 0.5 so this is you know not horrible but if we look closer at this accuracy of 0.93 we might want to say okay how does this even compare to a baseline let's baseline by predicting all zeros so this is basically predicting since we don't have an even balanced classes here we don't have in fact this data set is quite imbalanced if we just predicted everyone would not have a stroke we'd actually get a pretty good score so let's do that accuracy score with our target and then we'll just do numpy zeros like which will give us an array that's just the same shape as y but all zeros and then let's also do this with the area under the curve so we do see here that our baseline area under the curve would have a 0.5 but actually our baseline accuracy would be 0.95 so this this model is scoring worse on the holdout set than it would on the training set if we just guessed all zeros and that's not very good at all okay so we can see that this is a problem if we just train a model and assume that how well it does on our training data will work equally as well on any other unseen data and the way that we combat against this is by doing something called validation and the easiest way to do this is just a train test split even though it says train test i'm actually talking about splitting our training data into a training set and a validation set and i'm going to show you how that looks here all right so train test split is basically taking the training data making a training and a test or validation set and then we can evaluate it on that validation data before we put it into some sort of production environment where it's going to see unseen data and we'll be able to evaluate it this is the bare minimum that needs to be done when creating a machine learning model and trying to validate it if you remember before we imported train test split from sk learns model selection and we're going to use that right now first we're going to go ahead and get our x and y data again and then we're going to run our train test split on this data and it'll take in a number of arrays for us right now will be x and y and then we'll say how big we want the test size to be we could either give it a number of samples that we wanted the test size to be or we can make it a fraction so we want our test size to be a tenth of all of our data a tenth or twenty percent is a common split that you'll see when doing a train test split and this will actually return x train and x we're going to call it validation a y train and a y validation so now we can do what we did before and that's creating a classifier we can fit our classifier on an x train data and then we're going to predict on our validation data and we'll do the predict we'll do the predict probability as well let's go ahead and score the accuracy and the area under the curve for this model that was trained on our train split on this validation holdout split now we can see that our ariana curve and our validation accuracy are more reasonable and more in line with what we might expect to see on our holdout set or our future data this is a better representation of how good our machine learning model is but it also leads us to the possibility of fitting parameters and choosing a model that's over fit to this split and that is where cross validation comes in okay so we're ready to talk about cross validation and why it's so awesome now i'm creating some visualizations here to help us visualize what's going on and it's most of it's adapted from the sklearn library documentation itself i don't want to get bogged down in all this code itself so i've hidden it but the basically what you need to know is i've created a function called get fake xy which allows us to get x y and groups just like before but just for some fake data that'll be cleaner to for evaluating in this plot and then we have a plot cv function and what this takes is a different cross validation technique from sk learns library so here we're gonna put in kfold which will be our first example to show and then we will give it our data and it's gonna make a pretty visualization for us so yeah we're gonna first talk about k-fold this is a very common cross-validation technique and essentially all it is is taking our training data and splitting it up into different groups it by default it'll just split it up evenly into chunks depending on the number of splits that you give it so let's create a k-fold object here with k-fold and show you that you can set the number of splits you can also set it to shuffle or not it's always a good idea to shuffle it if you're not sure and then you can set a random state for the shuffling so that you can recreate the same results in the future if you would like to but this visualization shows us how this looks so imagine the class here being the ones or zero of we're predicting likely or not for stroke and then the group here would be our doctor column where we have different doctors who have gathered this information now k-fold splitting basically gives us however many splits so here we have five where we've split the training data into a training and a testing set or training and validation set and we've done that five different times so you can already see why this would be even more powerful than the train test split we're essentially doing this with all of our training data and we're evaluating five experiments instead of just zero now you can make the splits more or less five is a pretty typical number but the more splits that you have the longer your training time will be and there are other things you might want to consider like if you split it too small then your test set might not actually have enough test samples to be predictive there to show you how well your model is doing so basically k-fold it splits our data set into k consecutive folds without shuffling by default but you can set shuffling as true the next cross-validation technique is called stratified k-fold and it's very similar to the k-fold from above but we're going to show how this works so stratified k-fold let's call it skf and just to show you you can also choose the number of splits whether to shuffle or not in the random state and let's pass this through our visualization tool to see how the splits look now the difference here is stratified k-fold will automatically try to balance your target variable in both the training and the validation set and this is important especially if you have an imbalanced data set where there's a chance that you might have no positive cases in the validation set if you let it up to chance stratified just make sure that they're equally balanced groups and it's pretty common to use so this is k-fold but the folds are made preserving the percentage of samples in each class now next we're going to talk about something called group k-fold group k-fold is very similar to the other two cases we showed but that counts for if your data has some sort of grouping in which the data was gathered or the way samples are associated with each other so in our case we have a doctor who may have its own biases in how the doctor collects the data that we're trying to model and we want to make sure that our model is robust enough to predict well on unseen data in a brand new doctor so by using group k-fold we can ensure that our model is validated on a different group than it is trained on and that there's no leakage or information about the doctor itself that the model is learning and showing our results to be more confident than it actually is so i'm going to make a group k-fold object here and show you that in group k-fold you don't have the opportunity to shuffle or splits it's just going to choose the same groups for you each time and you can see here that the tr for each of these cross-validation iterations or folds you have an entire group staying together in either the train or the test set another example of this might be if you have a bunch of different observations for the same person you wouldn't want that person to be in both your train and validation set because your model might just predict that person very well so it is very important to consider things like group k fold when you have data that might have information spread across different observations that could make your validation over confident and how well it's predicting in reality it's just able to predict those groups which isn't very helpful on future data where those groups might not exist so in group k fold we have the folds are approximately balanced in this sense that the number of distinct groups is approximately the same in each fold all right so next up we have stratified group k fold and if you're tracking with me so far you probably have an idea of what stratified group k fold is it's the same thing as the group k fold but ensuring the equal number of positive cases in the each group so if i paste this here stratified group k fold i can show you that when you create this object you can put the number of splits shuffle true or false and then the random state and to visualize it here you can see now that the groups are chosen the training and test or validation set is split so that the groups are preserved but the groups are chosen such that there's an even distribution of the class in each of these samples if i shuffle the target class you'll actually see it a little bit clearer because the groups uh for our target class might have different amounts of targets and it'll choose the right amount of splits so that you've you can have multiple models created across folds and get a balance split of your target variable and if i show you the description from sk learns library about stratified group k fold it goes into a little bit more detail but essentially it's trying to preserve the percentage of samples in each class as much as possible while preserving the non-overlapping groups now the last type of validation technique i want to talk about isn't really applicable to this data set but it is very important if you're using any sort of time series data and that's called the time series split so let's make a time series split object and just show you you can also have the number of splits just like the other ones then you have some other parameters like the max train size and test size and a gap this will become clear when we actually visualize it so in a time series split your you want to make sure that none of the information about the future is fed into your model to give it observations about something it shouldn't have seen at that moment in time so it also is known as a sliding window approach where you grow the amount of training data that you're providing it in time and then testing on the amount of time right after the training period ends if i do a video on time series machine learning i will make sure that i cover this in detail okay now that we've learned about some of the cross-validation techniques let's go back to that first example and try to recreate what we did but with a proper cross-validation technique so we know we have a small data set that's very imbalanced so we want to make sure that we have a balanced number of samples in our validation set that are true means we're going to use stratified we also know we have some sort of grouping like the doctor information where some of the samples come from certain doctors we want to make sure the same doctor's observations are not split between the training and validation set so we're going to use a group validation technique so in this case i would use a stratified group k-fold split we're gonna set up it with number of splits equal to five we're gonna also make sure that shuffle is on to make sure that the sorting of our data doesn't play any role into how the splits are made so let's go ahead and create this stratified group k fold object we're going to give it the number of splits which is five we're also going to put shuffle as true and let's go ahead and give it a random state just so that the shuffling's the same every time we run an experiment so we compare apples to apples let's go ahead and run our get data again so now that we have our group scrat stratified k-fold object we can run the split method on our data like this x y we provided our groups you can see that the split method takes x y and the groups so if we run it we could see that this object is a generator and it runs these splits and it'll provide us every time we iterate over it training and validation indices so the locations where the train and test value samples should be taken from so we can loop over this by saying for train index and val index in this generator and if i break here i can just show you that the train index are just numbers of where in our training data set we should pull out the training samples and the val index are the samples that we should be using as validation because we're using group k fold they'll all be in the same group that's why you see these numbers starting at 500. and now we can similar to what we did before with trained test split is we can run the train we can pull out these samples and create a train in validation so let's locate an x where the train index appears we'll do the same thing with y and then we'll do this for our validation val index now there's a lot of things we can do at this point like saving the fold number into our data set or saving out of fold predictions that's beyond the scope of this video but just to show how this works we're actually going to do the fitting and predicting like we did before this just runs the fitting of the classifier on this training set invalidation set and this will loop over five different types because we set the number splits to five let's actually name each fold and then at the end here we're going to do a fold plus one so it increments each time and we'll print out our scores for each fold similar to like we did before there we go so we have a more robust way of evaluating this model setup anytime we change any parameters to this model or we're changing the features we create in the model we would want to do it within each cross validation fold so within each of these loops to really fully get the best out of this cross validation scheme additionally we can save off the scores that we have for each fold and then average across them to get a more robust idea of what our area under the curve score will be so creating a list here for aucs to be stored and then i'm appending it with the auc score of each fold now if we look at auc's we have our different area under the curve scores that are slightly different just because because of the randomness of each fold and we could take the average of this and say this is our average area under the curve score across five folds this is a much more robust way of evaluating our model and sharing how well we'll think it'll do on unseen data i'm going to call this o out of fold auc and there you have it we've done the same model evaluation with proper cross validation and we can actually report this score with confidence might not get published in a paper right away but you know if we improve the model maybe it'll become better and will actually have a good representation of how the model is so we've only scratched the surface about cross-validation and machine learning applications but hopefully this gives you a good place to start with and when you create machine learning models make sure that you're validating it in a cross-validated manner if you can if you found this youtube video helpful please let me know in the comments below let me know if there's anything i should expand on and i'll see you all in the next video thanks for watching"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Cross-Validation with Scikit-Learn",
        "Video Title":"Machine Learning Tutorial Python 12 - K Fold Cross Validation",
        "URL":"https:\/\/www.youtube.com\/watch?v=gJo0uNL-5Qw",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/gJo0uNL-5Qw\/hqdefault.jpg",
        "ID":"gJo0uNL-5Qw",
        "Publish Time":"2019-01-26T12:23:54Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"sometimes we get into this dilemma of which machine learning model should i use to solve my problem for example we worked on this iris flower dataset problem now you can classify those iris flowers using svm random forest logistic regression decision tree which model out of these is the best cross validation is a technique which allows you to answer the exact same question it basically allows you to evaluate a model performance when you are looking at machine learning models such as classifying emails as spam or not spam your typical procedure is you first train the model using whatever label data set that you have and once the model is built the next step is to use a different data set for testing your model and your model will basically return the results back and then those results you can compare it with the truth are to measure the accuracy of a model now there are several ways you can perform this training step the first way is all the training data set that you have available you just feed that to your model okay so you're using 100 of all your samples to train the model and then you use the same exit samples to test the model this in real life can be compared to an example of preparing this cute kid for a mathematic test so let's say you have 100 mathematical questions you train this kid for those questions and then when he goes for the exam you ask the exit same questions okay and then you try to measure his mathematical skills based on the score now this is not a very good way of measuring someone's math skills because he has already seen those questions before all right so what if he gets like 100 out of 100 there's no point he has already seen them so then the second option we have is we split the samples that we have available into training and test data sets so for example here i will i will out of 100 i will use 70 for training and 30 for testing okay and we have been using this train test split method in all of our supervised learning model tutorials and you will see that we have used train the split method in i think majority of our tutorials so that's what this technique is uh so again going back to our student example uh you give this person 70 math questions for preparation and the remaining 30 you will reserve for the test so that way when he goes for the test he has not seen these 30 questions which is good that way you can measure the skills in a good way but there is one problem with this approach also let's say 70 math question that you gave this person let's say they were all algebra and now remaining 30 questions are from calculus so now those questions he has not seen before or he doesn't have knowledge on that topic itself then he might not perform well so this this technique is also it kind of works okay but it's not like very perfect that's why we have k-fold cross validation now in this technique what we do is we divide our 100 samples into folds so i have five folds here each containing 20 samples and then you run multiple iterations in the first iteration you use fold number two to five for training the model and the first fold for testing the model and you not down the score second iteration you use first fold and then three to five for training and the second one for testing again not down the score you repeat the process till the last fold where you use fold number five for testing and remaining for training and then once you have every uh this course you just average them out now this technique is very very good because you are giving variety of samples to your models and then you are taking individual scores and then averaging them out here i have imported necessary libraries that i am going to use in this tutorial and the purpose of this coding exercise is to classify the digits data set which is an sklearn library you probably know about this um these are like handwritten characters that we can classify uh into one of the ten categories zero to nine we will use our different algorithms different models and then we'll evaluate performance of each of them using uh k-fold cross validation okay so here i imported my digits data set and the first thing i'm going to do here is split that data set into training and test data set so this is something you might be aware that using trend test split you can split it into our training and test data sets okay and once you have that we can use a different classifier so for example the first classifier i'm going to use is logistic regression and i'm going to measure the score so here i created a logistic regression classifier classifies basically your machine learning model which is trying to classify your samples and i trained it using extra in white trend and then when i tested the performance using the score method it returned me 0.959 which is pretty good now i use logistic regression but i have many other models as well for example svm i want to try on svm how how svm performs so you can see svm perform pretty poorly it's not the score is very low actually uh compared to logistic regression i can try random forest as well so in random forest case it is performing i think the best so this was a quick way of measuring the performance of these three models okay so logistic regression svm and random forest classifier rehabilitate the performance and we found that the random forest classifier is performing the best now this works in a practical situations but what happens is the distribution of samples in x strain and x test is not is not uniform right so for example when i run this method one more time now my samples change totally so now when i execute this code using control enter by the way that's a shortcut in jupyter notebook to execute your cell or you will see that uh the score will change so the previous code was 959 when i executed 953 so it changed a little bit here is 0.40 when i re-execute it you can see it become 0.62 now why did that happen because when i re-executed this cell x strain extensed white rain and vitis got changed the samples that we put into these four sets they got changed hence you expect the performance of your models to change when i execute this this is still performing better but you see that previously score was 0.97 now it's 0.98 so you see the problem with train test split method that you can't run it only one time and say that particular model is better than the other model you have to run this multiple times you know so see if i run it multiple time every time it is changing all right now let's try a k-fold so first what i'm going to do is uh use k-fold api to demonstrate what exactly it is doing all right so from sk learn dot model selection you can import k fold all right and kf is equal to k4 here you can specify how many folds you want to create so i want to create let's say three fourths just as an example so here it created that okay and the way you use this k-fold on the data set is you will say something like from train index test index in kf dot split okay now my k fold is ready we know that is going to make three splits so here in the argument you can supply the data set all right so let's say for simplicity sake i just want to supply number one to nine and then i can print train index and test index so when i run this what exactly is happening is this will return an iterator and that iterator will return train and test index for each of the iterations so it divided this into three folds three each and the first iteration it used one fold for testing which is this and remaining two folds for training which is this in the second iteration it moved this fold into training so you can see 0 1 2 is in a training set and then this fold into testing and it repeats the procedure like that so you can hear supply 10 folds also and it it should work accordingly now we are going to use k-fold for our digits example so to simplify the things i'm going to write a generic method called get score which can take a model as an input then x train x taste y train and y taste and i'll tell you what's the purpose of this method so this method calls model.fit which means i want to train my model using x train and y train and once the training is done it will return the model score using the test samples that you are supplying as an argument to this method now this method is pretty powerful we could have used this method to measure performance of these guys as well so for example let me just quickly show you if instead of doing you know repeating all these three lines for each of the model i could have just called the getscape score method here on x train x test white train y test and it should have returned me the score all right so same way i could have done svc here so if you do svc it's just modular modularizing our core all right so once we have this method ready i'm going to now use k fall on our digits data set so from sklearn dot model selection this time i'm going to use stratified stratified k-fold so stratified k-4 is similar to k-fold but it is little better in a way that when you are separating out your force it will divide each of the classification categories uh unif in a uniform way okay and this could be very helpful uh because imagine you are creating three folds for example our for our iris flower data set and two floor two folds have or two type of flowers and the one fold has just a very different type of flowers then it might create problems that's why using stratif stratified k-fold is better okay so here i'm going to say my n splits is equal to 3. people usually use 10 and splits but just to keep things simple i am using three here okay and once you have your folds ready so this method is exactly same as this okay so k4 and stratified k4 is same thing so we are doing we are repeating kind of the same thing that we did in these two lines here but we are using now our real example of digits data sets and i will prepare the scores array uh prepare the scores of our different models so l means the logistic regression scores and then random four score okay so i i need these arrays and i'll tell you why i need this array so here see same thing we are doing the exact same thing here okay but instead of this dummy data and now we are using our real digits data okay and what will now happen is in our digits data we have train index and then again in digits data we have our test index and these things i'm going to uh store in x train x test y chain y test okay digi start data so we have train and test index um now i want y train so y train is digit.target okay so y train uh train index actually and then in the digits.target now i want test index okay if you do this this and if you look at the length of the each of these sets you will realize it is doing exactly same thing as it did in this cell number 26 right now it's a time to measure the performance of three of our models in each iteration so since uh we have three folds this for loop is gonna repeat three times every time we'll take this different x test and x trains and y train and y test and we'll measure performance of our model and then we'll append the scores in these arrays okay so that's what we're doing so let's first start with getscore method getscore method as you know it will the first argument that it takes is the model then it takes x train x test y train and white test right let me just print this and i can do the same thing for three different models so the second model is svm and the third model is random forest classifier when i print this course it is printing the different scores okay so first iteration these are the three model performance second iteration and third iteration now instead of just printing why don't we append this the same score here so instead of printing i'll say append that exact same score here scores svm dot happen and scores rf dot happen all right so now my scores should be ready so i want to print them svm looks like this you you can see the svm performance is not that great and we have seen the same behavior before all right so logistic regression and random forest looks to be performing similar almost right on one instance this guy performed little better but what you can do is now you can take the average of these three scores and you can determine what is the best model for a given problem here it looks like based on this it looks like our logistic regression model might perform the best one optimization i can think of doing here is uh increased number of trees in my random forest classifier so if i increase the trees to 40 let's see how it performs all right i increase the trees to 40 and my scores are ready so now i will once again evaluate this course nice so you can see now random forest seems to be doing better because the 95 score match is here but here is 89.91 and here is 92 and 92. so now after i did little tweaking a parameter tuning in my random forest classifier my scores improved now this code looks little messy because we have to deal with so many indexes um but luckily sk learn library comes up with a ready-made method called called cross val score which you can use to do the exact same thing that i did here all right so this thing i i wrote this code just to explain you how k-fold works but in real life when you're solving machine learning problem you don't need to write this much code you can just call a crosswell score method which i can demo it here right now so to use that method you can import the method from escalant model selection it's called cross val score okay and once the method is imported you can call this method with first argument is your classifier the second argument is your x which is digits.data the third argument is your y which is digit.target so if you do shift tab to look at the the documentation it says estimator which is a model the second argument is your x and third argument is y so that's what we did and when you execute this line this is showing you that the similar score basically right that score is being shown here so internally this method did the same thing as this for loop basically it created folds and it measured the performance and it prepared this course uh array okay i will now call the same method on my svm classifier and see how that goes again svm didn't perform really well my accuracy is 39 percent 41 percent and so on versus here 89 94 percent it's much better and in the third case i'll just copy paste this code here so see all you have to do is just call one line man it's machine learning is not hard if you know the internals of these libraries then all you need to do is just just call one single method and say it's measuring the performance now we compared different classifiers you can compare same classifier with different parameters this is called parameter tuning for example we have random forest classifier right here and random forest classifier we ran with 40 trees we can actually run the same classifier with let's say 5 trees and get the score okay so you see this is 40 classifier this is the score with five classifier we say the score went down a little bit let me try 15 for example and the score went little bit up okay so looks like as i increase my trees my score is increasing so how about if i make my trees to be 50 so with 50 it increased even further 60 okay 60 i think 50 was better than 60. so so this way you can take a model and then you can do parameter tuning so you're using the same algorithm basically here random forex classifier but then you are tuning the parameters and trying to gauge uh which one delivers the best performance so you can see that cross-validation technique is pretty useful it not only allows you to compare different algorithms but the same algorithm with different parameters how it would behave for your given problem it can tell you that okay so machine learning is not like a scientific equation where uh for a given problem you use this model versus that model is something a little bit like trial and error based where for your given problem and given data set you need to try various models with various parameters and then uh figure out which one is the best for your use case all right now now comes the most interesting part which is the exercise so what uh we want to do uh this tutorial is take the iris flower data set and use different classifier random forest decision tree svm and logistic regression and use crosswell score method to measure the performance of the best classifier so for example if i'm a teacher and i'm telling you to solve the classification problem of iris flower which model will use so you will use a crosswell score with these four different models and you will tell me which model performs the best all right that's all i had for this tutorial i have a link of jupyter notebook used in this tutorial as well as exercise in the description down below in the video description section and uh stay tuned i'll be back with the next machine learning tutorial pretty soon uh and if you like my content uh please subscribe to my channel please give it a thumbs up it helps me with the youtube search ranking so please do that and thanks once again bye"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Cross-Validation with Scikit-Learn",
        "Video Title":"Cross Validation using sklearn and python | Machine Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=VGuxr5W5Nqg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/VGuxr5W5Nqg\/hqdefault.jpg",
        "ID":"VGuxr5W5Nqg",
        "Publish Time":"2019-06-13T14:53:08Z",
        "Channel":"Krish Naik",
        "Channel ID":"UCNU_lfiiWBdtULKOw6X0Dig",
        "Transcript":"hello all today we will be discussing about Krantz validation in machine learning this is a very important topic when we are actually calculating accuracy for a machine learning use case this is supervised machine learning use it's let it be a classification or regression beam now let us just go ahead and have a steep discussion about cross-validation but before that let us consider that I have a dataset and in that data set I have some features like f1 f2 f3 f4 and suppose I have my target output which is basically indicated by my p-value okay now usually whenever we whenever we take this input data and suppose let us consider that this we have thousand records this we have actually thousand records now when I have to give this particular data with respect to this particular output to a machine learning algorithm first of all what we do is that we do something called a strain testlet now when we do the strain test grid that basically means that we will be taking somewhere like 70 30 percent a strain test you know and it will be randomly picking up all the records so that you know there is no specific manner in which we are actually giving this record so machine learning algorithm then what we do is that after doing the train to split of 70 or 30 or it may be 75 and 25 it depends on the number of data set that we have number of records that we now when when we do this train displayed then what we do is that we give the 70% of our data set to our model now model gets trained with respect to our input and output feature and then once the model is getting trained after that we test this particular model we test the accuracy of this model by giving the input of the remaining test 30% of the data and then we see the output and then we try to verify this predicted output build with the real test output right with the real test output now when we see this particular I am getting my model a Chris is given somewhere around 83% okay now when when when we get this particular 83% it is with respect to one type of train and one type of test data now it may so happen that the 70% of the Train data and the 30% of test data some of the reports may come over here also in the next hydration suppose I want to do a separate train to split by using a different random number variable okay there is a parameter inside train test which is called as random number and suppose if I give a random number of something different and that and what happens is that it again randomly picks up some other 70% of the data as my train set and then I then then the test data is the remaining 30% of the date I'll it randomly picks up picks up it shuffles the data set and randomly picks it up now at that time when I give this kind of data again to my model then the model may be giving my accuracy somewhere like 78% or it may also increase or decrease based on the type of data that is basically selected in my training set and the test set so this random number shuffling and the train displayed actually works for one type of data itself it is it is I it completely depends on what kind of data basically on the 70% of the site what kind of data or basically on the 30 questions of the day now imagine that we should try to know in order to prevent this like every time we are shuffling in in the ran in the Korean test plane and giving to our given the data to a model for the training purpose right all the time we'll be getting some different accuracy so we will not be able to you know come to accuracy parameter that what is basically the accuracy of the model by just seeing one type of train and estimate so because of that we basically use something called as cross-validation now the cross-validation works somewhere like this now let us consider that I have total number of Records as 500 so 500 is my data set now if I try to divide this okay now I'll be considering that let me consider for 50 days I said for 50 a record should be always in my training data set and then 50 records should be always in my agitated this is basically test they can see okay now what happens is that cross-validation works in such a way that suppose I will take this 500 data silicon and I know that I have to have 50 tests it tests 50 data set in my test to 50 records in my test so what I do is that I will take all this 500 dataset once is particular square box let me just take the first 50 data set as my as my test data okay now the remaining 450 what I can do is that okay remaining 450 now okay I not consider 450 because I've just made five groups instead what I'll do is that if I'm taking 500 as my total data set then what I'll do is that 400 I'll keep it in my train and remaining 100 I will be keeping in my test okay now I can divide this into five different experiments in the first experiment my first hundred records will be my test dataset and then the remaining data set that I have this will actually become my train data set okay I always remember see how how the cross-validation will happen in the first iteration my first record first 100 record will basically my test and the remaining all will be my training data set now what I'll do I'll take this data set I'll give it I'll train it in my model I'll take this training data set of 400 over here give it to my model and then test with this remaining test data and get one kind of accuracy so this will be my accuracy one similarly what I will do is that in my second experiment I'll just move towards one ahead step that is my next hundred data set which will be my test data set over here I will consider this as matrix test data set and remaining data set will be my training data set then I will give it to my model okay my model will get trained and it'll give me an another accuracy similarly what I will do is that I will complete the citation for all the experiments based on the size of the test data now why I am saying based on the size of the test data we have taken my test data sizes hundred so if the total number of records are basically 500 then what will happen is that the first iteration I will get 100 remaining 400 will be my training dataset similarly I'll complete all the I could iterations then I will be seeing that based on the number of iterations I will be having back many number of accuracy so here I have five iterations I'll let you have a curacy one accuracy to a curiosity for Krishna now since I am having different accuracy what my final cross-validation will do is that it will try to find out the me nawfal this accuracy mean of all the secrecy now when I find out the mean of all the security this will then be giving me accuracy which is actually which is actually representable for the complete data set wherein you're shuffling the data and you're trying to take it as a train and test it so it is actually using different train and test data and actually calculating your accuracy and this is how a cross validation work simple guys serious in the first experiment what I am doing I'm just considering that my top hundred records will be my test data set remaining all remaining all this will be my training data set right now I'll give it to my model train it and this will happen for all the experiments now how many number of experiments will be there based on how many number of Records I have right and then how many data set I'm taking it as my test data set Ohan five hundred divided by 100 is nothing but Phi so this will actually be five experiments you know and based on this what will happen every one every experiment will have different different accuracy then finally all this accuracy mean will be we will be calculating mean of all this accuracy will we will be calculating and finally we get the type of accuracy that is representable for the whole dataset and this is pretty much pretty much good I am pretty much valid when compared to the tree line test ok trained and disciplined and sometimes cross-validation also helps you to choose like what algorithms you should use how I'm saying that suppose you applying logic regression aware for all these particular experiments okay and then in the second time you apply decision tree now suppose your logistic regression average is coming somewhere around sixty eight percent accuracy the decision tree may give use and accuracy and if three percent right so how do you change this algorithm might sell it by seeing the cross-validation score that is your mean accuracy of seventy three percent this is much more better than the sixty-eight percent so you may choose decision tree for this particular problem statement and similarly you can apply it for other other types of algorithms and this was all about cross-validation guys I just also help you to see a coding functionality just give me a sec can so here I have actually made a Jupiter notebook file of model selection and cross-validation here I've considered a data set which is called as purchased data set C is B and here you can see that after applying I'm trying to see with Kenny or near K nearest neighbors classifiers and logistic regression you can see that after applying cross-validation I get a mean score of 78 percent and a means call of sixty four percent can go through this particular file I'll provide this in these already uploaded in the detailing I'll provide the link in my description of this particular video I hope you particular I hope you like this video guys please make sure that is subscribe to general thanks for supporting everyone please share with all your friends or I do come up with very interesting content and I'm pretty much sure that I'll come again with much more contents in my next videos thank you one and all have a great day thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Hyperparameter Tuning Techniques",
        "Video Title":"Simple Methods for Hyperparameter Tuning",
        "URL":"https:\/\/www.youtube.com\/watch?v=vnXx7t2pnvQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/vnXx7t2pnvQ\/hqdefault.jpg",
        "ID":"vnXx7t2pnvQ",
        "Publish Time":"2022-12-02T12:00:35Z",
        "Channel":"M\u0131sra Turp",
        "Channel ID":"UCpNUYWW0kiqyh0j5Qy3aU7w",
        "Transcript":"All right now, it's time to talk about hyper parameter tuning. We will go into why there is a problem with neural networks. Like why do we have to talk about it even? And then I'll tell you some of the common ways of doing hyper parameter tuning, and also give you some ideas on what kind of libraries that you can use. So let's get. Let's say we have, uh, two hyper-parameters, right? Let's say we are trying to, um, optimize the number of neurons per hidden layer for one layer. That is because we can have different number of neurons in each layer in each hidden layer and the optimizer. So let's say we are trying gradient descent or miss prop momentum and item what's going to happen is that in this case we have 20 cases and that wouldn't even be very high. To try them. Right. We can just try them one by one. But if you also want to add learning rate into this, what's going to happen is all of a sudden you're going to have. 60 different possibilities because for every combination of optimizer and number of neurons on this hidden layer, you're going to, uh, have a different learning rate. So in total, you're going to have six different different cases that you need to try six different. Let's say you also want to optimize for a bedside. So if you try three different batch sizes, let's say 30 to 6,428. All of a sudden you have hundred 80 different settings that you need to try on to understand which one works better. And as you can imagine, this can get really big, really fast because right now you're only optimizing 1, 2, 3, 4 different Piper parameters, and we are trying four to five or even three. For some of them options for each of them. Whereas what could happen in not in a normal hyper parameter tuning problem is that we are trying to optimize five, six, maybe 10 different hyper parameters. And we are also trying out maybe attends of different values for all of these different hyper-parameters. So that can get quite big, very fast. And that's why we need some smart solutions when we're doing hyper parameter tuning. When we are searching for the best type of parameter setting. For our network. So let's look into some of the approaches that people use for. The first option is to use grid search. It is one of the very commonly used approaches when it comes to machine learning techniques, but it is a very slow because what it does is just tells us to try every single option that is in our hyper parameter space. And when we're building neural networks, this will take a very long time because as we just talked about, we have a lot of different combination of sales. That's why there is a bit of a different approach grid search that we use more commonly. And that is random search. What random search does is basically choose some random settings in your hyper parameter space and try them out. You can say out of the 20 options that I have here, I want to try five and then it tries out these five. And even though this sounds quite simple, random search is used very, very often. And most likely for your personal projects or your site project. Random searches going to be the one that you need out of all of the other hyper parameter tuning techniques that are out there. So, as I said, random search works really well for many problems, but there's one thing that you need to consider, and that is the range or the scale that you set for your hyper parameters. So if you're just sitting number of neurons in a hidden layer, you can just use normal distribution between 108 hundred. So that could be just your range and the scale could be a normal scale. But if you are trying to also optimize for learning. You shouldn't use normal scale because what's going to happen is this scale goes from 0.0001. So basically this is going to be seen as one. And while you're going at it, you're going to choose learning rates that are, that are bigger than 0.2, uh, bigger than 0.5, et cetera, et cetera, all the way until one, but the learning rates that are going to make the biggest difference for you and the ones that you're interested. Our old industry region. So when you are choosing them based on a normal distribution, you're going to choose the ones that are not going to make the biggest difference, because choosing one that is on 0.7 and one are not going to be. Whereas, what you can do instead of this is just use a logarithmic scale. So then when you are choosing your random values on the normal distribution, you will be able to choose the ones that are more relevant to what you're trying to achieve. So always keep in mind the range and the scale. That you're setting while you're doing random search, but random search is not perfect. Of course there are some shortcomings. So if you just do random search, once you might not be able to find a really optimal value for your hyper parameters. So what people do is do manual zooming in. So let's see what that. I'm again, giving the example of trying to optimize only for two hyper-parameters the optimizer and the number of neurons per hidden layer. So if we think about it in a three-dimensional sense, we can see actually that there is some sort of a area. And as we can see here, the ones that with the. Darker orange are going to be higher cost. And the lighter, the orange is the lower the cost is going to be. And basically you're going to have some sort of relationship between, uh, the values that our hyper-parameters take and the cost, right. Or the error, however you want to call it. So what we want to do with, to zooming in is we first again, do random search and then the try out somewhat to values. And based on these values, we look at the ones that are the most promising that give us a level of stereo. And then we say I'm going to zoom in zoom in, in that area and then do another random. And then we zoom in, as you said, the values here are changing because we are zooming in between 300 and 500 now, and we are getting closer. So of course we're basically getting closer to that, uh, low error area. And also, we also decided only to try RMS prop and monitor. And then you do another random search and then you get the ones that are really promising to you. So of course, this is a very simple example. Maybe we're able to find it with two or three tries. Normally what you do is just iterate this and zoom in, find the best value, zooming more, find the best value as they're consuming. But this is a very manual and time taking process because what you need to do is you need to run random search, check out the results, decide on the new range of surround random search again, but instead you can use a little bit more of sophisticated, sophisticated approaches."
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Hyperparameter Tuning Techniques",
        "Video Title":"Hyperparameters Tuning: Grid Search vs Random Search",
        "URL":"https:\/\/www.youtube.com\/watch?v=G-fXV-o9QV8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/G-fXV-o9QV8\/hqdefault.jpg",
        "ID":"G-fXV-o9QV8",
        "Publish Time":"2024-02-23T21:15:12Z",
        "Channel":"DataMListic",
        "Channel ID":"UCRM1urw2ECVHH7ojJw8MXiQ",
        "Transcript":"hey everyone let me start with a short story that those of you watching my videos most likely relate to imagine you've got all your data sorted out you have built this awesome machine learning model but there's this Crush step left how do you know what hyper parameters to use for the training for those of you that are not familiar with this concept the hyper parameters are either model or optimization parameters that are not learned during training but need to be said beforehand such as the learning the number of hidden layers in a neural network or the depth of a decision Tre to give an example let's imagine that we want to train a neural network and to do that we need to select among others the number of layers the number of neurons in each layer and the learning rate for the optimizer well if you wanted to find the best values for those hyper parameters you would have to exhaustively try all the possible combinations of hyper parameters which could be a very very very long process so what can we do to improve this one approach is to implement grid search which tries out all the possible combinations of hyper parameters but not in the whole parameter space but in a predefined grid and using the previous example we will Define a grid for the number of layers a grid for the number of neurons in each layer and a grid for the learning rate then griter systematically evaluates each combination of hyper parameters and select the one that yields the best performance on a validation set on the other hand we have random search which as the name suggests randomly samples combinations of hyper parameters from the hyper parameter space this method is less computationally expensive than grid search because it doesn't evaluate all possible combinations and instead it explores a random subset of the hyper parameters space nevertheless while this Randomness might seem less systematic it often performs surprisingly well and can be more efficient than grid search especially in high dimensional spaces however both G search and random search have their advantages and disadvantages G search guarantees that you'll find the optimal combination of hyper parameters within the specified grid but it can also Totally Miss the space where our model best performs as is depicted here also it can be computationally expensive especially if the hyper parameter space is large while on the other hand random search is less computationally demanding and can often find good hyper parameters configuration within fewer evaluations but there is no guarantee that you will find the optimal solution moving beyond research and random search there are more sophisticated hyperparameter tuning methods like basion optimization and evolutionary algorithms but I won't go into more details about them because I intend to create a separate video about each one in the future and that's basically it folks thanks for watching please hit the like button if you enjoy this explanation and don't forget to subscribe if you want to stay up to dat with the content I create on this channel see you next time bye-bye"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Hyperparameter Tuning Techniques",
        "Video Title":"Hyperparameters Optimization Strategies: GridSearch, Bayesian, &amp; Random Search (Beginner Friendly!)",
        "URL":"https:\/\/www.youtube.com\/watch?v=xRhPwQdNMss",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/xRhPwQdNMss\/hqdefault.jpg",
        "ID":"xRhPwQdNMss",
        "Publish Time":"2022-09-04T19:25:24Z",
        "Channel":"Prof. Ryan Ahmed",
        "Channel ID":"UC76VWNgXnU6z0RSPGwSkNIg",
        "Transcript":"hello everyone and welcome to this lesson in this lesson i'm going to show you guys three different strategies that we're going to use to perform model tuning and optimization so if you guys remember in the previous lesson we learned about the hyper parameters we learned about the difference between a parameter and a hyper parameter and we took an example of the learning rate as an example of a hyper parameter and we learned how just changing that learning rate can dramatically improve or worsen the response and the accuracy of the model as one so what i wanted to do next is i wanted to show you guys three different strategies that we're going to implement today the first one is what we call a grid search the second one is randomized search and the third one is going to be bayesian optimization so let's start with the first one which is grid search and please note don't worry about that code too much because that's actually what we're going to do together when we jump into the google collab but i just wanted to for you guys to know from a very high level the difference between these three different strategies so the first one which is a great search perform performs exhaustive search over a specified list of parameters basically think of it as more of a dumb search i have let's say here in this example i have four different parameters i wanted to optimize the maximum depth the learning rate the number of estimators and call sample by tree and here i have these are the guesses for each of the parameters for example for the max depth i don't know which one is the best one but i am guessing that it might be three or might be six or might be ten when it comes to the learning rate i have three guesses as well three candidates basically these are 0.01 0.05 and 0.1 number of estimators i have 100 500 and a thousand and cold sample by 3 i have 0.3 and 0.7 okay so when it comes to grid search what we're going to do is we are going to try every single combination for all these different candidates so i'm going to say well the first iteration the first run is going to be let's say three i'm going to grab 0.01 i'm going to grab a thousand and 100 sorry and then point three and that will be the first iteration i will go run the training process with these candidates with these hyper parameters and then we'll run another one maybe i'm gonna try 3.01 100 and maybe 0.7 and then keep repeating every combination that i have in there so basically here because i have three candidates here three i have another three candidates here there will be number three here as well i have another three candidates that's number three here i have two candidates that would be number two so in total if i multiply all these different parameters together i will end up with 54 combinations and please note that what we're going to do as well is i can go ahead here and specify when i call the grid search cv i can say well i wanted to perform cross validation five times meaning i wanted to essentially run all these different combinations the 54 of them and i want to try them five times so the overall number of runs and we're going to learn that today is going to be 54 times 5 because every combination i'm gonna run it five times okay and so the total number of runs is going to be 270. so that will be kind of an exhaustive search and it will take a very long time actually to perform the optimization please don't worry again about the code here we are optimizing an xgboost algorithm and we're trying to optimize these model hyper parameters and we're going to learn of course all these different lines of code when we jump into the google collab the second strategy is randomized search so grid search strategy works great if the number of combinations are limited just imagine maybe if you have let's say 30 hyper parameters and maybe for each of these hyper parameters maybe you have let's say 50 candidates it will essentially take you forever to run all these different combinations and that's why kind of think of it as more of an enhanced little bit of an enhanced version is randomized search so it works really well in scenarios when the search space is really large that's why we can do randomized search cv might be preferred and the algorithm works by evaluating just a very small select few of random combinations for example here i maybe have let's say number of estimators might be 100 500 900 1100 and 1500 i also have the max depth might be 2 3 10 15. the learning rate might be 0.05 0.1 0.15.2 i also have the minimum child uh weight 1234 and maybe the booster could be let's say gb3 or gb linear for example okay so instead of going there and trying every single combination as we have done before in the grid search i can essentially go ahead and say you know what please go ahead and maybe try for me 50 iterations 50 random runs and that will be essentially the randomized search it will essentially select just the 50 combinations of here that will be selected randomly and that's it and that will be the um we're going to of course select the best performing one out of these 50 iterations or 50 candidates okay so please note that in the grid search and in the randomized search actually there wasn't too much of an intelligence in there it was just kind of like we are doing trial and error basically blindly in a way and that's where the bayesian optimization comes into play and it can overcome the drawbacks of these two previous strategies that take a longer long period of time and it doesn't learn from the previous mistakes so bayesian optimization overcomes the drawbacks of random search algorithms by exploring search space in a more efficient manner basically if i'm searching in let's say specific combination of let's say learning great and maybe let's say called sample by tree an example of two hyper parameters and if i find an area that is a little bit more promising meaning that i calculated let's say the root mean squared error and i find my error very very low that means that i'm actually very close to the optimized value maybe right so if i find an area in the search space and this area appears to be a little bit promising which resulted in small error well this region need to be explored more i actually want to kind of double down on my gains and just not not ignore this region and maybe go and try a different different region right it makes sense that if i find something that works i need to go ahead and kind of capitalize on it and that's here the idea of bayesian optimization so basically i'm going to explore the areas that works and that increases the chances of achieving better performance and what you need to do here is that you need to specify the parameters search space and sometimes you can actually provide it in a form of a distribution for example i can say learning rate could be 0.011 and the distribution is log uniform and i'm going to show you guys again when you jump into the code there are various ways of defining your bayesian search um or bayesian space and you can specify here the number of iterations to be let's say 50 for example okay and that's it that's simply all i have for this lesson i hope you guys enjoyed it and now we're pretty much ready to go ahead and jump into our google collab and start coding together please stay tuned best of luck and i will see you in the next lesson"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Hyperparameter Tuning Techniques",
        "Video Title":"Bayesian Optimization (Bayes Opt): Easy explanation of popular hyperparameter tuning method",
        "URL":"https:\/\/www.youtube.com\/watch?v=M-NTkxfd7-8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/M-NTkxfd7-8\/hqdefault.jpg",
        "ID":"M-NTkxfd7-8",
        "Publish Time":"2021-01-25T12:57:15Z",
        "Channel":"paretos",
        "Channel ID":"UCJaL6SBbC7XaP7BbWKbR5nA",
        "Transcript":"hey geeks welcome to a new video today i'm going to talk about bias and optimization besides working a lot with our own multi-objective optimization so protested burritos i really enjoyed using bios opt the last weeks to solve single objective problems i think to best possible apply it you really need to understand it in a nice way that's what i'm gonna do today i'm gonna explain it to you step by step how it works that you can get most out of it if you enjoyed the video don't forget to like and subscribe the channel and if you have feedback or comments just drop them below let's get started let's start with an example for neural networks if you have a neural network you hardly know what is happening inside but what you want to achieve is that you want to optimize for example the precision that it finds stuff so what you need to do normally is you have different hyper parameters like the learning rate or the batch size and many others that you can tune before you start the training to optimize the precision if you look in more general on this problem we just can call it we have a black box problem where we don't know 100 what is really happening inside we have a lot of different input variables and what we try to do is to optimize the target value so what's coming out of the black box problem and this is exactly where bison optimization is really well suited to help you to find most efficiently or really efficiently the best target solution how it works is a iterative process in this case so we have five steps that are partially repeated iteratively i'm going to explain every step afterwards with an example in detail but yeah first you have an initial sampling set so you need to start with something so you start the initial sampling after having this sampling you evaluate all the samplings out of the initial samplings with the black box problem so for example before you would have different training runs for neural networks to see how they perform based on their hyper parameters based on these results you can start a training of a gaussian process regressor what this means in detail we get to this later based on these results you do a calculation of an acquisition function and you use this acquisition function the last step to identify the next to evaluate input so you try to minimize that function and see which evaluation am i going to do next in the black box problem and then you start the process over and over again until a certain criterion is met let's start with a simple example where we have a black box problem where we only have one input which is allowed to be in a range between 0 and 10 and we have also one target that we want to optimize in this case i take a mathematical function just for you to see clearly the conditions between and that we can later see how good the optimization were so we take the input and we multiply it with the sign of the input so let's take a look i said we start with the initial sampling which is in this case you have a lot of different possibilities to do initial samplings like latin hypercube sampling guessing or just yeah a random crop so this is you don't have one option that is mandatory i just took a random crop here and have five samples which when i one after the other evaluate them have different target values like you can see here based on these values we have now and the correct input values based on them we can now train our gaussian process regressor it looks like this so we have now two different indicators here i'm going to explain them to you so the difference in gaussian probes and regressors is you don't train like one regression function but you rather train a set of a lot of different tuned regression functions with different kernels different tails and what you do is the blue line is the mean of all predictions of all functions while the yellow area indicates the uncertainty of the model and is the standard deviation of all models and their predictions so you can see here obviously when we don't have noise at all points when we have a sample there's no uncertainty while the more the points are out away from each other the uncertainty rises in the next step we now have our gaussian process regressor and we start to do our acquisition function um what is an acquisition function basically you can have a lot of different approaches but it's somehow a mathematical function describing a gain or potential optimization volume by a function in this case i took a very common one it's called lower confidence bound some know it as upper confidence bound what it says is the acquisition function means that we take the normal standard we take the mean so the blue line and we take from that the standard deviation times kappa kappa at this place is a hyperparameter so it just you want to see it later depending how i choose this cover my optimization is going to be more locally focused or more global focused at this point i just want to let you know that we talk you about a minimization problem i forgot to tell this before so our goal is to get a target as small as possible i did the same acquisition function on the right taking a couple of 10 just for you to get a first feeling how it looks like what you can see like depending how big my copper is the more my uncertainty gains in value and at this point for example we see that for both we more or less sample at the value between four and six but still for the copper that is ten the value is more or less between five or six and for the copper with one it is nearer to 4. sampling now these two values will lead us to a new point and we start our iteration so we now have one more point that we evaluate so we retrain our model and what you can now see really beautifully here on the left side where we have the acquisition function with copper one the next sample that we should do is still very close to already the one that we did now and where we have the copper 10 on the other side you can see that it's far away so it's at a totally new point because the uncertainties are much higher prioritized so here we sample at 10 at the other one we sample at one uh not that one sorry at five and what you see here is now the model for copper 10 we have a really really good point actually there um but the model didn't expect the point to be so low so the uncertainties rise and this process now is actually repeated iteratively so it's done one more time and as you see the best point that is found by kappa one is more or less between four and six and you see that the samples are getting very close to each other already while with a couple of ten we still try to go in a wide variety so now the next sample point would be between zero and two we can now iterate this process as long as we want or we can say okay stop condition is i only have 20 runs because the training is expensive or i want to converge in such a way but in the end yeah this is up to you and it's probably an own topic or video to talk about this but now what is interesting in the end we can see it more or less already that in this time kappa with 10 was better but just taking a look in the end on the real function we see that the hyper parameter we choose is really mandatory or has a big impact if we find the best point or if we find just a locally best point and i'm going to do a video about hyper parameter tuning soon for exactly these optimization problems for now i just hope that you enjoy that's it that's all you needed to know to start with biogen optimization wasn't that hard wasn't it if you want to get even deeper into some parts like acquisition functions just drop in the comments below what you're missing or where you want to go deep in and i make a video about it in general don't forget to subscribe to stay always up to date with the topics that we are providing for you i wish you a nice day and keep optimizing"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Grid Search and Random Search with Scikit-Learn",
        "Video Title":"Machine Learning Tutorial Python - 16: Hyper parameter Tuning (GridSearchCV)",
        "URL":"https:\/\/www.youtube.com\/watch?v=HdlDYng8g9s",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/HdlDYng8g9s\/hqdefault.jpg",
        "ID":"HdlDYng8g9s",
        "Publish Time":"2019-11-23T19:58:19Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"in this video we are going to talk about how to choose the best model for your given machine learning problem and how to do hyper parameter tuning here is a list of topics that we are going to cover in this video let's say you are trying to classify SK learns iris flower data set where based on the petal and sample width and length you are trying to predict what type of flower it is now the first question that arises is which model should I use there are so many to choose from and let's say you figured out that SVM is the model I want to use the problem doesn't end there now you have hyper parameters what kind of kernel and C and gamma of LSU should I be using there are just so many values to choose form the process of choosing the optimal parameter is called hyper tuning in my jupiter notebook i have loaded iris flower data set here and it is being shown in a table format the traditional approach that we can take to solve this problem is we use trained test split method to split our data set into training and test data set here I am using 70\/30 partition and then let's say we first try SVM model okay so first I am going to show you how to do hyper parameter tuning and then we'll look into how to choose the model so just assume that you are going to use SVM model and using SVM model you can train the model and you can calculate the score okay here I randomly initialize these parameters I don't know what is the best parameter so I'm just going with some value the issue here is that based on your train and test set the score might vary right now my score is 95% but if I execute this again extraneous test samples are going to change so it will change from 95 to now it chained to 1 I cannot rely on this method because the score is changing based on my samples for that reason we use k-fold cross-validation I have a video on k4 cross-validation so if you want to pause here and take a detail look at it you can go there but I will just give you an as shown in the diagram what we do in a cave fold cross validation is we divide our data samples into n number of folds here I'm showing five holes and we take fiight raishin in each iteration one fold is test set remaining our training set we find the score for that iteration and we take these individual scores from each iteration and we make an average this approach works very well because you are going across all the samples okay and we have a method called cross well score which can tell you the score of each iteration here what I have done is tried cross well score four five fold so CV is equal to five means five fold and tied this matter on different values of kernels and see okay so here kernel is linear here it is RBF c10 and here C is 20 for each of these combinations I found the scores so these are like five you can see there are five values here and these are the scores from fire attrition you can take the average of these and find out what is your average score and based on that you can determine the optimal value for these parameters but you can see that this method is very manual and repetitive because there are so many values you can supply as a combination of kernel and C right C could be one two three hundred so for how many times you're going to write this line so the other approach you can take is you can just run a for loop okay so I'm doing the exact same thing but using a for loop so I have my possible values of kernel and then C and then I ran a for loop on both of this and I'm supplying those values here you can say see here key L and C well and then find average course right when I execute this I get this course okay so with RBF and one the score is this RBF in ten the score is this and so on just by looking at the values I can say that RBF and the value of C being either 1 or 10 or linear kernel and C being 1 will give me the best score you can see that these scores are low so this way I can find out the optimal score using the hyper parameter tuning but you can see that this approach also has some issues which is if I have 4 parameters for example then I have to run like four loops and it will be too many iterations and it's just not convenient luckily Escalon provides an API called grid search cv which will do the exact same thing okay so grid search cv is going to do exact same thing as shown in this code here in line number 14 alright so I'm going to do the same thing but you will not that we will be able to do that in a single line of code okay so the first thing you do is you import grid search cv from a scaler and model selection and then we will define our classifier the classifier is going to be Chris's CV where the first thing is your model okay my model is sv m dot SVC i am supplying gamma value to be auto if you want gamma to be in your parameters you can do that okay but for this example I'm just keeping it static now the second parameter is very important second parameter is your parameter grid okay in parameter grid you will see I want the value of C to beat one ten and twenty okay these are like different values that you want to try the second parameter is kernel and you want to try the kernel and you want the value of your kernel to be RBF linear okay so these are two values there are other parameters in grid CV for example how many cross validations you want to run grid cells CV is still using cross-validation okay it's just that we are making this particular code blog convenient and we are writing the same thing in one line of course okay so CV is this there is another value called a return train score if you this is some parameter that this method returns which we don't need that's why we are saying okay it is false once this is done you will do model training by saying is dot data and Irish dot target okay and once that is done we will print the cross-validation results when you execute this you get these results now if you look at these results you will notice that you got this mean test score CV results are not easy to view but luckily SK learn provides a way to download these results into a dataframe here I have SQL and documentation and it says that this can be imported into pandas dataframe so that's the next thing I'm going to do and all you guys are I think exports into pandas by now so you just create pandas dataframe and supply CV results as an input and when I run this I get this nice tabular view here you can see that these are the C parameter values and kernel values and these are the scores from each individual split okay B then five fold cross validation that's why you get spread zero to split four and then you have mean test score as well some of these columns in this grid might not be useful so I'm going to trim it down and just look at parameter values and means go so you can see that these are the possible values of param see and then Colonel and these are the scores I got based on this I can say that I can supply for three values into my parameters to get the best performance so we already did hyper tuning of these parameters you see that this how this works right and now you can have many many parameters all you have to do is supply them in parameter grid and this grid sir CV will do permutation and combination of each of these parameters using k-fold cross-validation and it will show you all the results in this nice pandas dataframe I can do dir on my classifier and see what other properties this object has and I see some of the properties such as best estimator best params and based score so let me try best score for so see laughs the dot based score and the base score it is saying point 98 which is well 0.98 is the base score I can also do see a left dot best params and it will tell me the best parameters in our case there are multiple parameters which gives you optimal performance but you can see the point you just run grid search CV and then call based Panem's to find out the base parameters and these are the parameters you are going to use for your model one issue that can happen with grits or cv is the computation cost our data set right now is very limited but just imagine you have millions of data points into your data set and then for parameters you have so many values right now see values I random it took them to be one to ten but what if I just want to try range let's say number one to 50 okay then my computation cost will go very high because this will literally try permutation and combination for every value in each of these parameters to tackle this computation problem Escalon library comes up with another class called randomized search cv randomizer cv will not try every single permutation and combination of parameters but it will try a random combination of these parameter values and you can choose what those iteration could be so let me just show you how that works here I imported analyze CV class from the esculent model selection and the API kind of looks same as grid search CV I supplied my parameter grade my cross-validation value which is again 5 fold cross validation and the most interesting parameter here is an iteration I want to try only two combinations okay here we tried total six you see zero to five so here it will try only two combinations and then we'll call fit method and then we will download the results into data frame when I run this you can see that it randomly tried C value to be 1 and 10 and then kernel value to be linear and RBF when I run this again it change the value of C to be 20 and 10 this way it just randomly tries the values of C and cano and it gives you the base core this works well in practical life because if you don't have too much computation power then you just want to try random values of parameters and just go with whatever comes out to be the best all right we looked into hyper parameter tuning now I want to show you how do you choose a based model for a given problem for our iris data set I'm going to try these two classifiers okay SVM random forests and the logistic regression and I want to figure out which one gives me the best performance you have to define your parameter grid and I am just defining them as a simple JSON object or simple Python dictionary where I am saying I want to try a sphere model with these parameters random forests with these other I want the tree value of random forests to be one five and 10 and this n estimator is an argument in random forest classifier ok similarly the value C is an argument or a parameter in logistic regression classifier once I have initialized this dictionary I can write a simple for loop so I'm just going to show you that for loop here and this for loop is doing nothing but it's just going through this dictionary values and for each of the values it will use grid search CV so you can see that grid search CV the first argument is the classifier which is your model so here you can see the model is classified so just trying each of these classifiers one by one with the corresponding parameter grid that I have specified in this dictionary you can see that parameter the second object second argument and then cross-validation is five I then run my training and then append the scores into this course list when I run this my scores list has all those values and all I'm going to do now is convert those results into pandas dataframe when I do that I see a nice table view which is telling me that for SVM model I am going to get 98% score random forest is giving me 96 and logistic regression is getting little more than 96 so here I have my conclusion that the best model for my iris dataset problem is SVM it will give me 98% score with these parameters so not only we did hyper parameter tuning but we also selected the best model here I have used only three models for the demonstration you can use 100 models for example here okay so this is this is more like trial and error error approach but in practical lives this works really well and this is what people use to figure out the best model and the best parameters now comes the most interesting part of my tutorial which is the exercise you have to do this exercise guys just by watching video you are not going to learn anything so please move your butt work on this exercise here we are going to take SK learns handwritten digits data set and then classify those digits using the listed classifiers and also you're going to find out the based parameters for it post your answer as a video comment below and if you want you can tell your answer with the solution I have provided now my solution is not the best one because I just tried only few parameters so you should try more parameters and I hope you can find better score than me alright so don't click on the solution link until you have tried it yourself thank you very much for watching this video if you like the content please give it a thumbs up subscribe to my channel and share it with your friends thank you very much I will see you next tutorial"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Grid Search and Random Search with Scikit-Learn",
        "Video Title":"GridSearchCV | Hyperparameter Tuning | Machine Learning with Scikit-Learn Python",
        "URL":"https:\/\/www.youtube.com\/watch?v=TvB_3jVIHhg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/TvB_3jVIHhg\/hqdefault.jpg",
        "ID":"TvB_3jVIHhg",
        "Publish Time":"2021-08-13T13:00:12Z",
        "Channel":"Normalized Nerd",
        "Channel ID":"UC7Fs-Fdpe0I8GYg3lboEuXw",
        "Transcript":"hello people from the future welcome to normalized nerd in this video i'm gonna talk about the grid search and how you can implement it using scikit-learn if you are new here then please subscribe to this channel because i make videos about machine learning and data science regularly so without any further ado let's get started okay first of all we need to import the two most important libraries for machine learning that is numpy and pandas and then we will be using the pandas read csv function to read the data set and store it into a pandas data frame now the data that i'm using here is publicly available on kaggle and i will share the link in the video description so make sure to check that then i'm just printing the first few rows from our data frame now from the column names you can probably guess that this data set is actually about the strength of concrete the last column strength is our target variable so given a set of features related to some concrete mixture we need to predict its strength and we can see that we have some features like amount of cement used blast furnace lag fly ash used water used etc obviously you can use all sorts of featured engineering and feature extraction methods on this data set to find the best set of features but in this video i'm not gonna do that the main focus of this video is to implement grid search so i'm just skipping the featured extraction part so now i'm just separating the feature matrix and the target vector into two variables x and y and don't forget to reshape the y vector into a 2d list because the scikit-learn models take only 2d lists and now i'm just going to print the shapes of x and y just to be sure that we haven't messed up before jumping into the grid search we need to first split our data set into train set and test set please remember that grid search is performed only on the training set during the training it does not and should not see the test set because you don't want to tune your hyper parameters based on the test set for the splitting we are using the train test split method and the test size is 20 now only one last step is left before jumping into the grid search and that is to define a base model here i'm gonna use the xg boost regressor why because most of the times you will find yourself tuning the hyper parameters of xgboost model so here i am defining a xd boost regressor because our problem is a regression problem right and in the parameter i am only passing a random state so that you can reproduce the results and please notice i am passing no hyper parameters because we are gonna tune them okay now that we have got a base model we are ready to implement the grid search the first step of grid search is to make a search space so what is the search space well it's nothing but the set of values of the hyper parameters that you want to search in order to find the best set of values for your hyper parameters so to do this we need to make a dictionary and here i am naming it as search space and the keys of the dictionary will be the names of the hyper parameters so for example in estimators denote the number of trees you want to have in the xg boost model and here i am passing three values 100 200 and 500 that means in the grid search process we are going to pick each value for the number of trees and train a model and then we are going to find which value works the best the next hyper parameter is max depth which simply denotes the depth of the tree then we have gamma and the learning rate so our dictionary is ready so from this dictionary can you find out how many models we are gonna train during our grid search process yes 72 because we have got three choices for the first one three choices for the second two choices for the third and fourth choices for the last so if you multiply all this you will get 72 so now we need to import the grid search cv class from the model selection module of sklearn then i'm just creating an object of this grid search now we need to pass all the information regarding our grid search as parameters so the first parameter is obviously the model then we need to pass the search space dictionary then we need to pass some scoring methods here i am using the r2 and rmse scoring matrix now notice that here the rmse is actually negative root mean squared error and to know all the available scoring matrix you can just print the sklearn.matrix.scorers.keys now the scoring matrix is actually very important because we need some number to compare all the 72 models right and scikit-learn will just use these scoring matrix to score each model now comes refit now here i'm passing r square that means the gs object will return a model that is best with respect to the r squared matrix then we have got the cross validation and here i am using five-fold cross validation now if you don't know about the k-fold cross validation then i will highly suggest you to watch the video that i did on k4 cross validation after that verbose to tell how much information we want to print okay so the grid search object is ready and now we need to train this on our training set now if you read them you will understand how different set of hyper parameters affect the scoring matrix okay so the execution is done and now i'm gonna print the best model and the way to do this is to use the best estimated method so here's the model with the best set of hyper parameters and if you want to get only the best set of hyper parameters then you can use the best params method so here's the optimal set of five parameters and you can see the best score of the model by using the best score method and the metric will be what you have used in the refit method so our best model has achieved an r squared value of 0.92 which is pretty good now if you really want to analyze your model in detail then i will highly suggest you to make a csv file of your grid search result so i'm just creating a file from the cv results and i'm sorting them based on the r square score now let me show you how the csv file looks like so in this file you will find all sorts of information corresponding to the grid search that we have just performed now i wanna show you something interesting that will actually help you in making a better and efficient model for example if you look at the parameter in estimators that is the number of trees used in the xgboost model so the top model uses 500 trees and the second best model uses 200 trees so there's a difference of 300 trees and that's a lot of computation power there now if you see the mean r squared value that we got from the fivefold cross validation the best model achieves an r squared value of 0.9228 and the second best model achieves 0.92218 now does it really make sense to use extra 300 trees for such a small improvement in r squared value i don't think so so that's how you can use this csv file to come up with the more efficient model so that was all for this video guys i really hope that you enjoyed learning about grid search and if you found this video interesting then please share this video and please don't forget to subscribe my channel stay safe and thanks for watching [Music] you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Grid Search and Random Search with Scikit-Learn",
        "Video Title":"Build Sklearn Grid Search CV with Random Forest Model",
        "URL":"https:\/\/www.youtube.com\/watch?v=OtCq4NGvTaA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/OtCq4NGvTaA\/hqdefault.jpg",
        "ID":"OtCq4NGvTaA",
        "Publish Time":"2021-03-24T13:00:09Z",
        "Channel":"TechEngineerSchool",
        "Channel ID":"UCssd_k9oZ0CtC_jafMxSVOQ",
        "Transcript":"hello everyone welcome to my youtube channel my name is evidence and in today's video i am going to show you how to use grid search cv to hyper parameter tune a random forest model all right so that's kind of our objective for today and before we get started let me go ahead and look at the documentation for greece source cv so you're going to be using circuit learn great start cv and as you can see right here the green search cv doesn't exist exhaustive search over specified parameter values from estimator and it has different parameters that you can choose from and then it has different attributes and then if you scroll down here you can see the different method that you can use with grid search cv so you can go ahead and read more about the grid source cv in the documentation if you want to but in this case we just go ahead and get started with the tutorial so earlier i did a random forest model and if you go through my youtube channel you find where i showed you how to build a random forest model and how to get the metrics for it so these are we are the initial results from the random forest model that i did we got a mean absolute error 4.2 and then we got a mean squared error of 32.4 so basically the idea with a regression problem problem is that you want a lower error score right you are trying to get a model that will give you a lower error score and the idea behind hyper parameter tuning is changing up the parameters of the model trying to see if you can get a better model or a lower score in this case and um when i did this project when i did the original random forest model i just used the default parameters and this video is going to be focused on hyperparameter tuning there a [Music] half by perimeter tuning using a random forest regressor model so with that being said let's go ahead and get started let's go ahead and import grid search cv module selection import grid search cv control enter runs the cell that are currently in and shift enter on the cell you are currently in and create a new cell below it so gs let's just call this um great search tv model is equal to random forest regressor so earlier i imported random first regressor but i'll go ahead and import it again here so i have random forest regressor here from sidekick to learn and i have gravestone cv from scikit learn and i'm just going to instantiate our model and i'm just going to do random states equal to 42 and basically this random state what it does is every time i run this code it just makes sure that i get the exact same results now let's go ahead and establish the parameters for our great search cv so whenever you are doing a great search cv it is best for you to experiment with one parameter at a time some people make the mistake of inputting a lot of different parameters in your research tv it is just better for you to just um search for one parameter at a time you could also use random search cv and in a different video i'll show you how to do a random search cv but random search tv is very similar to graystar cv but in this case we are just doing a great search cv so let's establish our parameters and the parameter that i want to look at here is the number of estimators that's the first parameter that i want to hyper parameter tune so i'll say n estimators if you look at the random forest regressor documentation you see that ns mirrors is one of the parameters that you can tune so let's give it a number of estimators let's do 50 esso 200 let's do 300 400 500 600 why not let's just do it to 700 for example and so we've um identified this is establishing what perimeter to hyper parameter tune now now that we've done this and the next step is so let's just call it search and let's do equal to great search cv and we are saying that our estimator is um the model that's we instantiated earlier so basically in this case the estimator is like what model should i be using in this case i mean i'm saying that um parameters grade [Music] is equal to params so basically we are saying this is the parameters we want you to search and then here i'm gonna also put n jobs equal to negative one so basically n jobs is like how many processors should i use and jobs the default is none and basically this allows you to do parallel processing using your processors on your computer so the default is none but if you do negative one that means to use all the processors on your computer and that's good if you're not going to be doing anything else but if you put something like negative two it will use all the processes on your computer except one so to if you have eight processors it will use seven and leave one so you have like one processor left to be able to do other tasks without your computer lagging you know so that's basically end jobs and in this case i'm saying just to go ahead and use other processors and also when you are working in google collab you can use gpu like if you are doing something that's like intensive or if you have a large data set and you don't want it to take forever you can use gpu and to use gpu in google collab you go to runtime change runtime type and then from here you choose gpu and whenever you do this you have to restart your wrong time all right so that's how to use gp in google app let's go ahead and run this so we've done our research cv we've instantiated our christmas cv model so now we are going to do search model it's equal to search dot fit we want to fit our training data and then we want to fit our training target so let's go ahead and run this and this might take a while and this is when the gpu comes in very handy and also the processor so finally our model is done so just so you know this is gonna take time to to do right so after our model is done running we want to find out the best parameters so this is the parameter we passed in for the graysearch cv to evaluate now we need to find out which one of these is the best estimator for this project so to do that we just do search model dot best params parameters we want to find out the best parameter so our research cv found that 700 is the best parameter for number of estimators for our random forest regressor model so that's pretty cool we can go ahead and let's experiment and do this whole process again but let's say try out um different parameters but before we do that let's go ahead and use the model that we just fit to kind of do predictions let's go ahead and do search model dot predict and let's use it to do prediction on our x test and our white test we want to use it to pressure on just our estes that was weird all right so we got our predictions from our great search cv model using 700 as the estimator and whenever you build a model you want to have a way to evaluate the model's goodness how good is a model or how bad is your model really you know so for the original random forest model that i did without any hyper parameter tunings just using the default parameters this is the score that i got from the mean absolute error and the mean squared error so using an estimator of 700 now to also get the mean absolute error on the mean squared error to see if it's better or worse than just using the default parameters so let's go ahead and get this matrix for our current model with that metrics you won't know how to evaluate your model right so let's go ahead and get the metrics and let's go ahead and import from sklearn dot metrics import mean absolute error mean squared error so let's evaluate our model let's call it mean absolute error for the search and let's say it's equal to mean absolute error and let's pass in our true value our true value is always the first thing and then our predicted value is the second thing so and now let's get the mean squared error for our model and again we're passing the true value and then the predicted value all right let's go ahead and print out this result to see what our score is so this is the mean absolute error from our grade search and this is the mean squared error from our grade search right let's compare that to our default without using any hyper parameters at all so you can see that using um the default random forest model gave us these error scores and using the random forest model with a an estimator of 700 gave us this result so the results are not significantly different we can further explore our model by doing something like so if we do something like search model dot best estimator we can kind of see more values and as you can see in a random forest regressor model everything all stays the same like all the default is here and then our number of estimators is 700 that's the only thing that we change right because that's what we hyper parameter tuned so now i'm gonna repeat um this hyper parameter tuning process but this time we'll be tuning a different parameter so basically what i have here is very similar to what i have above but but this time things are going to be different here since i know like the best estimator is 700 i'll just go ahead and put it here and put an estimator equal to 700 and this time the parameter that i'm going to be searching is going to be different this one i want to be looking at max depth [Music] and i'm going to give it numbers between 5 and 25 so 5. so the estimate i'm gonna be searching for right now is maximum depth and here i may have to put my end jobs here just to make this faster i'm gonna put it at negative one and basically everything else says stays and the same we can go ahead and name these two just so we know our estimator parameters is this and the number of jobs and i want to go ahead and print what the best parameter is now let's go ahead and run this code and basically we are going to repeat this process and two or three times to kind of um experiment with different parameters so we are done again this took some time um based on what we got here the maximum depth is 25 and again the score um didn't change much compared to what we have above so i actually didn't change these that's why the score didn't change i was like that doesn't look right so i made a typo here and i'm just gonna copy this part of the code and put it here and let's run this again so again well the score didn't change um that much but it's not the exact same thing as above yeah so basically our score didn't change much compared to what we did earlier and again we can repeat this process and let's say do something like maximum so basically we can repeat the same process again and this time since we know that the maximum depth is 25 we can use that here something like max depth let's set that equal to 25 and then in this situation let's search for maximum features and let's just take a guess for the maximum features let's do 2 4 6 8 10 for example and again the same thing nothing changes here so for this one we add hyper parameter tuning for maximum future to consider for each split and let's go ahead and run this and see what the maximum future is so this grid search cv is done and here right it's telling us that the maximum future is 10 but actually this time our score is worse than it was before so basically the point of this video was to show you how to do a great search cv using a random forest regressor and i'm gonna also do this again but i'm gonna do it to a random forest classifier and i'll probably do this again but i'm gonna do it with random search cv instead of graystar cv but the basics of this video was showing you how to run a grid research cv using a random forest regressor model and basically once everything is done you can combine all your results into one model so basically using um the best results that we've gotten based on the few great size cvs we've done so far we could put put it all together and to create one model that you could potentially use as your final model that's assuming that your score just keeps increasing and that's assuming your score keeps getting better our model keeps getting better so we are going to be using all the scores that uh we get from doing aggressor cv to do one last model and this may be better than the then using the default or it could be worse but we don't know yet so basically just using a combination of the factors we got from our christmas cv this is the error scores that we got and then but this is the error score that we got from just using the default value so just using the default parameters gives us a lower error score than using the combination of parameters with of minecraft cv but the whole point in this is not necessarily to get like the best score or to get the lowest error score i just wanted to show you how to use a great search cv for hyper perimeter tuning you can always find me online at evidenceend.com that's my primary website we have data science blogs and as time goes by i'm going to add more more stuff to my data science blog and if you go to my free data science resources you'll be able to get access to this notebook so i create a lot of youtube videos and a lot of blogs and i just find it easier more straightforward to take all my data science resources and put it in one place for you to get access to it and sometimes i release videos on this platform long before i release them on youtube or you can also go to machine learningeducation.com which is my primary platform and once you are here just click on free data science resources and you also be able to get to this page or you can just go to machine learning education dot com slash free to get access to my free data science resources if you like this video please give it a thumbs up and subscribe to the channel if you made it this far in this video but you didn't like it please give a double thumbs down and still subscribe to the channel thank you for watching and i'll talk to you on the next video bye"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Supervised Learning",
        "Topic":"Implementing Grid Search and Random Search with Scikit-Learn",
        "Video Title":"Hyperparameter Tuning in Machine Learning | Random Search | How it Works &amp; Sklearn &amp; Python",
        "URL":"https:\/\/www.youtube.com\/watch?v=pYM6L0H89PY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/pYM6L0H89PY\/hqdefault.jpg",
        "ID":"pYM6L0H89PY",
        "Publish Time":"2020-05-19T00:24:30Z",
        "Channel":"Quinn Wang",
        "Channel ID":"UCgc9e-Ma04hZWogxvw2lp9g",
        "Transcript":"welcome back to another video on hyper premier tuning in this video we're going to introduce random search but continuing off of the last video and talk about how random search makes up for some of the downfalls of grid search though I walk through how you scikit-learn random search to see me module as well as how to implement them manually if you haven't watched the video on grid search TV I highly recommend you checking that out first if you're interested in the details of the scoring and CV pureum there in these two search modules a random search like a grid search searches for a set of hot primers from a given search base that optimizes a predefined loss function but unlike grid search where M search were only utilized some random initialization of parameter values within the range redefine say we're tuning for the same three hyper parameters the number of estimators max features n min sample split and rainforest would sell a reasonable range for all three parameters respectively and the random search algorithm will randomly pick values and the corresponding ranges we can also specify a distribution in this range for example if you choose the uniform distribution within the range the algorithm is going to pick any value within this range with equal probability the idea is that we don't have to search for prey in their combinations so close to each other in space and if we allow for some randomness and picking a point in the search range we're likely to encounter a relatively good result without going over the entire space like a grid additionally our search space is not limited by lines on the grid now there are some very sophisticated theories behind why random search is generally more efficient than grid search as analyzed no paper buy extra and Benjo I'll leave a link to the paper below if you're interested for a more rigorous explanation both theoretically and empirically from a practical standpoint I would say each problem is different because of their innate loss function distributions so you can probably tell which one is better by simply running them for a small number of iterations and compare inputs to random search TV is very similar to that of grid search CB except now we want to pass in a value and either this is the number of times a random parameter combination is chosen to be evaluated for each other the algorithm is going to randomly select a set of koreander in combination to initialize the model train the model and evaluate the result based on the chosen metric and either is defaulted to 10th playing many cases with larger ranges in the search space we want to run more iterations will again look at how it works with the simplest example that s Kalyan provides so here I'm just copy and pasting the example from ramos horta cv documentation page first we're going to import libraries and our stea set directly from the secular datasets done for Amol they're using logistic regression the two parameters that will tune for in this example is C where C is a regularization term and a penalty type either l1 or l2 and I'll define the search space so that we randomly select a value for C between 0 and 4 with a uniform distribution and for the penalty will randomly choose one between l1 and l2 again we don't have to understand what these parameters mean to understand this example because the parameters in search space are going to be specific to the model you choose that such a problem then will define a random search CV passing our model the parameter distribution and here I'm studying and error to 5 and cd25 that one can fit the data the CV results will come out looking something like this which is very similar to the results of a grid search series so I won't go into too much details here we first look at the params these shows us all the combinations of hyper parameters that the random search picked as well as the order of which they're used notice that we have C between 0 and 4 and penalty either l1 or l2 the split test scores 0 1 2 3 & 4 are the respective scores of each fold ask the cross validation test set and as the summary we'll also get the mean test score order just take the average of different test groups to manually implement a random search CB we can talk just take what we had with a manually implemented grid search from the last video and modify that slightly so I'll start by pulling the data into a panda's data frame and prepare an empty CV result dictionary this time Aquarians we'll start with nothing in the list instead of a fully initialized combination so the only difference between a manual implementation of ran search and the grid search is that the number of iterations our grid search runs is determined by the number of parameter combinations in search space where is the number of iterations in a random search can be set by us so instead of looping through each of the possible parameter combinations well let this run for say 20 iterations and then each iteration we'll pick a ring but we ran for using a uniform distribution as our random C value and randomly select either l1 or l2 as the penalty since the parameter combinations are not predetermined or initialized the CV result dictionary with an empty parameter list until this list well the loop executes the result this loop is filled up version of the CV result dictionary which provides the same information as the range search CV module so it's more room for customization as we can see if we were to use the grid search with the search range first of all the search space cannot be a continuous distribution as that will lead to an infinite number of parameter combinations but even if we select say a hundred possible values within zero to four as the candidate value they'll give us 200 possible parameter combinations and the number of iterations will blog very quickly with more parameters to tune however with a random search we can always keep this 20 as the set number of iterations regardless of the range of our search space the problem with both types of tuning methods done neither uses information gained from previous iterations to help the succeeding tuning process let's take the example of the rain forest again and say our search range for a number of estimators includes the number 10 we soon find out that every parameter combination with an estimator close to 10 gives a bad result so this tells us that we might want to only search for combinations with a higher number and estimators however with grid search the algorithm will still take ten as a candidate for an estimators and run every combination with ten estimators and with random search algorithm still going to randomly try out values around ten with the same initial probabilities because both of them have no way of knowing this parameter value generally doesn't do well the problem at hand this means they're doomed to lack efficiency in some way and this is what genetic based search algorithms and bayesian based search algorithms tries to combat in the next few videos we'll introduce the genetic based search algorithm and implement it with a simple example if you're interested don't forget to Like and subscribe I'll see you next time"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Introduction to Unsupervised Learning",
        "Video Title":"Supervised vs. Unsupervised Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=W01tIRP_Rqs",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/W01tIRP_Rqs\/hqdefault.jpg",
        "ID":"W01tIRP_Rqs",
        "Publish Time":"2022-07-27T11:00:30Z",
        "Channel":"IBM Technology",
        "Channel ID":"UCKWaEZ-_VweaEx1j62do_vQ",
        "Transcript":"Supervised and unsupervised learning are two core components in building machine learning models. So what's the difference? Well, just to cut to the chase: supervised learning, that uses labeled input and output data, while an unsupervised learning model doesn't. But what does that really mean? Well, let's better define both learning models, go deeper into the differences between them and then answer the question of which is best for you. Now, in supervised learning, the machine learning algorithm is trained on a labeled dataset. So this means that each example in the training dataset, the algorithm knows what the correct output is. And the algorithm uses this knowledge to try to generalize to new examples that it's never seen before. Now, using labeled inputs and outputs, the model can measure its accuracy and learn over time. Supervised learning can be actually divided into a couple of subcategories. Firstly, there is a category of classification. And classification talks about whether the output is a discrete class label such as \"spam\" and \"not spam\". Linear classifiers, support vector machines, or SPMs, decision trees, random forests - they're all common examples of classification algorithms. The other example is regression. The output here is a continuous value, such as price or probability. Linear regression and logistic regression are two common types of regression algorithms. Now, unsupervised learning is where the machine learning algorithm is not really given any labels at all. And these algorithms discover hidden patterns in data without the need for human intervention. They're unsupervised. Unsupervised learning models are used for three main tasks, such as clustering, association and dimensionality reduction. So let's take a look at each one of those, starting with clustering. Now clustering is where the algorithm groups similar experiences together. So a common application of clustering is customer segmentation, where businesses might group customers together based on similarities like, I don't know, age or location or spending habits, something like that. Then you have association. And association is where the algorithm looks for relationships between variables in the data. Now association rules are often used in market basket analysis, where businesses want to know which items are often bought together. You know, something along the lines of, \"customers who bought this item also bought \", that sort of thing. The final one to talk about is dimensional ... dimensional reduction. And this is where the algorithm reduces the number of variables in the data, while still preserving as much of the information as possible. Now, often this technique is used in the pre-processing data stage, such as when autoencoders remove noise from visual images to improve picture quality. Okay, so let's talk about the differences between these two types of learning. In supervised learning, the algorithm learns from training datasets by iteratively making predictions on the data and then adjusting for the correct answer. While supervised learning models tend to be more accurate than unsupervised learning models, they do require all of this up-front human intervention to label the data appropriately. For example, a supervised learning model can predict how long your commute will be on the time of day and thinking about the weather conditions and so forth. But first you'll have to train it to know things like rainy weather extends the driving time. By contrast, unsupervised learning models work on their own to discover the inherent structure of unlabeled data. These models don't need humans to intervene. They can automatically find patterns in data and group them together. So, for example, an unsupervised learning model can cluster images by the objects they contain - things like people and animals and buildings - without being told what those objects were ahead of time. Now, an important distinction to make is that unsupervised learning models don't make predictions. They only group data together. So if you were to use an unsupervised learning model on that same commute dataset, it would group together commutes with similar conditions like the time of day and the weather, but it wouldn't be able to predict how long each commute would take. Okay, so which of these two options is right for you? In general, supervised learning is more commonly used than unsupervised learning, and that's really because it's more accurate and efficient. But that being said, unsupervised learning has its own advantages. There's two that I can think of. Firstly, unsupervised learning can be used on data that is not labeled, which is often the case in real world datasets. And then secondly, unsupervised learning can be used to find hidden patterns in data that supervised learning models just wouldn't find. Classifying big data can be a real challenge in supervised learning, but the results are highly accurate and trustworthy. And in contrast, unsupervised learning can handle large volumes of data in real time. But there's a lack of transparency into how that data is clustered and a high risk given accurate results. But wait, it is not an \"either\/or\" choice. May I present to you the middle ground known as semi-supervised learning. This is, well, a happy medium where you use a training data set with both labeled and unlabeled data. And it's particularly useful when it's difficult to extract relevant features from data when you have a high volume of data. So, for example, you could use a semi-supervised learning algorithm on a data set with millions of images where only a few thousand of those images are actually labeled. Semi-supervised learning is ideal for medical images, where a small amount of training data could lead to a significant improvement in accuracy. For example, a radiologist can look at and label some small subset of CT scans for tumors or diseases, and then the machine can more accurately predict which patients might require more medical attention without going through and labeling the entire set. Machine learning models are a powerful way to gain the data insights that improve our world. The right model for your data depends on the type of data that you have and what you want to do with it. And the choice between supervised and unsupervised learning is only the first step. If you have any questions, please drop us a line below. And if you want to see more videos like this in the future, please like and subscribe. Thanks for watching!"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Introduction to Unsupervised Learning",
        "Video Title":"Introduction to unsupervised learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=_Tf1Vi4s7Ec",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/_Tf1Vi4s7Ec\/hqdefault.jpg",
        "ID":"_Tf1Vi4s7Ec",
        "Publish Time":"2020-04-29T13:06:16Z",
        "Channel":"Herman Kamper",
        "Channel ID":"UCBu4J-JIs-UORp5pQ6M48nw",
        "Transcript":"hello everyone so we're now switching from supervised learning to unsupervised learning unsupervised learning is a really exciting area it's also a very very challenging area so in supervised learning not only what happened was we got a training set of data and each item in our training set will have a label maybe a class or a value that we're trying to predict in contrast in unsupervised learning we get a data set but the items aren't labeled we don't have a target time that we're trying to predict and we want to make sense of the data of this unlabeled data set and this video will look at a few examples of different tasks which try to make sense of unlabeled data sets and specifically I'll just introduce at a high level dimensionality reduction and clustering two very important unsupervised learning tasks in supervised learning we normally get a data set and the data set has some input vectors which we've denoted as X in previous videos so you would have your first training item X 1 you would have your second training item X 2 and so on up to xn and each one of these input vectors this is a set of features for your first training item for your second training item each one of these feature vectors will be paired with the target value that you're trying to predict and we've denoted that as Y so you would have a y1 for your first training item or y2 for your second training item and so on up to Y n for your end training item the goal in supervised learning is to take this training data set and build a model which can take in an unseen feature vector and then predict what the target value will be for that new input which we haven't seen before in regression the task will be to predict some continuous value of y and in classification the task will be to predict some discrete label y where each of the inputs belongs to one of a finite set of classes so as an example let's quickly look at binary classification let's say that each of our input vectors consists of two features x1 and x2 and let's say that our output variable Y can take on one of these two values either the input is in this class the orange square clause or it is in the green triangle clause and let's say our data set looks like this so for instance this might be the fifth training point so this vector there would be X 5 okay and that I'm training vector is of Cloths green triangle and from this data we might want to train a model which can predict what type of Clause an unseen input is and we can use different types of models for doing this we've looked at decision trees we've looked at logistic regression if we are using logistic regression then maybe the model puts a decision boundary around here and everything on on this side of the decision boundary is classified as it as a green triangle and everything on this side is classified as our inch square that is supervised learning where each one of our input vectors is paired with a corresponding label on the output side let's see how that compares to unsupervised learning in the case of unsupervised learning we also have a data set of course so we might have our first feature vector our second feature vector and so on up to our end feature vector but instead of having labels for each of these vectors as we did for supervised learning we simply just get the data and as it is here so they're not labeled as an unlabeled data set and we might want to make sense of this data so for instance let's pretend that again we've got two features x1 and x2 and let's say our data looks like this so in this case I've drawn all the data just with a blue circle a little blue circle some of them are really ugly like this one but they're all basically unlabeled you have no way of distinguishing this circle from this circle from this circle apart from the values that x1 and x2 takes on so there's no y target value with any of them and that's why they're all drawn with with these blue ugly circles and what we might want to do is we might want to look at this data in this form and try and make sense of of the data we might try and find some structure and if you look at this made-up dataset here you can clearly see that there is some structure in the data this for example a little grouping here a grouping yeah and a grouping here so the idea behind unsupervised learning is basically to see what can we learn just from the data itself without em any labels it's a really powerful idea because actually a lot of the time we have access to just way more data without labels then with with data with labels but it's also super challenging because it's actually and sometimes quite ill-defined exactly what you're looking for in supervised learning we know exactly in this case that the goal is the goal of the game is basically to classify things as orange squares or green triangles but in unsupervised learning when you get a data set like this it's a much harder to see exactly what the what the goal is nevertheless there's a number of things we can we can do by just looking at the data and we'll look at two specific examples of unsupervised learning tasks and in this video I'm just giving a high-level introduction to to these two tasks and then in follow-up videos we'll go into a bit more detail the first unsupervised task which will briefly look at in this video is dimensionality reduction so here you're given a dataset I basically just brings out the vectors in in Python normally what we would do is we would write this first vector we would write that as x1 and we use the convention of column vectors so we would have zero point five nine all of the values up to three point zero seven like this and so so this one would be the first data point this would be the second item third item and so on and we've got a number of these of these vectors and I saw some of the features actually this is this is some real data and actually each of the vectors has 256 different features so we could write that X our data has a dimensionality of 256 and we're just given this data set and now we basically want to do something with it so I guess in this case it's actually important to know something about where the data comes from and that's that's often the case in unsupervised learning so this data is actually a real data set where each one of these vectors actually represents a spoken word and I won't go into the details of exactly how we get the features for each of the spoken words but you can just know that each of these vectors is a 256 dimensional vector representing one word some of the words are the same so maybe x1 can be the word Apple and x5 can also be the word Apple x2 might be another word running x3 might be dog and so on we're not given the labels we don't actually know which word is being said or which word is represented for each of the vectors because this is an super starsk we're just given this data set with all these different vectors if we were in a supervised setting we might know that oh this first item corresponds to word Apple okay and then we could maybe train a classifier that reads in a unseen X vector and then predicts which word is being said but that's not the case here we're just given this this dataset this is actually a problem that I work on in my own research so where would this occur so imagine you have a robot that's in our house the robot doesn't know the language since that's spoken around it it doesn't know the words that's spoken around it but it wants to learn the language of of the other people living in the house and all that the robot observes is basically these vectors each vector representing a different word but the robot doesn't know what the words are so this is our actual data set is actually around 500 different vectors so we go up to an X 500 here what do we want to do with this day so maybe the first thing that we want might want to do is we might just want to look at the data and in this case it's actually quite difficult because I have no idea how to visualize something in 256 dimensions for my brain that's that's just really hard so maybe one thing we would want to do is we might want to map each of these vectors to something in two dimensions so we want to take X and we want to change it through some mapping to another vector and we let's call that set in two dimensions and what we would like is that this zet vectors these eight vectors will still capture some properties of the data set and that's actually the goal of dimensionality reduction you're given a data set and then from that data set you want to learn some mapping which allows you to take an input in the original feature space and then map it to another fake in a lower dimensional space and of course this mapping will throw away some information and the information you throw away that will basically depend on the type of problem that you're looking at but also the particular dimensionality reduction approach that you use so if we apply dimensionality reduction to this data set using a particular method what happens this is actually the output from a particular dimensionality reduction technique called TCE and what we've done here is we've mapped each of our vectors from the previous slide each of the x ends with which were in 256 dimension we've mapped this to as 8n in two dimensions and that allows us to plot the data on a two dimensional plot that you can see on a screen for example on youtubes and each one of these points is corresponds to one of the exes arm that we fed into this algorithm and that X was changed from a 256 I mention vector to a 2 dimensional vector and that allows us to look at this at the data set so this is one thing that we would often want to do with dimensionality reduction time internality reduction allows us to visualize high dimensional data ok so this is one particular use case and his visualization and TCE particular is a very very good dimensionality reduction technique specifically developed to some extent for visualizing data while it was very hard if you just looked at the 500 data points each 256 dimensions was very hard to make sense of that if you look at this plot here you can already start to see some structure in the data dimensionality reduction can also be used in other cases so even in a supervised setting where we've got a bunch of input vectors and some target values that we're trying to predict sometimes it might be computationally to expensive to fit a model directly on your original high dimensional vectors and in that case what you can do is you can use unsupervised dimensionality reduction technique to first take your original inputs and then map that to some lower dimensional inputs and then you can fit the machine learning model on those lower dimensional inputs so dimensionality reduction can be useful in cases where computational cost is a particular issue I should know that it's actually quite important to just think carefully and not just blindly do dimensionality reduction in all cases very often it's it's better to first try and fit a model on the original data and then see what happens before you're trying to save computational cost without that actually being so important for your particular situation nevertheless dimensionality reduction is very very useful in cases where computational cost is very very important another use case for dimensionality reduction is just in compression so if you have a very large data set of millions and millions of different points different items and each of the items have a very large dimensionality then it might be simply too expensive to save all of that data and in that case it might be useful to map your inputs to a lower dimensionality and then just save the data in the lower dimensionality so the third use case of dimensionality reduction is compression and different dimensionality reduction techniques might be appropriate depending on which particular problem you're trying to solve I've mentioned that this plot was generated using Teasley we will look in a follow-up video to one of the fundamental dimensionality reduction techniques called principal component analysis and we will step through the details of that technique the other unsupervised learning tasks we will look at is called clustering so in clustering the goal is to group your items so that each group contains items which are similar to each other and these items should be different from items that are put into a different group so if you look at this example for instance remember these are actually all each each of these items represents a spoken word so this is one spoken word this is another spoken word this is another spoken word some of the words are the same but there are actually many different types of of words and this relatively small data set of 500 points maybe I could ask you how many words to you guess and there isn't this data set it's a little bit hard to see but if I had to guess I would say like maybe all of this stuff here all of these items they might be the same word maybe these items here might be the same word not sure maybe these items may be these ones then you have a odd-looking cluster maybe the cluster runs somewhere yeah just I'm just guessing at this point right and maybe there is a cluster running running somewhere here so the goal in clustering is basically to take your data and to partition it in groups like this and this group the items within this group are more similar to each other than they are to the items for example in the group here so in the example here this might be a robot that observed different words that said it's in its environment it doesn't know what the words are all right it doesn't have a label for the different words but by just looking at the data and maybe applying a clustering technique you can say listen I don't know what this word is but I do think it's the same as this wood in this wood and those words are different from the words for example in the cluster down here again there are a number of different clustering techniques and and we'll look at a few of them so apart from this language learning example clustering has many applications imagine you're collecting data from tissue samples maybe for breast cancer detection and you have a number of measurements a number of features for different tissue samples maybe you would want to cluster that data to see if there are maybe subcategories of different types of of breast cancer maybe you're in marketing and you've got data from different types of of users and you want to see whether some users are similar to other users and then maybe recommend things that that other like-minded users have have chosen one area where clustering is often used these days is in social network analysis so for example this is a plot of I think a facebook friend network and you could maybe see that here we might think that this is a cluster and this is a cluster and maybe this is a cluster this video has introduced unsupervised learning by looking at just two particular tasks and in the next videos we will actually go into these two task clustering and dimensionality reduction in a bit more detail looking at some specific techniques"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Introduction to Unsupervised Learning",
        "Video Title":"Unsupervised Learning | Unsupervised Learning Algorithms | Machine Learning Tutorial | Simplilearn",
        "URL":"https:\/\/www.youtube.com\/watch?v=D6gtZrsYi6c",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/D6gtZrsYi6c\/hqdefault.jpg",
        "ID":"D6gtZrsYi6c",
        "Publish Time":"2020-05-22T04:30:04Z",
        "Channel":"Simplilearn",
        "Channel ID":"UCsvqVGtbbyHaMoevxPAq9Fg",
        "Transcript":"in this lesson you are going to understand the concept of unsupervised learning by the end of this lesson you will be able to explain the mechanism of unsupervised learning use different clustering techniques in python overview unsupervised learning is a machine learning technique used to train the machine learning algorithm using data that is either unclassified or unlabeled and allows the algorithm to act on that data without guidance unlabeled data is a designation for pieces of data that have not been tagged with labels identified by characteristics properties or classifications so the flow of unsupervised learning starts with training data that has no labels and depends on the feature vector the machine learning model defines the predictive model this is tested with an individual subset of data with its own feature vector here the predictive model defines the likelihood or cluster id or a better representation of unlabeled data let's look at the difference between unsupervised and supervised learning supervised learning technique deals with labeled data where the output data patterns are known to the system unsupervised learning works with unlabeled data in which the output is just based on the collection of perceptions supervised learning method is less complex the unsupervised learning method is more complex supervised learning conducts offline analysis unsupervised learning performs real-time analysis the outcome of the supervised learning technique is comparatively more accurate and reliable unsupervised learning generates moderately acute but reliable results while classification and regression are the types of problems solved under the supervised learning method unsupervised learning includes clustering and associative rule mining problems example and application of unsupervised learning let's understand unsupervised learning through an example consider a scenario where a child had no learning phase and is shown images without the labels now if the child is asked to identify if any range is a bird or an animal he will lack the information that can help him do so the best he can do is come up with the following groups based on common patterns wings and legs for example this explains how unsupervised learning works we show a lot of data to our algorithm and ask it to find patterns in the data by itself let's look at the application of unsupervised learning unsupervised learning can be used for anomaly detection as well as clustering to understand clustering let's look at a simple real-life example a mother asks her two children to arrange the pieces of playing blocks the children come up with two different groups as shown with different similarities in the blocks this is clustering each of our children came up with a different type of grouping one child grouped them based on the shape whereas the other grouped them based on the color there is no right or wrong way then how can you pick one set of clusters over the others this will depend on the similarity measure used by the mother in this case the arrangement of child 1 is better than child 2 if the similarity measure chosen by the mother was that blocks should have the same shape however the arrangement by child 2 is better if the similarity measure chosen by the mother was that blocks should have the same color therefore defining the similarity measure is important when performing clustering there may be different ways in which data can be arranged in different groups based on size shape color texture and other complex features anomaly detection is a clustering technique used to identify unusual patterns that do not conform to expected behavior anomaly detection has many applications in business such as intrusion detection system health monitoring and fraud detection clustering the method of grouping similar entities together is called clustering the goal of this unsupervised machine learning method is to seek out similarities within the data points and to cluster similar data points together need for clustering let's look at the need for clustering grouping similar entities together helps to merge the attributes of different clusters in other words this gives us insight into underlying patterns of different groups there are a lot of applications of grouping unlabeled data for example in order to maximize the revenue you can identify different groups or clusters of customers and market to each group in a different way another example is grouping books together that belong to similar topics clustering is needed to determine the intrinsic grouping in a set of unlabeled data organize data into clusters that show internal structure of the data partition the data points understand and extract value from large sets of structured and unstructured data types of clustering there are two types of clustering hierarchical clustering and partitional clustering hierarchical clustering can be agglomerative and divisive whereas partitional clustering can be k means and fuzzy c means a distinction among different types of clustering is whether the set of clusters is nested or unnested a partitional clustering is just a division of the set of data objects into non-overlapping sets or clusters such that every data object is in just one subset a hierarchical clustering is a tree structure that has a set of nested clusters hierarchical clustering the output of hierarchical clustering is a hierarchy how does the hierarchical clustering form a hierarchy assume you are going to create a three-layer hierarchy from six different data nodes so first combine a and b based on similarity and also combine d and e based on similarity combination of a and b is combined with c in the similar way combination of d and e is combined with f now combine c and f inside one cluster when you look at the final tree it contains all clusters combined into a single cluster let's understand the working of hierarchical clustering it works in four steps step one assign each item to its own cluster such that if you have n number of items you will have n number of clusters step two merge two clusters into a single cluster by finding the closest pair of clusters now you will have one cluster less step three compute distances between the new cluster and all old clusters step four repeat steps two and three until all items are clustered into a single cluster of size n let's understand the distance measure in hierarchical clustering let's look at the different kinds of linkage in clustering complete linkage clustering it finds the maximum distance between points belonging to two different clusters single linkage clustering it finds the minimum possible distance between points belonging to two different clusters mean linkage clustering it finds all possible pairwise distances for points belonging to two different clusters and then calculates the average centroid linkage clustering it finds the centroid of each cluster and calculates the distance between them what is dendrogram it is a tree diagram frequently used to illustrate the arrangement of the clusters produced by hierarchical clustering it shows the hierarchical relationship between objects it is most commonly created as an output of hierarchical clustering the main use of a dendrogram is to work out the best way to allocate objects to clusters the dendrogram also shows the hierarchical clustering of five observations and the relationship between each of them hierarchical clustering example let's understand hierarchical clustering through an example in the given example hierarchical clustering is used to find the distances between the different cities in kilometers the following matrix traces a hierarchical clustering of distances in miles between different cities the method of clustering is single link here as you can see from the given distance matrix the nearest pair of objects is t o and mi mi and tio are merged into a single cluster called mito as mi column has lower values than to column mito consists of mi column values mito column has one index with zero value this is because there is no distance between cluster m i t o and m i t o to get a new distance matrix we compute the distance from this new cluster to all other clusters now the nearest pair of objects is n a and rm these are combined into a single cluster called narm to get a new distance matrix we compute the distance from this new cluster to all other clusters in the similar way the nearest pair of objects is ba and narm these are combined into a single cluster called ba n a rm to get a new distance matrix we compute the distance from the new cluster to all other clusters similarly now the nearest pair of objects is ba narm and fi these combined into a single cluster called b-a-n-a-r-m-f-i to get a new distance matrix we compute the distance from this new cluster to all other clusters finally we merge the last two clusters this process is summarized by the clustering diagram on the right and the final distance matrix on the left demo clustering animals problem scenario consider the data set zoo dot data and look at the information provided in the first five rows the first column denotes the animal name and the last one specifies the high level class for the corresponding animal you are supposed to find a solution to the following questions one identify the unique number of high level classes two perform agglomerative clustering using the sixteen intermediate features three compute the mean squared error by comparing the actual high level class and the predicted high level class in a nutshell you just have to perform agglomerative clustering with the appropriate mse value let's import the required libraries and the data set since we have now loaded the data set we will extract some basic information from it as our first step with the info command it is clear that the data set has 18 columns in total and 101 entries also there are no null values let us now proceed towards the first question which is extracting the unique number of high-level classes most probably the unique function from numpy will help we can plot the unique number of labels obtained using the matte plot lib library create a figure and a set of subplots from the plot it can be seen that we have seven unique class labels now since we are about to group animals based on their features it's clear and quite predictive that clustering should be performed let's now extract the features leaving the labels column and store them in another data frame say features import the necessary modules for performing clustering specify the number of clusters as seven note that here we are specifying the total number of clusters as seven because there are seven unique class labels also specify the linkage method as average and the similarity method as cosine fit the agglomerative clustering model over the feature variable defined earlier let us extract the labels predicted by our model against the features we can see that we have predicted labels against all of our 101 animals although we have seven labels but it is numbered as six so in this case we can subtract one from our original label column such that it matches the predicted numbers now let us move ahead and predict the accuracy of our model considering the predicting parameter as mean squared error now evaluate the absolute error by applying square root operation on the mean squared error print the resultant error the root mean squared error we got is 2.43 approximately which is quite acceptable now that we have clustered the animals let's quickly recap the steps we have covered import libraries in the data set check for missing values identify unique labels and plot them extract features necessary for clustering within a single variable fit agglomerative clustering model on the feature data predict labels for each animal print the rmse of the model k means clustering let's look at the steps involved in k-means clustering k-means is an iterative clustering algorithm whose goal is to find local maxima in each iteration this algorithm works in these four steps specify the desired number of clusters k randomly assign each data point to a cluster compute cluster centroids reassign each point to the closest cluster centroid and recompute cluster centroids in order to check if the convergence criterion is met consider the dots given in the diagram as the data points first k-means randomly chooses k examples data points from the data set the three colored points as initial centroids this is because it does not know yet where the center of each cluster is a centroid is the center of a cluster assign data points to the nearest centroid then all the data points that are the nearest to a centroid will create a cluster as you can see there are three centroids as red blue and purple and all the data points of the same color is one cluster so in total we have three clusters now now we have new clusters that need centers a centroid's new value is going to be the mean of all the examples in a cluster centers are moving because a centroid will have the value of the mean of all the data points in a cluster we'll keep repeating steps 2 and 3 until the k-means algorithm is converged that is until the centroids stop moving optimal number of clusters determining the optimal number of clusters in a data set is a fundamental issue in partitioning clustering such as k-means clustering this requires the user to specify the number of clusters k to be generated if you plot k against sse you will see that the error decreases as k increases this is because their size decreases and hence distortion is also smaller the basic idea behind partitioning methods such as k means clustering is to define clusters such that the total within cluster sum of square wss or the total intra cluster variation is minimized the elbow method looks at the total wss as a function of the number of clusters one should choose a number of clusters so that adding another cluster doesn't significantly improve the total wss it works in the following way compute clustering algorithm for different values of k for instance by varying k from 1 to 20 clusters calculate the total within cluster sum of square wss for each k value according to the number of clusters k plot the curve of wss the location of bend in the plot is generally considered as an indicator of the appropriate number of clusters demo cluster-based incentivization problem scenario lithium power is the largest producer of electric vehicle e-vehicle batteries they provide batteries on rent to e-vehicle drivers drivers rent a battery typically for a day and thereafter replacing it with a charged battery from the company lithian power has a variable pricing model based on the driver's driving history battery life depends on factors such as over speeding distance driven per day etc you are supposed to create a cluster model where drivers can be grouped together based on the driving data and to group the data points so that drivers will be incentivized based on the cluster let's import the libraries numpy and pandas import visualization libraries namely matplotlib and seaborne import the warning module the warning module was introduced in pep230 as a way to warn programmers about changes in language or library features in anticipation of backwards incompatible changes coming with python 3.0 import met plot lib library for visualization and an instance of rc params for handling default met plot lib values please note for the sake of simplicity we will take only two features mean distance driven per day and the mean percentage of times a driver drove higher than 5 miles per hour over the speed limit let us go through each of the columns first and understand them the id column represents the unique id of the driver the mean underscore dist underscore day column represents the mean distance driven by driver per day and the mean underscore over underscore speed underscore purse represents the mean percentage of the times a driver drove higher than 5 miles per hour over the speed limit let's start with using pandas to read driverdata.csv as a data frame called df we will now use the info command to check the number of columns in total and entries also this will let us know if we have any missing values in addition to it we will use the describe function here to check the count mean and median values for each column now we will import k means from sklearn dot cluster and run the algorithm with k equals 2 which is the minimum number of clusters that can exist in a data set also let us create an instance of the k-means model with two clusters such that it becomes easier to call the same later please note that we have dropped the id column as it doesn't have any reference in forming clusters let's now fit the model to the data the algorithm is now fitted on our data and you can claim that it has created the clusters let us now use some commands to get some information on these clusters we will use the command cluster centers from k means to determine the cluster center vectors use the labels underscore command along with print to display the labels also we can go for the length of those labels now let us check how many unique drivers are there in the first and second cluster we will set the theme as white grid as it is better suited to plots with heavy data elements plot the clusters using the lm plot function from the seaborne library such that we have mean underscore dist underscored day feature on x-axis and mean underscore over underscore speed purse on y-axis we can clearly see from the graph plotted that there are two clusters one centered around 50 mean distance delay and the other around 175 also we can see that there are more drivers in the cluster with the delay centered at 175. since k means clustering gives optimum results when iterated multiple times let's try out the same with increasing the number of clusters say four print the cluster centers with four clusters and track the four unique labels along with their frequency of occurrence zip the unique number of cluster and their frequency counts within a dictionary we can clearly see the difference now in cluster centers also here we have a distribution of data points in each cluster let's now plot the same such that we have mean underscore dist underscored day feature on x-axis and mean underscore over underscore speed purse on the y-axis from the four cluster plot we can see that it's denser compared to the two cluster plot and hence more optimal now that we have clustered the data with the k means let's quickly recap the steps we've covered import libraries and the data set fit the k-means model on the data set evaluate cluster centers into labels plot the clusters to see the distribution of data points iterate the same by changing the number of clusters to four again evaluate the cluster centers plot the clusters to see the distribution of data points draw inference out of both plots this brings us to the end of unsupervised learning you are now able to explain the mechanism of unsupervised learning use different clustering techniques in python hi there if you like this video subscribe to the simply learn youtube channel and click here to watch similar videos turn it up and get certified click here"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Introduction to Unsupervised Learning",
        "Video Title":"ML 4 : Unsupervised Learning with Examples | ML Full Course",
        "URL":"https:\/\/www.youtube.com\/watch?v=3TbU9HI8Io4",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/3TbU9HI8Io4\/hqdefault.jpg",
        "ID":"3TbU9HI8Io4",
        "Publish Time":"2021-09-13T07:39:21Z",
        "Channel":"CS & IT Tutorials by Vrushali \ud83d\udc69\u200d\ud83c\udf93",
        "Channel ID":"UCapyHygDZ_pEJ5pwwZZKV3w",
        "Transcript":"good morning this is roshani and welcome to cs and it tutorials by richarlis in my last lecture we learned about introduction examples of machine learning than training versus testing data set and we also learn about supervised learning algorithm i have mentioned the link of that videos in below description box so in this video we will learn about the next type of learning that is unsupervised learning let's see following points are covered in this video which includes types of learning unsupervised learning algorithm introduction then working of supervised learning algorithm why use unsupervised learning then types of unsupervised machine learning algorithm type there are types like clustering association and advantages of disadvantages of this unsupervised learning algorithm so let's start so these are the types of learning we already learned in last lecture that is supervised learning unsupervised learning and reinforcement learning in this session we will learn about unsupervised learning algorithm next so what is mean by unsupervised learning algorithm in my last lecture we learn about supervised learning algorithm in supervised learning algorithm we will pass label input data to the machine for the training purpose right we already passed a particular image that is image of apple and their label is apple this kind of data input data to the machine for the training purpose this is a concept of supervised learning algorithm but now in unsupervised learning algorithm we will pass unlabeled data set to the particular machine for the training purpose means here we will pass the particular collections of images for example say in this diagram there are various collections of images of birds or animals right so we will pass this data sets to the machine for the training purpose so in unsupervised algorithm machine can be identify the structure of data set machine can identify the particular groups then particular functionality similarities than particular characteristics structure of each and every animals these all things can be identified the machine and after that they can predict a particular result so this is a concept of unsupervised learning algorithm we didn't pass label data to the machine for training purpose okay so for that purpose we will use different algorithms like k-means clustering k-nearest neighbor algorithm hierarchical clustering anomaly detection neural network then principal component analysis and a priori algorithm so we will learn all those algorithm in next session so here the concept of unsupervised learning algorithm is we will pass unlabeled data to the machine for training purpose and machine identify from those images and cluster them or group them as per their characteristics this is a concept of unsupervised learning algorithm next now see here this is just working of unsupervised learning algorithm in this image see in this image there is a there are total four images of there are two images for cat and two images for dog right we will pass these images as an input to the machine these are the unlabeled input data there are only images right this unlevel input data pass to the machine for the training purpose now machine by using some algorithms like k-means clustering algorithm decision tree algorithm machine can be identified some pattern of that particular images structure characteristics right this all kinds of specifications identify the machine for that particular images okay then after training and after identifying the machine the particular model can be generate the particular groups of that data set for example see here there is a docs and there are the kites this is the output of that particular machine by using unsupervised learning algorithm see this is a working of unsupervised learning algorithm machine identify the patterns of those images and as per their pattern group the data as per their characteristics machine can be grouped the data or cluster the data by using some algorithms so this is a working of unsupervised learning algorithms next now why we use unsupervised learning algorithm so unsupervised learning algorithm is very helpful for finding the useful insights from the data useful insights means some specification unsupervised learning algorithm use different algorithms for find out the feature extraction feature selection this kind of approach purpose they find out the hidden structure of each and every data right they find out the structure of cat image and they find out the structure of dog image so they find out the pattern of each and every image this is a main use of unsupervised learning algorithm then unsupervised learning algorithm works on unlabeled data and uncategorized data there is a combination of all kinds of data sets right so they can be grouped those images as per their features in real world we do not always have input data with corresponding output right nowadays there are large amount of data vast amount of data sets are available sometimes data are in sorted format sometimes data are not in sorted formats so at the time unsupervised learning algorithm is very useful to identifying the clustering of data set as per their similarities so that's why we use unsupervised learning algorithms now there are two types of unsupervised machine learning algorithms that is clustering and association so let's see the first type of unsupervised algorithm is clustering so what is mean by clustering so cluster cluster means group the group of similar functionalities this is called as cluster okay so clustering is a method to grouping the objects into the cluster which having the most similarities right see in this diagram suppose this is your raw data okay there are different images of strawberries and different images of apples and lemons etc so the particular machine identify the features the patterns this particular characteristics of that particular images and cluster those images as per their characteristics this is a working of unsupervised learning algorithms so cluster analysis finder commonalities between the data objects and categorize them as per the presence and absence of those commonalities this is called as clustering and now the next type of unsupervised algorithm is called as association so association means to find out the relationship between the variables the best example of association is market strategy or market basket analysis suppose a particular person buy x item then they also buy a buy item this is called as association for example see here suppose a particular person buy a bread from supermarket then they also buy a better so this is called as uh relation or relativity purpose of particular product suppose a particular customer buy a mobile then they also buy a headphones right so analysis of all those data or analysis of all those interrelated data this is called as association so it determine the set of items that occur and that can be purchased together this is called as association these are the different examples that is market basket analysis then statistical data analysis social network analysis for example facebook facebook always gives you a suggestion regarding your nearest friend or the particular persons that working on same organization same companies right it shows your suggestion regarding this it shows your suggestion regarding friends of friends like this so this is called a social network analysis then image segmentation anomaly detection so this is the concept of association and for that purpose we will use different unsupervised learning algorithms now these are the some advantages and disadvantages of unsupervised learning algorithm the advantages of unsupervised algorithms are that will be useful for more complex tasks because it requires large amount of data sets labels are not necessary right so that will be used for more complex tasks as compared to supervised learning algorithms then unsupervised learning algorithm preferable it's very easy to get unlabeled data and comparison to label data because nowadays unlabeled data is much available as compared to label data right so i identify their patterns and group them by their similarities this is the work of unsupervised learning algorithm now the disadvantages of unsupervised learning algorithms are that will be more difficult than supervised learning because it does not have corresponding output right then the result of unsupervised algorithm might be less accurate as input data is not labeled right suppose your input is with label data as like to supervise learning algorithm so your output is more accurate right but in unsupervised learning algorithm levels are not given so machine can be learned as per their pattern and as with their features so uh the identification and prediction is not much accurate as compared to supervised learning algorithm because sometimes some images are very similar to each other so this is the disadvantages of unsupervised learning algorithms so thank you hope so you should understand the concept about unsupervised learning algorithms so if you like my videos please subscribe the channel thank you keep practicing"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"K-Means Clustering",
        "Video Title":"StatQuest: K-means clustering",
        "URL":"https:\/\/www.youtube.com\/watch?v=4b5d3muPQmA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/4b5d3muPQmA\/hqdefault.jpg",
        "ID":"4b5d3muPQmA",
        "Publish Time":"2018-05-23T20:18:55Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"statcast [Music] stat quest stat quest stat quest hello I'm Josh stormer and welcome to stat quest today we're going to be talking about k-means clustering we're gonna learn how to cluster samples that can be put on a line on an XY graph and even on a heat map and lastly we'll also talk about how to pick the best value for K imagine you had some data that you could plot on a line and you knew you needed to put it into three clusters maybe they are measurements from three different types of tumors or other cell types in this case the data make three relatively obvious clusters but rather than rely on our eye let's see if we can get a computer to identify the same three clusters to do this we'll use k-means clustering we'll start with raw data that we haven't yet clustered step one select the number of clusters you want to identify in your data this is the K in k-means clustering in this case we'll select K equals three that is to say we want to identify three clusters there is a fancier way to select a value for K but we'll talk about that later step two randomly select three distinct data points these are the initial clusters step 3 measure the distance between the first point and the three initial clusters this is the distance from the first point to the blue cluster this is the distance from the first point to the green cluster and this is the distance from the first point to the orange cluster well it's kind of yellow but we'll just call it orange for now step 4 assign the first point to the nearest cluster in this case the nearest cluster is the blue cluster now we do the same thing for the next point we measure the distances and then assign the point to the nearest cluster now we figure out which cluster the third point belongs to we measure the distances and then assign the point to the nearest cluster the rest of these points are closest to the orange cluster so they'll go in that one two now that all the points are in clusters we go on to step 5 calculate the mean of each cluster then we repeat what we just did measure and cluster using the mean values since the clustering did not change at all during the last iteration were done BAM the k-means clustering is pretty terrible compared to what we did by eye we can assess the quality of the clustering by adding up the variation within each cluster here's the total variation within the clusters since k-means clustering can't see the best clustering it's only option is to keep track of these clusters and their total variance and do the whole thing over again with different starting points so here we are again back at the beginning k-means clustering picks three initial clusters and then clusters all the remaining points calculates the mean of each cluster and then re clusters based on the new means it repeats until the cluster is no longer change bit bit bit of bit of boop boop boop now that the data are clustered we sum the variation within each cluster and then we do it all again at this point k-means clustering knows that the second clustering is the best clustering so far but it doesn't know if it's the best overall so it will do a few more clusters it does as many as you tell it to do and then come back and return that one if it is still the best question how do you figure out what value to use for K with this data it's obvious that we should set K to three but other times it is not so clear one way to decide is to just try different values for K we'll start with k equals 1 k equals 1 is the worst case scenario we can quantify its badness with the total variation now try K equals 2 K equals 2 is better and we can quantify how much better by comparing the total variation within the two clusters to K equals 1 now try K equals 3 k equals 3 is even better we can quantify how much better by comparing the total variation within the three clusters to k equals 2 now try k equals 4 the total variation within each cluster is less than when K equals 3 each time we add a new cluster the total variation within each cluster is smaller than before and when there is only one point per cluster the variation equals 0 however if we plot the reduction in variance per value for K there is a huge reduction in variation with K equals three but after that the variation doesn't go down as quickly this is called an elbow plot and you can pick K by finding the elbow in the plot question how is k-means clustering different from hierarchical clustering k-means clustering specifically tries to put the data into the number of clusters you tell it to hierarchical clustering just tells you pairwise what two things are most similar question what if our data isn't plotted on a number line just like before you pick three random points and we use the Euclidean distance in two dimensions the Euclidean distance is the same thing as the Pythagorean theorem then just like before we assign the point to the nearest cluster and just like before we then calculate the center of each cluster and re cluster BAM although this looks good the computer doesn't know that until it does the clustering a few more times question what if my data is a heatmap well if we just have two samples we can rename them x and y and we can then plot the data in an XY graph then we can cluster just like before note we don't actually need to plot the data in order to cluster it we just need to calculate the distances between things when we have two samples or two axes the Euclidean distance is the square root of x squared plus y squared when we have three samples or three axes the Euclidean distance is the square root of x squared plus y squared plus Z squared and when we have four samples or four axes the Euclidean distance is the square root of x squared plus y squared plus Z squared plus a squared etc etc etc hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support stack quest well click the like button down below and consider buying one or two of my original songs alright tune in next time for another exciting stat quest"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"K-Means Clustering",
        "Video Title":"K-Means Clustering Explanation and Visualization",
        "URL":"https:\/\/www.youtube.com\/watch?v=R2e3Ls9H_fc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/R2e3Ls9H_fc\/hqdefault.jpg",
        "ID":"R2e3Ls9H_fc",
        "Publish Time":"2019-11-01T16:47:21Z",
        "Channel":"TheDataPost",
        "Channel ID":"UCo8qbws1-to6LDgMNECu2dw",
        "Transcript":"today I'm gonna talk about how the k-means clustering algorithm works I'll use Naftali Harris comm to help illustrate in a step-by-step fashion how the algorithm runs the goal of k-means is to group similar data points together into a predefined number of clusters we can see which data points are similar based on how far apart they are the closer together two data points are the more similar the farther apart the less similar k-means works through the use of centroids and centroids are just a term for the cluster Center therefore the number of centroids that are created correspond to the number of clusters that will be created in this illustration you can see lots of small circles these circles represent data points and exist in two-dimensional space think of the visualization as a graph and these points lie on an X and y axis this means each data point has its own numerical coordinates and we can easily calculate a distance between any data points as for the structure of these data points it's obvious there are three distinct groups and for that reason I'm going to create three clusters I'll do this by instantiating three different centroids these centroids will be represented as large circles seen here to be clear a centroid isn't a data point it's simply a representation of the clusters center this first centroid is red second centroid is blue and the third centroid is greened I'm choosing where the centroids are being placed in this example but in practice you can understand the centroids to be placed randomly notice that all data points are still white in the next phase of this algorithm each white data point will change to red blue or green the data points will be assigned to its mirror centroid thus if the nearest centroid is blue the data point will turn blue with that said I'll run this step of the algorithm you'll see that all of the data points have changed colors now that each data point has been assigned we can calculate where the center of each cluster is and ship the centroid to that calculated center so what you'll see is this red centroid here is going to shift toward the center of this red cluster will be about here this blue centroid is going to shift down here and towards the center of the blue data points and this green centroid will shift towards the center of these green data points now that the centroids have shifted we have to reassign all of the data points to the closest centroid so you can see all of these blue data points here are in green territory they are closest to the green centroid so in the next iteration these blue data points are going to turn green now that the data points have been reassigned we have to once again calculate the center of each cluster and shift the centroid to that calculated center so once again this blue centroid is going to shift further over here this green centroid might shift just slightly over here and this red centroid is going to stay exactly where it was I'm going to quickly move through further iterations and you'll see that further iterations don't change the clustering when data points stop changing clusters in the centroids are no longer shifting clustering is complete"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"K-Means Clustering",
        "Video Title":"k-Means Cluster Analysis",
        "URL":"https:\/\/www.youtube.com\/watch?v=GZj6ikx8PAc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/GZj6ikx8PAc\/hqdefault.jpg",
        "ID":"GZj6ikx8PAc",
        "Publish Time":"2021-01-13T07:05:43Z",
        "Channel":"DATAtab",
        "Channel ID":"UC3UwrWtAFlAkFl_3Nia756g",
        "Transcript":""
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"K-Means Clustering",
        "Video Title":"K-Means Algorithm Simple Explanation",
        "URL":"https:\/\/www.youtube.com\/watch?v=EoyTsiJNGN4",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/EoyTsiJNGN4\/hqdefault.jpg",
        "ID":"EoyTsiJNGN4",
        "Publish Time":"2023-11-15T17:00:10Z",
        "Channel":"Science Buddies",
        "Channel ID":"UCPrbh_9pghzmzkI1wJJRv7Q",
        "Transcript":"K means is an unsupervised machine learning algorithm used for clustering its goal is to categorize data into K number of clusters it begins by selecting K points randomly as the initial centroids then assigns each data point to the nearest centroid after every point is assigned to a centroid the centroids are recalculated by taking the mean of all the data points that were assigned to each centroid then the data points are reassigned to the nearest CID based on the updated ones this process repeats until so the Clusters no longer change"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing K-Means with Scikit-Learn",
        "Video Title":"Machine Learning Tutorial Python - 13:  K Means Clustering Algorithm",
        "URL":"https:\/\/www.youtube.com\/watch?v=EItlUEPCIzM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/EItlUEPCIzM\/hqdefault.jpg",
        "ID":"EItlUEPCIzM",
        "Publish Time":"2019-02-04T12:03:39Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"machine learning algorithms are categorized into three main categories supervised unsupervised and reinforcement learning up till now we have looked into supervised learning where in the given data set you have your class label or a target variable present in unsupervised learning all you have is set of features you don't know about your target variable or a class label using this data set we try to identify the underlying structure in that data or we sometimes try to find the clusters in that data and we can make useful predictions out of it k means is a very popular clustering algorithm and that's what we are going to look into today as usual the tutorial will be in three parts the first part is theory then coding and then exercise let's say you have a data set like this where x and y axis represent the two different features and you want to identify clusters in this data set now when the data set is given to you you don't have any information on target variables so you don't know what you're looking for all you're trying to do is identify some structure into it and one way of looking into this is these two clusters just by visual examination we can say that this data set has these two clusters and k-means uh helps you identify uh these clusters now k in k means is a free parameter wherein before you start the algorithm you have to tell the algorithm what is the value of k that you are looking for here our k is is equal to two so let's say you have the this data set you start with k is equal to two and the first step is to identify uh two random points which you consider as the center of those two clusters we call them centroids as well so you just put two random points here if your k was let's say three then you will put three random points okay and these could be placed anywhere in this 2d place doesn't matter next step is to identify the distance of each of these data points from these centroids so for example this data point is more near to this centroid hence we'll say it belongs to red cluster whereas this data point is more near to green so we'll say this belongs to green cluster the simple mathematical way to identify the distance is to draw this kind of line connecting the line between the those those two centroids and then draw a perpendicular line anything on the left hand side is red cluster on right hand side is green cluster so there you go you already have your two imperfect clunky clusters and now we try to improve these clusters okay so you started you only got your two clusters now we'll make them better and better at every stage and the way you do that is you will try to adjust the centroid centroids for these two clusters for example for this raid cluster which is these four data points you will try to find the center of gravity almost and you'll put the red center there and you do the same thing for green one so you get this when you make the adjustment and now you repeat the same process again again you recompute the distance of each of these points from these centroids and then if the point is more near to red you put it them in a red cluster otherwise you put it in a clean green cluster okay so you repeat the same method and see now these points got changed from green to red so they're more near to red that's why they're in red cluster and you keep on repeating this process you just recalculate your centroids then recalculate the distance of individual data points from these centroids and readjust the clusters until the point that none of the data points change the cluster so here right now see there is only one green which is changing its cluster so now it's in red but after this we are done even if you try to recompute everything uh none of these data points will change their position hence we can say that this is final so these are now my final clusters now the most important point here is you need to supply k uh to your algorithm but what is a good number on k because here we have two dimensional space in reality you will have so many features and it is hard to visualize that data on a scatter plot so which case should you start with well there is a technique called elbow method okay and we'll look into it but just to look at our data set we started with two clusters but someone might say no these are actually four cluster third person might say oh they are actually six clusters so you can see like different uh people might interpret these things in a different way and your job is to find out the best possible k number okay and that technique is called elbow method and the way that method works is you start with some k okay so let's say we start with k is equal to 2 and we try to compute sum of square error what it means is for each of the clusters you try to compute the distance of individual data points from the centroid you square it and then you sum it up so for this cluster we got sum of square error one similarly for the second cluster you will get uh the error number two and you do that for all your cluster and in the end you get the total sum of squared errors now we do square just to handle negate value there is nothing more than that okay so now we computed ssc for k equal to 2 you repeat the same process for k equal to 3 4 and so on okay and once you have that number you draw a plot like this here i have k going from 1 to 11 and then on the y axis i have sum of squared error you'll realize that as you increase number of uh clusters it will decrease the error now it's kind of intuitive if you think about it at some point you can consider all your data points as one cluster individual where your sum of square error becomes almost zero okay so let's assume we have only 11 data points at 11 value of k the error will become zero okay so error will keep on reducing and the general guideline is to find out an elbow so the elbow is on this chart this point is short of like an elbow okay so here is a good cluster number okay so for example for whatever the data set this chart is representing uh a good k number would be four all right so that was an elbow technique let's uh get into python coding now all right so the problem we are going to solve today is cluster uh this particular data set where you have age and income of different people now by clustering these uh data points into various groups what you're trying to find out is some characteristics of these groups maybe the group belongs to a particular region in u.s where the salaries are higher or the salaries are lower or maybe that though that group belongs to a certain profession where the salaries are higher versus less okay so you try to identify some characteristics of these groups so right now we have just name age and income and first thing i'm going to do is import that data set into pandas data frame so you here you can see that i imported essential libraries and then i have my data frame ready with that and uh since the data set is simple enough i will first try to plot it on a scatter plot okay so when you plot it on a scatter plot of course i don't want to include name i just want to plot the age against the income so df dot h df income in dollar i'll just use the same convention you can use dot also but since there's a bracket here i'll use the same convention okay when you plot this on scatter chart you can kind of see three clusters one two and three so for this particular case choosing k is pretty straightforward so i will use k means so k means is something imported here okay and of course you need to specify your k which is n underscore clusters and by the way in jupiter notebook when you type something and when you hit tab it will auto complete okay so it creates this k means object for you and it has all these default parameters you can tweak all these parameters later but i'm just trusting on the default parameters the second step is fit and predict so in previous supervised learning algorithms we used to do fit and then calculate the score here i'm just directly doing fit and predict so fit and predict what okay i'm going to fit and predict the data frame excluding the name column because name column is string and it's not going to be useful in our numeric computation so i want to ignore it all right so you do fit and predict and what you get back is y predicted so now what this statement did is it ran k-means algorithm on agent income which is this scatter plot and it computed the cluster as per our client criteria where we told algorithm to identify three clusters somehow okay and it did it it just assigned them different labels so you can see three clusters 0 1 and 2. now visualizing this array is not very very much fun so what we want to do is we want to plot it again on on a scatter plot so that we can see what kind of clustering result did it produced okay so i am in my data frame i am going to append uh this particular column so that my data frame looks like this so now this is a little better where i can see these two guys belongs to same group these two belongs to same group and so on but it is still not as good as scatter plot okay so let's do this plot dot scatter plot all right now uh what we need to do is we need to separate these three clusters into three different data frames so let me do that df1 is equal to df df dot cluster cluster is equal to zero okay so what this is doing is it's returning all the rows from dataframe where cluster is zero and the second one will be this and the third one will be this so now we have three different data frames each belonging to one cluster and i want to plot these three data frames onto one scatter plot okay now just to save some time let me just copy paste the code here okay i will come at this little later but see three different data frames and we are plotting these uh data frames into different color okay so cluster zero is green then red and black let's see how that looks okay so df oh i'm made a mistake here i had a typo good all right so i see a scatter plot here but there's a little problem so this red cluster looks okay but there is a problem with these two clusters you know they are not grouped correctly so this problem happened because our scaling is not right our y-axis is scaled from let's say 40 000 260 000 and the range of x-axis is pretty narrow see it's like hardly 20 versus here is 120 000. so when you don't scale your features properly properly you might get into this problem that's why we need to do some pre-processing and use min max killer to scale these two features and then only we can run our algorithm all right so we are going to use min max scalar so the way you do it is you will say scalar is min max scalar and this is something if you already noticed we imported here okay all right so scalar is this and scalar dot fit df so now i want to fit first the income all right so my scalar min max scaler will try to make the scale 0 to 1 so after i'm done with my scaling i will have a scale of 0 to 1 on y as well as x axis all right so df let me just uh copy paste this guy here is equal to scalar dot transform okay so now scalar will um scale the income feature all right so df this okay let's see how that did it so you can see that the income is a scale right it's like say 0.38 and so on so it is in a range of one to zero you will not see any value outside zero to one range we want to do the same thing for our age also okay so let's do that scalar dot fit df dot h df dot age is equal to scalar dot transform df dot h and then we print our df and you can see the age is also scaled okay i have this extra column because i made a mistake previously but you can ignore that you can ignore cluster also so we have age and income features properly scaled now okay and even if you plot these on to scatter plot they will look structure wise at least they will look like this okay all right so the next step is to use k-means algorithm once again to train our scale data set so it's gonna be fun now let's see what scaling can give us and as usual y predicted is equal to km dot fit and predict so again i started with three clusters and i am using um i'm just fitting my scale data age income all right and let's see my y predicted so it predicted some values which yet don't know how good they are so i will just do cluster is equal to y predicted i will also just drop the column that we typod and then let's look at df okay in places in place is equal to true okay so now this is my new clustering result uh let's plot this on to our scatter plot i'm just going to remove this for now now you can see that i have a pretty good cluster see black green and red they look very nicely formed uh one of the things we studied in the theory section was centroids so if you look at km which is your train a k-means model that has a variable called cluster centers and these centers are basically your centroids okay so this is x this is y so this is the first centroid of your first cluster second centroid and third centroid and if you can plot this into a scatter plot uh it can give a nice visualization to us right so pld dot scatter so first let's plot x axis okay so x axis for this will be it will be what okay so using this syntax you can say i want to go through all the rows which is three rows here and then the zero means first column which is this okay and your y is your first column and just to differentiate them with regular data points i will use some special marker and color so you can see that these are the centers of my clusters all right let's look into now elbow plot method see this data sort was simple but when you're trying to solve a real life problem you will come across data set which will have like 20 features it will be hard to plot it on scatter plot and it will just get messy and you will be like what do i do now well you use your elbow plot method so in elbow plot um as we saw in theory we uh go through number of case okay so let's say we'll go from k equal to 1 to 10 in our case okay and then we try to calculate sse which is sum of square error and then plot them and try to find this elbow so let's define our k range let's say i want to do 1 2 10. this will be 1 2 9 but whatever okay and then sum of squared error is an array so for k is equal to 1 you'll find sse k equal to 2 you will find sse you will store all of that into this array and then use matplotlib to plot the result okay so 4k in k range so i'm just going through one to nine and then each iteration i create a new model with clusters equal to k and then i call fit okay and what what do i try to fit okay i try to fit my data frame but i use this syntax because my data frame has name column i don't want to use name column all right you'll be like what the heck this guy is doing all the time using this crazy syntax but that's to avoid name if you want you can just create a new data frame just drop name column that is fine too and all right so now what is my sum of square error how do i get that when you call km dot fit after that on your k means there is a parameter called inertia that will give you the sum of square error and that error we want to just append it to our array that we have all right that was pretty fast because our data set is very small okay let's see what is sse so sse you can see that sum of squared error was very high initially then it kept on reducing and now let's plot this guy into nice chart okay when you do that you get our elbow plot remember elbow plot elbow all right where is my elbow where is my elbow okay here is my elbow you can see that k is equal to 3 for my elbow and that's what happened see i have three clusters for exercise we are going to use our iris flower data set from sklearn library and what you have to do is use pattern length and width features just drop sample length and width because it's it makes your clustering little bit difficult so just drop these two features for simplicity use the pattern length and with features and try to form clusters in that data set now that data set has a class label in the target variable but you should just ignore it okay you can use that uh just to confirm your results and in the end you will draw an elbow plot to find out the optimal value of k alright so just do the exercise post your results into the video comments below also i have provided a link of jupyter notebook used in this tutorial in the video description so look at it when you go towards the end you will find the exercise sections also don't forget to give it a thumbs up if you like the content of this tutorial you can also share it with your friends"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing K-Means with Scikit-Learn",
        "Video Title":"K-Means Clustering - Methods using Scikit-learn in Python - Tutorial 23 in Jupyter Notebook",
        "URL":"https:\/\/www.youtube.com\/watch?v=ikt0sny_ImY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ikt0sny_ImY\/hqdefault.jpg",
        "ID":"ikt0sny_ImY",
        "Publish Time":"2017-09-08T06:48:26Z",
        "Channel":"TEW22",
        "Channel ID":"UCcBIq3zYCbiq3jYWAq45q3w",
        "Transcript":"k means clustering is an unsupervised machine learning algorithm that you can use to predict subgroups from within a data set with k-means clustering you usually have an idea of how many subgroups are appropriate with the a means model predictions are based on one the number of cluster centers that are present and second the nearest mean values between observations some popular use cases for k-means clustering are market price and cost modeling customer segmentation hedge fund classification and insurance claim fraud detection there are a few things you need to keep in mind when you are using the k-means model the first thing is you always need to scale your variables before clustering your data and second you need to look at a scatter plot or a data table to estimate the number of cluster centers to set for the k parameters in the model it might not make a lot of sense to you now but i am going to show you how to do this in practice in just a few seconds all right let's get some practice with the k-means method in this demonstration you are going to need numpy and pandas so we will input those and we are also going to be using matplotlib but most of this demonstration relies on sidekick escalant so make sure to import all of the library and all of the correct modules and packages within it i am going to type them out now we are going to say from sklearn.com the cluster module import k-means and then from mpl toolkits dot m-plot 3d import axis 3d from sklearn pre-processing module import skill we are going to use that to scale our variables and then we want to import sklearn matrix this is going to be to evaluate our model and we will import that as sm from scale on import data sets we are going to use a built-in data set from sidekick learn and from the metrics module we are going to import the confusion metrics in the classification report these are what we use to evaluate the model confusion metrics and classification report run that and then we have our libraries let's set our plotting parameters for the data visualizations in this jupiter notebook and then as you have seen earlier in the lectures we will call the load iris function in order to load our iris data set that is the built-in data set we use from sidekick learn and next we want to scale the data so we will call this scale version of the iris data set x and we will say scale and pass in irs.data and then let's set our target variable which will be y and we are going to make a data frame so we will call the data frame constructor and we will pass in iris dot target for our variable names we will create an object called variable names and that will be equal to our iris feature name so iris dot feature names and then let's just print out the first 10 records of our x data set so that's what the iris data set looks like from the inside now we are going to cluster this data to do that we need to initiate a k-means object we will call it clustering and we will set clustering equal to k-means the k means function and we are going to pass in n clusters this parameter tells python how many centroids to use for the clustering and for the iris data set we know that there are three species types of irises in their data so we are going to say the number clusters should be equal to three we also need to pass in a parameter for random state so we will say random state and this initializes the centroids before clustering and sets the seed so that you get the exact same results here as produced on my computer next we call the fit method off of the model and pass in our data set so here is our model and we take dot fit and we pass in x this computes the k-means clustering and what it has printed out here is a summary of our model with default model parameters and then the parameters we set for n clusters and random state next i am going to show you how to plot our model outputs in our scatter chart so let's create a data frame called iris df and going to call our data frame constructor and we are going to pass in our iris data let's set the column names for their data frame those are sample length sample width petal length and petal width and let's also set a column name for out target so we will say why dot columns is equal to target and then i run it now let's create a color theme for our scatter chart so i will say color theme and then we will make this equal to an array of color values from the math plot library we will pass in the list dark gray light solomon and powder blue for this data visualization let's create a subplot that has one row and two columns in order to do that we will call the plt.subplot and then we will pass in one row two columns and in here i am going to pass in a third parameter to specify exactly what position i want this next plot to combinate so we are going on the first plot within our subplots now we will generate a scatter plot like you learn in the data visualization section in the previous lectures we will plot our petal length along the x-axis and petal width along the y-axis so to do that we say plt dot scatter and we will set x and y make x equal to better length y equal to pattern width remember that our target variable is the variable that contains the species label there are three different types of species in the irs data set and we want to color our data points by their species label to do that we write the name of our color theme and then select the labels in our target variable these are the actual species labels so we will say here c is equal to color king and then let's select iris target the values in our target variable also let's set a size for our markers say s equal to 50 and add all title for the chart we will call it ground truth classification now we call this subplot function again and for the third parameter this time we pass in the value of two to tell python that we want the next scatter plot to be the second of the two charts it might generate an identical scatter plot at this time we will color our data points according to their predicted species levels instead of their actual ones so i am going to copy and paste this code we used here to generate the first scatter plot but then i am going to change the target variable and i am going to say instead we want to color our chart according to the clustering labels the labels that were predicted by our clustering model so we say clustering labels clustering dot labels and then change the title to k means classification print it out the model appears to have predicted clusters pretty accurately except for the fact that the clusters themselves are mislabeled that's easy to fix using numpy's choose function using this function we can easily reassign label names to predicted labels we will say that level 0 should be changed to 2 label 1 should be changed to 0 and label two should be changed to one so we will call this relabel and we will call the choose function np dot choose and we will say of the clustering labels clustering dot labels we want label 0 to be changed to r2 label 1 to be changed to a 0 and label 2 to be changed to a 1 and then we want this to be an integer data type so we will say s type np dot in 64. then let's regenerate our charts so what i'm going to do is i'm just going to copy and paste the code we already wrote but then instead of coloring the second chart by the clustering labels we are going to color it by the relabel object we will run that and it looks like a pretty good fit it looks like the k-means model did a pretty good job of predicting the clusters but in order to verify that quantitatively we will use sidekick learns classification report to score the model to do that i will say print and then i will call the classification report function classification report and then pass in y our target variable and relabel our predicted values for our target variable and run that these are some metrics that you can use to evaluate the accuracy of your results but before looking at actual numbers i want to explain to you that precision is a mayor of the model's relevancy and recall is a mayor of the model's completeness what you are going for in machine learning is high precision and high recall that indicates that you have highly accurate model results so looking at the results of our model evaluation you can see under the precision return for record 0 for all points projected to have a 0 level 100 of the retrieved instances were relevant that is indicated by 1.0 here and then for all points that were predicted to have a one level 74 percent of the retrieved instances were relevant looking over at the recall metric for all of the points that were labeled two 72 percent of the results were returned as truly relevant of the entire data set 83 percent of the results that were returned were truly relevant just remember you are going for high precision and high recall that reps up things up for the k-means method next"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing K-Means with Scikit-Learn",
        "Video Title":"K-Means Clustering Algorithm with Python Tutorial",
        "URL":"https:\/\/www.youtube.com\/watch?v=iNlZ3IU5Ffw",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/iNlZ3IU5Ffw\/hqdefault.jpg",
        "ID":"iNlZ3IU5Ffw",
        "Publish Time":"2021-11-17T17:30:03Z",
        "Channel":"Andy McDonald",
        "Channel ID":"UCn1O_4_ApzbYwrsUdRoMmOg",
        "Transcript":"hey friends and welcome back to the channel in today's video we're going to look at a machine learning algorithm known as k-means clustering we will see how we can apply that to well-logged data and split it up into separate groups but before we get on to the practical side of running through k-means clustering in jupiter notebooks we first must understand what k means clustering is clustering is an unsupervised machine learning process which learns from the data itself rather than from labeled examples as you would expect within supervised learning it splits the data into distinct groups based on the features that are supplied to it there are a number of clustering methods available such as dbscan gaussian mixture modelling and also k means clustering which is going to be the focus of this video the main objective of the k-means clustering algorithm is to reduce or minimize the sum of the distances between the center of the cluster and the other data points so let's have a closer look at how k-means clustering actually works to illustrate the process of k-means clustering we can use a simple data set plotted on a scatter plot the first step in the process is to define the number of clusters that we're going to want to generate there are a number of ways that we can identify the optimum number of clusters and we will see an example of one of these methods in the python tutorial section so to keep things simple for illustration we're going to work with free clusters which means that the k in k means is equal to free so after defining the number of clusters that we want our next step is to select k random points within our data set in this case we are going to be selecting three random points these selected points will form the starting point for our clusters we can then calculate the euclidean distance between the cluster center point and all other points if we take this point here for example we can see that it is closer to the red cluster compared to the green and yellow clusters as a result it gets assigned to the red cluster we then repeat this process with all other points on the scatter plot until they have been assigned to an appropriate cluster after all their points have been assigned to a cluster we can then calculate the main point of each cluster which are represented here by the x's after the new mean points have been calculated we can then repeat steps three to five and check if the points belong to the same cluster or to a different one as we see here a few points have changed their color as they are now closer to different mean points once the points have been reassigned we can then recalculate the main point and repeat eventually we will reach a point where the main points of the clusters do not change and then we have our final clusters and this is just a very simple illustration of how k-means clustering works now that we have the basics of k-means clustering covered we can now go to our jupiter node we can see how we can apply k-means clustering from the sk learn python library to some well-logged data so let's go over to our notebook and get started in this tutorial we are going to see how to run a simple k-means clustering on some well-logged data don't worry too much if you're not familiar with well log data the same process can be applied to other data sets so the first step is to import the main libraries that we're going to be using we will be using pandas which is commonly imported as pd and this will be used to load data from a csv file and also view our data we then have our import map plot dot pi plot as plt and this is the main plotting library that we're going to be using to display our results and finally we have the main library and modules we will be using for our machine learning process these are the standard scalar from sklearn.preprocessing and k means from sklearn.cluster once our libraries have been imported we can then move on to loading the data the data we are using today is a single well from a much larger data set that was used for a machine learning competition hosted by zeek and force 2020 you can find a link to the full data set and the competition details in the description below to load in our data we will call upon df is equal to pd.read underscore csv and then we pass in the location and the file name to make things easier for us and to make it easier to work with our data i'm going to set the index column to depth underscore md and this is just the measured depth that has been acquired along the wellbore and once we've run that we can check our contents by calling upon the data frame or the df variable and then we can see that we have our measurements from each of the logging tools we have our row b which is our bulk density gr which is gamma ray n phi which is our neutron porosity pef which is photoelectric factor and dtc which is acoustic compressional slowness as we can see from the data frame we have a series of nands in within our data and this means not a number or missing values and many machine learning algorithms can't handle the missing values so we have to either repair them or remove them and in this situation what we're going to do is we're going to remove them and just to keep things simple i have done a video previously looking at various ways to fill the data using pandas you can check that out on my channel if you're interested so to drop our missing values we can call upon df dot drop n a and in the brackets we'll set the argument in place is equal to true so rather than creating a copy of the data frame what we're going to do is apply this function drop n a to the actual data frame here and now if we look at our data we can see that some of the rows have disappeared so the debt value of 494.528 is now no longer within the data frame and we can see that we don't have any nans within here and we can further confirm this by calling up on the described method and we can see that the count is equal for all of the individual columns within our data frame and this just indicates that there are no missing values so the next step within our process is to transform the data so before we apply our k-means clustering algorithm we need to standardize the data as our data set contains features that have been measured in their own units and have their own data ranges some features may end up having more influence over the final results compared to others to reduce this impact we need to take each feature subtract the mean of that feature from each value and then divide it by the feature standard deviation so we can see for instance that if i call up on the new data frame here just to get our values above this df.describe method we can then have a look at the values and how this process works so we have the first value here within the row b column which is 1.77 and then we have a mean of 2.1499 and we also have a standard deviation of 0.25 so what we do is we take 1.77 and take away the mean from that and then that result is then divided by the standard deviation and then this is repeated for each of the measurements within this column once that's been done it will then be applied to the other columns within this data frame so let us apply the the standard scalar so first we have to create an instance of the scalar and we can do that by typing scalar is equal to standard scalar open brackets and that will initialize the class so then we need to fit that standard scalar to our data and then transform our data using that scalar what we need to do is within our data frame i prefer to have those curves or those outputs put back into the data frame rather than having them as an independent array so what i've done here is i've added the curves with an underscore uh t tend to say that they've been transformed set that up as a list within the dataframe object so this is going to add new curves to your data frame then over on the right hand side well we need to call upon scalar and then we call upon this method called fit underscore transform and then we pass in our data frame with our original curves that we want to transform so we can run this and then when we view our data frame we can now see that we've got our row b underscore t and phi underscore t et cetera here in the data frame and we also have our original values so why am i doing this well it just makes things easier when we come to plotting so if i had done this without applying it back to the data frame we would be working with a numpy array we would then have to work out how to bring that back into the data frame which can be done with a few lines of code but this is probably the in my opinion there's the simplest way to do that to bring into the data frame now we're going to move on to actually applying the k-means clustering to our data but you may have one important question that you're asking and that is how do we know how many clusters to initialize our algorithm with you would have seen in the presentation at the start that we we need to provide a number for k to begin the process of clustering there are a number of methods we can use to identify the optimum number of clusters and the one that i'm going to show here involves a plot known as an elbow plot and to keep things simple for visualization later on we're just going to work with two variables n phi underscore t and rho b underscore t so the simple idea behind this function is we run our k-means algorithm multiple times and plot the inertias which is simply a measure of the sum of the square distances to the nearest cluster center as the number of clusters increase the inertia will decrease and there will be a point in this plot where the value will go from a large change between clusters to a very small change between clusters it is at this point that we have an idea of what our best number is for starting our k-means algorithm so with this function i've got two lists that are being created here means and inertias and then we're going to look through k so we're going to range from 1 to max k which is our argument here so we can pass in any number here and it will loop through that until we've reached the maximum number of k and then we're going to apply the k means algorithm and then fit that algorithm to the data and then we append the inertias to our list as well as a k number to our means list and then we're just going to generate the elbow plot down here by just creating a simple matplotlib figure uh showing our number of clusters versus inertia so to keep things simple for visualization we're just going to work with the two variables n phi underscore t and row b underscore t and then we're going to pass in 10 for our maximum number of clusters so let's run this function once it's run we then get back this elbow plot and we can see that we've got our inertia up here on the y-axis and the number of clusters here on the x-axis so here we've got cluster one which has a high inertia which means that the sum of the squared distances between the cluster center and the points is very very high as we move into two clusters we then reduce that number uh for our inertia down significantly to around about six thousand and then as you can see we've got a now we've now got a gradual decline in our inertia as we move along we could select our point here at two clusters or we could potentially select three clusters as that slope is getting more and more gradual as we go above three clusters so two clusters might be just a bit simplistic for this kind of data so i'm going to go with three for this this data set so now that we have our initial value for k we can then create a new variable called k means and that is going to be equal to k means which is our algorithm that we've imported from sk learn and then we're going to set the n so the number of clusters or n clusters is equal to free so now that we've initialized our k means clustering algorithm and we've set the k the number of clusters or the k value to free we now need to apply that to our data and we can do that by calling up on k means dot fit and then we're going to pass in the data frame and then from that we're going to select our two curves which will be n phi underscore t and row b underscore t so that is the k means algorithm fitted to our data let's view the output of this and what we can do is we can just create a new column within our data frame and we will call this k means underscore free and we will set that equal to k means dot labels and we need to add the underscore at the end here and then when we view our data frame we now see that we've got k means underscore free in our data frame and we can also see that we've got different numbers here so we've got a number one for these data points up here and we've also got a number two for these data points down here at the bottom of the data frame it's all going well knowing that we've got the data within our data frame uh as a column but it doesn't really tell us much about our data but just by looking at these raw values so what we need to do is actually plot the results and we're going to display a scatter plot of our density neutron data and we can do that by simply calling up on plt.scatter and then we add an x is equal to d f n phi so we'll have our n phi curve on the x axis and then we have the y axis which is going to be equal to d f and that will be rho b we will also set the color argument c to equal to our new k means column and we just call upon df and then pass and k means underscore free for tidying up our plot we can then set up the x limits and we will set that from minus 0.1 to 1 and plt.ylim to go from 3 to 1.5 and then we'll do plt dot show and now we have our density neutron cross plot divided up into different clusters and we can see that we've got our one cluster down here in the bottom left and then we've got one in the middle and then we've got another cluster up here and the top right so if you're familiar with reading these density neutron cross plots we generally have our cleaner intervals down here towards the matrix points of 2.65 for our sandstone or 2.71 for limestone and then as we move up here towards the right we get into their shallower intervals and same with up here um which could also be from shallower intervals as we've got quite a large range of data going from shallow to deep just for a bit of fun if we want to see how the k-means clustering algorithm splits this data up using different k values we can simply create a little for loop so we'll go for k n range so we're going to go between 1 and 6 and then what we're going to do is run the k-means algorithm again so similar to our optimization function and we're going to set that to k-means and we'll set n underscore clusters is equal to k and we just need to change that back to a capital m and then we'll do k means dot fit and then we pass in our data frame and also our transform data so we'll call upon ruby underscore t and n5 underscore t and then we will add our new curves back to the data frame and we will need to create a formatted string by just putting an f at the start and we type in k means underscore and then the curly braces uh followed by the the single quote and within the curly braces we can then pass in the k number so that we automatically add a new new column to data frame with the incremental number or the number representing the number of clusters and we'll set that to k means dot labels underscore so that's that being done so if we call upon our data frame now we can now see that we've got k means underscore free from our original one and then we've got k means underscore one two three four and five so let's use matplotlib to create some subplots so we'll create some we'll create fig and then ax s is equal to plt dot subplots n-rows is equal to one with the number of columns equal to five and we'll also set the fig size is equal to 20 by five this declares the size of the figure so next we need to go through a little for loop so for i and then x and enumerate and then we're going to enumerate over fig dot axis so all of the individual axis within our subplots and we will also set the starting value to one so when you generally use enumerate it starts from zero but you can use this keyword argument start to specify the starting number so if we go through this loop it's going to start from start counting from one and then we just type in x dot scatter we'll just copy what we had above so where we've got x is equal to neutron porosity and then y is equal to row b and then over here on the the color so what we need to do is change the km to capitals is that's what we've used within our data frame and then here we need to put in the curly braces and we will use a value of i so that is going to get the number from the current loop so as we're using x rather than plt we need to change the syntax slightly so ax dot set ylm and then we're passing the values of 3 to 1.5 and then ax dot set exelem going from zero to one and then we'll set the title ax dot set underscore title and then again we use a formatted string so f and then open quotes n clusters colon and then we pass in our curly braces with the value for i and then we'll just close that out and here we have our final subplots with the n clusters going from one up to five see going from one cluster where everything is just the same which we would expect with one cluster and then as we go up to n clusters is equal to two we can now see we've got a single division between the cleaner interval down here and our shaler data up here and the clusters become smaller and smaller as we go along as the number of clusters increases so this is where it would take some domain expert knowledge to understand whether the k-means clustering algorithm has been applied correctly so this was just one example of k-means clustering but there are a number of other different methods and you can see examples of these on the sk-learn or the scikit-learn website and you can see here we've got gaussian mixture modeling we've got db scan and we've got many other different methods and you can see from this image here how each of these methods split up that data so instead of importing k means we can import one of these other methods and then apply it to our data so if you've enjoyed today's video be sure to click that like button and also if you want to see more content from this channel be sure to hit that subscribe button and ding that notification bell so thanks for watching and until next time bye for now"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing K-Means with Scikit-Learn",
        "Video Title":"K-means Clustering From Scratch In Python [Machine Learning Tutorial]",
        "URL":"https:\/\/www.youtube.com\/watch?v=lX-3nGHDhQg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/lX-3nGHDhQg\/hqdefault.jpg",
        "ID":"lX-3nGHDhQg",
        "Publish Time":"2022-07-11T16:00:32Z",
        "Channel":"Dataquest",
        "Channel ID":"UC_lePY0Lm0E2-_IkYUWpI5A",
        "Transcript":"hi today we're going to walk through how to build a k-means clustering algorithm from scratch k-means clustering is an unsupervised machine learning algorithm that you can use to find patterns in your data so if you're looking to implement this algorithm building it from scratch can help you understand it better it can help you build your portfolio or help you through an interview it can also help you if you want a more flexible and powerful way to create the algorithm for custom use cases let's dive in and learn how to create this algorithm from scratch by the end of this tutorial you'll have a full working implementation of a k-means clustering algorithm and you'll be able to run the algorithm and actually see how the cluster assignments change with each iteration of the k-means algorithm and the clustering will eventually converge and give you values that correspond to the centroids for each cluster k-means clustering is an unsupervised machine learning technique and what that means is that we're not trying to predict something instead we're feeding some data into the algorithm and it's going to find interesting patterns in the data set for us k-means clustering is an iterative algorithm which means that we go through multiple iterations to actually update the algorithm and get to the final cluster assignments that we need so these steps roughly outline how you build a k-means clustering algorithm so the first thing you want to do is specify the number of clusters you want which is what k means so k can be three if you want three clusters five etc then what we do is we randomly initialize the centroid for each cluster and the centroid is just the central point in each cluster if we look at this gif the centroid is represented by the black plus sign and that just corresponds to the cluster center once we initialize the random centroids then we determine which data points belong to each cluster once we do that we can assign labels to each of our data points based on which cluster or which centroid is the closest to that data point then we update our centroids based on the geometric mean of all the data points in the cluster and then we run steps three and four we continually iterate until the centroid stopped changing and ideally you'll hit a point where the centroids actually stop changing and your algorithm has converged towards stable clusters and that's when the algorithm is complete so we're going to implement all of this today and let's go ahead and dive in we're going to be working with player data from fifa a popular football video game and we're going to be clustering the players and that's how we're going to test our k-means clustering algorithm so in this data set each row is going to be a specific player so this first row is lionel messi the second row is robert lewandowski and so on and each column corresponds to some attribute of that player the positions they play their overall rating in fifa etc the five features that we're going to look at for clustering are overall rating potential rating so the highest rating they could achieve in the future their value to their current club their wage so their annual salary and their age so we're going to cluster players based on these five features the first thing we're going to do is just read in this player data and clean it up and then we'll implement a k-means clustering algorithm to actually cluster the players so i'm using jupiter lab and this is just a notebook open inside jupiter lab you can use google collab you can use jupyter notebook or you can use jupyterlab or any other ide that supports jupyter notebooks and the first thing we'll do is we'll import pandas and numpy so we can just type import pandas as pd and then import numpy as np and that will import both of them and then we want to load in our player data so that's just pandas.readcsv players22.csv and that's data for fifa players from the 2022 season so let's go ahead and read that in we can take a look at the player's data frame and we'll just verify that it is the same the data frame that we just looked at in the csv viewer then we're going to define a list of the features we want to cluster based on so that's going to be overall rating potential rating wage eur so the salary value eur so the total value to the club and age so these are the features that we're going to use to cluster our players based on and then what we're going to do is we're going to use the drop n a method in pandas drop any rows where one of these columns has a null or missing value so that's what this subset parameter does it says look at these five columns and if there is a null or missing value just drop the row this makes sure that we don't have any missing values in what we're going to be clustering and most clustering algorithms aren't able to work with missing values so this just ensures that the data is properly set up for clustering then we're just going to copy some of our player data into a new data frame and this data data frame is what we're going to use to actually do our clustering all right so if we look at data now we can see it only has the five columns that we care about and given this we can actually go ahead and start implementing our k-means clustering algorithm so the steps we're going to use to do our k-means clustering i'm just going to briefly write out so that we can come back to them and refer to them so the first thing we want to do is scale the data so what this means is we want the data to go from a certain value to a certain value so if you look at our data here some of the columns have values that are much larger than values in other columns so when we do our clustering these columns with the largest values will have the most importance in our clustering but that's not necessarily what we want right we want every column to be treated equally when it comes to doing the clustering so we're going to rescale the maximum and minimum value in each column so that all the values are in the range 1 to 10 and what this will help us do is it will ensure that no one column dominates the others in the clustering so that's scaling the data then we initialize random centroids so we talked about this earlier when you create and start your k-means algorithm you initialize random centroids at first then we're going to label each data point based on how far that data point is from each centroid so that will give us our cluster assignment for each football player each data point is one row in our in our data and each row corresponds to a football player then we're going to update our centroids so we'll look at each player and we'll look at their label and then we will find the center point of each cluster and then we're going to repeat steps three and four until centroids stop changing so we'll wait till the algorithm converges and gives us our final cluster assignments so this at a high level is the steps that we're going to follow so the first thing we're going to do is scale our data and we're going to use something called min max scaling so the first thing we'll do is we'll subtract the minimum value in each column from each value in the column and what that does is it sets the minimum value in each column to zero because we're subtracting the minimum value from each item then we're going to divide by the range so data.max minus data.min so what this does is it rescales everything from zero to one then we're going to multiply it by nine and what this will do is rescale everything from zero to nine and then we'll just add one so that everything is on a one to ten scale you can definitely do a different scale if you wanna do a zero to one scale or even a one to a hundred scale that will work out actually you don't want you don't want to do a 0 to a 10 scale you basically don't want a 0 or a negative value in your scale so your minimum needs to be greater than 0 and other than that you can feel free to do whatever scale feels comfortable to you and then what we can do now is we can say data.describe and what we'll see is that our minimum value in each column is a 1 and our maximum value in each column is a 10. so everything is now on a 1 to 10 scale which means that no one column will dominate the others in the clustering we've also preserved the variance in our original data so if one player had a much higher potential than another player that's still preserved in our data and that's what we want to preserve all right let's take a look at the first few rows in our data so we can see lionel messi has the highest overall rating in fifa out of any player and his rating is now a 10 which is the maximum value that's possible and then you can see this player had the highest possible wage so that that shows up as a 10 and then everything else is is is scaled down according to the min and the max all right so we've we've loaded our data in and we've processed it and scaled it properly so the next thing we need to do is initialize our random centroids so to do that we need to create essentially a value for each feature so a centroid is just going to be similar to a row here it's going to have a value for each feature for overall for potential for wage but we need those values to be random from our data set so what we're going to do is we're going to say actually let me get rid of this for now we're going to say centroid so if we want to create a single centroid we're going to say centroid equals data.apply lambda x float x dot sample so what's this doing so the apply method on a data frame is going to iterate through each column in the data so we're going to look through each column so overall then potential then wage and then we're going to call x dot sample and what that does is it selects a single value from that column a random value so we're going to randomly sample a single value from each column and then we're going to turn it into a float because x dot sample returns a panda series and we want it to actually be just a single number so we we then call float to turn it into a number so if i run this and i look at centroid centroid is now a pandas series and it has a random value well randomly sampled from our data value for each column you could instead just find a random value from 1 to 10 and use that as your centroid also that's another way that you could initialize these random centroids so we have a single centroid now we need to scale this method up to get multiple centroids so if we're looking for k clusters we need k centroids so if k is five we want we want there to be five centroids that we initialize so we'll initialize a list called centroids and we'll say for i in range k create the centroid and then add it to our list of centroids and then we will wrap this into a function so we'll call a function called random centroids we'll take in our data and we'll take in the number of clusters that we want right if you remember each centroid is just the center of each cluster so the number of centroids we create is equivalent to the number of clusters we want and then what we'll do is we'll return pd.concat centroids axis equals one so what this does is it combines all of the individual panda series remember each centroid is a panda series it combines all those individual series into a data frame so now we can run this to generate our centroids let's call random centroids data and let's say we want five clusters so let's take a look at our randomly initialized centroids here so each column is a centroid and each row is a different feature so cluster zero the centroid would have this overall value this potential value so on then cluster one cluster two cluster three cluster four also have those values if you changed k you would generate a different number of centroids right if you only want three clusters you would only generate three centroids and so on and that's what this loop is doing so we've completed the first step in our algorithm for the first two steps we've scaled the data and we've initialized random centroids the next thing we need to do is write a function that labels each data point according to the cluster centers and what this function is going to do is it's going to look at each player in our data each each data point in our data frame and it's going to find the distance between the euclidean distance so geometry between that data point and each cluster center and then we're going to find the cluster center that is the closest to that data point so for this first data point which represents lionel messi we're going to evaluate the distance between lionel messi's overall rating potential wage and each centroid and then we will take the centroid that's the closest and assign lionel messi to that cluster so we'll we will do this by writing some code to calculate the distance so to calculate the distance you may be familiar with this formula from geometry we are going to subtract the the centroid so this will give us the centroid for cluster zero so we square that and then we take the square root so we subtract the values for the centroid from the from each data point then we square that and then we take the square root of the overall result and that is a distance formula from geometry and then what right okay so inside here sorry i missed a parenthesis we want to add all the distances up before we take the square root okay so what we're doing here is we are subtracting from our from each row we're going to subtract the values for the centroid so we do overall minus the overall value for centroid zero we'd do potential minus the value for centroid zero and so on and then we would square those each difference we would add all the differences together and then take a square root which is just a distance formula and then what we are going to do these parentheses are tricky to make sure i'll make sure they're all in the correct place otherwise the formula is going to be incorrect then what we'll do is we will apply this to all of our centroids so we'll say centroids dot apply lambda x and then we'll replace this with x so what this will do is it will apply this function to each of our centroids because each centroid is a column and this is going to apply a function to each column and we can assign the result to a variable called distances so let's take a look and see what distances is so this gives us a number that represents the distance from each player to each cluster centroid so this first row would represent this first value here represents the distance from lionel messi to the center of cluster zero and we can see here the closest cluster to him is actually cluster four because it has the least distance so the next thing we need to do is automatically figure out which cluster people are in and we can do that by doing distances dot idx min axis equals 1. so this finds the index of the minimum value in each row so the index of the minimum value in this row is 4. the index of the minimum value in this row is also 4. and you can see the index of the minimum value is also the cluster assignment so this will tell us which cluster each player should be in so we can see we now have the cluster assignments for each player and then we'll wrap this into a function called get labels and we'll pass data and centroids into this function and then we will return this piece the label so this is calculating the distance between each centroid and each data point and then this is finding the cluster assignment for each data point and each data point is just a player so it's finding the cluster assignment for each player so that's get labels and then what we can say is we can just write some code to say labels equals get labels data centroids and this will return the same thing we just saw the labels for each player a cluster label rather for each player and then we can see how many players are in each cluster by looking at labels dot value counts so value count will count up how how many times each unique value occurs in a column so we can see 7 600 players are assigned to cluster 4 and so on okay so we've now assigned each player to a cluster based on our random centroids the next step is to actually update the centroids based on who's in the cluster so what we want to do is find all the players in a cluster and then take the geometric mean of each feature so the geometric mean is a little bit different from the mean you're probably familiar with the mean most of us are familiar with is the mean where you add a bunch of numbers together and then divide by the count we're going to take the geometric mean which is the mean of points the center of points in euclidean space and what this gives you is the center of each cluster so you find the geometric mean by multiplying each point together and taking the nth root based on the number of points we're going to do a small modification to how we calculate the geometric mean so that we don't have to multiply a lot of large numbers together and just get a huge result that takes a lot of memory to store so what we're going to do instead is we're going to say np.exp exp is e to a certain power then we'll take np.log which is the natural log and then the mean so what this is what this does is it takes the natural log of of each data point so it's going to take the natural log of for example each of these values in a given row for a given player then it's going to find the mean of all of the values in a row and then it's going to do e to that value to give us back a result and that's going to be the geometric mean and then we'll apply this to each of our labels so we'll say data.group by labels dot apply lambda x is this so this splits our data up by label so we create one group for each cluster essentially and then to each of those groups we're going to apply this which will calculate the new centers for each column grouped by instead of group let's run that okay and what we end up with is our centroids are in each row so this is the geometric mean of all of the values uh all of the overall values for players who are assigned to in cluster zero this is the geometric mean for all of the overall values for players assigned to cluster 1 and so on so the next thing we need to do is if you remember our centroids from earlier the columns were actually the centroids and the rows were the features but here the columns are the features and the rows are the centroids so we just need to call dot t to actually turn our data frame and what that does is now each column is a centroid and each row is a feature which is similar to what the output of our random centroids function is okay so this this may have been a little bit complicated so let me just explain it one more time the first thing we're doing is we're splitting our data frame our data data frame which contains our player data we're splitting it up by cluster so each the labels the labels series gives us the cluster assignments so we're splitting up data by the labels and then to each group we're applying this function which calculates the geometric mean of each feature and the geometric mean of each feature is the cluster centroid so it gives us the new centroids so let's wrap this into a function and we'll run that okay so we now have all of the pieces we need to go ahead and write our loop and our loop will actually make all this work together and it'll go through our iterations now one thing we want to do this isn't necessary but it's very nice to have is actually visualize what's happening as our loop as we iterate in the algorithm and as the loop continues so we're going to write a quick function to actually plot the cluster assignments at each step in our algorithm at each iteration and to do that we're going to need to import a couple of things so from sklearn.decomposition we're going to import pca and then we're going to import matplotlib plot lib.pi plot as plt and then we are going to say from ipython and dot display import clear output so pca stands for principal components analysis and what it's going to help us do is visualize our data because we have five dimensional data so our data if we scroll up and look at our data data frame we have five features for each player you can only visualize easily two-dimensional data you can also visualize three-dimensional data but it's a lot easier to visualize two-dimensional data so we need a way to turn this five-dimensional data into two-dimensional data and pca will help us do that it'll look at these five columns and essentially summarize them into two columns so that's what pca will do so this will help us graph our clusters then matplotlib will actually do the plotting so it's a python plotting and charting library and then this will actually help us clear the jupyter notebook output each time we plot a new graph so we clear the output then put a new graph in so let's run that and then we can go ahead and write our function to plot our clusters so our function is going to be creatively called plot clusters and we'll take in our data we'll take in the labels the centroids and the iteration so we haven't done anything with iteration yet but the iteration is just going to be a counter indicating which iteration it is okay so the first thing we'll do is we'll say pca equals pca and components equals two then data 2d is going to be our pca dot fit transform data so we're initializing a pca model and we're saying we want to have two columns returned by our pca so we're going to pass in data with several columns and we want that turned into data with only two columns and then the fit transform actually runs the model on our data it first fits it to the data and then it transforms our data into two-dimensional data and we'll do the same with our centroids we'll say centroids 2d equals pca.transform centroids.t and the dot t is because this pca model expects our data to be in the form where the columns or the features and the rows are the data points so we're just switching our format there then what we're going to call is clear output so if we have a graph displayed this will clear it out so we can display the new graph over it and then what we'll say is plt.title so this title is just going to represent the iteration that we're currently on then we will plot each data point into our graph and color it according to which cluster the data point is part of so data 2d 0. so that's going to be the x-axis and then the y-axis is going to be the second component that got returned by our pca and then what we're going to say is color is the labels so we'll color each point according to its cluster label and then we will plot over top of that the centroids so x is going to be centroids 2d comma 0. and then y is going to equal the second component and then we'll call plt dot show and that will show the plot so it is not necessary to write this but it's nice to be able to visualize the centroids and cluster shifting as the algorithm iterates so this is first transforming our data into 2d data then it's plotting our clusters and coloring them and then plotting our centroids and then it's going to show the plot so we'll call this at each iteration of our algorithm and now we can actually go ahead and write the body of the k-means clustering algorithm so we'll say max iterations equals 100 centroid count equals three and then we'll go ahead and write our logic so the first thing we're going to do is initialize our random centroids actually instead of centroid count i'll call this k just to make it clear so this is our number of clusters and then this is our total number of times the algorithm will iterate unless the cluster stopped changing first and i'll talk a little bit about that in a second but but we will go ahead for now and initialize our random centroids and that's just data and k and then we will initialize our old centroids so we will stop the algorithm when our old centroids are equal to our current centroids like that indicates that the centroids have stopped shifting and then we'll specify that we're starting on iteration one so we're going to say while iteration is less than max iterations so basically if we if our iteration counter hits 100 we want to stop our loop and and not continue iterating our our centroid values anymore and not centroids dot equals old centroids so what this is saying is stop the loop if we've hit our maximum number of iterations or our centroids are equal to our old centroids which mean the centroids haven't changed in this updated iteration so we'll say old centroids equals centroids then we'll call our get labels function data centroids so we we did this before so we initialized our random centroids here then we're going to get the labels that correspond to each player so we're going to figure out which cluster each player goes into then we're going to update our centroid values based on the geometric mean of all the players assigned to that cluster so in here we'll pass in data labels pressing k and then what we're going to do is plot our clusters okay so a lot of this is just wiring together what we've already done so this loop is basically just continuing it'll keep running and it'll keep doing the steps that we already saw until it hits the stopping criteria which are here and then we're just going to increment our iteration counter okay so let's go ahead and run this and we can see our clusters and we can see how they change from iteration to iteration so early on we saw that the clusters changed a lot now we can see that they're stabilizing and then around iteration 22 the algorithm the clusters actually stopped changing so the algorithm stopped i'll run this again just so you can get a closer look at what's happening you can see early on the cluster boundaries are shifting a lot the centroids are shifting later on it starts to stabilize and eventually it just stops changing the reason why the algorithm doesn't always result in the same output and sometimes it stops on iteration 17 sometimes it stops on iteration 22 is because the clusters are randomly initialized so depending on how the clusters start out the algorithm could take longer or it could run in less time it really just depends on this random initialization i can also increase the cluster count and we can see how that changes our graph and how it changes the cluster convergence so kind of cool we end up with one cluster in the center and the rest around the outside and generally the more clusters you have the longer the algorithm will take to converge so you can see it's going through more iterations here to try to stabilize okay and then at iteration 47 that's when it stopped changing and the algorithm ended once the algorithm ends we can take a look at the centroids and see what they represent so this is the fun part is interpreting the output of this so you can see from the clusters each cluster has slightly different values for each of these key columns so you can see cluster zero here represents star players so these are players who the age is kind of in the middle of the distribution so they're in their prime their wage is pretty high relative to the the other the wages in the other clusters and their overall and potential scores are actually high as well so we can confirm this hypothesis by looking at players label labels equals equals zero and we can see which players this is actually referring to and let me actually look at fewer columns so we can look at short name plus features okay so we can see these are star players right so this is players like lionel messi cristiano ronaldo neymar et cluster one looks like younger players you can see that the age is their age is lower their salaries are also lower and their overall rating is is lower than average but they have high potential so let's take a look at this and we can see that these are all players who are younger their age is pretty low but they have high potential and then we can also look at cluster two this appears to be older players so their their age is kind of greater than the other clusters and their overall and potential are about the same so we can take a look at this as well yep so we can see these are players who have hit their potential and are are older and so on so once you develop your your once you finish running your algorithm and you get the values for each cluster you can actually go ahead and start to interpret it and that's the really cool part about k-means is it can find patterns in your data that maybe you didn't know were there and if you follow football you may know of some of these patterns but one it can help you automatically categorize players into each group so you can then process them differently if you're building a system to for example rate players or do something along those lines and then two it can help you find patterns that you can use in your business or to to draw interesting insights so those are the centroids the next thing we can do is just compare what we did to the scikit-learn algorithm so i'm going to go ahead and reduce the cluster count to three just makes it a little bit easier to compare so we can see the algorithm is running and then once this finishes running i'll take a look at how the actual cluster assignments shook out or how the actual centroid shook out okay and then we can see our centroids here and then what we'll do is we'll just compare this to the implementation that's in scikit-learn just to see how we did so to initialize this we do k means three so three specifies the number of clusters we want and then what we call is k-means dot fit data and then what we to get the centroids we say centroids equals k-means dot cluster centers and then we can create a short small data frame here to actually look at the centroids and specify the columns the columns are going to be the features and then we'll just turn this so it looks like our other centroids so each column is the centroid and each row is the feature and then if we compare this to what we got before which i'll drag down here so these were our previous centroids that we got with our algorithm and these are the centroids that we ended up getting with scikit-learn so we can see a lot of the same patterns right so there is cluster zero in both is a group of players who've kind of hit their peak rating they tend to be older and they their overall and their potential are about the same cluster one is younger players who are still rising they still have a lot of potential and then cluster two is kind of these star players who make a good amount of money and are close to their their over their potential rating they're in their prime still all right so the values aren't the exact same because the underlying algorithms are implemented slightly differently and the randomization that i mentioned before but they're close enough that we have some confidence that our implementation is working okay and seeing how quick it was to run this in scikit-learn why would you want to build your own k-means clustering so there are a few reasons so one is learning so implementing your own k-means clustering algorithm is the best way to learn how it works under the hood and really know which situations you can apply it to and which ones you can't another is that it gives you more power and flexibility so if you wanted to modify this k-means algorithm to work in a low memory situation work on multiple machines etc you can do that with your own algorithm more easily it also looks good in a portfolio if you implement an algorithm on your own it can also look good during interviews if you're able to talk through how an algorithm works in a lot of data science and machine learning interviews you will be asked how does this algorithm work and having implemented it yourself can really help you in that interview so those are some reasons why you would want to implement this yourself and i really hope you enjoyed this tutorial"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Hierarchical Clustering",
        "Video Title":"Hierarchical Cluster Analysis [Simply explained]",
        "URL":"https:\/\/www.youtube.com\/watch?v=8QCBl-xdeZI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/8QCBl-xdeZI\/hqdefault.jpg",
        "ID":"8QCBl-xdeZI",
        "Publish Time":"2023-01-26T11:33:33Z",
        "Channel":"DATAtab",
        "Channel ID":"UC3UwrWtAFlAkFl_3Nia756g",
        "Transcript":"what is the hierarchical cluster analysis and how is it calculated that's what we'll discuss in this video let's start with the first question what is the hierarchical cluster analysis a hierarchical cluster analysis is a clustering method that creates a hierarchical tree of objects to be clustered the tree represents the relationships between the objects and shows how objects are clustered at different levels but let's look at this with an example we asked people how many hours a week they spend on social media platforms and in the gym we now want to know if there are classes in this data set and perform a hierarchical class analysis how do we calculate a hierarchical class analysis the first thing we do is plot the people on a scatter plot with this we can now start to create the Clusters the first step is to assign a class to each individual point so we have as many classes as we have people the goal now is gradually merge more and more clusters until finally all points are in one cluster but how do we do that in each step the classes that are closest together are always merged what does closest together mean for this we need to establish two things one how to measure the distance between two points two how points are connected in a cluster let's start with the question how do we calculate the distance between two points here are the most famous ones the euclidean distance the Manhattan distance and the maximum distance let's take the distance between Max and Caro the difference on the y-axis results with one and the difference on the x-axis is 4. the euclidean distance is just the square root of the sum of the square differences it is calculated as follows square root of 4 squared plus 1 squared which is 3.162 for the Manhattan distance the sum of the absolute differences is used so we simply calculate four plus one and thus get a distance of 5. the maximum distance simply uses the maximum value of the absolute differences in this case 4. now that we know what ways there are to calculate the distances between points we still have to determine how the points within a cluster are linked let's say we have a cluster with the points Joey and Lisa and the class with mags and Carol now how do we determine the distance between these two clusters here are the most popular methods single linkage complete linkage and average linkage single linkage uses the distance between the closest elements in the cluster so the distance between Caro and show complete linkage uses the distance between the most distant elements of the cluster so between Max and Joe average linkage uses the average of all pairwise distances from each combination the distance is calculated and from it the average value for our example we use the euclidean distance and the single linkage method so we need the distance from each cluster to the other clusters for this we must first calculate the distance Matrix in the distance Matrix we plot the Clusters on each of the dimensions and then calculate the distances from each cluster to every other cluster the distance between Allen and Lisa is given by the square root out of 5 minus 2 squared plus 2 minus 3 squared which is 3.16 the distance between L and Joe is given by the square root out of 5 minus 2 squared plus 3 minus 3 squared which is 3. we can now do this for all other combinations until we have calculated the entire distance Matrix now we can merge the first clusters for this we look at which two clusters have the smallest distance between them this is the case between Joe and Lisa so the closest distance we have is between Joe and Lisa with this we now connect Joe and Lisa to form a cluster in our tree diagram or dendrogram we can draw the first connection now we need to update our distance Matrix we decided to use the single linkage method so the distance between two clusters is given by the elements that are closest to each other to the Clusters Ellen Max and Carol from the class Lee Sancho respectively Joe is always the closest person so we calculate the distance from L to Joe the distance from Max to Joe and the distance from Cairo to Cho now we put back together the classes that are closest to each other this is Max and Ellen in our tree diagram or dendrogram we can draw the second connection now we update the distance Matrix again we calculate the distance between Ellen and Joe Caro and Joe and between Cairo and Allen we obtain the smallest distance between the cluster Caro and the cluster Lisa and Dro so we connect these two classes and draw the third Connection in the tree diagram now there are only two classes which we merged in the last step and we get our finished dendrogram and now I'll show you how you can easily calculate a hierarchical cluster analysis online with data tab if you like you can load the data set and calculate with it in parallel you can find the link in the video description to calculate a hierarchical cluster analysis online just visit datadeep.net and copy your own data into this table or use the link to load this data set then the variables social media and Jim appear here below now we click on cluster and select hierarchical cluster if we now click on social media and Jim a hierarchical cluster analysis will be calculated additionally we can specify the label in our case the names of the people now we can specify which connection method should be used and how the distance should be calculated again we take the single linkage and the euclidean distance now we get the results down here we see the tree plot a scatter plot and the elbow plot in yellow plot we can now read how many classes we take we can see a king here so we take four as the cluster count we can select the number of classes here and then we get the four clusters highlighted by different colors in the tree plot we see the first cluster the second the third and the fourth cluster if you don't know what the elbow method is please watch our video about it I hope you enjoyed the video see you next time"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Hierarchical Clustering",
        "Video Title":"StatQuest: Hierarchical Clustering",
        "URL":"https:\/\/www.youtube.com\/watch?v=7xHsRkOdVwo",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/7xHsRkOdVwo\/hqdefault.jpg",
        "ID":"7xHsRkOdVwo",
        "Publish Time":"2017-06-20T12:33:24Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"[Music] going on a quest on a stat Quest stat Quest hello and welcome to stat Quest today we're going to be talking about hierarchical clustering hierarchical clustering is often associated with heat Maps if you're not already familiar with what heat maps are just know that the columns typically represent different samples and that the rows typically represent measurements from different genes red typically signifies High expression of a gene and blue or purple means lower expression for a gene hierarchical clustering orders the rows and or the columns based on similarity this makes it easy to see correlation in the data for example these samples express the same genes and these genes behave the same on the left we have a heat map without hierarchical clustering and on the right we have a heat map with hierarchical clustering so you can see that the clustering makes a big difference on how the data is presented heat Maps often come with dendrograms so we'll talk about those too let's get started we'll start with a simple example here we've got a simple heat map that has three samples and four genes for this example we are just going to Cluster or reorder the rows or the genes conceptually the first step is to figure out which Gene is most similar to Gene number one genes number one and two are different we can tell because the colors are very different Gene one is highly expressed in Sample number one so it has a red color Gene 2 however is not highly expressed on Sample number one so it has a blue color in Sample number three Gene one is lowly expressed so it's blue and Gene 2 is highly expressed so it's red genes 1 and three are similar so that means in Sample one both Gene 1 and three are red they're highly expressed and in Sample three they're both blue meaning they're lowly expressed genes one and four are also similar however Gene number one is most similar to Gene number three so the second step is to figure out what Gene is most similar to Gene number two so we do all the comparisons and we see that Gene number two is most similar to Gene number four and then we do the same thing for Gene number three and then Gene number four in Step number three we look at the different combinations and figure out which two genes are the most similar once we've done that we merge them into a cluster in this case genes number one and three are more similar than any other combination of genes so genes 1 and three are now cluster number one step four go back to step one but now treat the new cluster like it's a single Gene so in step one we figure out which Gene is most similar to Cluster number one cluster number one is most similar to Gene number four and we figure out which Gene is most similar to Gene number two in this case Gene number two is most similar to Gene number four but notice that we compared Gene number two to Cluster number one and then we do the same thing for Gene number four of the different combinations figure out which two genes are the most similar now merge them into a cluster in this case genes 2 and four are the most similar combination so we've merged them into a cluster now we go back to Step One however since all we have left are two clusters we merge them bam we're all done hierarchical clustering is usually accompanied by a dendrogram it indicates both the similarity and the order that the Clusters were formed cluster number one was formed first and is is most similar it has the shortest Branch cluster number two was second and is the second most similar it has the second shortest Branch cluster number three which contains all of the genes was formed last it has the longest Branch now let's go over a few nitpicky details remember the first step figure out which Gene is most similar to Gene number one well we have to Define what most similar means the method for determining similarity is arbitrarily chosen however the ukian distance between genes is used a lot let's look at an example we'll use a very simple heat map that just has two samples and two genes now we're displaying the values that underly the the colors that we have in the heat map the ukian distance between genes 1 and two is just the square root of the difference in Sample number one squared plus the difference in Sample number two squared here we'll just plug in the values for sample number one we have 1.6 minus 0.5 now let's plug in the values to calculate the difference in Sample number two we have 0.5 minus -1.9 doing the subtraction gives us the square < TK of 2.12 + 2.4 2ar we can think of these values within the parentheses as sides on a triangle so on the x axis we have the distance between Gene 1 and Gene 2 in Sample number one and on the Y AIS we have the distance between Gene 1 1 and two in Sample number two the hypotenuse is the distance between genes 1 and two the Pythagorean theorem says that the hypotenuse equals theare < TK of x^2 + y^2 in this case that means the Square t of 2.12 + 2.4 SAR and that gives us 3.2 the distance between Gene number one and Gene number two when we have more samples we just extend the equation it's no big deal the ukian distance is just one method there are lots more including the Manhattan distance the Manhattan distance is just the absolute value of the differences so instead of squaring the differences and then taking the square root all we do is take the absolute value of the differences we can think of the Manhattan distance in geometric terms by imagining that each difference is a line segment if we take all those line segments and put them together head to tail head to tail and then add that total length of all those line segments together that's the Manhattan distance yes it makes a difference here's a heat map Drawn using the ukian distance and here's the same information drawn as a heat map but now we're using the Manhattan distance the heat maps are very similar but there are also a few differences the choice and distance metric is arbitrary W there is no biological or physical reason to choose one and not the other pick the one that gives you more insight into your data now do you remember how we merged genes 1 and three into cluster number one and compared it to other genes well there are different ways to compare clusters too one simple idea is to use the average of the measurements from each sample but there are lots more and these have effect on clustering as well so let's talk about the different ways to compare clusters for the sake of visualizing how the different methods work imagine our data was spread out on an XY plane now imagine that we have already formed these two clusters and we just want to figure out which cluster this last Point belongs to we can compare that point to the average of each cluster this is called the centroid the closest point in each cluster this is called single linkage or we can compare it to the furthest point in each cluster this is called complete linkage and there are other methods as well here's a heat map that compares the furthest points in the clusters by the way if you use R this is the default setting for the hclust function this heat map compares the average points in the Clusters and this last Heat Map compares the closest points in the Clusters these heat maps are all very similar but there are also differences in the way the data is presented in some summary clusters are formed based on some notion of similarity you have to decide what that is however most programs have reasonable defaults once you have a subcluster you have to decide how it should be compared to other rows columns or subclusters Etc and most programs have good default settings for this as well and the height of the branches in the Dinger gram shows you what is most simple similar hooray we've made it to the end of another exciting stat Quest if you liked this presentation please subscribe to my channel and you'll get more like it also if you'd like me to do something specific feel free to mention it in the comments below"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Hierarchical Clustering",
        "Video Title":"Flat and Hierarchical Clustering | The Dendrogram Explained",
        "URL":"https:\/\/www.youtube.com\/watch?v=ijUMKMC4f9I",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ijUMKMC4f9I\/hqdefault.jpg",
        "ID":"ijUMKMC4f9I",
        "Publish Time":"2019-01-15T08:50:00Z",
        "Channel":"365 Data Science",
        "Channel ID":"UCEBpSZhI1X8WaP-kY_2LLcg",
        "Transcript":"We will have a short lecture about\u2026 clustering of clustering! Originally, cluster analysis was developed by anthropologists aiming to explain the origin of human beings. Later it was adopted by psychology, intelligence and other areas. Nowadays, there are 2 broad types of clustering: Flat and hierarchical. K-means is a flat method in the sense that there is no hierarchy, but rather we choose the number of clusters, and the magic happens. The other type is hierarchical. And that\u2019s what we are going to discuss in this lecture. Historically, hierarchical clustering was developed first, so it makes sense to get acquainted with it. An example of clustering with hierarchy is taxonomy of the animal kingdom. For instance, there is the general term: animal. Sub-clusters are fish, mammals, and birds, for instance. There are birds which can fly, and those that can\u2019t. We can continue in this way, until we reach dogs and cats. Even then we can divide dogs and cats into different breeds. Moreover, some breeds have sub-breeds. This is called hierarchy of clusters. There are two types of hierarchical clustering: agglomerative or \u2018bottom up\u2019 and divisive or \u2018top down\u2019. With divisive clustering we start from a situation where all observations are in the same cluster. Like the dinosaurs. Then we split this big cluster into 2 smaller ones. Then we continue with 3, 4, 5, and so on, until each observation is its separate cluster. However, in order to find the best split, we must explore all possibilities at each step. Therefore, faster methods have been developed, such as k-means. With k-means, we can simulate this divisive technique. When it comes to agglomerative clustering, the approach is bottom up. We start from different dog and cat breeds, cluster them into dogs and cats respectively, and then we continue pairing up species, until we reach the animal cluster. Agglomerative and divisive clustering should reach similar results, but agglomerative is much easier to solve mathematically. This is also the other clustering method we will explore \u2013 agglomerative hierarchical clustering. In order to perform agglomerative hierarchical clustering, we start with each case being its own cluster. There is a total of N clusters. Second, using some similarity measure like Euclidean distance, we group the two closest clusters together, reaching an \u2018n minus 1\u2019 cluster solution. Then we repeat this procedure, until all observations are in a single cluster. The end result looks like this animal kingdom representation. The name for this type of graph is: a \u2018dendrogram\u2019. A line starts from each observation. Then the two closest clusters are combined, then another two, and so on, until we are left with a single cluster. Note that all cluster solutions are nested inside the dendrogram. Alright. Let\u2019s explore a dendrogram and see how it works. Here is the dendrogram created on our \u2018Country cluster. Okay. So, each line starts from a cluster. You can see the names of the countries at the beginning of those lines. This is to show that, at the start, each country is a separate cluster. The first two lines that merge are those of Germany and France. According to the dendrogram, these two countries are the closest in terms of the features considered. At this point, there are 5 clusters: Germany and France are 1, and each other country is its own cluster. From this point on, going up, Germany and France will be considered one cluster. Here\u2019s where it becomes interesting. The next two lines that merge are those of the Germany and France cluster, and the UK. At this point there are 4 clusters: Germany, France and the UK are 1, and the rest are single-observation clusters. At the next stage of the hierarchy, Canada and the US join forces. The next step is to unite the Germany, France, UK cluster with the Canada-US one. Australia is still alone. Finally, all countries become one big cluster, representing the whole sample. Okay. Cool! What other information can we get from the dendrogram? Well, the bigger the distance between two links, the bigger the difference in terms of the chosen features. As you can see, Germany, France and the UK merged into 1 cluster very quickly. This shows us that they are very similar in terms of \u2018longitude\u2019 and \u2018latitude\u2019. Moreover, Germany and France are closer than Germany and UK, or France and UK. The USA and Canada came together not long after. However, it took half of the dendrogram to join these 5 countries together. This indicates the Europe cluster and the North America cluster are not so alike. Finally, the distance needed for Australia to join the other 5 countries was the other half of the dendrogram, meaning it is extremely different from them. To sum up, the distance between the links shows similarity, or better: dissimilarity between features. Alright. Next on our list is the choice of number of clusters. If we draw a straight line, piercing these two links, we will be left with two clusters, right? Australia in one, and all the rest in the other. Instead, if we pierce them here, we will get three clusters: North America, Europe, and Australia. The general rule is: when you draw a straight line, you should count the number of links that have been broken. In this case, we have broken 3 links, so we will be left with 3 clusters, because the links were coming out of those 3 clusters. Should we break the links here, there will be 4 clusters, and so on. Great! Finally, how should we decide where to draw the line? Well, there is no specific rule, but after solving several problems, you kind of develop an intuition. When the distance between two stages is too big, it is probably a good idea to stop there. For our case, I would draw the line at 3 clusters and remain with North America, Europe, and Australia. Okay. When most people get acquainted with dendrograms, they like them a lot. And I presume that is the case with you, too. Let\u2019s see some pros and cons. The biggest pro is that hierarchical clustering shows all the possible linkages between clusters. This helps us understand the data much, much better. Moreover, we don\u2019t need to preset the number of clusters. We just observe the dendrogram and take a decision. Another pro is that there are many different methods to perform hierarchical clustering, the most famous of which is the Ward method. Different data behaves in different ways, so it is a nice option to be able to choose the method that works better for you. K-means is a one-size fits it all method, so you don\u2019t have that luxury. How about a con? The biggest con, which is also one of the reasons why hierarchical clustering is far from amazing is scalability. I will just show you a single dendrogram of 1000 observations and you will know what I mean. 1000 observations and the dendrogram is extremely hard to be examined. You know what else? It\u2019s extremely computationally expensive. The more observations there are, the slower it gets. While K-means hardly has this issue. Thanks for watching!"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Hierarchical Clustering",
        "Video Title":"Clustering: K-means and Hierarchical",
        "URL":"https:\/\/www.youtube.com\/watch?v=QXOkPvFM6NU",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/QXOkPvFM6NU\/hqdefault.jpg",
        "ID":"QXOkPvFM6NU",
        "Publish Time":"2019-01-28T06:20:05Z",
        "Channel":"Serrano.Academy",
        "Channel ID":"UCgBncpylJ1kiVaPyP-PZauQ",
        "Transcript":"hello I'm lisis Sano and this video is about clustering we're going to learn two very important algorithms K means clustering and hierarchical clustering clustering is a type of unsupervised learning and it basically consists in grouping data so if your data looks like it's all over the place the algorithm will say okay you got a group here you got a group here a group here Etc so let's take a look so let's start with an application the application is going to be in marketing in particular in customer segmentation and the situation is the following we have an app and we want to Market this app we've looked at our budget and we can actually make three marketing strategies so that's our goal to make three marketing strategies so the idea is to look at their potential customer base and to split it into three well-defined groups when we look at the customer base we realize that we have two types of information we have their age and years and uh we have their engagement with a certain page p in number of days per week so one of the columns is demographic age and the other one is behavioral which is engagement with the page and the engagement of the page can be a number from 0 to seven since it's in days per week so we look at the potential customer base and this is it there's eight people with their age and their engagement so by looking at this list of people what groups can you come up with let's take a look feel free to pause the video and think about it for a minute so just by eyeballing I can think that for example this two people are similar they have similar ages and similar engagements maybe I could put those in the same group uh I don't know maybe these two are similar as well we can take a while and we can actually write them down and maybe come up with groups but there's got to be something easier or at least something mechanical the computer can do automatically so one of the first things to do with data is to plot it so let's let's plot it in some way let's plot it like this this so in the horizontal axis we put the H and in the vertical axis we put the engagement and now it looks more clear right there are three groups here is one here is another one and here is the other one so that's our three marketing strategies the first one is for people around the age of 20 who are have a low engagement with the page two three and four days a week then strategy two for people that are around their late 30s and early 40s and high engagement with the page and then the last one is for people that are around their 50s and very low engagement with the page and that is pretty much what clustering is basically if our data looks like it's a bunch of points like this then a clustering algorithm will say hey you know what I don't know much about your data but I can tell you that it's kind of split into these groups so what we learn in this video is how to do this clustering how does it a computer identify these groups because for a human in this small case it's easy but for a computer it's not and in particular if you have many many many points and and many columns or many dimensions it's not easy so in this video I'm going to show you two important methods the first one is called K means clustering and the second one is called hierarchical clustering so let's start with k means clustering and the question is how does the computer look at this points all over the place and figure out that they are forming these groups so when I try to imagine points in the plane I just imagine places in a city and trying to put pizza parlors so let's say that we are the owners of this pizza place and we want to put three pizza parlors in this city and what we want to do is we want to put them in such a way that we serve our clientele in the best possible way so we look at our clientele and it looks like this this is where they live so what we want to do is locate three pizza parlors in the best possible places that will serve a clientele so if you take a look at it uh you can come up with three places right it seems like we should have a red one that serves the red points a blue one that serves the blue points and a yellow one that serves the yellow points however for a human this is easy but a computer has a harder time so what the computer is going to do is like in most things in machine learning start at a random spot and start getting better and better so so how does it start well first it locates three random points and puts three pizza parlor there and now what we're going to do is a series of slightly obvious logical statements that when put together will get us to a better place so the first logical statement is it seems like if we have the pizza parlor in these places everyone should go to the closest one to them that makes sense right so we're going to plot all the people that go to the red to the blue and to the yellow pizza parlor basically you go to the one that is the closest so here's another logical statement if all the red people go to the red pizza parlor it would make sense to put it in the center of all those houses right and the same thing with the blue and with the yellow basically you move the pizza parlor to the center of the houses that it's serving so the yellow one will serve these houses over here the blue one is serving these houses over here and the red is serving these houses over here so we move each one of them to the center of the houses that they're serving and now let's apply the first logical statement again we have three pizza parlors and everyone's going to go to the one that is closest to them so something's change right because let's take a look at these three Blue Points well now they're closer to the yellow pizza parlor so these people move and now they're going to go to the other to the yellow pizza parlor what about these two red points over here here well now they're closer to the blue pizza parlor so they're going to start going to the blue pizza parlor now so let's go back to the other logical statement which is that the best location for a pizza parlor is the center of the houses that it serves so we move every pizza parlor to the center of the houses that it serves and again let's go back to the first logical statement which is every person goes to the closest pizza parlor so if you look at these points over here they are red but now they're much closer to the blue pizza parlor so they move to the blue pizza parlor now and you can see that we're getting better and better right because now when we apply the other statement which is every pizza parlor should be at the center of the houses that it serves then now we move everything to the center where the houses where it serves and we're done so that's pretty simple right and a computer can do it because a computer can find the center of a bunch of points by just averaging the coordinates and can also determine if a point is closer to one Center than to the other one because it simply just applies the Pythagorean theorem or the distance formula and and Compares numbers these are these are decisions that a computer can make very easily so we managed to think like a computer and not like a human which is basically the main idea in machine learning so this is the C's clustering algorithm now you may be noticing that we took one decision that seemed to be taken by a human and not by a computer right we decided that there were three clusters but as we said that's hard for a computer to decide a human can see it but a computer can't so here's a question how do we know how many clusters to pick and for this we have a few methods but I'm going to show you what's called the elbow method so the elbow method basically says try a bunch of numbers and then be a little smart on how to pick the best one so let's try with one cluster we can do this algorithm with only one cluster and we're probably going to get something like this every house go to the same pizza parlor then we can run it with two clusters and you can start seeing that this algorithm actually depends on where the original Point starts sometimes it works sometimes it doesn't sometimes give you different answers so let's say we try two clusters and we got this then we tried three clusters and we got the solution that we got then we try with four clusters and let's say we got this with five clusters and we got this and with six clusters and we got this so by eyeballing this we can see that the best solution is with three clusters but again we need to teach the computer how to find the three clusters we need to think like a computer so we can't rationalize things we have to do things like measuring distances comparing numbers averaging coordinates Etc so with those tools how do we find that three is the best well what we need is a measure of how good is one clustering and uh maybe the following measure will make sense basically what we're going to do is we're going to think of the diameter of a clustering and the diameter is simply going to be the largest possible distance between two points of the same color that basically tells us how big each group is in a in a in a rough way so let's look at the first uh one cluster solution the longest possible distance between two points of the same color is this one those two red points are the farthest apart so that distance is is in a way telling us how good is that clustering let's do it with two clusters so the longest distance let's say it's this distance over here that tells us how good the clustering is with two clusters now let's do it with three clusters and let's say that the longest distance is this one over here again with four clusters the longest distance is this one with five clusters longest distance is this one and with six clusters is this one now I just eyeballed these distances so if you think there's another one you may be correct but conceptually what we're trying to do is to to define the next method which is the elbow method so what we're going to do is we take all these distances and we graph them in the following way on the horizontal axis we're going to put the number of clusters so 1 2 3 4 5 and six and on the vertical axis we're going to graph the diameter so we get get the following points and now what we do is we join these points and now this is somewhere where human can intervene a human can look at this graph and say okay you know what I want the elbow to be here there are also some automatic methods to do this but at some point in in the machine learning algorithm is is good to actually have a consistency check because you may have an idea of how many clusters you want or you may have an idea of how many clusters you you would like to have or a Max maximum or a minimum so anyway in some way or another we figure out that the number is three um another thing that's important is that this elbow ma is very easy for a human uh if our if our data has many many columns we're looking at points in very high Dimensions however the elbow method the graph is always going to be two dimensional so that's it that's how we decided that three clusters are the best and that is the c clustering algorithm in a Nell okay so now let's go to our second method which is hierarchical clustering and we're going to do a similar problem except now with this data set we're going to find a clustering into let's see how many groups we can find so another way to do it is the following let's think about this let's think of the two closest points it would make sense to say that these two points that are the closest will belong to the same group maybe yes maybe no but it's a sensical thing to ask right so let's go on that statement let's say these are the two closest points so these two are going to be part of the same group now what are the next two closest points let's say it's this two so these two belong in a group and we're going to keep going in this direction the two closest points are these ones so these two belong to the same group the two closest points are this ones so now what do we do well we just join the two groups so now it becomes a group of three the two closest points are these two so they join like this the two closest points after that are these two so we join the two groups the group of two and the group of three into a group of five and then the next pair of points that are the closest are these two so we're going to join them but let's say that's just too big so we have maybe a a measure of how much is too far so we stop here and that's it that's hierarchal clustering it's pretty simple right now again there seemed to be a human decision here right why did we decide on that being the distance or for example why did we decide on two being the number of clusters so we can make this decision but let's actually look at an educated way to make this decision so let's answer this question how do we decide the distance or the number of clusters so a way to do it is by building something called a dendrogram so what we're going to do is the following we're going to put our points in a row over here one up to 8 and then in the vertical axis we're going to graph the distance and I'm going to show you how let's pick the closest two points which are four and five so we join four and five and we join them over here and uh this is not up to scale but the height of that little curved line between four and five let's say it's the distance between four and five so we join this two and then we go to the next two which is one two and so we're going to join one two here and we're going to join them in the dendr graph in the right and again assume that that height of that little a curved line is the distance between 1 and two now we join the next pair which is six and seven so we join six and seven and again the height is the distance we keep going six and8 so now we're going to join six and 8 how do we join them well we join them like this the group of 67 and the group of eight and the next group is three and four five so they get joined like this and now the next group is going to be two and three so we join the group corresponding to two and the group corresponding to three in the dendrogram and notice that the dendrogram goes up because these distances increase so every time we make a new join it's higher than the previous ones the next ones that we join are three and six so we end up joining these two trees like that and so that's it we have a lot of information about this set in this dandr and now how do we decide where to cut well let's say we cut over here at a certain distance and that gives us two clusters which are this one 1 2 3 4 and 5 and this one which is 6 seven and 8 so notice that we made the decision on cutting based on how much a distance is too far away or how many clusters do we want to obtain let's say that we want to obtain four clusters so we cut at this distance over here which gives us four clusters the cluster formed by one and two the one formed by three by itself the one formed by four and five and the one formed by six seven and eight so again these decisions are taken by a human but think about it again let's say we have billions of points and let's say that they live in a thousand dimensional space it doesn't matter the dendogram is still a two-dimensional figure and we can easily make decisions on it so again a combination of a computer algorithm and some smart human decisions it would giv us the best clustering and that's it that's hierarchical clustering in a nutshell clustering has some very interesting applications and let me mention some of them things like genetics or evolutionary biology The genome carries a lot of information about a species and if you manage to Cluster them you get to understand a lot about species and how they evolved into what they are right now other things like recommender systems use a lot of clustering for example the way you may have got this video recommended was using several methods that include clustering users grouping them into into similar users so maybe some somebody very similar than you uh watch this video and that's why you got to recommend it and that brings us to social networks which is another place where clustering is used a lot in a very similar example than the one we did uh social networks use these methods to group users into certain similar groups based on demographics based on behavior and then be able to Target information to them that they want to see or suggest your friends that are similar to you Etc so that's all for now thank you very much for your attention as usual if you would like to see some more of this content Please Subscribe or hit like uh feel free to share with your friends and feel free to throw in a comment and tell me what you think of the video or if you have any suggestions for other videos you'd like to see and my Twitter handle is Louis likes maath if you'd like to tweet at me so thanks again and see you in the next video"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing Hierarchical Clustering with Scikit-Learn",
        "Video Title":"CS 320 May 1 (Part 3) - Agglomerative Clustering in sklearn",
        "URL":"https:\/\/www.youtube.com\/watch?v=_snvL4CQNWc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/_snvL4CQNWc\/hqdefault.jpg",
        "ID":"_snvL4CQNWc",
        "Publish Time":"2020-04-30T23:17:56Z",
        "Channel":"Tyler Caraza-Harter",
        "Channel ID":"UCBnAH0kpEfn71UGqp7t-OHw",
        "Transcript":"well that last video I talked about some of the problems with k-means and then i introduced this idea of a collaborative string they measure how that actually solves these two problems we have of arbitrary cluster shapes and also hierarchical patterns so here we have these moon shaped patterns and you can see with the K mean that it's not quite capturing that was non round or non circular patterns and so I'm going to try doing this with with agglomerative clustering instead and let me type that I mean autocomplete that is it's not a word I can easily spell and I'm every name this AC AC here and and I think that's good and I run it and oh but I think I could grab the wrong data there let me just run this again quick and and I see it's not doing a whole lot better and the reason why is that there's this linkage parameter that we talked about last time and if I look at the documentation over here it says well I have these four options for like linkage and the default is Ward and what word is doing is it's minimizing the variance within the clusters being merged and so what that means is it doesn't want to really have a cluster that's all spread out like that it's trying to have I have a smaller or more round cluster to minimize the variance and so just using a collaborative testing doesn't fix the issue I had with k-means we have to do a running page what I'm gonna have to do is use a different option and in particular if I use a single then it's okay to have this big cluster where maybe there's just a few points tying it together into this contiguous thing when I do that and I say linkage equals singular and but don't misspell that Ostra single sorry linkage equals singing I can't type anything single there we go and now we can actually see that it's neatly dividing into those two clusters as we might expect okay so let's head down here and do some hierarchical clustering and the challenge here is really gonna be about visualization right I can't neatly put every point into just one cluster and so we're going to do is where I lose learn a new visualization um technique called the dendogram oh it's popular in biology and it's showing the evolution of different species and so I'm going to do it down here I'm an example of this data alright I have these kind of two big clusters and you have some sub clusters and so what I may be doing this I am going to let me head up here and grab my code I am trying to run this and and I mix this up a little bit before I was using imposters and what that would do is it would just keep grouping it until it has that number of clusters remaining well now what I want to do is I want to get a complete free right where I have kind of one starting node that there's a cluster that contains everything and it's going to be a binary tree so that one big cluster is going to be divided into smaller clusters each of those might be divided into two smaller clusters let's not go all the way down until we have clusters that each just contain one point and so the way I'm going to do this is by using n clusters but this other option which is distance threshold and I'm going to say that distance threshold equals zero if that means um it's still gonna prefer to merge together things that are near each other but it will always merge things together until I end up with just one cluster at the end let me run that and done one of my problem here oh my problem is that there was still a default for m+ ters and i have to disable that I'm going to do that and let me peek at these categories quick and I see that of the way I've done this is that every single different point is in its own own fluster right so it might be a little hard to see what's drawing on on so to do that I'm going to have to make a dendogram and I'm gonna look over here at the documentation that they provide for this I can see that this is actually part of site PI clusterings and that part of scikit-learn um specifically so I'm gonna have to import a dendogram like that and then and then what are we doing there's lots of stuff here that has to do with the labeling of the dendogram and I'm gonna ignore that for now but what it's doing is its pulling out the children from the model and the distances from the model those are really what it needs to create this picture that I may be introducing called the dendogram and so let me just look at those two things let me look at the children and the distances and maybe I'll just leave this like that for now I'll say AC children Hey and what this is doing is it's kind of a new encoding for a binary tree that we haven't seen before on each each node and the tree is represented by one of these rows and then these two numbers and there are children of that node right so this node has children 185 and 347 and and while this is true for most of the children you'll notice that all of these have exactly two children so we're parently leaving out something we're leaving out the Leafs in our tree we're only showing the non non leaf nodes here right but I have all of those and I'm every node whether or not it's a leaf has a number here and and there's a formula and I'm not going to go to right now but I could actually figure out what 185 may be either 185 is a point and in my original data or it might be another node in this tree that just not made up whenever whenever I merge two data points together I'm printing a new node for them that somehow representing those two is a cluster right so we have lots of more nodes and we had original data points so I have each of these and then another thing I have is I have an AC dot distances right and these are how far are the two children are apart from each other right you notice that here right I have you know this is a child I'm sorry this is a node and it has children 185 and 347 and how far are those two children from each other they are this far from each other okay well to create a dendogram which is going to really visualize this tree of clusters we have to create a special matrix with four columns matrix with four columns and it's going to be like it's the first column is going to be child one and the next one column is going to be child to the third column is going to be the distance between them and then the fourth one is going to be the label or that node um and and I'm not going to go into detail here in this video you can read the documentation more if you want but that time gets tricky for that part but I really set up these three pieces nicely and and I have these right this one gives me those first two columns and the distance is Tiffany of all their columns so let me take a look at this AC dot children shei okay so I have 499 rows and then the columns and what was the shape of other one if I look at AC distances dot shape okay that one is only one dimensional and so what I'd really like to do with this one is reshape it and want this one to be having as many rows as necessary in one column and when I do that I'm like okay great I have I have 499 rows in each case two columns here one column here I can glue these things together to cover these first three columns okay so let me let me do that and you get rid of this now and the way I'm going to do that is with numpy horizontal stacking I do that and two horizontal stacking I have to pass in a tuple okay let me check out the shape of this thing great so I have my three columns 499 rows that's great and ultimately I'm not done the ideas I may have put a label there even though I'm not gonna go into detail oh but maybe I'll call this d4 dendogram what I really have to do is just to call this dendogram function that we created like so I can pass in D and it's going to complain because I'm not done yet right it's going to complain that well it basically wants to have that or column even though I strive confusing error message let me just do that for now I'm just making about a dummy column and then some just might call it labels oh I wish I put zeros in it for now I don't share and that's not going to be part of this demo um what will the shape of this thing be I know I want 499 rows and one column um but a little bit better if I just say a length of AC children that's maybe a little bit more robust because there may is two columns here or the children the distance between them and then my labels on those children I'm gonna do that and and now I'm concerned because I wasn't working and then it worked the second time which I never like to see um this is time running a very long time it's spitting out a bunch of stuff because this not only draws a picture but it returns something and so maybe actually what I'm going to do is I'm just really putting on here at the end and so I can just see my see my picture and it's creating and it's going to be a mess until we clean it off Oh okay so what what the problem right now is that it's drawing every single point in this scatter along the x-axis and that's just too much right so there's these features that we can do we can say truncate mode and we can pass in how many levels we want it to be truncated to so I'm going to do that runs much faster and and what does this mean if it's really tall that means there's a lot of separation between between the two children right if it's really short it means that it's not that meaningful splitting into two subcategories and so I can play with this and I can see that you're what's going on is that well I have two two main children and then on one side I kind of split into two children and that has some significant height so those two clusters are fairly different but splitting those two sub clusters into sub sub clusters doesn't help much because this is pretty small here and so this green part over here and this left Oh what is that those are these two guys right here Oh what does it look like when we have three sub clusters in a binary tree well it looks like this it looks like it looks like well I have one child and then do grandchildren they're all kind of different from each other so I can kind of see the patterns there and if I wanted to do more with these labels down here I could actually show how many are in each of these clusters and and it's some ugly code but they have an example I'm how to do that that's most of what they're example just doing is figuring out those counts and just copy paste if I'm not gonna bother with that with that right now alright so we can see we can by learning how to read this we can see that all of all there's do big clusters which are a distinct and then we can also see that there's three clusters right I can see that in this dendogram even if I wasn't able to get a scatterplot of the original data maybe it's multi-dimensional or something like that so it would might be hard to see in the original picture but I could still see that hierarchy here"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing Hierarchical Clustering with Scikit-Learn",
        "Video Title":"How to Perform Hierarchical Clustering in Python( Step by Step)",
        "URL":"https:\/\/www.youtube.com\/watch?v=v7oLMvcxgFY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/v7oLMvcxgFY\/hqdefault.jpg",
        "ID":"v7oLMvcxgFY",
        "Publish Time":"2019-09-09T20:39:28Z",
        "Channel":"Kindson The Tech Pro",
        "Channel ID":"UCppSe7mf1kdxA3Ga49rPWBg",
        "Transcript":"as you know I'm kinda on the tech probe on today I'm going to teach you how to perform hierarchical clustering in Python using Gupta notebook so this is the diagram where you got you get when you create a hierarchical cluster and this is called a dendrogram and then this is the plots where the clustering has been performed at this point so I'm going to take this out let's so that we can start right from the scratch to perform here right hierarchical clustering so let me start a new dirt underdog so I'm gonna go to file new Python 3 and I'm going to import course I'm going to import numpy pass NP and I'm going to of course import math plot light and forth cloth slide the pipe locks PRT and then I'm also going to import make blobs and big blobs is imported from SK line dot dataset so basically either use it to create data sites that you can use to perform clustering of war analysis from sq Lane the data sites import make blobs right so I'm going to run it just to make sure everything is fine mark plot line midpoint lot team ports must work like the pie plots as P only there is no module named Matt plot line in Padma must plot like pipe lot o find plots times disciples can be a problem so I have multiplied this pipeline all right so let's just create a set of 200 data points that we are going to use for for clustering min well let me just to clarify what we're writing so let me just say do the necessary parts and here I'm going to create a blob of 200 data points okay so let's create this 200 at a point and then we proceed from there so let's call it theta sex is a call to make so these syntax make the globs and you specify number of data point 1 so in this case we want the number of samples to be 200 and we want it to be just two dimensional data let's keep it simple so let's have two dimensional data so number of pixels is equal to 2 in this way we can have okay I because we don't need three dimensional data in this case so then we have to choose the number of clusters we want so I'm going to say st. as equal to four and then I need to just specify the standard deviation of these data points is clockwise TD is equal to I can choose any number between let's say 1 and 2 so 1 point 6 and then random States you have to include this is a koala fixing all right so I created 200 data points all right so now these data points actually is made up of two parts they decide 0 and dataset 1 so if you dig a fete at this point you see that is made up of dataset 0 and it is 1 so these it doesn't 0 the 200 data points and we also have data set 1 which is kind of the clusters from they belong but we are not interested in this this one all right so I'm going to just trucks dataset 1 so let me just call it points points is involved two data sites zero so let's just take the data set no not really the the closest alright so at this point we want to know perform hierarchical clustering so the default hierarchical clustering you can import import this isn't is important CPS keep I disclosed other therapy so the hierarchy is in a sub model and skip idle pasta and you have to use it so imports sleep I got closer the clustered of hierarchy as a CH let's call it a sexy a we also have to import agglomerative clustering from s killing the plaza import the receive clustering so agglomerative clustering is simply another name for erotic or clustering so I'd like to run it this time make sure there is no error alright so at this point we want to now create a dendrogram of course you know that we have the form in hierarchical clustering the key thing you want to generate is a dendrogram because you want to start with everything in the same cluster and individual clusters then you add the ones that are close together on your gate you get everything in one bit closer so let's create a dendrogram then program dendrogram is in for sch so this sc8 requited this object we imported dot dendrogram so use a function then draw from and specify the linkage you specify the linkage as parameter to the dendrogram function linkage and the linkage now takes the data sets so the data set in this case is points and the methods could either be forward linkage for the backward linkage so in that way this called what w8 Rd so at this point I'm going to just put a comment here that says create a then program alright so this is what I have let's try to run it to see if everything is fine yeah so we have this dendrogram but I would like to show the plots of these data just to kind of see how this data looks like in a scatter plot so I'm going to just copy the I'm going to just display because I don't want to keep typing these so I'm going to just let me take out this let me just do a scatter plots of these data sites so is actually datasets to do a scatter plots just to see how the data looks like when you're on so these how the data looks like when we created a dendrogram at this point now we want to see the clustering actually where the clusters are so I want to put kind of puts the centroids if you will the centroid I center point of each cluster we want to specify it right there all right so how do we do this it happens that you can do it first let's let's assign each of the items to close us so I'm going to say y HC this just add verbal so these variable is kind of written a dataset that assigns that first the the closer where each data point along these are patched mm a pointer you have to point 1 to point 2 then has it also closer one and so on so how at this point for cross has lost a 0 plus a 1 plus a 2 plus a 3 um so alright so we say the first thing is to we build the dendrogram we change the scatterplot but we actually need to perform the actual clustering so let me just say perform our commander you do this a couple of times so that you get your 3 but from the actual clustering so let's call it hierarchical clustering does just me my husband rival I go Mary go Mary see clustering yeah so these functions are giving number of clusters you want and closes it was this time one for clusters and how do you measure the distance between the data points you can actually use different metals maybe a Manhattan distance Euclidean distance Mahalanobis and and so on but this time let's use the common 1finity specifies the distance methods metric you want to use so yeah let's use Euclidean ok and finally you use the same linkage which is one alright so hopefully everything should be fine all right so this Haiti now performs the clustering now after performing the clustering we know how to fit the data set using this HC so we are actually going to assign each of these data points to clusters so to do that I'm going to just create a new data straight called y MC equal to HC dog piece so we are fitting the data site and hey Y hates ahead set of feet predict predict which clusters of data points belong and I'm going to say points all right so at this point we have performed the clustering and we have the data points each of them belongs to won't close up or the other all right so let's now visualize how this how it looks like so I'm going to say it PRT just gotta point where points why I can see is they fall to 0 0 okay why hit C is equal to 0 0 points against y HC is equal to 0 1 so this is cluster 1 cluster 1 for all the data points in cross the one that is what we are trying to plot let's say size s is equal to hundred and color is equal to CM all right so I'm gonna plot this and this is invalid syntax we as is coming from so we have points points as well at this point points Ron why hit C is it fall to is default we are checking weight is zero point one and if I roll it I should have only one cluster only one cluster so let me just plot a second cluster at this point so this is 0 0 1 0 1 this is 1 0 1 1 1 1 0 and 1 1 the second cluster on the color let's give it blue or yellow okay so I'm gonna Ronny and we have two pluses at this point now instead of just pi pinning I'm going to just copy and paste so we have this should be 2 0 on to 1 and this should be 3 0 this is 3 1 is 2 and this is 3 ok let's give this a color of red these a color of green I'm gonna run everything now now we have the clothes rain has been performed now let's try 2 plus let's try two plots to put in the the centroids in between us and we want to put in the centroid at this point so PRP dots Kutta so the centroid should be so how let's see let's see Santa's so the Centers of these plots you need to extract it so the Fantas k-means those clusters sorry not not came in this process is hate see that clusters dots st. us that should be ah let me just check oh maybe I I don't think maybe this is not necessary but I'll just change because I'm trying to confuse some kind of confusing k-means inherited philosophy now at this point we don't need to go and call it so we actually perform hierarchical clustering at this point and we have our dendrograms are designed for us as well so I recommend you take some time to go through it try to do it yourself to get your head around it and if you have any questions feel free to leave for me below in the comment box below if this has been informative for you please subscribe to my channel only seeing the next lesson"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing Hierarchical Clustering with Scikit-Learn",
        "Video Title":"Hierarchical Clustering in Python using Scikit-learn | Step-by-Step Coding Tutorial",
        "URL":"https:\/\/www.youtube.com\/watch?v=PVyILZow2Y4",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/PVyILZow2Y4\/hqdefault.jpg",
        "ID":"PVyILZow2Y4",
        "Publish Time":"2023-05-20T08:30:08Z",
        "Channel":"Codanics",
        "Channel ID":"UCmNXJXWONLNF6bdftGY0Otw",
        "Transcript":""
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing Hierarchical Clustering with Scikit-Learn",
        "Video Title":"Clustering with scikit-learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=PAK6rK4_1OQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/PAK6rK4_1OQ\/hqdefault.jpg",
        "ID":"PAK6rK4_1OQ",
        "Publish Time":"2022-05-21T09:20:14Z",
        "Channel":"Alexander Jung",
        "Channel ID":"UC_tW4Z_GfJ2WCnKDtwMuDUA",
        "Transcript":"so this week the topic of this round is clustering so clustering is an unsupervised learning method where we're unsupervised means that we don't have any label data and the idea is to find a way to to group data points based on how similar they are or more specifically how similar their their features are and some examples of that so probably the most common example is how advertising companies um segregate their customers and clients and make groups of people and then recommend products to a certain type of person and so on so yeah this is clustering used in market research then another common thing where clustering is used is image segmentation you can think of that also as minimizing the size of the image so for example we would cluster or group the colors of the image that are similar and then we would substitute the similar colors with the centroids or with like the mean of these similar colors and this way we can get a smaller size of image yeah and then recommendation systems for movies or again products or whatever so um clustering is typically divided into categories we have soft clustering and hard clustering and the question that heart clustering tries to answer is does the data point belong to your cluster why so the answer is always yes or no while on the other hand soft clustering asks a question how probable is it that the data point belongs the cluster so each data point has a probability of belonging to a certain class yeah and we will also discuss about dd scan which is a density based clustering algorithm and yeah it's also hard clustering and then we will also see the problem yeah k means probably the most popular clustering algorithm and then we will also see uh dmm gaussian mixture models for soft clustering so the learning goals are learn how to use k-means learn how to use gmm and db scan yeah and how to choose between different clustering methods and different number of clusters because typically you also have to decide on how many clusters you are expecting okay let's start with the basics so assume we have m data points denoted by x the task is to assign its data point to one of the of k groups so k here is a hyper parameter that mean which means that we are the ones that should divide define it and [Music] yeah to define a clustering algorithm we also need a similarity function so a data point x i have like how similar is it to data point x j so if they are too similar then maybe they belong to the same cluster um yeah and now since we don't have any labels uh you can understand that it's not so easy to evaluate how good the clustering algorithm is or the outputs of the clustering method are yeah and we will see different ways of how to counter that so um k-means is a hard clustering method that works following this tutorial cheetah code so we first initialize some sentries [Music] yeah i will show this animation first and then go to the algorithm so as you can see here we first initialize these four uh centuries and then we calculate the similarity or the distances to each and every other of the data points and then as you saw from here the center it changed to here so this is like the middle point of all of these data points of all of these similarities so we want the century to be at the center of all of uh the rest of the data point so um yeah if you have any questions about this feel free to ask them but i will go to the computer code maybe it's easier this way so here m denotes this four data points we saw before and then while we are we haven't reached conversion so while true we assign the data points to the closest centroid so for each data point in our data set we calculate the similarity to to the centroid to one of the random starting centroids and we assign it to the closest one and then uh yeah so for this data when we calculate this um yeah and this is the similarity function used in k-means so this is euclidean distance simply the norm2 square [Music] okay and then we update the means by operating samples in each cluster so we're we're practically operating the distances and we take the medium the mean one and then our convergence check is that if the means have not changed in a while then there's probably no reason to go on so yeah we break the loop [Music] okay so a good thing about game means is that even though it's simple it is guaranteed to converge so we will not have like an infinite loop that doesn't add um however it might converge the local minimum so it is not always guaranteed to find the best uh possible sentries and there are certain techniques to help us avoid such cases um a common one is to try out different starting centroids because as as you saw in the animation the starting centroids are usually initialized in a random way so if you do that random initialization many times then you will get the you may get different results every time so you will see like the local minima if you see different local minimums then one of these must be a global optimum most probably if you do that many times at least okay um then another common technique is yeah so this is exactly what i said now sorry and when implemented this in sklearn you need to make sure that you set this uh number of suggestions to one because in the assignment you actually have to implement the for loop like the initializations yourself [Music] yeah there are instructions there but make sure that you know of this and init parameter okay yeah now as we mentioned choosing the number of clusters is a is a hyper parameter that you are the ones that have to choose so how can you know which one is the best number of hyper parameters um well we will again use the validation set that we introduced before and we will try out different k values and we will pick the one that has the smallest error on the validation set and yeah and to pick the one we typically use the elbow method which will be shown in the next slide but let's first discuss about what's the error so the error is just summing the euclidean distances of its data point so we want this sum to be well ideally we don't want this to be zero but that's not possible so we would want these distances to be like small the smallest possible um yeah again if you have any question feel free to to interrupt me so uh if the majority of data points are far from the centroid then the error will be high because the individual euclidean distances will be very high okay now let's see the elbow method so here this is taken from the assignment we tried out different values of k so from one to eight different clusters and um the album method says that the the best possible number of clusters is the one at the elbow point of the plot so it's the point uh where the the error starts decreasing in a linear function so here it will be between three and four because after three this seems to be like a straight line after four this is even more obvious so this would be up to you but [Music] yeah personally i think that four may be the best number of clusters okay if you have any questions about k-means um okay assume not so let's move to soft clustering then so um yeah with heart clustering it is not possible to to see how probably it is that the development belongs to another cluster other than the one that we uh assigned it to and uh well that's not really what we always want we we ideally want to be able to assign the probability for for each and every cluster and in soft clustering we actually assign a confidence score so for example in this plot um the data points that are inside the circle have a high confidence score so like the the algorithm knows that these data points do belong in this cluster but the database outside the circle could belong to another cluster so that's what this says says to us or they could even be outliers so you could use this sort of methods to to simply discard bad examples in your data [Music] right and gaussian mixer models are probably the most common way to perform soft clustering so um as we said we assign probabilities so instead of assigning a data point to one class we are now assigning scores in a class in a vector of classes so it's y i would contain a score that the i data point belongs to the first class and so on okay and in the gmm we assume that the data are drawn from k different gaussian distributions and its distribution is parameterized by its own mean vector and covariance matrix so we will not get into many details about the gaussian distribution or how how we update the mean and covariance metrics but in the notebook there is a small optional section if you are interested in that [Music] and yeah so our task is to find the best mean and covariance matrix and we also want to find the best probability assignments that correspond to this means the covariance metrics so we denote the assignments by the pi and c is the their id of the cluster [Music] so this is the multivariate gaussian distribution oh no that's the the one-dimensional one so yeah as you can see by tweeting the mean and the variance we can have many different distributions like the tails are either more flat or or the whole graph is wider and so on so yeah it's one of these separate gaussian distributions could denote a separate cluster for example [Music] some drawbacks of dmms are that it's usually slower than gay means um i also have to add here that it's kind of more complex maybe for someone without a probabilistic background and it uses the expectation maximization algorithm which may not be so easy to familiarize yourselves with if you don't have the necessary background but in any case you will not need that in this course so you should be fine um yes and also it is very heavily dependent on the initial values of the parameters and then uh if the measure is not a gaussian distribution so if the underlying distribution is not gaussian then the gmm soft cluster will not work right and it is also like a game it's it can also converts the local minimum instead of a global minimum and we can try out the same approaches as with means to to get over this issue um yeah let's also see another animation of that so as you can see here the wider the the circle is the more no sorry the denser the circle is the more probable it is that the data points there belong to the certain cluster so as you can see with the yellow data points [Music] uh when it converges there are these yellow data points that don't seem to be probable at all to belong to that cluster so maybe we could consider them as outliers or still accept them [Music] and then there is this page with documentation about clustering from sklearn and they have this really nice image which i will show you later and also this seven minute lecture on expectation maximization is very a very good one if you are interested to learn more about how dmms work like underlying all that but again you will not need to implement the algorithm itself the scale and already has it there [Music] so any questions about gms [Music] okay then maybe yeah we will move to density based clustering and db scan in particular so the methods we have seen so far both use the euclidean distance to measure similarity and the error but in many cases we don't have a euclidean structure and these algorithms tend to fail in such cases so what density-based clustering does is it reform relates the problem into a graph theory problem and it uses the connectivity relation to define similarity between data points so two data points are considered connected if one can read the other by intermediate data points and if those intermediate data points have a small duplicate distance among them then we can say that [Music] the node at the start of the path and the nodes at the end of the path are similar so for example here this is also taken from the notebook with the classical euclidean similarity measure you would say that these two are not similar just because the euclidean distance which is this line here is kind of large but on the other hand when using connectivity to to define the similarity we see that from this data point we can jump to the next one then to the next one the next one and then to the last point so these similar these uh distances are pretty small which means that probably these two data points are similar so these two databases are similar and so we assume that the first and the last one are similar too yeah so that's like the big picture of how db scan works um so again it's a hard clustering method so if a data point either belongs or not to the corresponding cluster and another good thing about db scan is that it does not require the number of clusters to be predefined so you do not need to tweak the number k of clusters or anything but instead it just finds out the best possible clusters in your data [Music] and it can also it is also actually used a lot in outlier detection because sometimes there is a cluster of data points that are too far from the other and these are well categorized as outliers in maybe you can also use that in normal classification tasks in order to identify which of your data points or set of features are very non-relevant to your problem right um so db scan requires the definition of what constitutes a dense neighborhood so as we said here so what what's really the thing that makes this dense is that these similarities are small like the each individual similarity is small but we still need to define how small we needed them to be so as to consider the the neighborhood dance um so this is not as simple as it sounds maybe but in sklm they have found a pretty good way to do that they define these two hyper well what's hyper they define these two parameters mean samples and the epsilon threshold so there is a documentation in these two links i have here you can check it but the epsilon value is usually fixed and you may need to tweak the mean samples parameter in some cases [Music] but they have a pretty good explanation of how these two works this will work and specifically i would recommend the second link because it contains yeah i think that's the wrong link i will fix it [Music] yeah this should be the link so here it says how many samples and the eps parameters work anyway so uh this is the subset of the picture we saw before so here we have k-means agglomerative clustering which we did not discuss and we will not discuss i'm just including it here so you will see that there are other clustering algorithms too and then we have db scan and gms so like we said k-means and gmms completely failed to classify the two clusters that are in this [Music] sort of circular yeah they follow the circular movement um and the same also happens with these semi circles but on the other hand with the graph based or the density based algorithms we see that we are able to define that you know this data point and the data point on the other side are similar just how we humans identify that these two are similar just because they they are interconnected through nodes that have uh good similarity scores [Music] right um [Music] yeah some other examples with pure noise and um yeah some other structures do you have any questions here yeah yeah here in db scan maybe you could also notice that they have this black data points here so this would be the outliers these don't belong to a certain cluster [Music] okay so this was the presentation um don't have anything else to say maybe [Music] we could quickly go through the notebook if you don't have any questions if you do have any questions feel free to interrupt me of course so the notebook starts with some theory about clustering then you you are using the uh cafe in helsinki data and you are implementing pk means clustering algorithm so here is the pretty much what we saw in the pseudocode but you don't have to worry about implementing this because sklearn already provides a solution [Music] yeah so here you have an example of how to first define a class of payments and then create it on your data and then you have all of these information in the parameters of the instance [Music] then the task here is to apply it means yourself and you should get something like this here so these are the sentries that you should see [Music] and here it's like there is an invisible line that separates the two clusters from each other then there is this way to handle local minima instead of global optimal solutions so we have already discussed about that you need to implement it and get an output like this so here the smallest error is this assignment while the largest error is this assignment which makes sense even to the human eye because this separation of two clusters is not so intuitive and then there is the elbow method to decide on the number of clusters [Music] and then there is the soft clustering some information about how dmms work so this is optional material you don't have to read it and in sklen you can use the this class gaussian mixer class again it has a ton of parameter well not done but it has many parameters you can use um not so many attributes that you can use after you have fixed the model but the the methods you use to to do the training to find the clusters is the same so you still use the fit method all right and then we have pretty much the same stuff as i mentioned so this is a nice visualization of db scan um yeah if you have any particular questions about the notebook i would be happy to answer them now because there is not much uh i could say for the notebook here [Music] yeah okay yeah after the discovery is also an example with uh of course combining clustering with logistic regression and there are some quiz questions so yeah i will stop the the recording then"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"DBSCAN Clustering Algorithm",
        "Video Title":"Clustering with DBSCAN, Clearly Explained!!!",
        "URL":"https:\/\/www.youtube.com\/watch?v=RDZUdRSDOok",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/RDZUdRSDOok\/hqdefault.jpg",
        "ID":"RDZUdRSDOok",
        "Publish Time":"2022-01-10T12:51:02Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"dp scan clusters just like a person can statquest [Music] hello i'm josh starmer and welcome to statquest today we're going to talk about clustering with db scan and it's going to be clearly explained now imagine we collected weight and height measurements from a bunch of people and we plotted the people on a two-dimensional graph like this where we have weight on the x-axis the first dimension and height on the y-axis the second dimension by eye we can see two different clusters by identifying two different but relatively dense clumps of people in contrast these people that are far from everyone else look a little bit like outliers so by eye clustering this data is pretty easy however because these clusters are nested meaning the green cluster wraps around the blue cluster a relatively standard clustering method like k-means clustering might have difficulty identifying these two clusters instead because of the nesting a simple clustering method might get something weird like this where these points are assigned to the blue cluster even though they look like they belong to the green cluster so we need a clustering algorithm that can handle nested clusters also remember this two-dimensional graph only uses weight and height data but if we wanted to include each person's age we would have to add a third axis and now our graph is three-dimensional drawing a three-dimensional graph on a two-dimensional computer screen is awkward but possible however if we wanted to include four or more features we'd need to draw a four or more dimensional graph and that's not possible and if we can't draw and look at a four or more dimensional graph then we need a way to identify nested clusters that we cannot see by eye the good news is that there are clustering algorithms that can identify nested clusters in high dimensions one of these algorithms is called db scan and that's what we'll talk about today so let's go back to the original two-dimensional graph and see how db scan tries to mimic what we can easily do by eye now remember by eye we identify clusters by the densities of the points clusters are in high density regions and outliers tend to be in low density regions so let's see how dbscan uses the densities of the points to identify these two clusters bam now starting with the raw unclustered data the first thing we can do is count the number of points close to each point for example if we start with this red point and we draw an orange circle around it then we can see that the orange circle overlaps at least partially eight other points so the red point is close to eight other points note the radius of the orange circle is user defined so when using dbscan you may need to fiddle around with this parameter now this red point is close to five other points because the orange circle overlaps at least partially five other points this red point is close to six other points and this red point is close to seven other points this red point is only close to two other points and this red point is not close to any other point because the orange circle does not overlap anything else likewise for all the remaining points we count the number of close points now in this example we will define a core point to be one that is close to at least four other points note the number of close points for a core point is user defined so when using db scan you might need to fiddle with this parameter as well anyway these four points are some of the core points because their orange circles overlap at least four other points hooray but neither of these points are core points because their orange circles do not overlap four or more other points ultimately we can call all of these red points core points because they are all close to four or more other points and the remaining points are non-core now we randomly pick a core point and assign it to the first cluster next the core points that are close to the first cluster meaning they overlap the orange circle are all added to the first cluster then the core points that are close to the growing first cluster join it and extend it to other core points that are close by here we see two core points and one non-core point that are all close to the growing first cluster and at this point we only add the core points to the first cluster that said eventually we will add this non-core point but right now we are only adding core points ultimately all of the core points that are close to the growing first cluster are added to it and then used to extend it further bam note at this point every single point in the first cluster is a core point and because we can no longer add any more core points to the first cluster we add all of the non-core points that are close to the core points to the first cluster for example this point which is a non-core point is close to a core point in the first cluster so we add it to the first cluster however because this is not a core point we do not use it to extend the first cluster any further that means that this other non-core point which is close to the non-core point that was just made part of the first cluster will not be added to the first cluster because it is not close to a core point so unlike core points non-core points can only join a cluster they cannot extend it further now we add all of the non-core points that are close to core points in the first cluster to the first cluster and now we are done creating the first cluster double bam to summarize how the first cluster was formed we picked a random core point and it started the first cluster then neighboring core points joined and extended the first cluster [Laughter] and non-core points only joined the first cluster bam now because none of these core points are close to the first cluster they form a new second cluster because they are close to each other and the non-core points that are close to the second cluster are added to it lastly because all of the core points have been assigned to a cluster we're done making new clusters and any remaining non-core points that are not close to core points in either cluster are not added to clusters and called outliers and that is how the db scan algorithm works triple bam note as we just saw clusters are created sequentially that means if we had a non-core point close to both clusters then when we built the first cluster beep we would add this non-core point to the first cluster because it is close to a core point along with all of the other non-core points that were close and now that this point is part of the first cluster it is no longer eligible to be in any other cluster small bam now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the statquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"DBSCAN Clustering Algorithm",
        "Video Title":"DBSCAN Explanation and Visualization",
        "URL":"https:\/\/www.youtube.com\/watch?v=_A9Tq6mGtLI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/_A9Tq6mGtLI\/hqdefault.jpg",
        "ID":"_A9Tq6mGtLI",
        "Publish Time":"2019-10-31T02:12:49Z",
        "Channel":"TheDataPost",
        "Channel ID":"UCo8qbws1-to6LDgMNECu2dw",
        "Transcript":"DB scan is a clustering technique which can cluster any shape it is a widely used algorithm and only relies on two parameters epsilon and minimum points epsilon is a distance parameter you can think of it like a radius from a given data point that broadcasts out in all directions forming a perimeter around the data point minimum points is a second parameter you must specify before we get to that I'll define two key terms the first I'll call the starting point starting point is a randomly selected data point that will attempt to start a cluster in this image the green circle represents the starting point the second term is core point core points are data points which fall within epsilon of the starting point in this image all gray circles are core points the white circles are not core points because they aren't within epsilon of the starting point minimum points is the minimum number of core points including the one starting point that must exist for a cluster to be formed in this example there are seven points within epsilon of the starting point including the starting point itself thus if minimum points is set to seven or less the cluster would be formed however if minimum points was set to eight or more these data points could not initiate a cluster once a cluster is formed that cluster can continue to grow each member of that cluster will broadcast out their own perimeters looking for new data points that could join the cluster in this step you can see the core points did in fact find new members to join the cluster these points have been made blue all the blue points in this example are considered border points this means they are part of the cluster but not with an epsilon of the starting point now these new members will cast out their own perimeter and search for even more members to join the cluster but in this case they don't find any because new members are not found this cluster is finalized I've turned all members of the first cluster blue however you will notice there are three white points still on clustered now we will restart the process by randomly selecting one of the remaining uncluttered points I have again designated the starting point green I won't run through the steps again but this iterative process continues until every uncluttered data point has been looked at as this algorithm finishes running it may eventually form multiple clusters as seen in this example but note this red group could only form if the minimum point was set to three or less now then you understand how DB scan works I'll show you a start to finish visualization with the running algorithm found at Naftali Harris comm"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"DBSCAN Clustering Algorithm",
        "Video Title":"DBSCAN Clustering Easily Explained with Implementation",
        "URL":"https:\/\/www.youtube.com\/watch?v=C3r7tGRe2eI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/C3r7tGRe2eI\/hqdefault.jpg",
        "ID":"C3r7tGRe2eI",
        "Publish Time":"2019-06-05T14:58:21Z",
        "Channel":"Krish Naik",
        "Channel ID":"UCNU_lfiiWBdtULKOw6X0Dig",
        "Transcript":"hello all today we will be discussing about DB scan algorithm so the full form of DB scan is we have something called as density based spatial clustering or application with noise don't worry about so big full form of this particular topic but it is a very easy topic to understand machine learning algorithm so DB scan algorithm is one of the most better unsupervised machine learning technique which sometimes performs better than k-means clustering and - in clustering but it has its own advantages and disadvantages which we'll be discussing at the last in this particular video so make sure you watch this particular video till the end because I have explained as explained the book intuition part and the implementation part with the help of Python and it's a LAN and how you can actually implement this particular algorithm to begin with guys the most important thing over here is that it has four important components one is something called as epsilon a epsilon the second component is something called as minimum points then you have something called as core points then you have something called as border points and you have something called as noice points so as you know that DB scan is an unsupervised machine learning technique so basically an unsupervised machine learning technique it happens in such a way that it works in such a way that whenever we have some data points populated in a particular graph it may be two-dimensional three dimensions what we do in unsupervised machine learning technique is that we try to make up clusters you know clusters helps you to find out the most similar points in our distribution and they've shown that particular clusters you know whenever a new point comes like for the next test data whenever I get a new data and if I try to populate that point whichever cluster that particular point will fall into I will consider that that point falling into that particular group so that is how a clustering algorithm works I have already created videos on k-means clustering and - in question if you're interested you can go and have a look onto that but we'll try to understand how does density based spatial clustering works and this is included with noise okay now first of all what we do is that we will be having initial points so this will be basically are all our points so what we do is that we initially consider some epsilon value and some minimum points value so suppose if I am considering away here as minimum value minimum points as for and suppose I say that my epsilon value is something like 3\/4 it depends on the distance now the epsilon value indicates very important thing that basically indicates is that suppose this is my point a ok now what does epsilon indicates that I have to take the radius of that particular value of Epsilon ok and then I have to create a circular path across it that is I have to create a circular boundary across it of that particular radius so this is my radius ok now when I create this particular radius the next the next component that is minimum points comes into existence now this epsilon will be based based on like within this point I have to minimum ly considered I mean I have to consider a minimum number of points of four points in this so if I if I have four points like this which is populated within this then what happens is that I will be considering this point a as my core point so very anything to understand that to make a point considered as a core point there should be two conditions that has to be you know that there should be two conditions that it has to follow one is that I have to consider a boundary with the radius of our salon Valley then the next thing is that I have also considered that the minimum number of points that fall within this boundary should be less than or equal to four in this particular case right sorry it should be at least four points should be following in this particular thing right or if it is greater than four point it is well and good but at least four points you know at least four points should be following in this particular boundary then only I will be considering this a point as my power point so my next component that I was discussing about is something called as four points now there is also something called as border point a border point indicates something now suppose I have a point C okay now I'll be using my epsilon value and creating my epsilon value right the creating that radius and with respect to that I will be creating my boundary now this particular C point suppose it does not satisfy the minimum points is equal to four okay it does not satisfy this minimum point is equal to four but we have at least one core point present inside this particular boundary okay now just understand is there's two condition first of all I had drawn the boundary with the help of epsilon the second condition is that this minimum points is not getting fixed I that display means I don't have that many minimum number of points that I've actually initialized in this but I have one at least one core boundary inside it so when I have this one core boundary inside it then this points actually becomes a bound border points so this point actually become a border point now three things guys first of all I mean we need to understand what is epsilon with the help of epsilon and creating the boundary second condition is that I have to consider that minimum number of points up has to be present within that boundary then only I'll be able to consider this point as my core point okay if it is not falling then what do we do is that we consider another scenario wherein we consider that at least one core point should be falling inside the if one core points four points basically means this kind of point if it falls inside this boundary then I will be considering this as my border point okay now the next thing is that suppose if I have one point like B now I've taken the epsilon radius I have created my boundary now what if none of this is getting satisfied if I don't have this minimum number of points s4 and if I don't have any four points if I don't have any four points then this point basically becomes a noise point noise point is basically like my outlier now this outlier is being very nicely handled by DB scan algorithm that basically means that whenever it finds an outlier it will never draw this kind of boundary across it you know so it will just keep that noise point and it will never treat it as a separate group of Cuesta but whereas in the case of border point and whereas in the case of core point it will be considering this kind of circular borders or boundaries or this is just like treated like a clusters no this is like a cluster now you see this let us see this particular example that I have I have red points over here you can see that this red point basically indicates that if I am considering a epsilon value of some value and if I am considering minimum points as for suppose if I consider this I have drawn I have taken an epsilon length over here as my radius I've drawn a circle now I will go and see that at least four points are falling inside this circle so I have one two three and four so four points are actually falling inside this circle so I can consider this as my four point so this all red points that you see is basically my core points okay now suppose tomorrow after our my model is getting trained if I if I if I if a new point comes over here and this is basically treated as the point that belongs to this particular cluster where my four point is this similarly with respect to all the other values but just now see about this particular yellow point at this yellow point when I draw a circle around it or a boundary along it I don't see more than two points over here I have just and to write man this particular point if it is considered as a core point then this basically becomes my yellow border point yellow which is my border point over here so similarly in this case this also becomes a border point because I have at least one core point within this particular boundary of the yellow point now this is how it is actually used you know this whole DV algorithm DB scan algorithm works in this way goes from one point to other the price to find out this particular value whether it it has that particular ice to create the you know it tries to create the diameter of this particular radius that is of my epsilon value and then what happens is that after that it it it tries to then create you know or try to find out the necessary core points in this by satisfying both these conditions and then it classified itself as a clover point or a yellow border point and wherever a yellow border point or a core poor I mean what this yellow is basically the colors fine saying yellow water points but when I see a core point I will be trying to create a cluster when I see a yellow point I will be trying to create a cluster but whenever I find out a noisy point I don't create a cluster so this DB scan algorithm works very well with the noisy points the cause noisy points that are never taken into a cluster so it basically skips or it does not take care of those noisy points so DB scan algorithm works well with the noisy one and this was an example of this guys let me just go to the next slide and show you some more things about this like how the classification happens you know the left side in the left side of the image that you see its grouping has been done by some other algorithm like k-means okay and hierarchal mean higher clustering okay k-means clustering and haiku mean questioning whereas in the right hand side of the graph you can see that it is this whole clusters has been happened with DB scan this was some of the disadvantages in k-means and - clustering because when our data is actually distributed as you know that in came clustering what we do is that we try to make clusters of points ID based on Euclidean distance I mean clustering again there is a Euclidean distance again we follow some thing called as you know our talk to bottom high P who actually solve that and similarly in DB scan algorithm we have actually discussed about this write in DB scan you can see that it has been able to group the points very nicely and you see this this particular points are not grouped because these all are noisy points these are left like that it is these are all left like that you know this all are left like that this dissolve are left like that but whereas the other point that you can see that it has been classified a group very nicely so that is the vantage of DB scan of cartoons it works very well with the noisy data right it works well with the noisy data now as we go ahead guys I'll also show you some of the advantages of DV scan so it is very great at separating clusters of high density it focuses on high density of points because we are considering two things that is that epsilon and the minimum points that should follow within the boundary of that and so it it is very great at separating clusters of high density versus cluster of low density and it also defines well defines the noisy points and it is also a great with handling outliers which also my noisy points within the data set some of the disadvantages are it does not work well with dealing with clusters of where very in densities because suppose if I have some extreme dense density in my clusters in ever if I have less number of dense clusters so all this kind of spike the DB scan will not work okay so it also struggles with hide and dimensionality of data because as the number of dimension increases it becomes very very difficult for grouping together by using this particular density clustering so so let us let us go and just understand how we can implement a DB scan algorithm because I'm just going to show you a very good use case wherein we will try to apply DB scan in order to apply DB scan algorithm guys we will be using a library that is present inside scalar dot cluster dot DB scan so let me just go ahead over here you can see that we have epsilon value we have minimum sample we are metric Euclidean distance you know and this minimum samples is basically indicating your minimum points now and based on the Euclid is Euclidean distance it can consider again there are two types of metric that we can use either euclidean distance or distance it is up to you okay and how well it works with respect to this particular data so let me just show you a very good example that I have for the VC an algorithm I have a data set which is filed as mall customer dot CSV okay now let me just show you the data set now this particular data set you can see that I have lot of information like customer ID jen-jen ray or gender so it should be gender it is written agenda me then we have age then we have annual income then we have spending scopes now what our aim should be that I just created it from statement that I'm going to consider annual income and spending scope taking these two features I'll just apply a DB scan algorithm and based on the density of the points it will try to group this particular points together okay so let let us go ahead and try to see so what I have done is that I've just taken my third and fourth column which is my annual income and spending school so this is my third and fourth column and I've basically used data set or log so as usual let me just open a notepad and let me just write down the steps for you all so the first step is as usual let me just make the font size a little bit better bold 16 Oh let us know so the first step as usual is basically to import the data set and guys this is a unsupervised machine learning technique so you don't have any output in these features you just have independent feature and finding the similarity in the feature what we do is that we try to based on a density scan algorithm we will try to group those into clusters okay after importing the data set will just and I have my independent features over here and in my X then I will be just applying my DB scan algorithm now DB scan basically you know I have to import import the DB scan algorithms so as usual dbstyle Gotham is stepping in supported import the DB scan from SK learn all right so I'll just write it down so that you will always be able to remember these steps now you can see that way we taken the DV scan important a DB scan from a cylinder cluster and over here DB scan I've taken as epsilon value as 3 you know what is epsilon with with the help of which I'll be actually creating you know or radius and based on that I'll be creating the considering epsilon as my radius and creating a boundary around that particular point and the minimum number of samples and just considering it as for to in order to just show you how to work how this is working now you can see that my DB scan has worked properly over here it has got executed now the next thing is that I'll be fitting the model you know so as usual my third step is basically fitting the model you know that basically means that my training will start over here now this is fine done as soon as I do this after doing the fit if I say model dot labels I automatically get all the labels you can see that now my labels is like this now from the labels that basically means that how many clusters of groups aren't getting it you can see that over here there are a lot of values which will have minus one now can you just imagine what this minus one will be okay so there are one zeroes minus one you can see one way at two three four five six seven eight and remaining all at once okay so zero to one I have lot of this understand this minus one basically indicates that these points are the outliers okay honey they do not fall in any of the clusters whereas other points you can see that it is starting from zeros than once then these are basically my zero cluster of first first two cluster so many groups of clusters are there now what I will do is that I will try to find out how many unique groups are there okay so for that first of all I'll just I'll just create array array by using numpy where I will consider everything as false so this is the code to do that you have everything as false right now then what I will do I will compare this sample underscores and labels and wherever these label values are true so suppose if I just write DB dot core underscore sample underscore indices this basically indicates that on which all index which all index I'm having a different value I'm having a different value which is like or groups you know apart from minus one okay so what I do is that I will take all these indexes and I'll make it as true because this basically indicates that this meaning from the sample and disco cool whichever all Falls remaining all are basically my group's different different groups so you can see that all the two values are there then I'll just calculate the number of clusters if I do that I just use that length length offset of labels minus this you can see that my total number of clusters are nine not be considering minus one as my cluster because minus 1 is the noisy points the total number of customer hat from 0 to 8 0 to 8 is basically my nine clusters these are my nine clusters then I can also see my score which is called as silhouette score which I use it with respect to my X and labels based on the number of sorry I did not import this SQL on the matrix and this score will be getting calculated based on the average mean of the number of points that are indicated as noisy and when compared to the other points that are basically indicated as growth plus now this is a very good algorithm many when you want to basically divide or divide the data set or cluster the data set based on density I hope you like this particular video guys all you have to do is that like share comment he's put your valuable comments or just like it if you like this particular video or make sure you share this but with all your friends who ever require it and this is how a DB scan algorithm will be done I'll be uploading this code in my github so you can see that particular bit of you are in your description and don't forget to subscribe the channel I will come up with more interesting content I'll see y'all in the next video till then have a great day never give up keep on learning thank you one and all I'll see you all in the next"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"DBSCAN Clustering Algorithm",
        "Video Title":"DBSCAN Clustering Algorithm Explained Simply",
        "URL":"https:\/\/www.youtube.com\/watch?v=Lh2pAkNNX1g",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Lh2pAkNNX1g\/hqdefault.jpg",
        "ID":"Lh2pAkNNX1g",
        "Publish Time":"2022-08-01T12:07:34Z",
        "Channel":"Greg Hogg",
        "Channel ID":"UCJublDh2UsiIKsAE1553miw",
        "Transcript":"hey guys it's Greg today I'm going to explain the DB scan clustering algorithm and DB scan stands for density based spatial clustering for applications with noise I would not blame you for being intimidated by that long name no one ever says that we call it DB scan and the only important part of that is that it is density based and we'll see what that means shortly now it is best explained in two Dimensions so I'm going to draw two axes imagine that you had a set of observations and we put them on a graph like this on a scatter plot and say just so that we can have some interpretation it doesn't actually matter what they are but we're going to call this axis height and we're going to call this access weight just so that we have some sort of name for these if you have a different data set with many different variables that is expected and okay now because I'm drawing this in two Dimensions it will visualize well but if you were trying to do this in one dimension or 4 or 10 or 100 then it would probably work as well as you added the number of variables it does start to get pretty computationally complex but technically the math would still work out now drawing it in two Dimensions well we should have something like this we had different points so we have each of their heights and weights maybe we had a bunch of people over here like this and that was probably considered a cluster this cluster here if we have say another scoop over here I'm going to line it up so that we kind of curve around here like this and maybe we had some kind of other weird points that that didn't fit quite as well just to make it a little bit more realistic now what we'd hope out of this clustering algorithm is that we identify the Clusters and what it would probably be is something very close to this where we specified or found that this was one cluster maybe it got around here it may or may not identify that point as part of it or not and then probably this other scoop actually should do a different color a different color over here specifying that this is a different cluster so we'd hope that it identifies that we have one cluster in purple over here we'll just call this number zero and that we had another one which is cluster one over here and then the rest of these we kind of hope that it uh it just caused them noise and so we're going to specify that each of these are minus ones because they're just outliers they really don't fall any sort of cluster that's what we're going to Hope happens and if we do the DB scan algorithm then it will actually output this or something extremely close if we did the K means algorithm whether you know how that works or not it would actually not get the right answer most likely DB scan is great with this kind of thing and we'll see what it does right now so basically how this works is we have to tell DB scan a couple different things we have to tell it uh Epsilon which is a distance value we have to set Epsilon which I'll call EPS because that's what pyit learn uses and we'll set this equal to for now we'll say 0.5 and we also have to specify this thing called Min samples Minore samples and we'll set this equal to say three okay so when you're actually trying to find these correct values well there's a way of optimizing and fitting that which I'll show you in the coding video if you want to see that uh but for now we're just going to assume that we have some fixed values that you've decided Epsilon is .5 and mid sample is three now what this Epsilon value does is that Epsilon = 0.5 implies that the radius of a circle is 0.5 and so we could draw like a whole circle around like that I know not perfect but something like that if we drew that circle around every individual Point well we'll start with this one here so we'll draw that Circle and maybe it looks something like this and what it happens to do is hit three other points hey sorry for this piece I realized I need to explain this better when I say the word hit I don't necessarily mean that the circle actually hits as in absolutely touches another point I mean that if the circle absorbs another point so as long as there is other points that are within that Circle either on the boundary of it or anywhere within that Circle then that's what I mean by hit and if I say that carrying on forward then that's what I mean as well anything within that Circle Okay carry on it hit this point this point and this other point if it hit three points well we'll just calculate that for now and remember that that first point that hit three with that circle now this second point we'll do it for that that Circle if we draw it again happens to hit four other points and we'll remember that if we do that same process for every individual point we will have memorized and we can store that in the computer how many different points that Epsilon of 0.5 Circle touches so if we have that information we have a whole number associated with each individual point and what we can do with that is compare it to this Min samples value which is basically a threshold if that threshold is set at three then what we'll say is that if each point hits at least three different other points it is considered what is called a core point so many core points will be defined here and although this may not be perfectly correct I will say that this many core points are defined we'll say this one and this one and this one and this one and this one are all core points we'll say that this is a core Point that's a core point this is a core Point all of these other ones as well and I'll just circle all of the ones that are most likely core points now that we've specified whether each point is a core point or not we can do another process which is draw draw that Circle again around each of the core points so for each of those core points we will draw a circle around them we'll start with say this one over here if we draw that Circle it hits these points and now we are going to start specifying actual clusters starting one in not green because this green is not specifying a cluster it is specifying whether something is a core point we will start in blue and looking at this core Point here we'll start randomly with that one if we draw our Circle it hits all of these points and so we will specify that this point is that cluster and all of the other ones that it touches now for all of the other core points that it touches we're actually going to draw that circle around them and so since this is a core point right here we'll draw that Circle and then for each core point that it touches we'll assume that those are part of that cluster as well and we'll do that same process now for the ones where it hits non-core points since that Circle hit these non-core points we are going to specify that these are part of the cluster except we're not going to chain react and bounce off of them and draw the circle off of these ones we won't be drawing a circle like this and seeing what they're touching we're just going to classify them as those points now what should happen if you bounce around and do that Chain Reaction Every Time We Touch a core point we'll bounce off of it it and collect everything around it we should get something like this where all of these are assigned to that cluster and then this one here since it's only close to this non-core point here and we don't Branch off it we don't chain react off of that that point is not going to be specified as that cluster this cluster is now fully done identified and we'll just draw a zero to represent this is cluster one we need to identify more clusters because we have more core points so for a random core point we start at one of those and start creating a new cluster so we'll start calling this the red cluster and since this is a core Point here we'll draw a circle around it maybe it hits these points and we'll color each of those as a red cluster so all of these are considered red and for every core point that that Circle touches we're going to draw a circle around that as well so since these are core points we'll draw that circle around chain react off of it until we hit all of these other points and then these ones are going to be hit as well and it stops there since at this point there is no other core points well we are going to draw this as a one because that cluster is fully identified and we are fully done with all of the core points we still have non-core points left over and so what those non-core points will be specified as are1 for outliers so this is1111 1 1 all of those are considered outliers and that is the final result of our dbcan algorithm this algorithm is density based which allows it to find this nonlinear pattern over here where this is one group and this is another group if you were to do K means it would likely just say hey maybe all of these are going to be one cluster and all of these will be another cluster and actually it won't even be able to identify the out wires either DB scan is well fit for this particular data set Epsilon Min samples are userdefined variables or often called hyperparameters these have to be tuned and you can see the coding tutorial on how we optimize those variables to get the best model we also haven't specified what makes a good DP scan model or what makes any good clustering model you can see that as well in the coding tutorial if you'd like to see that have a great day guys and I hope this was helpful"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing DBSCAN with Scikit-Learn",
        "Video Title":"DBSCAN Clustering Coding Tutorial in Python &amp; Scikit-Learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=VO_uzCU_nKw",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/VO_uzCU_nKw\/hqdefault.jpg",
        "ID":"VO_uzCU_nKw",
        "Publish Time":"2022-08-02T18:40:33Z",
        "Channel":"Greg Hogg",
        "Channel ID":"UCJublDh2UsiIKsAE1553miw",
        "Transcript":"hey guys it's greg today we're going to be implementing the dbscan clustering algorithm in python and scikit-learn now two notes before we start this this is not going to fully explain the algorithm in particular if you do want that you can check the video description and i will have a link there for a separate video where we don't talk about the coding and point number two is that this is going to be a full tutorial on how to actually get good results with this not some two-second uh scikit-learn copy where you just fit a model we're going to do the whole thing over here and how you would need to do this in practice and what everything you need to understand okay so let's get started we will do import pandas as pd i think i can even do one more zoom for now import paint it says pd import matplotlib dot pi plot as plt we're actually going to be using plotly as well although for now matplotlib is easier import numpy as np and this is in google collab by the way if you guys didn't know but you can use whatever site or ide that you want as long as you get this google colab californiahousingtrain.csv file here i'm going to be using it so we'll actually load that right away with df is equal to pd.read csv and then that file is located at sample underscore data california housing train dot csv and then we'll just output the data frame df okay so we just loaded that csv as a panes data frame and basically what it is you've probably seen it before but as a quick overview these are many different rows in the data frame we have 17 000 rows and each of those rows has nine columns each of these rows represent a particular area in california which is marked geographically by its longitude and its latitude and we have different descriptions of those areas in california such as the median age of all the houses there the total number of rooms in those houses the total number of bedrooms the population and so on i'm not going to go over everything especially because in this video i'm not really going to use everything we'll use a couple different pieces starting with the latitude and longitude because a really nice way to graphically represent or show what clustering does is to use two-dimensional graphing data and this will be very clear in a moment we'll get lat long as a subset of the data frame with df sub we grab those columns latitude and longitude okay and we'll just for now separate that into two different variables latin long we need to do long with two g's is equal to df dot latitude and df dot longitude okay so now that we have that we'll just do a quick plot of plt.scatter with long and lat it has to be in that order so that we get what california actually looks like since each of these points are an area in california with its longitude on the x and its latitude on the y this is actually going to show california uh and so we don't have if we fully painted in california you would kind of fill in all the cracks and get the the whole area but these are just different areas in it okay so why is this important well we're going to do a db scan on these two on these two columns here on these two different variables and so we can get loaded up with that right away it's not overly difficult to fit a db scan model we do from sklearn.cluster import db scan okay so that's their class for building and fitting db scan algorithms and we'll do x because as always with machine learning you usually have some matrix or something that's kind of like a matrix maybe a tensor we have for now x is equal to lat long which is that subset of the data frame dot to numpy okay so that would mean that x dot shape better be sorry i'd miss must have mixed something there i just put two double equals or a double equals uh x dot shape here it's going to be x is going to be a numpy array with that same number of rows 17 000 and two columns because it's just the latitude and the longitude now to fit the model which i'm actually just going to put in here instead we do grab it in a variable we do db scan underscore cluster underscore model is equal to db scan and then we set two different variables we set epsilon and we set the minimum number of samples if you don't know what those are well you could kind of get along with this video if you don't really want to completely fully understand what they do but i will give a little bit of a hint if you haven't seen how the algorithm works in particular for now i don't actually want to set anything and so we'll just agree with their defaults whatever they are db scan and we dot fit just x and it's just x because this is an unsupervised learning algorithm we're not trying to do some function that maps an input to an output we are just doing learning something about the input space so we got fit x now if we output our dp scan cluster model we will get just db scan okay it just fit a model and it didn't really say anything interesting we fit a model uh what is interesting about this but it doesn't tell us is that we have db scan underscore cluster underscore model dot labels underscore that's probably the most important thing for any clustering algorithm the point is to label different things you know label all of the points as a given cluster and so if we do i'll just output that for now it's an array and if we convert this to a panda series we'll actually add it to our data frame if we do df-subcluster is equal to that labels and then if we output the data frame well that shows that we basically put in a new column on the end there that is each of our each of our labels and so i'm saying cluster because it's what cluster or what group it was assigned to what label it was given and it was assigned to each of the points we have 17 000 of these clusters we have one for every single point uh and that's great because what it allows us to do is uh well we've labeled every single point and now each of them belong to some particular group we can actually look at the different distribution with df subcluster dot value counts and what this does is this is saying that 16982 of the points were assigned a cluster label of one uh eight were assigned a cluster label of zero and ten were assigned a uh cluster value of negative one basically what the negative one means is noise or equivalently outliers it's basically saying each of these 10 points they didn't really fit into any particular cluster it's not like they're all part of the this this negative one cluster it's basically saying that they're all outliers or all noise and we'll we'll see that visually in a bit okay but anything that's not the negative one it's considered to have found a part of a particular cluster according to the parameters that we gave in the algorithm dbscan with whatever epsilon and pin samples we gave again if you don't know what those are it doesn't really matter for now so basically what we have is the values of each of the different clusters and these ones are noise and the other ones are part of some actual cluster well what we can do is well graph where we have the same plot as above in matplotlib actually we're going to convert to plotly because it's a lot easier for this type of thing but now we want to give each different cluster a color and plotly also does a really good job of you can hover over each point and see exactly what it is referring to so we can just grab wobbly with import plot flea dot express as px and we'll do fig is equal to a px dot scatter with x is equal to long as before y is equal to lat and most importantly here we set color equal to the df sub cluster and basically it's best explained by just showing you what this does uh plotly is really smart about this color thing and since we set it like that it's going to say that each of the each of the values that have the same color it is going to pick a color for them and so that'll be consistent so each of these ones you can hover over and see that they're the blue ones here our color of negative one we can actually see that in this uh scale over here as well i should really zoom out a little bit so you can see it better um but yeah all of this yellow stuff this is considered the one cluster and we had apparently we had some tiny little cluster over here now it turns out that this doesn't really make all that much sense and it's kind of up to you know either a visual discretion or what we'll see later as some metrics as to what makes a good clustering model but you know this doesn't really make that much sense like if i i could get that all of these are maybe a cluster because it's like inland stuff and then we have some of the outskirts which are maybe outliers but i i really don't like that it's just that there's this little kind of cluster over here supposedly and if this is your first clustering experience then basically all we did is according to some unsupervised learning algorithm we covered each of the points or we assigned each of them to a group depending on the db scan algorithms parameters in this case and it just it doesn't really make sense all these different the the three different clusters that it has technically only two we have these quest this cluster in yellow this cluster in pink and then these are outliers what i would have preferred to see is i don't know just something different um and there is uh if you go to scikit-learn's page they have a really nice page on the different clustering algorithms and their effects but i can tell you that dbscan can do a better job of what we see here and what i am going to show you in particular is that this is not going to get a good score on what we call the silhouette score in inside kit learn okay because we don't because in it because this is an unsupervised learning approach we don't have really any way of really easily verifying like how good this model is at least one that makes really good sense uh what i'm going to use without explanation is something called the silhouette score which attempts to do that um and it's a little bit complicated as to what exactly it's doing but it is a score from uh from negative one to one which is one is really really good and negative one is really bad and zero is also you know not not very good as well okay so if we do from sklearn dot metrics we import sil uh silhouette silhouette score and i'm just going to write it as ss because that's really irritating to write over and over again and if we do ss if we pass it i should probably zoom in a little bit more again and we pass it our input matrix x and the cluster labels that were outputted df sub cluster that's a different way of getting those labels that we got it's going to output a score for us and it actually takes a little bit of time to compute that it gets 0.26 which is okay but you could definitely get better for this this cluster model like this it makes a little bit of sense but it's not amazing so what do we do how do we try and uh how do we try and adjust this well db scan you can play with two different components you can play with epsilon which is the distance that marks a core point and you can you can play with min samples which says how many points how many points are required for something to be a core point epsilon also does slightly more than that but it's not really a big deal as i said i don't want to explain the full algorithm here you can check it out in my my other video and that will explain okay so most importantly we do need to play around with epsilon and min samples and we could do this randomly for now we could go back to where we changed that sorry it's a little too far we didn't change anything here at all and their defaults whatever they are we're going to change epsilon and min samples so if we do epsilon maybe we'll set it equal to a smaller point two and we could also set um maybe min samples to be i don't know 15. and when that's let's run that again okay it's not going to look all that different for now what we should see is a different number of clusters okay so here's all the different distributions we have there's a 84 get assigned to the zero label 15 goes sign to the one 906 5 to the 2 and lots of different ones what's important for db scan as you see here is that we don't choose beforehand the number of clusters that are going to be outputted um and so that is kind of figured out automatically by the algorithm depending on those parameters that you gave so it got a lot more clusters um and we can run this again and what we see now is something that oh makes a lot less sense than it did before i really don't like this at all um yeah this this doesn't make a lot of sense for california and so what do we get according to a silhouette score well marking our visual guess it is about the same i guess according to this metric it's still not very good and i guess it thinks that it's okay but i i i tell you i really don't like this result now although we may not get a perfect result for any data set and i'm not trying to pretend that this is something that you can really get an a super awesome result on for california what is important is that we can optimize this somehow and to do that well we usually use in machine learning some sort of a grid search and you may see some other techniques for finding epsilon and bin samples in db scan but what's easiest to code even though actually what's coming up coding it's going to be a little bit confusing i i apologize for that but it is really really useful so just stick with me on it um we're going to optimize uh via grid search on epsilon and min samples and if you don't know what a grid search is it's not very complicated it basically just says try all possible combinations of these two lists of things and so if we get say a bunch of epsilons to try epsilons is equal to np dot lin space with we'll say starting at 0.01 you don't want to start at 0 because you don't want zero distance that would not make the algorithm happy uh np.linspace 0.001 and we'll go to one and then we'll do num number of those is equal to 15 so that our various epsilons to try are these different values here okay so spaced evenly between 0.01 and 1 we have these different epsilons to try we could of course get more or less but i don't want this to take super duper long and i do want to get a decent search as well we can get different min samples is equal to np dot a range between 2 and 20 and a step equal to 3. this is just something i played around a little bit on my own and you can feel free to play with them by yourself there's really no limit to how many different values you need i'd pick more than one so at least two and generally between zero and one for epsilons we'll actually get to that because this isn't standardized although longitude and latitude are pretty much on the same scale so we don't have to worry that to that about that at the moment we will in a bit but still either way between zero and one is going to be probably proper and min samples so some values between say 2 and 20 are decent to try so and i don't need an equals min samples we're just going to do 2 5 8 11 14 and 17. and so a grid search is just going to try well 0.01 and two and zero point zero one and five and zero point zero one and eight and all of those combinations and then zero point zero eight with two zero point zero eight with five so it's just trying all these different combinations to try and get a good one an easy way to get those combinations is just import iter tools and then you can do combinations combinations is equal to the list of iter tools dot product with epsilons and min samples okay and then combinations combinations is probably what you'd expect it's actually with tuples and so it's a list of the tuples of all of those combinations as i said so in the first column you have all of your different epsilons and the second column you have all your different min samples and the combinations between those okay i'm gonna i'm gonna exit that because it is kind of a long list and just so that we have this we're going to do n is equal to the length of the combinations and we can see that n is equal to 90 in this case now this is by far the most complicated coding we're going to do so stick with me but most tutorials won't do this because they don't place an importance on finding an actually good model and showing you how to do this i will show you how to get an actually good model for db scan so we can do define get scores and labels if you if you get confused in this just wait it out and i'll explain everything i promise define get scores and labels with combinations and x okay so that combinations you you can probably guess what will be passing in this combinations over here and x that's kind of the same or possibly a different matrix as before it should be a numpy array which represents a matrix as an input so we'll start with these two empty lists list scores is equal to empty list and all labels list is equal to an empty list this is going to be a list of scores that is going to be for each db scan model that we try so we're going to have 90 of those because we'll be trying 90 combinations and this is going to store all the different lists of labels so the first component of this will be the labels associated with the first combination and the second the labels associated with the second combination would be the second element we're just going to try many different combinations of db scan or we're just going to try many different db scan models with each of those combinations and record its main results now what we're going to do is first a for loop where for getting i and epsilon and num samples in enumerate the combinations okay if you haven't seen enumerate before basically it's just the best of both worlds for iterating through something this is a list of tuples and so we get the index starting at zero and then going up until the length of it minus one as usual and this we get immediately access to epsilon and num samples which is that particular combination that we're going to be iterating through since we're enumerating through the combinations now we'll make a model db scan underscore cluster underscore model is equal to db scan and then epsilon is going to be equal to epsilon whatever epsilon we're iterating through or whichever one from the combination we have which is dependent on you know our combinations and we're doing a grid search so we're going to be each time we get a model we're going to be storing its results and we're going to be trying to find the best one so min samples it's going to be that min samples that we have that we're iterating through as well which will be num samples and then we'll dot fit with x just like it is above we'll get the labels which is equal to the db scan db scan cluster model dot labels underscore just like it was before we'll convert that to a set labels underscore set is equal to the set of the labels and then what we want to do to count the number of clusters well first it's going to be this number of clusters is equal to the the length of the label set except then to get the actual number of real clusters because if something's assigned to negative one then it's not really a cluster it's just noise the number of real clusters is going to be reduced by one if the if negative one is in that set so if negative one is in the labels set if we actually labeled anything as as noise then we're just going to decrease num clusters by one minus equals one okay so why do we need that well it's because we don't really want to make or even spend time on models that make really no sense and it's a little bit up to us as to what makes no sense but i don't want to do if num clusters is less than two so if we this is even disregarding the noise already so this basically means if we've only made actually one real cluster i don't like that model so i'm not even going to consider it if this is true or something that's a little bit up to our discretion technically there should really no be not be an upper bound on um on a number of clusters it's not really realistic to say hey if there's something bigger than say like 50 clusters or so this is getting a little bit complex now again many people would disagree about this practice it's not this is not some set in stone practice but if something if we're getting something massive for this particular problem i'm just going to ignore it now what we're going to do is if this is true then what we want to do is ignore it to ignore this properly we will still append some garbage into the scores because we're still trying to calculate this is this should be the score for this model but we're going to ignore it so we're going to add some noise here and we'll also match this up with all labels list dot append bad so we're going to append again just some garbage into this list which basically just says ignore this score ignore this set of labels because we're not using this model uh if we wanted to we could make this nice printout which i'm actually just going to copy in because it's not super relevant if we do c is equal to that tuple's there we can output this nice string which basically says hey i want to still just write some output to see what we're doing okay so now what we want to do is basically since in here we have a continue which basically just says if something's wrong here we're just going to go over the next iteration grab the next epsilon and number of samples well if we here down here we actually do care about the model and so that means we should have some score since we already made the model up here so will scores not append the actual score that we got which is computing that ss with x and the labels that's how we did it before and we'll also do all labels list all labels list dot append with the labels okay so now here we added the score for that set of parameters and we added the labels now we're going to add a nice print here as well i am actually going to copy this in here as well because it's not super important but again i'm just gonna really skip over that okay so now we wanna do is return things that we actually care about what do we care about well we probably care about the parameters that were best the labels that were best and the score that was best okay so what parameters got the best score and um and what labels were associated with that model the reason we need to actually get the labels right now instead of running the model again we could technically just extract the same parameter the best parameters and then run that model once more to get the best one well the reason we want to actually extract the labels is because db scan is not deterministic due to the way it sequentially makes something kind of at random a cluster well that's going to make it not necessarily get the same output every time and so we want to get the labels that were associated with it the first time that it ran so what do we need to make sure it's best what we'll do best index is equal to the numpy.arg max of the scores because that says which index maximizes the scores that's the one we care about now we can basically just index everything with this we'll do best parameters or our epsilon best combination basically best parameters is equal to combinations sub the best index because that's the combination that has the best score now we can do best labels is equal to all all labels list and then give that the best index again so those are the best labels that are associated and finally we'll also get the best score which is just going to be scores sub best index okay very long function i know but it is done after we simply just return this dictionary which i'm going to do as return we'll get the best epsilon which is best parameters sub zero it is going to be the best min samples with best parameters sub 1. since this thing is a tuple we need to get sub 0 and sub 1. best labels which is that labels there and best score which is those scores there so we're just returning a dictionary our results basically now what we can do with this absolutely massive function that probably no one else is really using is best [ __ ] is equal to get scores and labels that's what we call this function up here get scores and labels technically it gets a little bit more than that but get scores and labels on combinations and x now if we run this you will see that it takes a pretty long time and i probably will skip to the end but it shouldn't take too long and i made sure that this doesn't take a massive amount of time on google collab it takes a little bit to do a grid search and that's understandable with machine learning sometimes you do have to wait a little bit of time but with our output you can see some stuff being shown when i write moving on here that's our output or that's uh that's our our print out here that says hey it has this many clusters this is why we're moving on and we're not really we're just adding trash and we're not actually uh thinking about that one i will just resume the video after that's over so as you can probably see if you followed along even if you didn't here you can see after it hits a point basically after the epsilon and pretty much after the epsilon gets too high past 0.64 so it's just getting way too many or way too few clusters it's getting one pretty much every time and so it pretty much gives up but there is a lot of different a lot of different options in here especially in around this range where we have the epsilon uh where is it around around this combination here around epsilon is 0.15 or so with min samples different values we're getting some pretty decent scores in here we have a 0.32 and a bunch of negatives as well it definitely varies but you can see we are beating our previous score of uh i think it was 0.25 when we tried it on our own we're getting many different scores that are higher than that which is a good sign that's uh that's the best we can really hope for okay so let's extract our best dictionary because we got that stored in a variable best dick is the best epsilon is 0.29 and that is not true sorry that is true best i was thinking that was the score uh best score we found a 0.4066 which is much better uh these are our labels associated we'll grab those and these are our parameters epsilon is 0.29 and best min samples was 14 associated now let's just go ahead and override what we had before so we'll do df sub cluster is equal to now the best dict sub the best labels really all that we actually need is the labels although we did store this other stuff because it's kind of interesting for you and i to see what values were actually making this work maybe if you wanted to further kind of do a closer grid search around these areas you could do that but really we're just going to take these labels and we will again look at the df subcluster dot value counts looking at the distribution that it was outputted and it outputted this that's a nice amount of clusters usually something like in the five or six range is quite often a nice number uh and minimal noise a little bit of noise 202 but if you do have outliers like that's they should be clarified classified as outliers and a nice different distribution so what's good about this is there's not some where there's like so few like we're not getting like three or four points and it's not like all dominated by just one cluster we are getting uh mostly dominated by two clusters and then some various other things so just like before we are going to make our graph and it's the exact same so i'll just copy it in fig is equal to px.scatter x with actually that should be long and flat i think because i believe my code is actually wrong over there we should be outputting it as yeah long and then what not what and then wrong sorry about that so long should be over here and flat over here and then we output this again and we see something that maybe makes a little bit more sense you know i think this color it's definitely better than um one of the earlier ones we've seen and that first one you know i think the first one didn't really make too much sense here you could kind of see some some uh distribution over here another cluster sort of makes sense in the middle it seems to have i'm kind of surprised found one over here and that wasn't classified as just pink uh yellow i can definitely see why there's kind of its own cluster in the middle over there and everything over here is classified as an outlier which is really what we wanted to see because the stuff on the outskirts that kind of did strike me as either maybe it could be marked as its own cluster or if it's not then at least it should all around the edge be kind of classified as noise instead of some of the other stuff in the middle here definitely just seems like that one you wouldn't really associated that with anything other than an outlier okay so this is just kind of showing you that you can actually optimize to make a a visual cluster model that actually makes sense now a big question here is well what if you have more than two variables because you can't really you can't really color this and you can't really visually look at it well if it's more than two variables well you would actually just do the exact same thing that we had before you wouldn't really need to change anything at all because we optimized this for the silhouette score however one thing that you should change even in our example it wouldn't probably wouldn't do anything although it doesn't hurt because it's latitude of longitude i said it earlier you should scale your variables and so the important one or at least it's really obvious to show that this is a problem if we look at the full data frame so i should really be i don't know why i'm not fully zoomed in here if you look at df and you did try to do a clustering on this well housing median age age is generally very small you're not going to get a housing median age bigger than like a thousand or probably not even close to that range but if you have total rooms and total bedrooms you'll definitely be in the thousands and other variables here especially if there's money involved money is very usually a very large number and so median house value if you have that in there you have some variables that are on a very small range like 0 to 100 and you have other variables that are like zero to like a hundred thousand and so basically basically what would happen if you just uh put that raw into the algorithm well it's just gonna really ignore those smaller uh those smaller pieces because the distance between say this point and this point is like 15 000 but the distance between like this variable this very this variable is only four and so it's going to say like hey i really don't care about this four distance but i have to maximize this when i have to worry about this 15 000 distance over here okay and just to show this graphically i will pick a couple of variables that explains it very well uh you'll do a plt dot scatter on df dot total rooms that's one column and df.housing median actually yeah i'll just stick with that for now it's good enough okay so here you can see it's kind of all slanted over here but most importantly you look at the range of the axis as we just explained 0 to 60 or so and 0 to 35 000 or so the db scan algorithm would be completely dominated and generally just confused uh by these different columns here and i can show that if we were to do x2 so just getting another matrix here x2 is df with those two columns total rooms and the housing median age and we put that converted that to numpy and we'll just show x2 not that we really need to show that that's x2 a different matrix and we will since we have that awesome function from before that's why we put in so much work to get this great function we can just do best dict do is equal to get scores and labels calling that function if i can type it get scores and labels with combinations and x2 then we should be able to actually i will let that run and then while it's going in a new okay it's not even letting me type so basically the point here it's moving on every time it's uh it just finished because it didn't really do much of anything it kept making these models but it kept having zero here's zero clusters we're getting or this is 863 clusters um and so it's very very confused here you see most of the time we're not getting any clusters and that's because our scaling is really messed up here and so if you look at which i tried to do best sticked two uh we're we're stuck with bad okay we have in best epsilon and at best mint samples which is the first one because that's associated with garbage and that's because literally all of it was garbage there was not a single cluster model that really made any sense and so if we do go ahead and scale our variables instead from sklearn dot pre-processing import standard scalar x2 scaled is equal to standard scalar dot fit transform we don't really need to keep the transform so i'm just going to do a fit transform like that and not worry about it after plt.scatter x2 scaled will show all of the first column there and then off the second column just a different way of showing and we're just scaling the variables and then we're using the standard scalar in particular and then we are going to graph those so this is the first column second column all the rows and the second column plt dot scatter that it looks the same here it's still going to have that shape but if you look at the range here it's much much closer still not perfect but uh much closer and so this is 0 to negative 2 between 2 and negative 2 or maybe negative 3 and 3 and that's this is about negative 1 to about 16. so again not perfect but they are much much much closer to each other and that basically says to the algorithm hey we're going to treat these uh relatively the same as the same as we can we can get uh and now what we can do is just that same thing as above so i'm going to copy this exactly i will remove that that's way too much writing best stick two i i could make it we'll just make it best dick three that's it that's a three uh and then this should be the only change that we have to do is x2 scaled and let's let that run now what we're going to see is sometimes it still runs into trouble we don't expect these clustering algorithms to be magical but hey uh index 5 here we did get num clusters with 32 uh 54 and or sorry 34. and so these are looking not so bad and i since it's actually working again i will jump to the end of end of it since this is going to take a little bit okay so we're all done over there and what's nice is there's many different uh scores that actually were not too bad uh and i'm not really i'm not really concerned about the actual score although we will take a look at it uh we can take a look at best [ __ ] what is now best x3 and we see these different options and what we can go ahead and do is i'm not going to overwrite any of the columns because we're just about done here but we can take a look at the the new coloring and so we will just make a new fig equal to the px dot scatter with x is equal to x2 underscore scaled from zero sorry all the rows and then the first column y is equal to x2 scaled what sorry colon first column and color is equal to best dict 3 sub the best labels okay we have those labels scored stored in there that we got from our optimization and we should just be able to do a fig.show which might not make perfect results but it should make something okay so this actually kind of makes sense we have all of this stuff over here got assigned to the zero cluster all this stuff over here just these four points got assigned to that one cluster and then we have some outlier over there which is really nothing so it not to say that you can make some clustering that really really makes sense for anything and you can judge that by the fact that yes this best score well actually it thinks that the score is very good because you know you can see it does kind of picture good whether that means anything it's kind of hard to tell and that's up to you um that is a that is a good silhouette score since we did scale the variables we at least got something and so the lesson is that you should always scale your variables um unless you you're purposely trying to make a mismatch and to try and treat one variable more than another most the time we're not trying to do that and you want to treat them as well letting the algorithm figure out what's what's most important to really think about okay so that's the that's the idea um i think it's pretty cool db scan is uh much different than k-means because it's it is able to say all of these ones actually it's mostly in uh in gonna be shown in this california piece here k means uh you would have specified the number of clusters or maybe done a an elbow method to figure out um what's the best number of clusters but either way i would have kind of done hey this is an area this is an area and this is an area whereas db scan allows you to have these little pieces and flag noise as some of the outside stuff it's a very nice algorithm and i hope you learned a lot about how to use it you can uh feel free to copy that function that i that i made and scale your variables and use it because it is it will work very well and you can compare them via the silhouette square method on what is best most people genuinely will not show a tutorial that actually shows you how to make a good model in db scan you could also i believe you should be able to use any clustering method you just would change the clustering method in that and if you wanted you could build your own sort of class class where you had you could specify the clustering method that you wanted maybe the different distance metric since we did i did use the euclidean distance by default most people would use that and you could do something cool with that i hope this video was helpful uh if you are listening to this right now please drop a like because it took a lot of effort to put together and yeah have a great day guys i hope this was helpful see you later"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing DBSCAN with Scikit-Learn",
        "Video Title":"DBSCAN Algorithm | Machine Learning with Scikit-Learn Python",
        "URL":"https:\/\/www.youtube.com\/watch?v=Q7iWANbkFxk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Q7iWANbkFxk\/hqdefault.jpg",
        "ID":"Q7iWANbkFxk",
        "Publish Time":"2020-05-25T05:30:00Z",
        "Channel":"Normalized Nerd",
        "Channel ID":"UC7Fs-Fdpe0I8GYg3lboEuXw",
        "Transcript":"hello people from the future welcome to normalise nerd in this video I'm gonna be talking about DB scan algorithm it's an unsupervised learning algorithm for clustering first of all I'm gonna explain every conceptual detail of this algorithm and then I'm gonna show you how you can code DB scan algorithm using scikit-learn if you are new to my channel then please subscribe to my channel and hit the bell icon I make videos about machine learning and data science regularly so let's get started [Music] first of all let me tell you the full name of this algorithm well it's a big one it is density based spatial clustering of applications with noise well there are three particular words that we need to focus on from the name there are density clustering and noise from the name it's clear that the algorithm uses density to cluster the data points and it has something to do with the noise maybe it can identify the noise very well we'll see later first of all let us understand what is density well from the physics we know that density is just the amount of matter present in a unit volume well we can easily extend this idea of volume into higher dimensions or even in lower dimension okay so for example we have this region here okay and we have some data points in this region and we have another region of same area and we have got these many data points here so from the idea of density obviously the density of first region is greater than the second region because there are more data points more matter in the first region okay and DB scan actually uses this concept of density to cluster the dataset now to understand the disk got them clearly we first need to know two very important parameters the first one is epsilon and it is actually a measure of neighborhood well what is the neighborhood well suppose this is our point that we are considering right now and let me draw a circle around this point making this as Center and add a distance Epsilon so we are gonna say this circle as this points neighborhood okay so epsilon is just a number which represents the radius of the circle around a particular point that we are going to consider the neighborhood of that point okay and the second parameter is mean sample and by the way I'm gonna represent min samples as Z throughout my video because it is convenient for me but if you're gonna study this DB scan from the internet then probably you're gonna encounter this term mean samples so just keep that in mind okay and this is actually a threshold on the least number of points that we want to see in a points neighborhood suppose we are taking Z is equal to two and in this points neighborhood we have actually two points this point itself and this point so this point actually passes this vessel Z equal to two and it is focus on this point and suppose this is our neighborhood of this circle I know it's not a perfect circle but please bear with me and here you can see that we have got four points in this neighborhood 1 2 3 4 so this point also satisfies this threshold is equal to 2 now if I said Z equal to 3 then this point won't satisfy this because we have got only 2 points in its neighborhood but this point will satisfy it because we have got more than three points ok now based on these two parameters we are first going to classify every point in our data set in two three categories the first one is cool points second one is boundary points and the third one is noise let us see the cool points first okay so this is a cool point why well if I say this point is cool point then it must satisfy one condition the condition is number of neighbors must be greater than or equal to our threshold min samples are Z okay so if I set Z equal to 3 then this point actually satisfies this condition so we say this is one core point now let us see other core points in this dataset okay so we have got this core point this core point this core point and these three points are also core point but remember this point this point and this point are not core points okay now let us see the second type of points that is boundary points okay so if I say one point is boundary point then it has to satisfy two conditions the first condition is number of neighbors must be less than Z and the second condition is the point should be in the neighborhood of a cool point okay for example this point has actually less than three neighbors in his neighborhood but it is in the neighborhood of another cool point so this point is actually a boundary point and same thing goes for this point also so we have got two boundary points in this data set okay now let's see the last kind of point that is the noise point well this definition is very simple neither core point nor boundary point so this point is neither a cool point nor a boundary point so we say this point is a nice point okay so we have classified every single data point into three categories so the first step of our DB scan algorithm is actually done congratulations now you need to understand another concept suppose we have got two cool points here well I'm not drawing the neighborhood points but remember that this is X and this is y and they both are cool points okay so if x and y are neighbors then we join them by an edge that we call as density edge okay now let's see four points here here here and here and assume that every point is a core point and say this is point a and this is point B assume they are connected via density edges let me name this also so you can see C is in the neighborhood of a D is in the neighborhood of C B is in the neighborhood of B but a and B are not neighbors so if this kind of situation arises when two core points are connected via density edges then we say a and B are density connected points okay a and B are density connected points now let me write the rest of the steps of this algorithm so our first step is actually done which was to classify the points second step is to discard noise so we are going to discard this point okay and the third step is assign cluster to a cool point for example I am taking this core point and assigning it a cluster red okay and the fourth step says color all the density connected points of a core point okay so we have to color all the density connected points to this point the same color red okay so by following this we have to color this and this remember we are not going to color this and this just now because boundary points can never be a part of density connected point only core points can be density connected okay now our algorithm says repeat this two steps for every uncolored cool points so here I'm gonna take this core point first and I am coloring this a new color that is I am assigning a new cluster to it plus tableau and we have to assign all the density connected points the same cluster so these three points will belong to the blue cluster now comes the fifth step which is by the way the last step it says that color boundary points according to nearest whole point so if you just look at our example then you will see that we have two boundary points here and for both of the boundary points the nearest core point belongs to the red cluster so we have to assign the boundary points also red and voila the DB scan algorithm is done let me explain a couple of very important points about this algorithm well the first point that I want to say is how to determine epsilon and Z well to be honest this is a difficult question because the DB scan algorithm is very sensitive to its initial parameters so if you change this epsilon and Zed even slightly then your algorithm can actually produce very different results so that is one downside of this algorithm but you can choose them wisely if you have the proper domain knowledge as a rule of thumb if you have a large number of examples then you can choose Z in the order of your dimensionality so if you are working with a 10 dimensional data then it is preferable to choose a value of Z close to 10 like 12 15 like that okay now to know the value of Epsilon here is a thing you can try suppose you have chosen Z equal to 5 so after that what will you do you will just find the distance of 5th neighbor from every data point so you will have a distance array and the eyath entry in that array will represent the distance of 5th neighbor of ayat data point and then you are gonna sort this distance array and you are gonna plot it like this in the y-axis you will just have the distance and in the x-axis you will have the index that is simply the eye okay so ideally you should get a graph like this as we have sorted this so as the index will increase the distance of the fifth data point from that point will also increase so if your luck favors then you can find this elbow type of thing and you can just cut it by a horizontal line and the horizontal line will cut the y-axis at some point and you can just take this value as your Epsilon but obviously in real world you might not get this smooth elbow but this is a trick that can be applied in some cases okay now let me come to the second important point very effective in noise elimination as you saw in my previous example we were classifying the points into three categories and there was a category of noise points so this algorithm can be applied in noisy datasets very well and the last point is DB scan can't handle higher dimensional data of very well well this is actually a fault of many clustering algorithms as I have talked in my demeans video also as the dimensionality increases we have to look into a larger volume to find the same number of neighbors so the similarity between the decrease and that will result in clustering errors so I guess that was a pretty decent explanation of DB scan algorithm and now let's jump into the code section first of all I'm gonna make a fake dataset and if you have watched my previous video on k-means clustering then you will find that I am using the same method of producing fake data points so it will be really helpful if you just go and watch my k-means video too after this video in this way you will actually learn to clustering algorithms at once isn't it awesome so please go and check that out too okay so to make the fake dataset we are using our favorite library scikit-learn and we need to import the function called make blobs from SK learn dot datasets okay and this function takes these arguments the first one is n samples well this just represents how many data points we need to produce and the second argument is Centers this tells us that how many clusters will be there and the third argument is n features well this is just the dimensionality of our data set here I'm choosing two because two-dimensional data sets are really easy to represent visually okay and if you want to produce the exact same results that of mine then you can actually use random State argument and set it to 20 okay and this function will actually return two things the first thing is the data points that I am assigning to the variable X and the second thing it's gonna return is the array of labels now remember that DB scan is an unsupervised learning so we don't provide the labels that is the ground truths at the beginning we let our algorithm find those labels on its own so I have left it blank with an underscore okay and we need some other libraries like pandas matplotlib and numpy okay so moving on with the visualization so you can clearly see that we have got three clusters here one two and three and now our DS can we'll try to find the labels on its own okay and the DB scan algorithm is really simple to implement in Python using scikit-learn because the scikit-learn library has already implemented this for us we just need to import this class from SK learn dot cluster module okay so the class name is DB scan and we need to create an object out of it the object that I have created here is named as clustering and we need to input the two most important parameters that I have discussed in the conceptual portion and the first one is epsilon eps and the second one is Z or min samples now look here that I have given epsilon as one and min samples as five as I have told earlier that this totally depends on the domain knowledge so please choose them wisely and like every other algorithm in scikit-learn we need to use the dot fit method on our training set X and then we are just getting the labels okay labels of the clusters so it will give us an array and to see how many clusters has it found on the data set we can just convert this array into a set and we just print the length of the set now you can see that here is four but what happened we saw that our data set contains three clusters wait the first cluster that is the minus one level cluster is actually represents than noise in this DB scan so you can see if we just visualize our clusters by this simple piece of code then we will find something like this and you can see that these two points first point and second point I don't know if you can see this clearly but you can just see the legends here so minus one label corresponds to the noise so even though we have got four clusters here the first cluster actually represents the noises so we discard them and if we just look at the other points you can clearly see that our algorithm has found three clusters blue cluster yellow cluster and green cluster so that's how easy it is to implement DB scan using scikit-learn I hope that you found this video helpful and please share this video and subscribe to my channel and if you want to suggest a new topic that I should make a video on then please comment in the comment section stay safe thanks for watching [Music]"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing DBSCAN with Scikit-Learn",
        "Video Title":"Clustering with DBSCAN, Clearly Explained!!!",
        "URL":"https:\/\/www.youtube.com\/watch?v=RDZUdRSDOok",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/RDZUdRSDOok\/hqdefault.jpg",
        "ID":"RDZUdRSDOok",
        "Publish Time":"2022-01-10T12:51:02Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"dp scan clusters just like a person can statquest [Music] hello i'm josh starmer and welcome to statquest today we're going to talk about clustering with db scan and it's going to be clearly explained now imagine we collected weight and height measurements from a bunch of people and we plotted the people on a two-dimensional graph like this where we have weight on the x-axis the first dimension and height on the y-axis the second dimension by eye we can see two different clusters by identifying two different but relatively dense clumps of people in contrast these people that are far from everyone else look a little bit like outliers so by eye clustering this data is pretty easy however because these clusters are nested meaning the green cluster wraps around the blue cluster a relatively standard clustering method like k-means clustering might have difficulty identifying these two clusters instead because of the nesting a simple clustering method might get something weird like this where these points are assigned to the blue cluster even though they look like they belong to the green cluster so we need a clustering algorithm that can handle nested clusters also remember this two-dimensional graph only uses weight and height data but if we wanted to include each person's age we would have to add a third axis and now our graph is three-dimensional drawing a three-dimensional graph on a two-dimensional computer screen is awkward but possible however if we wanted to include four or more features we'd need to draw a four or more dimensional graph and that's not possible and if we can't draw and look at a four or more dimensional graph then we need a way to identify nested clusters that we cannot see by eye the good news is that there are clustering algorithms that can identify nested clusters in high dimensions one of these algorithms is called db scan and that's what we'll talk about today so let's go back to the original two-dimensional graph and see how db scan tries to mimic what we can easily do by eye now remember by eye we identify clusters by the densities of the points clusters are in high density regions and outliers tend to be in low density regions so let's see how dbscan uses the densities of the points to identify these two clusters bam now starting with the raw unclustered data the first thing we can do is count the number of points close to each point for example if we start with this red point and we draw an orange circle around it then we can see that the orange circle overlaps at least partially eight other points so the red point is close to eight other points note the radius of the orange circle is user defined so when using dbscan you may need to fiddle around with this parameter now this red point is close to five other points because the orange circle overlaps at least partially five other points this red point is close to six other points and this red point is close to seven other points this red point is only close to two other points and this red point is not close to any other point because the orange circle does not overlap anything else likewise for all the remaining points we count the number of close points now in this example we will define a core point to be one that is close to at least four other points note the number of close points for a core point is user defined so when using db scan you might need to fiddle with this parameter as well anyway these four points are some of the core points because their orange circles overlap at least four other points hooray but neither of these points are core points because their orange circles do not overlap four or more other points ultimately we can call all of these red points core points because they are all close to four or more other points and the remaining points are non-core now we randomly pick a core point and assign it to the first cluster next the core points that are close to the first cluster meaning they overlap the orange circle are all added to the first cluster then the core points that are close to the growing first cluster join it and extend it to other core points that are close by here we see two core points and one non-core point that are all close to the growing first cluster and at this point we only add the core points to the first cluster that said eventually we will add this non-core point but right now we are only adding core points ultimately all of the core points that are close to the growing first cluster are added to it and then used to extend it further bam note at this point every single point in the first cluster is a core point and because we can no longer add any more core points to the first cluster we add all of the non-core points that are close to the core points to the first cluster for example this point which is a non-core point is close to a core point in the first cluster so we add it to the first cluster however because this is not a core point we do not use it to extend the first cluster any further that means that this other non-core point which is close to the non-core point that was just made part of the first cluster will not be added to the first cluster because it is not close to a core point so unlike core points non-core points can only join a cluster they cannot extend it further now we add all of the non-core points that are close to core points in the first cluster to the first cluster and now we are done creating the first cluster double bam to summarize how the first cluster was formed we picked a random core point and it started the first cluster then neighboring core points joined and extended the first cluster [Laughter] and non-core points only joined the first cluster bam now because none of these core points are close to the first cluster they form a new second cluster because they are close to each other and the non-core points that are close to the second cluster are added to it lastly because all of the core points have been assigned to a cluster we're done making new clusters and any remaining non-core points that are not close to core points in either cluster are not added to clusters and called outliers and that is how the db scan algorithm works triple bam note as we just saw clusters are created sequentially that means if we had a non-core point close to both clusters then when we built the first cluster beep we would add this non-core point to the first cluster because it is close to a core point along with all of the other non-core points that were close and now that this point is part of the first cluster it is no longer eligible to be in any other cluster small bam now it's time for some shameless self-promotion if you want to review statistics and machine learning offline check out the statquest study guides at statquest.org there's something for everyone hooray we've made it to the end of another exciting stat quest if you like this stat quest and want to see more please subscribe and if you want to support statquest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time quest on"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing DBSCAN with Scikit-Learn",
        "Video Title":"DBSCAN implementation using Python, Sklearn",
        "URL":"https:\/\/www.youtube.com\/watch?v=FpTHdx9oUxs",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/FpTHdx9oUxs\/hqdefault.jpg",
        "ID":"FpTHdx9oUxs",
        "Publish Time":"2020-09-01T15:26:21Z",
        "Channel":"Machine Learning - CTW",
        "Channel ID":"UCh7zZ9ut9ytyVoC5DKs2P6w",
        "Transcript":""
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Principal Component Analysis (PCA)",
        "Video Title":"StatQuest: Principal Component Analysis (PCA), Step-by-Step",
        "URL":"https:\/\/www.youtube.com\/watch?v=FgakZw6K1QQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/FgakZw6K1QQ\/hqdefault.jpg",
        "ID":"FgakZw6K1QQ",
        "Publish Time":"2018-04-02T20:06:10Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"StatQuest breaks it down into bite-sized pieces, hooray! Hello, I'm Josh Starmer and welcome to StatQuest. In this StatQuest we're going to go through Principal Component Analysis (PCA) one step at a time using Singular Value Decomposition (SVD). You'll learn about what PCA does, how it does it, and how to use it to get deeper insight into your data. Let's start with a simple data set. We've measured the transcription of two genes, Gene 1 and Gene 2, in 6 different mice. Note: If you're not into mice and genes, think of the mice as individual samples and the genes as variables that we measure for each sample. For example, the samples could be students in high school and the variables could be test scores in math and reading, or the samples could be businesses and the variables could be market capitalization and the number of employees. Okay, now we're back to mice and genes, because I'm a geneticist and I work in a genetics department. If we only measure one gene we can plot the data on a number line. Mice 1, 2, and 3 have relatively high values and mice 4, 5, and 6 have relatively low values. Even though it's a simple graph, it shows us that mice 1, 2, and 3 are more similar to each other than they are to mice 4, 5, and 6. If we measured 2 genes, then we can plot the data on a two-dimensional X-Y graph. Gene 1 is the x-axis and spans one of the two dimensions in this graph. Gene 2 is the y-axis and spans the other dimension. We can see that mice 1, 2, and 3 cluster on the right side and mice 4, 5, and 6 cluster on the lower left hand side. If we measured three genes, we would add another axis to the graph and make it look 3D, i.e. 3-dimensional. The smaller dots have larger values for Gene 3 and are further away. The larger dots have smaller values for Gene 3 and are closer. If we measured 4 genes, however, we can no longer plot the data. 4 genes require 4 dimensions. So we're going to talk about how PCA can take 4 or more gene measurements, and thus 4 or more dimensions of data, and make a 2-dimensional PCA plot. This plot will show us that similar mice cluster together. We'll also talk about how PCA can tell us which gene, or variable, is the most valuable for clustering the data. For example PCA might tell us that Gene 3 is responsible for separating samples along the x-axis. Lastly, we'll talk about how PCA can tell us how accurate the 2D graph is. To understand what PCA does and how it works, let's go back to the data set that only had 2 genes. We'll start by plotting the data. Then we'll calculate the average measurement for Gene 1, and the average measurement for Gene 2. With the average values, we can calculate the center of the data. From this point on, we'll focus on what happens in the graph - we no longer need the original data. Now, we'll shift the data so that the center is on top of the origin in the graph. Note: Shifting the data did not change how the data points are positioned relative to each other. This point is still the highest one, and this is still the rightmost point, etc. Now that the data are centered on the origin, we can try to fit a line to it. To do this, we start by drawing a random line that goes through the origin. Then we rotate the line until it fits the data as well as it can, given that it has to go through the origin. Ultimately, this line fits best. But I'm getting ahead of myself, first we need to talk about how PCA decides if a fit is good or not. So, let's go back to the original random line that goes through the origin. To quantify how good this line fits the data, PCA projects the data onto it and then it can either measure the distances from the data to the line and try to find the line that minimizes those distances, or it can try to find the line that maximizes the distances from the projected points to the origin. If those options don't seem equivalent to you, we can build intuition by looking at how these distances shrink when the line fits better, while these distances get larger when the line fits better. Now, to understand what is going on in a mathematical way, let's just consider one data point. This point is fixed and so is its distance from the origin. In other words, the distance from the point to the origin doesn't change when the red dotted line rotates. When we project the point onto the line, we get a right angle between the black dotted line and the red dotted line. That means that if we label the sides like this: A, B, and C, then we can use the Pythagorean theorem to show how B and C are inversely related. Since A, and thus A squared, doesn't change if B gets bigger then C must get smaller. Likewise, if C gets bigger then B must get smaller. Thus, PCA can either minimize the distance to the line, or maximize the distance from the projected point to the origin. The reason I'm making such a fuss about this is that, intuitively, it makes sense to minimize B and the distance from the point to the line, but it's actually easier to calculate C, the distance from the projected point to the origin, so PCA finds the best fitting line by maximizing the sum of the squared distances from the projected points to the origin. So, for this line, PCA projects the data onto it and then measures the distance from this point to the origin, let's call it D1. Note: I'm going to keep track of the distances we measure up here. And then PCA measures the distance from this point to the origin, we'll call that D2. Then it measures D3, D4, D5, and D6. Here are all six distances that we measured. The next thing we do is square all of them. The distances are squared so that negative values don't cancel out positive values. Then we sum up all these squared distances, and that equals the sum of the squared distances. For short we'll call this SS distances, for sum of squared distances. Now we rotate the line, project the data onto the line, and then sum up the squared distances from the projected points to the origin. And we repeat until we end up with the line with the largest sum of squared distances between the projected points and the origin. Ultimately, we end up with this line. It has the largest sum of squared distances. This line is called Principal Component 1, or PC1 for short. PC1 has a slope of 0.25. In other words, for every 4 units that we go out along the Gene 1 axis, we go up 1 unit along the Gene 2 axis. That means that the data are mostly spread out along the Gene 1 axis, and only a little bit spread out along the Gene 2 axis. One way to think about PC1 is in terms of a cocktail recipe. To make PC1 mix four parts Gene 1 with one part Gene 2. Pour over ice and serve! The ratio of Gene 1 to Gene 2 tells you that Gene 1 is more important when it comes to describing how the data are spread out. Oh no, terminology alert! Mathematicians call this cocktail recipe a linear combination of Genes 1 and 2. I mention this because when someone says PC1 is a linear combination of variables, this is what they're talking about. It's no big deal. The recipe for PC1, going over 4 and up 1 gets us to this point. We can solve for the length of the red line using the Pythagorean theorem, the old A squared equals B squared plus C squared. Plugging in the numbers gives us A equals 4.12. So the length of the red line is 4.12. When you do PCA with SVD, the recipe for PC1 is scaled so that this length equals 1. All we have to do to scale the triangle so that the red line is 1 unit long is to divide each side by 4.12. For those of you keeping score, here's the math worked out that shows that all we need to do is divide all 3 sides by 4.12. Here are the scaled values. The new values change our recipe, but the ratio is the same. We still use four times as much Gene 1 as Gene 2. So now we are back to looking at the data, the best fitting line, and the unit vector that we just calculated. Oh no, another terminology alert! This 1 unit long vector, consisting of 0.97 parts Gene 1 and 0.242 parts Gene 2, is called the Singular Vector, or the Eigenvector for PC1, and the proportions of each gene are called loading scores. Also while I'm at it, PCA calls the average of the sums of the squared distances for the best fit line the Eigenvalue for PC1. And the square root of the sums of the squared distances is called the Singular Value for PC1. Bam! That's a lot of terminology. Now that we've got PC1 all figured out let's work on PC2. Because this is only a two-dimensional graph, PC2 is simply the line through the origin that is perpendicular to PC1 without any further optimization that has to be done. And this means that the recipe for PC2 is -1 parts Gene 1 to 4 parts Gene 2. If we scale everything so that we get a unit vector, the recipe is -0.242 parts Gene 1 and 0.97 parts Gene 2. This is the singular vector for PC2 or the eigenvector for PC2. These are the loading scores for PC2, they tell us that, in terms of how the values are projected onto PC2, Gene 2 is 4 times as important as Gene 1. Lastly the eigenvalue for PC2 is the average of the sum of the squares of the distances between the projected points and the origin. Hooray! We've worked out PC1 and PC2! To draw the final PCA plot, we simply rotate everything so that PC1 is horizontal. Then we use the projected points to find where the samples go in the PCA plot. For example, these projected points correspond to sample 6, so sample 6 goes here. Sample 2 goes here. And Sample 1 goes here. Etc. Double bam! That's how PCA is done using singular value decomposition. Okay, one last thing before we dive into a slightly more complicated example. Remember the eigenvalues? We got those by projecting the data onto the principal components, measuring the distances to the origin, then squaring and adding them together. Well, if you're familiar with the equation for variation, you will notice that eigenvalues are just measures of variation. For the sake of this example imagine that the variation for PC1 equals 15 and the variation for PC2 equals 3. That means that the total variation around both PCS is 15 plus 3 equals 18. And that means PC1 accounts for 15 divided by 18 equals 0.83 or 83% of the total variation around the PCs. PC2 accounts for 3 divided by 18 equals 17% of the total variation around the PCs. Oh no another terminology alert! A scree plot is a graphical representation of the percentages of variation that each PC accounts for. We'll talk more about scree plots later. Bam. Okay, now let's quickly go through a slightly more complicated example. PCA with 3 variables, in this case that means 3 genes, is pretty much the same as 2 variables. You center the data. You then find the best fitting line that goes through the origin. Just like before, the best fitting line is PC1. But the recipe for PC1 now has 3 ingredients. In this case Gene 3 is the most important ingredient for PC1. You then find PC2, the next best fitting line given that it goes through the origin and is perpendicular to PC1. Here's the recipe for PC2. In this case Gene 1 is the most important ingredient for PC2. Lastly, we find PC3, the best fitting line that goes through the origin and is perpendicular PC1 and PC2. If we had more genes, we just keep on finding more and more principal components by adding perpendicular lines and rotating them. In theory, there is 1 per gene or variable, but in practice the number of PCs is either the number of variables or the number of samples, whichever is smaller. If this is confusing, don't sweat it. It's not super important and I'm going to make a separate video on this topic in the next week. Once you have all the principal components figured out you can use the eigenvalues, i.e. the sums of squares of the distances, to determine the proportion of variation that each PC accounts for. In this case, PC1 accounts for 79% of the variation, PC2 accounts for 15% of the variation and PC3 accounts for 6% of the variation. Here's the scree plot. PC1 and PC2 account for the vast majority of the variation. That means that a 2D graph, using just PC1 and PC2, would be a good approximation of this 3D graph, since it would account for 94% of the variation in the data. To convert the 3D graph into a two-dimensional PCA graph, we just strip away everything but the data and PC1 and PC2, then project the samples onto PC1 and PC2. Then we rotate so that PC1 is horizontal and PC2 is vertical. This just makes it easier to look at. Since these projected points correspond to sample 4, this is where sample 4 goes in our new PCA plot. etc. etc. etc. Double bam! To review, we started with an awkward 3D graph that was kind of hard to read, then we calculated the principal components, then, with the eigenvalues for PC1 and PC2, we determined that a 2D graph would still be very informative. Lastly, we used PC1 and PC2 to draw a two-dimensional graph with the data. If we measured 4 genes per mouse, we would not be able to draw a 4-dimensional graph of the data, but that doesn't stop us from doing the PCA math, which doesn't care if we can draw a picture of it or not, and looking at the scree plot. In this case, PC1 and PC2 account for 90% of the variation, so we can just use those to draw a 2-dimensional PCA graph. So we project the samples onto the first 2 PCs. These 2 projected points correspond to sample 2, so sample 2 goes here. Bam! Note, if the scree plot looked like this, where PC3 and PC4 account for a substantial amount of variation, then just using the first two PCs would not create a very accurate representation of the data. However, even a noisy PCA plot like this can be used to identify clusters of data. These samples are still more similar to each other than they are to the other samples. Little bam. Hooray! We've made it to the end of another exciting StatQuest. If you liked this StatQuest and want to see more, please subscribe. And if you want to support StatQuest, please consider buying one or two of my original songs, the link to my bandcamp page is in the lower right corner and in the description below. All right until next time quest on!"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Principal Component Analysis (PCA)",
        "Video Title":"Principal Component Analysis (PCA)",
        "URL":"https:\/\/www.youtube.com\/watch?v=FD4DeN81ODY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/FD4DeN81ODY\/hqdefault.jpg",
        "ID":"FD4DeN81ODY",
        "Publish Time":"2021-09-29T18:39:16Z",
        "Channel":"Visually Explained",
        "Channel ID":"UCoTo2gtN527CXhe7jbP6hUg",
        "Transcript":"what makes a country happy in 2021 the un published a report that gives a score to each country on earth according to the following six factors in order to analyze and draw conclusions from this data we need to be able to understand or at the very least visualize it obviously we cannot visualize six dimensions all at once but we can pick two or three factors for example gdp social support and life expectancy and visualize that here is norway for example the us china russia and the rest of the countries but if we do it this way however then we lose some possibly valuable information that might be contained in other factors of the data like freedom or generosity pca is all about taking all factors combining them in a smart way and producing new factors that are one and correlated with each other and two are ranked from most important to least important these new factors produced by pca are called principal components and they are constructed in such a way that if you restrict your attention to the first few components only you would still get a fateful representation of the data let us now explain how pca picks its components for that let's take the same data as before but limit ourselves to the first three columns only for simplicity and drop a few countries so that the plot is not too cluttered to pick the first component pca asks the following question how can we arrange these points on a line in a way that preserves as much information as possible a first attempt is to project all of these points on one of the 3d axes but that corresponds to throwing away all the other columns of the data so maybe there is a better less wasteful way to answer that question let's take a small detour to explain how projection works when you project a point x on a unit vector u you get a new point x prime whose magnitude is given by the inner product between x and u and we can think of the square of this inner product as the amount of information about x that is preserved after projection on u in particular this quantity is maximum when x is parallel to u and is minimal when x is orthogonal to you back to pca the first component that pca picks is a unit vector that tries to preserve as much information as possible so it maximizes the sum over all countries of the square of the inner product while solving this optimization problem might look intimidating at first glance it is actually surprisingly easy to solve if you use the so-called lagrange multipliers method if you are not familiar with the terms optimization or lagrange multipliers you can either simply ignore the next 20 seconds and just take the results we're going to reach for granted or first go watch my beginner video series on optimization so in order to solve this problem let's simplify the objective function a bit with some not terribly complicated manipulations we can rewrite this expression as u times the matrix c times u where c is known as the covariance matrix of the data we then form the lagrange function take the gradient with respect to u and set it equal to zero in conclusion we know that the direction u that preserves information the most after projection satisfies the equation c times u equals lambda u for some unknown scalar lambda and if this equation looks familiar to you there is a reason this is exactly the equation of the eigenvectors and eigenvalues of c and interestingly the amount of information preserved after projection on an eigenvector is given by the corresponding eigenvalue so clearly the best direction to pick is the eigenvector with the largest eigenvalue so for our example this is what the eigenvector with the biggest eigenvalue looks like and one way to interpret this component is that roughly all three original factors have equal contribution and for this reason let's call this component power when we project the data points on this component we know that icelandic countries that we normally associate with happiness like norway for example are all positioned high on this axis while countries like niger are positioned low of course this representation is not perfect since you have countries like singapore that rank high simply because they have an unusually high gdp but this is to be expected though because we have only looked at the first component that pca gives let us now discuss the other components of pca how do we pick the second component for example ideally the second component is a unit vector that does not contain information that is already contained in the first component or in geometric terms we want the second component to belong to the subspace orthogonal to u1 but other than that it should maximize the same quantity as before and following reasoning similar to what we did before we find that the second component is given by the second eigenvector of the covariance matrix of the data and looks like this in english this component is the difference between individual factors and social factors so let's call this component balance if we project the countries on the plane spanned by the first and second component we find that the happiest countries seems to be the most balanced ones and countries that are either very high on the balanced axis like benin or very low like turkmenistan are generally less happy and it's very interesting to revisit the case of singapore so while it's doing great in terms of the power axis the individualistic nature of its citizens seems to make it less happy of a country overall more generally the directions picked by pca are exactly the eigenvectors of the covariance matrix you see every symmetric matrix like c has n eigenvector and eigenvalue pairs that are orthogonal to each other the eigenvectors are exactly the components that bca picks and the eigenvalues give you a sense of the importance of the corresponding eigenvectors for our case we see that pca suggests to look mostly at the power component and only look at balance for a more refined analysis or to compare countries of comparable power in fact we can make things even more precise if we divide all the eigenvalues by their sum we see that the component power explains about eighty-five percent of the data while balance explains about ten percent of the data and the third component explains the remaining five percent this was the basics of pca and how it can help you analyze high dimensional data if you want to dive deeper into the topic you can look at the references in the description and if you like the video like and subscribe and see you next time [Music]"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Principal Component Analysis (PCA)",
        "Video Title":"StatQuest: PCA main ideas in only 5 minutes!!!",
        "URL":"https:\/\/www.youtube.com\/watch?v=HMOI_lkzW08",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/HMOI_lkzW08\/hqdefault.jpg",
        "ID":"HMOI_lkzW08",
        "Publish Time":"2017-12-04T20:18:06Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"that quest is the best if you don't think so then we have different opinions hello I'm Josh stommer and welcome to stat quest today we're gonna be talking about the main ideas behind principle component analysis and we're going to cover those concepts in five minutes if you want more details than you get here be sure to check out my other PCA video let's say we had some normal cells if you're not a biologist imagine that these could be people or cars or cities or etc they could be anything even though they look the same we suspect that there are differences these might be one type of cell or one type of person or car or city etc these might be another type of cell and lastly these might be a third type of cell unfortunately we can't observe differences from the outside so we sequence the messenger RNA in each cell to identify which genes are active this tells us what the cell is doing if they were people we could measure their weight blood pressure reading level etc okay here's the data each column shows how much each gene is transcribed in each cell for now let's imagine there are only two cells if we just have two cells then we can plot the measurements for each gene this gene gene one is highly transcribed in cell one and lowly transcribed in cell two and this gene gene 9 is lowly transcribed in cell 1 and highly transcribed in cell 2 in general cell one and cell to have an inverse correlation this means that they are probably two different types of cells since they are using different genes now let's imagine there are three cells we've already seen how we can plot the first two cells to see how closely they are related now we can also compare cell one to sell three cell one and cell three are positively correlated suggesting they are doing similar things lastly we can also compare cell two to cell three the negative correlation suggests that cell two is doing something different from cell 3 alternatively we could try to plot all three cells at once on a three dimensional graph cell one could be the vertical axis cell two could be the horizontal axis and sell three could be depth we could then rotate this graph around to see how the cells are related to each other but what do we do when we have four or more cells draw tons and tons of to sell plots and try to make sense of them all or draw some crazy graph that has an axis for each cell and makes our brain explode no both of those options are just plain silly instead we draw a principal component analysis or PCA plot a PCA plot converts the correlations or lack thereof among the cells into a 2d graph cells that are highly correlated cluster together this cluster of cells are highly correlated with each other so are these and so are these to make the clusters easier to see we can color-code them once we've identified the clusters in the PCA plot we can go back to the original cells and see that they represent three different types of cells doing three different types of things with their genes BAM here's one last main idea about how to interpret PCA plots the axes are ranked in order of importance differences among the first principal component access PC one are more important than differences along the second principal component access PC two if the plot looked like this where the distance between these two clusters is about the same as the distance between these two clusters then these two clusters are more different from each other than these two clusters before we go you should know that PCA is just one way to make sense of this type of data there are lots of other methods that are variations on this theme of dimension reduction these methods include heat maps tea Snee plots and multiple dimension scaling plots the good news is that I've got stat quests for all of these so you can check those out if you want to learn more note if the concept of dimension reduction is freaking you out check out the original stat quest on PCA I take it nice and slow so it's clearly explained hooray we've made it to the end of another exciting stat quest if you like the stat quest and want to see more of them please subscribe and if you have any ideas for additional stat quests well put them in the comments below until next time quest on"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Principal Component Analysis (PCA)",
        "Video Title":"Principal Component Analysis (PCA)",
        "URL":"https:\/\/www.youtube.com\/watch?v=fkf4IBRSeEc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/fkf4IBRSeEc\/hqdefault.jpg",
        "ID":"fkf4IBRSeEc",
        "Publish Time":"2020-01-28T03:29:55Z",
        "Channel":"Steve Brunton",
        "Channel ID":"UCm5mt-A4w61lknZ9lCsZtBw",
        "Transcript":"[Music] welcome back so now we're going to talk about how you can use the singular value decomposition to compute the principal component analysis or PCA PCA is the bedrock dimensionality reduction technique for probability and statistics and it's still very very commonly used in data science and machine learning applications when you have big data that might have some statistical distribution and you want to uncover the low dimensional patterns to build models off of it PCA has been around for a long time since 1901 pearson paper okay so it's a really really old method very well-established ton of theory about the statistics of this and I'm going to essentially pigeonhole this as what we're gonna call the statistical interpretation interpretation of the SVD and principal component analysis is in particular going to provide us with a data-driven hierarchical coordinate system so it gives us a hierarchical high or that's not how you spell hierarchical a hierarchical coordinate system hierarchical coordinate system based on data to represent the statistical variations in your data sets okay so it's a coordinate system based in terms of directions in your data that captured the maximum amounts of the variance in your data okay and so I'm going to the notation here is gonna be a little different than what we're used to and that's because the PCA literature and the SVD literature how kind of different conventions about what the matrix looks like so in the PCA literature we still have a data matrix X and it still has a bunch of measurements from experiments independent experiments but here we're going to represent those independent experiments as big row factors x1 x2 and so on and so forth okay so each each row vector X are essentially measurements from a single experiment measurements from a single experiment and what we're hoping is that these are kind of individual experiments so this measurement might be the demographic information from a specific human you know age weight sex race etc etc etc and then X 1 would be person 1 X 2 is person 2 and so on and so forth so the same basic idea as before with our data matrix X except now instead of columns having the information for a specific individual kind of that that measurement from a single experiment now we're gonna have those B rows and that's because that's consistent with the PCA literature so I just want to show you how it looks this way okay and the idea here is that we're going to try to find and we're going to assume that this data X has some statistical distribution it's not deterministic there are some statistical variability to this information and we're going to try to uncover the dominant kind of combinations of features that that describe as much of the data as possible okay so we're gonna do it using the SVD but we're gonna write it a little bit differently okay so because this is the statistical interpretation of the SVD there's a few steps that are extra that we don't normally do with SVD that we're gonna do here so kind of step one in this procedure is that we're gonna compute the mean the row-wise mean the average row so we're gonna compute the mean row and we're just gonna call this X bar equals one over I'm gonna say that I still have n rows 1 over n sum of each of these X J's from J equals 1 to n ok so this is the average row I just average all of the rows the next thing I'm going to do is I'm going to build an average matrix so the average matrix is going to be obtained by taking a vector of ones and multiplying it by that X bar vector okay so I just literally create n copies of X bar and that's my X bar average matrix and so step two is I'm going to subtract the mean from my data matrix okay so I'm going to subtract the mean so now I have B equals X minus X bar and the way we say this in in PCA language is that this is the mean centered data so if I have some distribution of data where there's some you know average value to all of this when I subtract that out it brings everything down so that the center of my distribution is at the origin okay so we're going to be modeling this data matrix X assuming that it is a zero mean Gaussian and so this is where we we subtract off the mean so that it's zero mean okay now what we're gonna do is we're going to compute the covariance matrix of this mean Center data so the covariance matrix again this is kind of just the correlation matrix from the SVD but we're calling it a covariance matrix in this context of the covariance matrix of the rows of B and we're going to call that matrix C C is equal to B transpose B okay good so at this point all we've done is essentially take our data matrix we've written it in a transpose from how we normally do it we've subtracted off the mean and we've computed this correlation matrix or this covariance matrix so this looks a lot like the X transpose X from before but we've subtracted off the mean now what we're going to do is we're going to compute the eigenvectors the leading eigenvectors of this correlation matrix and that's going to be related both to the singular vectors of X and also to its principal components okay and I'm gonna try my best to get the notation and the terminology correct again this is in section 1.5 in our book so you can go there to refer for more details good so now what we're going to do is we're going to compute the eigen decomposition we're going to compute the eigenvalues and eigenvectors so the Ides of C and in particular we're going to compute for example let's call it V 1 transpose B transpose B V 1 that would be the biggest eigenvector of this matrix B transpose B is V 1 then I would compute V 2 then V 3 then V 4 and so on and so forth just like in the SVD and there's corresponding eigenvalues just like in the SVD and essentially what we're gonna get is this matrix C times V equals V times D where these are my eigen values and these are my eigen vectors okay good so all we've done is we've computed the singular sorry the eigen value decomposition of this covariance matrix that you could actually compute it using the SVD I'll show you that in a minute and you get these eigen values and eigen vectors and here's where the principal components come in so if I take this matrix T which is equal to my my mean subtracted data B times these eigen vectors V these are called my principal components these are my principal components okay principal components and this vector V of these eigen vectors are called the loadings so essentially what you do is you decompose this matrix into kind of directions of maximal variance just like in the singular value decomposition called the principal components and the loadings are kind of how much of each of those principle components each of each of the experiments has the loadings in a particular experiment of those principal components columns okay and oftentimes in terms of the singular value decomposition language so let's say X was equal to u Sigma V transpose here then what we would say is that T is simply equal to u times Sigma okay because and if I write B as sorry I should be a little bit more careful if I took the singular value decomposition of the mean subtracted data B equals u Sigma V transpose then T would be either B times V and B times V is simply u times Sigma okay so these are also a representation of the principal components okay so you can get the principal components and the loadings directly from the SVD of the mean subtracted data I guess that's the headline here is that this very important statistical representation of your data can be achieved just by computing the SVD of your mean subtracted data it's the same is finding these eigen vectors of the covariance matrix which is what you would kind of do in the classical principal component analysis good ok now what's also important is these eigen values here or the singular values in Sigma give you an indication of the amount of the variance of this data set that these principal components capture or these loadings capture so if I only want to describe this high dimensional data in terms of the first two principal components or the and the first two vectors of loadings I would be able to compute how much of the variance is captured by computing kind of how much energy or variances in those first two eigen values of this this D matrix and so what I could literally do I want to make sure I'm being careful here these eigen values lambda are equal to the square of the singular values it's literally equal to the variance of that principal component in the data and so if I want to know how much variance is being captured in the first let's say four modes I would take the sum from k equals 1 to R of lambda K divided by the sum of all n of my lambdas so I would basically see how what's the fraction of variance captured by my first are lambdas divided by the total of all of my eigen values all of the variants in the data and so for example I might decide to keep only as many principal components as are needed to explain 95% of the variance and so that would give you a criterion for how many principal components to keep okay again we're going to code this up I'm actually going to create a couple of data matrices real data matrices that have distributions one of them will be a random data matrix another one will be a data matrix consisting of genetic markers for people with and without ovarian cancer and we'll compute this principal component analysis and look at the results so I'll point out that again in MATLAB it's pretty easy to compute this so it's something very simple like V score and then some extra variable s 2 equals PCA of the B matrix okay so really really easy to compute in MATLAB also easy to compute in R and Python and so we'll do examples of that the last thing I want to do is just show you a picture so again if I have data that is you know some high dimensional data and it has some distribution you're hoping that it has some Gaussian kind of white noise distribution or some some normal distribution not drawing a great normal distribution here then what the principal component analysis is going to do is it's essentially going to find these ellipsoids of maximal variance in terms of 1 & 2 & 3 standard deviations so you can actually quantify if I have a new point how is it given the distribution of the old points and not only that but that prints the singular value decomposition in the principal component analysis will tell you exactly what directions are account for the most variance the second most variants the third most variance in the data and so on and so forth okay so a very useful statistical technique to build models statistical models from your data okay thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing PCA with Scikit-Learn",
        "Video Title":"PCA Analysis in Python Explained (Scikit - Learn)",
        "URL":"https:\/\/www.youtube.com\/watch?v=6uwa9EkUqpg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/6uwa9EkUqpg\/hqdefault.jpg",
        "ID":"6uwa9EkUqpg",
        "Publish Time":"2023-09-07T13:31:41Z",
        "Channel":"Ryan & Matt Data Science",
        "Channel ID":"UCKq-lHnyradGRmFClX_ACMw",
        "Transcript":"in today's video we're going to be taking a look at how you can Implement principal component analysis also known as PCA in Python with the help of scikit learn so first off why are you going to want to use PCA well when you're working with data often in the real world there's going to be a lot of specific columns to work with and not all of them are going to cause specific variants or even be important to your model PCA allows you to get rid of non-important features and getting rid of these non-important features are going to allow your machine learning model to run a lot faster now I'm going to show you two different methods on how you can use this you can either specify the percentage of variants that you would like to see based off of components or call out the specific number of components as well as this I'm going to take you in a practical example all the way from splitting up our data the scaling it and then also applying PCA hope you guys do enjoy this video and let's start coding okay let's get started I have a brand new jupyter notebook here and we're going to do the basics of reporting pandas as PD shift and enter runs the sill builds a new one down below then import numpy as NP shift and enter as well now we're going to throw in our data frame so I'll show you how we did this and also show you how I grab this data because it's actually pretty helpful if you do like baseball like I do you can grab a lot of stats so MLB teams.csv and this is the CSV I created I'll put this in my GitHub too I do want to also start building out my GitHub repository so probably a lot more coding that I'll be throwing up there in the future and also this is our data frame over here uh baseball stats for different teams in 2022 so I want to show you real quick how I actually grab this data so first there's a website called baseball reference has tons and tons of data based off of baseball both on the singular level of a player all the way through different teams or leagues now what I did is went over here share an export either grab a CSV or Excel workbook now I up I now I open this up and I open this up and then I went over here to playoffs and I built out this specific column other reason why is there wasn't any playoffs mentioned and I just looked up who made the baseball playoffs in 2022 but one for that team if they did zero if they did not so pretty cool we grab that Theta and then just upload it straight back into the Jupiter notebook so essentially how I did that I'm gonna copy me or you can just grab this specific CSV down in GitHub it'll be in the description too so okay now that we have that we can actually start working on this video so I will also mention I don't need this specific team so I am going to drop it so here's how you can do that so df.drop on your team like this axis equals one and then you put in place equals true so that way impacts the direct data frame right and now the team is dropped again if I just throw a DF right it's gone I don't need to show that over here uh you just got to believe me and the reason why is this is categorical data and teams aren't guaranteed to spot in the playoffs every year so there's like literally no reason to have this and if we're going to build out a Model A little bit later so okay and we aren't going to be running any models in this video but I'll still show you uh essentially how PCA works so first thing I'm going to do now is set up X and also y so uh X is gonna be everything over here or Y is going to be Target if they made the playoffs or not so I tend to use DFI lock so we'll do DX equals DF dot ilock then we'll put over here colon and then we'll put 0 to 27 then we have our X and then we're going to do our y next so again y equals DF Dot ilock over here colon then we'll just put 27 in this instance the reason why we both have 27 is this ends at 27 does not include it this one will include 27 which is playoffs and always make sure capital x and also lowercase y otherwise you're gonna have people yelling at you for improper formatting okay now we're going to import in train test split so from sklearn dot model selection Imports train test split make sure you have that before we apply PCA then our favorite right X train X test flight train y test equals train test splits X Y test size equals 0.2 then random State and um State equals 21. boom feel free to copy what you want uh that means we're going to use 80 of our data in training 20 testing and then just random state if you want to copy the exacto randomization that I am using okay now we have this we can Implement our scaling which you need to have scaling before you apply PCA in this case and why use baseball stats is just to show you like teams have at-bats or played appearances right like six thousand five thousand but then there's stats like Ops which has a League average of 100 and you have good teams have like 109 or 102 is a little bit above average right 115. so it's important to scale your features now what I'm using is standard scaling so what standard scaling allows you to do is have a mean of zero and the standard deviation is around one so I do a full video on the channel of scaling you do want to check that out I really appreciate it um because I do put a lot of effort into these videos so sklearn.pre processing [Music] import standard scalar like that shift enter and we run that now we have to call our standard Skiller so we're going to say scale standard equals standard scalar like that's now that is over here and um we have to fit our data so I'm going to say x train equals and then we have the skill standard right here there's a DOT fit score transform then throw X train in here again okay but we are not done because if you just put X train over here right we have this array and I want it to go back into a appendix data frame so easy way to do that you put first DF columns and you're going to get all the columns from your data frame and made our DF earlier and then I'm just going to rewrite X train so this time we're going to say x train equals PD dot beta frame then we're going to throw in here our X train like this then we're gonna throw in here our columns equal and all you got to do is copy this over and there's one thing you're gonna have to remove which is playoffs because we don't have that in there so remove playoffs keep it at lob which is left on base and boom we have that and then just to show you xtrain.head pull through a 10 in here right this is now scaled which is great right obviously this team did really bad with a bunch of negatives over here uh this team right did all better we have ones the 1.8 0.09 not really many negatives here except for home runs so now I want to run describe so we can just copy this x train and we can put describe here now if I don't do a round it's going to look pretty disgusting if you see this right here yeah I I don't like that so just put round three over here around we put three inside and this data is much much cleaner across the board so we see our count means here in deviation Min 25 50 75 and Max like I said about mean right all zero across the board which means we implemented it correctly I'm gonna build out a bunch of brand new cells and we're going to continue on in fact now we're going to import in PCA so from sklearn dot decomposition import PCA and since we're gonna be using PCA a lot in this video I'm going to start off with pca1 promise you we're gonna go over a ton of examples now that we have this data cleaned up okay and I'm not going to put any parameters in here we're just going to run it basic next what we're gonna have to do is set our X set so xpca one equals pca1 dot fits transform then we'll put over here our X train from earlier and now we can take a look at the variance ratio so essentially with the variance ratio shows is like how much variance each of these have in this data set and you'll understand a little bit later when I show you some graphs and also in a few other versions of this but just to show you everything right and I know the formatting isn't the best on here um but this is from greatest variance the least amount of variants that you can see over here like this is 0.5 but this is e to the negative 34 so there's like no variance on it but I think a few charts will show a better visualization of this and I am going to give a shout out over here to Prasad Oswalt from GitHub because he actually had both of these charts built out and I just copied the code because I think they look really nice and why recreate something so there you go and our first thing we're going to do is Import in matplotlib.pi plot and then I'm going to show you this first example so just copy this code over here looks like all right so we have our different components over here all the way through 27 you can see like once you get to 15 it's pretty much gone we still have a little bit at 10 but it really is down to 15. but really these first few components make the most and then you can see the cumulative over time going up right and um it's good because you can determine how many components you want to have or put a specific number like 95 variance which will be our next example now I also wanted to show you real quick another graph so you can see how much is variance is on each of these right 0.512 and probably about like 0.8 and again just windows down kind of like the like this over here but just in a line variation of it which I think is kind of cool just to show you guys so into that a little bit let's show 95 now that's gonna be pretty easy uh so we're gonna name this as pca2 since we just did pce1 and we're going to say PCA and throw in here 0.95 okay and let's start building this up so again like earlier xpca 2 equals pca2.fits transform and we'll throw in over here x now if you want to see how many components for 95 you can just literally throw in shape so copy this over here dot shape and um should probably be around 10 because it does dwindle down a bit over here and you can see 10 was the correct answer on that and like earlier right so I showed you this over here this variance ratio it's not going to show everything this time it's only going to show these 10 so all we're going to do this time it's like this you see two Dot ratio and you can see 0.512908 and all this will add up to 0.95 Great really cool okay now I'm going to show you if we're going to only use two components so I showed you 95 here well these first two components right 0.5 and then let's say this is 0.13 right that's 63 percent of the variance with just two components so let's build that up so I'll say PCA 2C equals PCA and then instead of just putting 095 this time you're going to put n components so n components equals two right now we have this implemented over here and um we're going to do the same thing where we do our fit transform so X will say PCA to C and I do apologize for how disgusting this is this again I'm doing a lot of these in one video so PCA to see and we'll do a fit transform fit transform throw in our classic X train over here and now I want to show you a specifically a chart on this and I actually built this out of chat GTP so pretty easy on here first of all we're going to set up a color map we're going to use cool warm and then show you how this works so uh we're taking a look at teams that ended up making the playoffs or not and you can see there's a pretty clear separation now there are some teams that probably should have made the playoffs like this team looks like it probably should have made the playoffs that this team shouldn't but there is some luck involved with baseball right obviously see the separation right over here one to zero this is nice to see and I also want to show you real quick a version with three components so it is a bit more complicated once you get into three dimensions but it'll be nice all right so now we're going to do n components of three for each of these over here and x p c a 3C equals and let's say this bit score transform over here x train now I have that over here and another that I built out from chat GTP and we'll just throw this over here and here we go another visualization different teams that made the playoffs or ones that did not with the three dimensions on it so a tldr specifically how this works right first thing that you want to do is clean up your data make sure that it is clean and ready to run you split up your X and your one next you're going to scale your data is super important that you scale your data before you apply PCA that way we can know what has the most variance and again in this example over here right 5000 compared to a hundred it can make sense why you have to standardize it okay now we have that over here now we can start working on PCA so call PCA you have two different versions that you could really specifically use you could use components which we showed you at the very bottom or you can show the percentage of variance now be sure it's a percentage of variance you might have a lot more components like in this example we had 10 components in comparison to two or three at the very end your choice to do so and again how some of these different charts show up like this not as important but just wanted to show you on at least on the visual side of things hope you guys enjoyed this video and now you have learned how to apply principal component analysis if you did make sure to subscribe to the channel as these videos do take quite a bit of effort to make now if you want to learn even more about machine learning or scikit learn I have a full playlist right over here and right now I'm uploading over three videos every single week so make sure to check that out"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing PCA with Scikit-Learn",
        "Video Title":"Principal Component Analysis (PCA) using Python (Scikit-learn)",
        "URL":"https:\/\/www.youtube.com\/watch?v=kApPBm1YsqU",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/kApPBm1YsqU\/hqdefault.jpg",
        "ID":"kApPBm1YsqU",
        "Publish Time":"2017-12-10T08:17:48Z",
        "Channel":"Michael Galarnyk",
        "Channel ID":"UC2QvKS5O6QhqaZi4LyAEUnA",
        "Transcript":"hello everyone this tutorial goes over pca using python pca or otherwise known as principal component analysis is when the most commonly used unsupervised learning algorithms so pca is inherently a dimensionality reduction algorithm and we'll go over what that means throughout this tutorial okay so we're gonna be going over a PCA for data visualization basically when you have higher dimensional data than two or three dimensions that you want to reduce to two or three dimensions so you can visualize it so you hopefully understand your data better when you do any sort of machine learning a lot of times it's nice to understand your data just in general really okay the second part we'll be going over is PCA to speed up machine learning algorithms so in a previous tutorial logistic regression using Python one of the things we briefly briefly briefly briefly went over was that just by changing one of the parameters to logistic regression by changing the solver ie the optimization algorithm we greatly sped up how long it took to fit our algorithm how long it took to fit our algorithm okay so in this tutorial we're me doing a way way way more common way of speeding up or algorithm by using PCA to speed up the fitting of our machine learning algorithm and that's the second part this tutorial and I should note that the code used in this tutorial will be down below as well as this blog post that I'm going through throughout this video so all I have to do is click on these links and you'll have access to all the code ok so I'm using anaconda in this tutorial if you need help installing it I have so many tutorials on this it's nine funny ok and feel free to ask questions on that too okay so we're gonna be using the iris dataset to you know apply PCA for data visualization so the important thing to note is first we're loading the IRS data set into a panda's data frame and one thing you'll notice is that this data is four dimensional we have you know four features sepal length sepal width petal lane petal width okay four features it's really hard to visualize four dimensional data okay so we're going to use PCA to reduce our four dimensional data into two dimensions so that we can plot and understand our data okay and of course we have our target and our target is typically what people use in supervised learning algorithms what they're trying to predict basically so one thing I really want you guys to get out of this tutorial is that use PCA you need to standardize your data okay so PCA is an unsupervised learning algorithm that is affected by scale most algorithms are gonna be affected by scale to some degree so we're be using a standard scaler to standardize the data sets features ie the pedaling little sepal width put a length petal width and that's it onto the unit scale which means mean of zero and variance of one okay just to emphasize this point even further so I could learn has a wonderful section on it that goes over the importance of standardizing your data and this could have a major impact if you ever do machine learning with prediction accuracy as well as you know PCA for data visualization okay so what I have over here and I should note that the code that produced all this stuff this is just a jupiter notebook and it's right here and you'll have the ability to downloaded use it you know whatever you want okay it I just find this a little bit easier to visualize at least for tutorial purposes so basically what this code is doing is separating out the features because you standardized the features not the target okay and the target is just the this column over here called target okay and we're fitting and transforming our features into unit scale with a mean of 0 and a variance of 1 okay so now we do a PC a projection to two dimensions so what you have to do is you have to import PCA and we're basically making an instance we're basically saying I want a PCA and I want to keep to principal components okay so we make an instance of the PCA class okay and then we're gonna fit and transform our features and we're gonna get a two-dimensional data okay with two principal components I should say okay and basically we have something that's four dimensional and after we apply PCA we have two dimensions okay so this over here is basically just combining these two prints of component two principal components with the target again so that we have our final data frame which is basically the two principal components and our target okay so the whole point of this exercise was to go from four dimensional data into something that we could visualize something we could plot so that hopefully we can understand our data better okay so I have some matplotlib code here if you have any questions about it let no I'm happy to help okay and what you're gonna see in this plot over here is that iris atossa is very different than iris versicolor and iris virginica okay so we understood something a little bit more about our data okay um one thing I really want you guys to you know learn about pca is we went from a four dimensional space to a two dimensional space okay so anytime you go from a higher dimensional representation so as you can see here we have four dimensional data okay and then you know after running all this code we went down to two dimensional data okay there is some information that's gonna be lost okay so the way PCA accounts for this or the way you can think about it is by a explained variance ratio so these first two principal components take up ninety five point eight percent of the variance or the information okay um with the first principal component accounting for seventy two point seven seven percent and the second twenty three and the remaining two with the rest of the variance one thing to really you know note that's important is that if you try to go down to two principal components and you're below eighty five percent of the information it may not be the best idea to to visualize that as an accurate representation because you lost a lot of your variance or your information when you went from four dimensions or however many dimensions to two dimensions okay but since we're above you know 85% roughly this is more than a valid way to visualize our data okay um so next we're to go over PCA to speed up our machine learning algorithm okay so okay so the just about the most common application of PCA that I know of is for speeding up machine learning algorithms so the reason why we're not going to use the iris dataset like we use up above here is because that's a very very small data set the data set is a like concerted 22 set which means it's a like a data set you know just applying algorithm it's very small so if we use you know PCA here and then a machine learning algorithm we wouldn't really see a difference in how long it took to fit or algorithm because it's already so fast on the iris dataset it's a small so instead we're gonna use the mints database of handwritten digits otherwise no one is missed okay this has 784 features Barriss only had four as you remember it has 60,000 training examples and a test set of 10,000 okay what it means by 784 feature columns is that we'll have 28 by 28 images okay and inside each one those columns will just be pixel intensities okay so the first thing you have to do and again all this code will be provided down below feel free to use it as your own okay so the first thing you have to do is get the data set so we're gonna use SK learns fetch ml data to get the Mint's data set there's a bunch of different ways to get this data set this is just the way I chose okay so inside this data set when we download it you'll see that mints dot data these are the images in the data set it's you know seventy thousand total images that are 784 dimensional or 28 by 28 images by the way 28 times 28 is 784 okay and mints thought target these are just the labels corresponding to each one of these images above okay so what we're gonna do I'm sorry this is this notebook okay is after we download this data set we're gonna do with a typical splitting our data into training and test sets so typically you split your train set into or your data set into 80% training and 20% tests in this case I chose 6\/7 of the data to be training because I wanted 60,000 training images and 10,000 test images okay so this is just SK learns train test split okay and one thing I fret but over here is that import train test fling okay you'll see over here that you have to import it from sq learn model selection okay so like before we have to standardize our data I mentioned this early in the tutorial so again we use standard scaler so one thing you'll start noticing about a lot of machine learning you know algorithms you have some sort of process or some sort of pipeline where you first you know standardize your data then you'll import and apply PCA so I import PCA and then I make an instance of the model okay and the difference from before is that what it means this the model before we had number of components equals to let me just go up so I can show you we had PCA and components equals to okay we basically said Ref the bat that we want to go down to two components okay in this case what I'm telling SK learn is that please choose the minimum number components such that 95% of the variance is retained okay so what SK learn is gonna do is it has a curve where it'll find out what's the minimum number of components such that 95% of the variance is retained okay and if you want to find out how many components that is you can do PCA dot and components okay and that really amounts to 154 principal components okay and again we're doing this to speed up our machine learning algorithm okay so important thing a note is just like with any sort of algorithm where you fit your algorithm on the training set you do the same thing with PCA on your training set and then once you fit PCA a new training set you apply that transform to both the training set and the test set okay and then from there it's just like you have a normal algorithm from SK learned you import the model you want to use in this case from SK learn linear model import logistic regression and let me just do this inside the jeepers notebook so you can see okay okay let me wait till my computer catches up so I made an instance the model I'm finding PCA on just the training set okay on your computer it probably be faster because right now I'm recording this video so it's slowing down my computer quite a bit okay and I applied my transform to both the training and test set okay and from here it's just import whatever them all you want to use I chose with just other algorithms will actually be a lot better for this case it just live discretion is a very common algorithm I make an instance my model okay I am fitting logistic regression okay so when I'm fitting the model it's learning the relationship between the digits and the labels okay so this is really what we're trying to speed up when we do PCA to speed up machine learning algorithms so because I went down from 784 components in the original data to I think 95 percent of the variance corresponds to 154 components this is really what's you know suppose to be sped up by using PCA okay so I just got done fitting my algorithm and then from here you can measure your model performance okay so this blog post goes over the steps my little bit cleaner than just giving you guys do clear notebooks you guys like the format let me know if you don't let me know as well okay and then one thing I want to note is the timing of a logistic regression after PCA so for different number of principal components kept if I kept more percent of the variance back up 100 percent of the variance basically if I'd really been applied PCA it took roughly you know forty eight point nine four seconds on my macbook with an accuracy of 0.9 one five eight and then you know variate and then from there I just kept various percentage of the variance okay so with 85 percent of the variance retained that amounted to 95 principle components and it took eight point eight five seconds to train my model okay to fit the model and it really didn't affect the accuracy in this case okay so one thing I want to note is we can also do an inverse transform so we went from 784 components down to 59 okay or 784 dimensions down to 59 dimensions okay and speed up an algorithm so this entire tutorial up until now we've gone from higher dimensional data to lower dimensional data so PCA can also take the lower dimensional data ie the compress representation the data back to an approximation of the original high dimensional data okay so after I ran PCA for 95% with keeping 95% of the variance I was able to do an inverse transform to get back to an approximation of the original 784 dimensional data okay if you want to see this code and how it works I have a link to it over here okay so you can see it's really just you know after fitting the transform going from high dimensional data to lower dimensional data ie 154 components you can also do the inverse transform to get back to an approximation of the original data okay and then the image I have in this blog post at the top this just is basically just after applying PCA going down to like let's say a city nine components on this rightmost image I just did an inverse transform to get this to get back to the 28 by 28 image okay so that's really it for this tutorial please let me know if you have any questions I'm happy to help and I should note that if you have a good question I might just answer it on this blog post or in the comments down below the YouTube video and that's it please subscribe if you want more content leave feedback if you want and that's it bye"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing PCA with Scikit-Learn",
        "Video Title":"Principle Component Analysis (PCA) using sklearn and python",
        "URL":"https:\/\/www.youtube.com\/watch?v=QdBy02ExhGI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/QdBy02ExhGI\/hqdefault.jpg",
        "ID":"QdBy02ExhGI",
        "Publish Time":"2018-07-02T15:13:57Z",
        "Channel":"Krish Naik",
        "Channel ID":"UCNU_lfiiWBdtULKOw6X0Dig",
        "Transcript":"hello all today we'll be discussing about the principal component analysis principal component analysis is not a machine learning algorithm but it is a technique which is used for reducing the dimensionality suppose if we take a dataset which is having around three columns or three independent features and if you want to convert that three independent features into two independent features by using a new vector space and that time we can actually use this principal component analysis so not only with three independent features any number of independent features and if you are trying to actually reduce those number of features to the number that you want you can actually use this principal component analysis so this technique usually involves creating lot of vector space or you can say no number of independent components based on the components on or the number of dimensions that you want we can convert any dimension into that particular dimensions so before moving ahead guys please do subscribe the channel because I don't want you all to miss any content I try to upload as more as videos on Saturdays and Sundays on every weekends and I also upload videos on deep learning so I'll try to upload one video on Saturday on machine learning and on Sunday I try to upload a video on deep learning so please before moving ahead please do subscribe the channel so now let me just move ahead with this principal component analysis so to start with I'm just going to input some of the libraries that I'm going to use like matplotlib which is used for actually plotting the diagrams numpy for creating arrays ponders to read the data set then Escalon provides data set within itself from this library I am just going to import a particular library a particular data set called as load breast cancer so just let me execute this and show to you what this particular data set has what is the details in this data sets so I'll just write load breast cancer with a method over here and here is my cancer dot keys once I execute the cancer dot keys I have all these things so I have all the data in this particular data key and my target is basic what is my output so this is a basically a data set which says that what type of breast cancer it has and if I see the target there are two different types of target and we can see the description in the target names okay and there is also feature names this feature names are nothing but the data that we have so all the columns all the column name is having this feature names over here okay now you can also see the description of this particular data set from here so you just have to write print cancer cancer of description I'll be uploading this file in my github link please do see in the description below and you'll be able to see the github link along with all this code okay so in this breast cancer or diagnostic database you can see the description over here so it shows basically like there are number of instances of so many records how many different kind of attributes are there this is basically columns right so if I see a 30 new Breakaway M that means 30 different independent features so you can say this as a it is 30 dimensional okay and we'll try to actually reduce this 30 dimension into two dimension and we see how well the new dimensions will be created by considering all these features such that it will not lose that much amount of data also okay so you can see all these descriptions from here and as I said that there are two types of targets names that one is malignant cancer one is benign so malignant I think it is given a value as 0 and 1 so if you see if you just go down there'll be something called as target I guess I'll just let me display the target away in the below so what I'm going to do I'm just going to create a data frame where I'll be taking period or data frame my first attribute will be my data and the second attribute I'm going to put the column names which will be my feature names over here okay so all the feature names will be appended as my column names and it will be having all the data from this particular data key okay so once I execute this I can see my data frame ahead as five you can see all the features all the columns that have all the independent features over here and from this particular data value we are getting all the values over here so you can see how many features are there they're mostly around 30 features as we saw from the help section so these are the 30 features that we have over here now what we try to do is that by using PCA we'll try to reduce this 30 dimension into two dimensions which will be creating a new vector space okay so always remember that in the PCA while running a piece here the first thing that you have to do is do the standard scaling okay the main reason is that you can see the values right the difference between these values will be very high and if you are trying to create a new vector space the value should be the difference on between this value should be very very minimal so for this purpose I'm just going to use a standard scalar again standard scalar is present in this SQL under pre-processing library by using the standard scaler as you know that by using standard deviation is equal to m1 and mean is equal to zero by that it will try to scale down all these values you can also use min max scalar so suppose if you want to use min max scalar I'll just write it down for you so you can just write from a scale on dot pre-processing I'm just going to import min max Kaila so min max Keller will convert all your values between 0 to 1 right so you can use this and you can again do the same operation initialize object and then do a fit operation on the data set that's it and with this you'll be able to do the scaling again I mean to finish the scaling you need to also add and transform operation on the data set so once you do this you'll be able to do the scaling ok once I do the scaling you can see that it has been converted it into an array and this is based on the standard deviation as 1 and mean is equal to 0 then after this you can apply now the PC algorithm ok the PCA technique so as I said that PCA technique it is basically used for dimensionality reduction so if I want to reduce the dimensions of 30 dimension to two how can I do it so I'll just write it as from a scale on dot decomposition this is my library I'm trying to import this class called as PCA okay in this PCA if I execute this piece here and you can see that if I press shift tab over here you can see the the first attribute is something called as n components this is the components like the number of components you want to reduce a dimension to so from I mean from from the total 30 dimensions I want to convert this into two dimension only by using a new vector space so my whole 30 dimensions just to understand my whole 30 dimension is getting converted into two dimensions isn't it cool yeah so this is a wonderful features where you can actually convert so many dimensions so if you have many dimensions right now at that time it usually provides an under fitting problem because because of those kind of dimensions the model will not be able to predict properly so by this we are just using n components is equal to 2 and I have created my PC object over here ok then I'm going to fit my scale data scale data is my this data which I have scaled down by using a scale standard scalar method okay now if I go here and transforming the PCA by using the PCA object for this scale data now you can see my scale data had a shape of 569 comma 630 okay 569 is basically my number of rows and 30 is the number of columns so 30 was the number of different kind of features now after applying the PCA technique you can see over here for the variable X underscore PCA dot shape when I did it it gave me around so just let me execute this once again so ok execute everything once again because I was just trying to do something I'm just going to restart and run all so that it starts executing faster just give me a moment it will run off okay no perfect now you can see that way I'm showing you the scale data dot shape right I have converted the dimensions into N and this component is equal to 2 okay so that means my 30 dimensional is getting converted into two dimensions now if I see my scale data dot shape it is 569 comma 30 that means I have 569 rows and 30 different dimensions that it 30 independent features and now after applying this transform method I get this X underscore pca value you can see the shape of this X underscore PCA dot shape it is 569 comma 2 now see see this this 30 dimensions has got converted into two dimensions so this is a very good way of reducing the dimensions once the dimension is getting reduced you have the target value over there you can apply any algorithm you want like if you want you can apply logistically it already linear regression or you can also apply some some kind of decision trees or any classification kind of algorithm to determine which particular target name it is of like whether it is or whether it is of men and or it is for so if I show you my target names over here whether it is my malignant or benign okay so based on this target values you can then apply your algorithm so for this X and X underscore piece here dot shape okay data now let me just go down little bit you can see that I am just creating a plot figure over here and just plotting my X and the scope is PCA since I have two independent features I am going to plot it based on the target value right so based on the target whiling I'm going to plot this particular figure from these two particular dimensions now you can see even though the from 30 dimensions this we have just created two dimensions the splitting is perfect you can see that so it does not loss that much data so it is possible that you can actually don't worry about the number of dimensions that you have okay by using various this principal component analysis you can actually convert into any number of dimensions you want now I have converted into two dimensions for a purpose because I could draw this diagram and show to you so now this diagram you can see that this is a zero values and this is a one values so this is malignant and this is benign okay this value indicates that this value basically indicates that from 30 dimensions you are trying to take out two dimensions by using a new vector space still that particular difference right the points that are belonging to the malignant the points that are belonging to the benign benign target names is correctly splitted up so there is no loss such of no such loss of data now you can take this an X underscore PCA and apply to any machine learning all classification algorithm because I have the data right I have to date away so any machine learning algorithm you can use it and your y-axis that will be your output will be this target values so based on all the xn I mean the two down in dimensions will be having the target values whether it is 0 and 1 so by this way you can actually apply principal component analysis right now in deep learning you have something called as Auto encoder and decoder right so auto encoders and decoders are basically used as a PCA for dimension reduction in case of deep learning where as in machine learning you basically use as principal component analysis so this was the lecture for today I hope you liked the video please do subscribe the channel and please do share with your friends and feedback is happily expected accepted thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing PCA with Scikit-Learn",
        "Video Title":"PCA Tutorial with scikit-learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=ftMBLceSF9U",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ftMBLceSF9U\/hqdefault.jpg",
        "ID":"ftMBLceSF9U",
        "Publish Time":"2023-06-25T21:21:44Z",
        "Channel":"TekMinded - Python Recipes",
        "Channel ID":"UCdHC62kngffvzsuHofvqCBg",
        "Transcript":"welcome back to Tech minded in today's tutorial we'll be diving deep into principal component analysis PCA using the scikit learn Library we'll explore the concepts step-by-step implementation and practical examples to help you grasp PCA thoroughly so let's get started before we jump into the code let's take a moment to understand what PCA is all about principal component analysis is a dimensionality reduction technique used to discover patterns and relationships within High dimensional data sets it accomplishes this by transforming the data into a new set of variables called principal components these components capture the maximum amount of variance present in the data alright let's dive into the code we'll start by importing the necessary libraries great now that we have our libraries imported let's load the data set we'll be working with for this tutorial we'll use the famous Iris data set which contains measurements of Iris flowers it's a well-known data set with four features simple length sepal width pedal length and pedal width our goal is to classify iris flowers into three species setosa versicolor and virginica excellent we have successfully loaded our data set into variables X and Y now let's perform PCA on the data set we'll create an instance of the PCA class and fit it to our data perfect we have performed PCA on our data set and the transform data is stored in the variable X underscore PCA now let's explore the explained variances and cumulative explained variances to gain insights into the significance of each principal component in this code snippet we calculated the explained variances and cumulative explained variances using the explained underscore variance underscore ratio underscore attribute provided by the PCA object we then plotted them using a line plot the x-axis represents the number of components while the y-axis represents the variance explained looking at the plot we can observe how each additional component contributes to the explained variance the explained variance line indicates the individual variance explained by each component while the cumulative explained variance line represents the accumulated variance explained by the components the cumulative line provides insights into how much of the total variance is captured as we increase the number of components now that we have a good understanding of the explained variances let's take a closer look at the transformed data we'll create a scatter plot to visualize the iris flowers in the reduced dimensional space in this code snippet we use the transform data X underscore PCA to create a scatter plot each point represents an iris flower and the color indicates its species by visualizing the transform data we can see how PCA has compressed the high dimensional Iris data set into a 2d space while preserving the discriminative information PCA is not only useful for visualization but also for dimensionality reduction we can choose to keep a specific number of principal components that retain most of the variance while reducing the dimensionality of the data for example let's say we want to reduce the dimensionality of the iris data set to only two principal components we can achieve this by modifying our PCA initialization as follows by specifying an underscore components equal sign 2 we restrict the PCA transformation to retain only the first two principal components this reduction can be handy when working with large data sets or preparing data for machine learning algorithms and that wraps up our tutorial on principal component analysis using scikit-learn we covered loading the data set performing PCA exploring explained variances and cumulative variances and visualizing the transformed data the complete code used in this tutorial can be found in the description below feel free to experiment with different data sets component selections and explore the numerous applications of PCA if you found this tutorial helpful please give it a thumbs up and consider subscribing to our channel for more exciting tutorials on data science and technology if you have any questions or suggestions for future tutorials please leave them in the comments section below thanks for joining me today on techminded stay curious keep exploring and I'll see you in the next video"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"t-SNE and UMAP for High-Dimensional Data Visualization",
        "Video Title":"Latent Space Visualisation: PCA, t-SNE, UMAP | Deep Learning Animated",
        "URL":"https:\/\/www.youtube.com\/watch?v=o_cAOa5fMhE",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/o_cAOa5fMhE\/hqdefault.jpg",
        "ID":"o_cAOa5fMhE",
        "Publish Time":"2024-08-05T09:42:34Z",
        "Channel":"Deepia",
        "Channel ID":"UCJLoMm3XdBoMnQrKoboRkPA",
        "Transcript":"You've probably seen these kind of images representing the latent space of a deep neural network, but have you ever wondered how they are created ? Well reducing data from 50 dimensions to just 2 or 3 is no simple task. Today, we'll explore three famous and powerful dimensionality reduction techniques: PCA, t-SNE, and UMAP. By the end of the video, you'll know how each of these methods work, and what type of data they can handle. First, let's talk about PCA, or Principal Component Analysis. Imagine we have gathered data on the weight and height of a thousand people. We can plot this data on a 2D graph to observe how it is distributed. And additionally, we can normalize the data to range from 0 to 1. You can see that the data varies along two main directions represented by the red and green arrows, and the variation along the red arrow seems more significant than that along the green arrow. These directions are known as the 'Principal Components' in Principal Component Analysis. Now, suppose we want to represent this 2D data on a single axis. One way to do it is to project the data onto the largest principal component, which is the axis given by the red arrow. And here is the final 1D graph representing our population. However, you might wonder why we can't just project it onto the horizontal axis ? Indeed, if we revert back to our original distribution, it seems that just projecting to this axis yields a very similar result. Let's change the dataset: imagine we have data on the blood pressure and cholesterol levels of several patients. When we plot this distribution, we observe that the patients form three distinct clusters. If we project the data along the x-axis, we only see two clusters, meaning we've combined two different populations in our low-dimensional representation. And if we project along the other axis, we run into the same issue. Now what happens if we project onto the principal component? Well, it seems that this time, we successfully retain all three clusters in our low-dimensional representation. This low-dimensional representation successfully preserves the main feature of our original data, which is much better than the simple projections we tried previously. Now that we've seen some examples, let's see how we can calculate these principal components in practice. The first step in Principal Component Analysis, is to calculate the covariance matrix of the data set. Let's look at a simple example using our previous 2D data set. To calculate the covariance matrix, we first need to centre our data. This means calculating the mean of each variable, and then subtracting that mean from each data point. Next, we multiply the transpose of our centred data by the centred data itself and then scale it by the number of samples. Since we are working with 2D data, we end up with this nice and simple 2 by 2 matrix, which is called the covariance matrix. The second step is to find the eigenvectors of the covariance matrix, which are the vectors whose direction remains unchanged when the matrix is applied to them. These are precisely the vectors that satisfy this equation. Solving for these vectors is a very common task in linear algebra, and can be done using various methods. Once we have the eigenvectors, we sort them in descending order by their eigenvalues. The eigenvectors with the largest eigenvalues are our 'principal components'. Finally, we create a matrix to project our data onto the principal components, finishing the PCA. There are lots of advantages to using Principal Components Analysis instead of other more complex methods. The main benefit is that it relies only on basic linear algebra, which makes it very fast. Indeed, if we look at the time complexity for a second, PCA is linear in terms of the number of samples. This means that the time required to project the data increases proportionally with the dataset size. The other great thing about PCA is that when you use it to visualise the latent space of an auto-encoder, you also get the eigenvalue information. This means we can now identify which dimensions of our latent space are the most important, and by how much. However, there is one thing that PCA struggles with: it doesn't work well with non-linear data because it relies on linear projections. Take spiral-shaped data, for example. PCA struggles to separate such data into distinct clusters effectively. The problem is not the direction of projection, but the fact that we are using a linear operation at all, whereas the relationship between the data is non-linear. Despite this, it is still the fastest and most interpretable method for dimensionality reduction, which is why it is used so widely in research and engineering. However, there is still a need for methods that can effectively deal with more complex data, as the relationship between latent variables may not be linear. Another well-known method for visualising latent space is t-SNE, but before we look at how it works, let's give it some background. t-SNE stands for t-Distributed Stochastic Neighbour Embedding and was developed in 2008 by Laurens van der Maaten and Geoffrey Hinton. Geoffrey Hinton is a big name in deep learning, often referred to as one of the \"godfathers\" of the field, along with Yoshua Bengio and Yann LeCun. Hinton has come up with several key ideas in deep learning, such as backpropagation, dropout, autoencoders, knowledge distillation, and contrastive learning, and the list goes on and on. t-SNE is actually an improved version of an earlier technique called SNE, which Hinton published in 2002. We'll start by looking at the basics of SNE, and then quickly go over what t-SNE adds to the mix. The main idea behind SNE is pretty straightforward: points that are close to each other in high-dimensional space should also be close to each other when we bring them down to a lower dimension. To make this happen, we measure the distance between a point and its neighbors in the high-dimensional space, and then we try to make sure those distances stay similar in the lower-dimensional space. We do this by using a special kind of loss function that tells us how different the distances are in the high-dimensional and low-dimensional representations. Next, we simply minimize this loss function to improve our low-dimensional representation. Let's take a look at how we actually calculate all this in practice. The first step in the SNE algorithm is to turn the distances between a point and its neighbors into a probability distribution. More specifically, we convert these distances into a Gaussian distribution. You can think of this Gaussian distribution as the probability that a blue point is a neighbour of the red point. To get the probability for each point in our high-dimensional space, we use this formula, which is basically the exponential function applied to the distance between the points. If you know a bit about Gaussians, you\u2019ll know that the shape of the bell curve changes based on the variance sigma. The shape of this Gaussian is crucial because if it's wider, more points will be considered as neighbors of the red point. And if it's narrow, only the closest points will be considered as neighbors. We adjust the variance of this Gaussian using a hyperparameter called \"Perplexity,\" which we'll talk about later. Now that we have the probabilities for the neighbors of this point, we repeat the same process for each point in our high dimensional representation. To make all these probabilities add up to one, we normalise each Gaussian by the sum of all the others, which is very costly in terms of computations. In our low-dimensional space, we start by placing points randomly. Then, we repeat the same process we just did with our high-dimensional data: we calculate the same kind of probabilities based on the distances between these points. As you can see, it is exactly the same formula, except we use different variables since we describe the low-dimensional representation. Now that we have two different probability distributions representing our high-dimensional and low-dimensional spaces, let's see how we bring each of these distribution closer together. The most common way to measure how similar two probability distributions are is by using the Kullback-Leibler divergence. You can think of it as a kind of distance, but between probability distributions. If the KL divergence is large, the distributions are very different and if it small, they're very similar. For instance let's take these two Gaussians: the Kullback-Leibler divergence between these two functions is the blue area under the blue curve. As these Gaussians move further apart, the KL divergence increases, and as they move closer together, the KL decreases. Finally if the two functions match perfectly, the KL is 0. We can explicitly compute the KL divergence between the distribution representing our high-dimensional space and the distribution representing our low-dimensional space. Then we can calculate the gradient of this KL divergence with respect to our low-dimensional embeddings, which gives us this nice formula. This allows us to adjust the low-dimensional representation to be closer to our original distribution. Let's take the example of these four clusters, and see how their 1D representation evolves using t-SNE. As you can see, after 80 iterations we indeed have 4 clusters, just like in the higher dimensional space, which is already something that PCA would not be able to do. So now that we saw the whole process, what about that perplexity hyperparameter we mentioned earlier? Perplexity helps us choose the correct sigma parameter, which controls the width of the Gaussians in the high-dimensional representation. More precisely, it is calculated with the entropy of each distribution P using this formula. So what happens in practice is that the algorithm iteratively selects different values of sigma until this quantity is roughly equal to the perplexity chosen by the user. Theoretically, the perplexity should represent the number of other points that a given point will consider as neighbors, but in practice this is not so obvious. Let's take the example of the mnist dataset: here is the 2D representation computed by SNE with a perplexity of 30. With its default parameters, the algorithm already provides a decent visualisation, with some easily identifiable clusters. Let's increase the perplexity and see what happens. As the perplexity increases, the effects on the size of the clusters and their distribution is not all that clear, but the clusters are supposed to be more diffused now. Perplexity is the most important parameter of SNE, but you kind of have to test different values, and adjust until you are happy with the visualisation. As you may have guessed the main advantage of SNE is that it can handle non-linear data, which is a big improvement over PCA. For example, take this spring-like 3D dataset: on the right is the 2D representation of this dataset using PCA and SNE. As you can see, PCA produces only one mixed-up cluster, whereas SNE successfully separates the two classes in the 2D representation. Even if the visualisation is better, SNE has one major flaw, it runs terribly slowly. t-SNE introduces several changes to speed up and improve the visualisations of SNE, but we won't cover all of these in this video. The most important tweaks are in the probability distributions, like using symmetric distributions in the high-dimensional space and Student's t-distribution in the low-dimensional space. Indeed these distributions are easier to compute because they don't use the exponential function. Even with many modifications and hacks, t-SNE is still very slow compared to PCA: as you can see when running it on 10 thousands MNIST samples, it takes almost 90 seconds to get the result. Another drawback of t-SNE is the perplexity hyperparameter. The big problem with perplexity is that changing this parameter even a little can completely mess up our visualisation. For instance here we just went from a perplexity of 30 to a perplexity of 31, and many clusters have completely changed. This means that when using t-SNE, we have to check the visualization each time we change the perplexity until we get a good representation, which is very time-consuming. And the bigger the perplexity, the slower the algorithm runs, making it even harder to tune this parameter. Still, for a long time, t-SNE was the most widely used visualization method because it could handle non-linear relationships way better than PCA. In 2018, a new dimensionality reduction technique called UMAP was introduced, which stands for Uniform Manifold Approximation and Projection. While the algorithm has a strong mathematical foundation, particularly in topology and category theory, we won\u2019t be diving into the theoretical details in this video. If you're curious about the deeper mathematical justifications, there's a link to the original paper in the description. Now, let\u2019s break down the main steps of the algorithm. The core idea of this algorithm is somewhat similar to t-SNE: we look at the distance to each neighbor of every high-dimensional sample and aim for a low-dimensional representation that roughly matches those distances. But instead of using Gaussians and probability distributions, we'll use graphs to represent both our high-dimensional and low-dimensional data. So, how exactly do we compute these graphs? Well first, we will need to find the nearest neighbors of each samples. There are several algorithms to achieve this, all based on comparing the distances between points. For each neighbour, we construct an edge between the two points and move on to the next neighbour until we have k neighbours. In this example, k is equal to 3: keep in mind that this is in fact the main hyperparameter of UMAP. This gives us a first binary graph, which we'll transform into a weighted graph to represent how close each point is to its neighbors. We achieve this by applying an exponential decay over the distances between a point and its neighbors. We offset this decay by the distance rho to the closest neighbor so that the closest neighbor keeps a weight of 1. As you can see, we have a similar formula to t-SNE, but we don't normalize this quantity, making it much faster to compute. Now that we have the weights for this sample, we will repeat the same process for each of the other samples. Although we only show three graphs here, we end up with one weighted graph per sample, which we need to combine into a single weighted graph. So, how do we proceed ? Well, to keep a weighted binary graph, we need to symmetrize it so that there's only one edge between any two points instead of two. We simply use this formula that does just that, combining edges and keeping the weights between 0 and 1. For instance if we want to combine the edges between the white and red vertices, we simply take their respective weight and apply the formula, and end up with a weight of 1. Now that we have our final graph construction, we repeat the same process for our initial low-dimensional representation, which gives us another weighted binary graph. So, can you guess what the next step of the algorithm will be? You've guessed right, we'll try to match both representations with the help of a loss function. To calculate how close the two graphs are, we use their adjacency matrices, which represent the weight of each edge in the graph. We then compute the cross-entropy over these matrices and simply minimize it using the stochastic gradient descent, and we adjust the low-dimensional representation accordingly. Let's see some visualisations now. Let's look at the MNIST dataset again, but this time we'll visualise it using UMAP. Amazing, right? The classes are beautifully clustered, and the result looks almost perfect. UMAP's strong theoretical foundations really shine here, both in visualization quality and speed. It took UMAP just 5 seconds to generate this visualization, compared to the 40 seconds it took t-SNE. And if we compare the runtime versus the number of samples, you see that UMAP grows much slower than t-SNE. Another advantage of UMAP is its main hyperparameter\u2014the number of nearest neighbors in the initial graph construction. Let's look at the evolution of the projection of this mammoth as we increase the number of neighbors. In the beginning we have just one big fuzzy set, and as we increase the parameter, we get more and more details coming through. As you can see, the behaviour of the algorithm is much more predictible than t-SNE when changing the main hyperparameter. With the perplexity, on the other hand, it was difficult to determine whether results were actually improving when we modified the value. Finally, UMAP is pretty good at preserving global structure: as you can see, the parts of the mammoth that are close together in 3D stay close together in the 2D projection. Even though UMAP often outperforms PCA and t-SNE, these two techniques are still widely used in engineering and research. As a rule of thumb, if you need a quick visualization, PCA is a great starting point. For more complex data, t-SNE can provide good results, but I believe UMAP is the best choice overall. Additionally, there are even more recent methods, like TriMAP and PACMAP. I also linked the papers for these in the description below. Thank you so much for watching this video ! Stay tuned for the next one which will be"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"t-SNE and UMAP for High-Dimensional Data Visualization",
        "Video Title":"StatQuest: t-SNE, Clearly Explained",
        "URL":"https:\/\/www.youtube.com\/watch?v=NEaUSP4YerM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/NEaUSP4YerM\/hqdefault.jpg",
        "ID":"NEaUSP4YerM",
        "Publish Time":"2017-09-18T14:55:30Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"I'm drawing a graph doesn't it look cool but I didn't know how until I watched a quest hello and welcome to stack quest stack quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about T Snee or tis knee to be honest I don't actually know how it's pronounced but it's gonna be clearly explained I know that bet also this stack quest is by request a couple of people put it in the comments below and I got a couple of emails from other people so I'm doing it because you guys want it here goes if you're watching this stack quest chances are you've seen an example of a tea sneak graph before what tea Snee does is it takes a high dimensional data set and reduces it to a low dimensional graph that retains a lot of the original information if you're not familiar with those terms of taking a high dimensional data set and reducing it to a low dimensional graph you might want to watch the stat quest for pca because i explain what that means in that video here's a basic 2d scatter plot let's do a walk through of how tea stain would transform this graph into a flat one dimensional plot on a number line I'm going to use this super simple example to explain the concepts behind tea Snee so that when you see it applied to a much larger data set a much more complex data set you'll still know how that graph was drawn note if we just projected the data onto one of the axes we just get a big mess that doesn't preserve the original clustering if we project it on to the y axis instead of two distinct clusters we just see a mishmash and the same thing happens if we just project the data onto the x axis what T Snee does is find a way to project data into a low dimensional space in this case the one-dimensional number line so that the clustering in the high dimensional space in this case the 2-dimensional scatterplot is preserved so let's step through the basic ideas of how tasty does this we'll start with the original scatter plot then we'll put the points on the number line in a random order from here on out T sneem moves these points a little bit at a time until it has clustered them let's figure out where to move this first point should it move a little to the left or a little to the right because it is part of this cluster in the two-dimensional scatter plot it wants to move closer to these points but at the same time these points are far away in the scatter plot so they push back these points attract while these points repel in this case the attraction is strongest so the point moves a little to the right BAM now let's move this point a little bit these points attract because they are close to each other in the two-dimensional scatter plot and this point repels a little bit because it is far from the point in the two-dimensional scatter plot so it moves a little closer to the other orange points double bam at each step a point on the line is attracted to points it is near in the scatterplot and repelled by points it is far from triple bail now that we've seen what tea Snee tries to do let's dive into the nitty-gritty details of how it does what it does step 1 determine the similarity of all the points in the scatter plot for this example let's focus on determining the similarities between this point and all of the other points first measure the distance between two points then plot that distance on a normal curve that is centered on the point of interest lastly draw a line from the point to the curve the length of that line is the unscaled similarity I made that terminology up but it'll make sense in just a bit so hold on now we calculate the unscaled similarity for this pair of points now we calculate the unscaled similarity for this pair of points and now we calculate the unscaled similarity for this pair of points etc etc etc using a normal distribution means that distant points have very low similarity values and close points have high similarity values ultimately we measure the distances between all of the points and the point of interest then plot them on a normal curve and then measure the distances from the points to the curve to get the unscaled similarity scores with respect to the point of interest the next step is to scale the unscaled similarities so that they add up to 1 um why do the similarity scores need to add up to 1 it has to do with something I didn't tell you earlier and to illustrate the concept I need to add a cluster that is half as dense as the others the width of the normal curve depends on the density of data near the point of interest less dense regions have wider curves so if these points have 1\/2 the density as these points and this curve is half as wide as this curve then scaling the similarity scores will make them the same for both clusters here's an example where I've worked out the math this curve has a standard deviation equal to 1 these are the unscaled similarity values this curve has a standard deviation equal to 2 these points are twice as far from the middle the unscaled similarity values are half of the other ones to scale the similarity scores so that they sum to one you take a score and you divided by the sum of all the scores that equals the scaled score here's how the math works out when the distribution has a standard deviation equals to one we get zero point eight two and zero point one eight as the scaled similarity scores and here's the math for when everything is spread out twice as much we get zero point eight two and zero point one eight the similarity scores on top are equal to the similarity scores below they are the same that implies that the scaled similarity scores for this relatively tight cluster are the same for this relatively loose cluster the reality is a little more complicated but only slightly T Snee has a perplexity parameter equal to the expected density around each point and that comes into play but these clusters are still more similar than you might expect now back to the original scatter plot we've calculated similarity scores for this point now we do it for this point and we do it for all the points one last thing and the scatter plot will be all set with similarity scores because the width of the distribution is based on the density of the surrounding data points the similarity score for this node might not be the same as the similarity to this node so T Snee just averages the two similarity scores from the two directions no big deal ultimately you end up with a matrix of similarity scores each row and column represents the similarity scores calculated from that point of interest red equals high similarity and white equals low similarity I've drawn the similarity from a point of interest to itself as dark red however it doesn't really make sense to say that a point is similar to itself because that doesn't help the clustering so T's knee actually defines that similarity as zero hooray were done calculating similarity scores for the scatter plot now we randomly project the data onto the number line and calculate similarity scores for the points on the number line just like before that means picking a point measuring a distance and lastly drawing a line from the point to a curve however this time we're using a t-distribution a t-distribution is a lot like a normal distribution except the tea isn't as tall in the middle and the tails are taller on the ends the t-distribution is the tea in tea stay we'll talk about why the t-distribution is used in a little bit so using a t-distribution we calculate unscaled similarity scores for all the points and then scale them like before like before we end up with a matrix of similarity scores but this matrix is a mess compared to the original matrix the goal of moving this point is we want to make this row look like this row t Snee moves the points a little bit at a time edit each step it chooses a direction that makes the matrix on the left more like the matrix on the right it uses small steps because it's a little bit like a chess game and can't be solved all at once instead it goes one move at a time BAM now to finally tell you why the t-distribution is used without it the clusters would all clump up in the middle and be harder to see triple bam and now we know how teeny works i've used a really simple example here but the concepts are the exact same for more complicated data sets hooray we've made it to the end of another exciting stack quest if you like this stack quest and want to see more like it please subscribe and if you have any ideas for future stack quests just put them in the comments below until next time quest on"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"t-SNE and UMAP for High-Dimensional Data Visualization",
        "Video Title":"Visualizing High Dimension Data Using UMAP Is A Piece Of Cake Now",
        "URL":"https:\/\/www.youtube.com\/watch?v=015vL0cJfA0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/015vL0cJfA0\/hqdefault.jpg",
        "ID":"015vL0cJfA0",
        "Publish Time":"2022-08-16T16:08:51Z",
        "Channel":"Krish Naik",
        "Channel ID":"UCNU_lfiiWBdtULKOw6X0Dig",
        "Transcript":"hello all my name is krishnayak and welcome to my youtube channel so guys it is always a very difficult task to basically visualize a higher dimension data let's say if you have 100 features let's say if you have 200 features yes we know some of the dimensionality reduction techniques like pca tsme and many more right but what is the problem with this dimensionality reduction techniques you will lose some amount of information let's say if i have 100 features and if i probably apply pca then and if i try to convert that into two dimensions or three dimension because as a human being right we can only see two dimension features or three dimension features right so if i try to convert this all features or reduce this dimensionality into two features or three features you know the problem with pca or tsne techniques are that you lose good amount of information and then you probably try to work with that specific data or apply some kind of machine learning algorithms today in this video i am going to talk about a new technique again this is a dimensionality reduction technique which is called as umap uniform manifold approximation in this specific technique okay similar to tsne it will try to reduce the number of dimensions but this is far more superior than pca or tsne techniques why i'm saying you i'll show you a practical example right now and we'll try to see that how umap can be used to basically visualize higher dimensions in short when we reduce this into some smaller n number of dimensions at that point of time like suppose if i reduce it to two dimensional three dimension the amount of information that is lost is little bit less when compared to pc or tsa and again there is an amazing research paper regarding this you should definitely have a look on to that if you probably want the theoretical explanation make sure that you hit the like button at least make it to 500 i'll definitely upload the theoretical explanation about your map but today in this video i really want to show you how umap actually works now before we go ahead there is a quick announcement regarding the tech neuron offered by i neuron so in tech neuron if you don't know it is an ott platform with 200 plus courses currently it has 240 courses and along with this you know you also have all the live classes that is included so like batches whichever we run in every month that will also be included in tech neuron and currently we are launching the lifetime offer which will be for a week this is just for the on the eve of independence day we really wanted to help many people out who are not able to afford other courses so through this they will be able to get all the other courses along with that all the live sessions also the price of the course has also been reduced to 10 000 rupees and if you're not satisfied just with 10 000 what you can do is that you can use a coupon code that is crush and then you will be getting a 20 discount so in 8 000 rupees all these courses along with many services seven days you'll be having a doubt clearing session you'll be having one to one mentorship you'll be having probably uh mock interviews resume discussion you can also raise a request with respect to any kind of courses that you want so yes uh the offer just uh is uh for another four to five days please utilize this opportunity okay now let me go back over here now over here i have first of all taken an irish data set here you can see sepal width sepal length petal width petal length and i've just created a scatter matrix and here you can see how my diagram is basically generated right so this is what is the plot that you can see and obviously this is not like this is just like a two-dimensional plot right with respect to different different features you can do the similar task with the help of c bond where you just need to write snh dot pair plot with the entire df and here you can just write hui as the species so that it can actually categorize so once you execute this here you will be able to see that you will be getting a pair plot wherein it will try to show you the correlation between each and every features right specifically independent features that is what we are actually focusing on right now now as you know this will probably take some amount of time so here you can see that your graph is ready now already you know that we have four features sepal width sepal length petal width petal length right so four dimension now let's see when we reduce this number of dimension into two with the help of tsne how my data will look like so here it is project the data into 2d with tsne so here i have taken all my features and then i'm just using tsne and when i write n underscore components is equal to 2 that basically means i'm converting that four dimension into two dimensions and then i do fit transform on the feature and then i'm plotting the scattered plot uh with respect to all these projections and my x and y value right and here you can see i'm also provided my labels along with the color that i really want to specify with respect to my output feature so once i probably execute this let's see how we will be able to see it and here is how clearly we can see fine we have we have reduced this number of dimensions into two right all the four features and here by seeing this yes you can clearly separate it but what if i make or project this into three dimensions okay and obviously if you have many number of features then again some amount of information is definitely lost now let's go ahead and see projecting the data into 3d with tsne right the same technique that is dimensionality reduction technique here you can also use pcm so here what i am doing i have taken n underscore components as three that basically means it is going to convert this into three dimension and that is the reason why i have used scatter underscore 3d plot okay so once i execute it and here you will be able to see this here obviously you'll be getting a three dimension so this is how it looks like now here you can see that okay fine my data points are completely jumbled right like yes there are a lot of overlapping still we are not able to properly see this entire dimensions you know they are still overlapped data points and yes it is not that good when compared to what we really want i really want this clusters to be completely separate now let's see this technique with the help of umap okay so first of all to start with umap you have to install slash pip install umap learn so once you install this the installation will happen i have already installed it then all you have to do is that apply umap and again it is an efficient technique when compared to pca and tsne and if i give an underscore components is equal to two that basically means it's going to convert into two dimension and then i've also made sure that i convert this into three dimensions so umap2d you map 3d i do fit transform on the specific feature and i'm scat i'm just creating a scattered plot with respect to the two dimension and with respect to the three dimension and once i see this you will be able to see how beautiful diagram i'm able to get with respect to two dimension right over here all the data points are clearly separable with respect to two dimension but the most interesting thing what we saw over here with respect to three dimension now you see with respect to three dimension the magic is here now see this how well your data points are clearly clustered right with respect to the data points that i have this is definitely preserving more information when compared to pca and tsne this is the recent technique that has actually come and you can definitely use dimensionality reduction using umap right what an amazing technique again uh i hope you are able to understand this video definitely for any dimensional reduction technique try to use umap it is an amazing library altogether yes try to make the likes to 500 at least i will also upload the theoretical video uh and we'll try to understand what is the in-depth mathematical intuition behind umap and how it is basically doing this dramatically reduction such that much information is not lost so yes this was it from my side i will see you all in the next video have a great day ahead keep on rocking and yeah i will see you all in the next video bye take care"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"t-SNE and UMAP for High-Dimensional Data Visualization",
        "Video Title":"Python Tutorial: t-SNE visualization of high-dimensional data",
        "URL":"https:\/\/www.youtube.com\/watch?v=85XaciPBCkw",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/85XaciPBCkw\/hqdefault.jpg",
        "ID":"85XaciPBCkw",
        "Publish Time":"2020-03-20T03:29:59Z",
        "Channel":"DataCamp",
        "Channel ID":"UC79Gv3mYp6zKiSwYemEik9A",
        "Transcript":"in this video you'll learn to apply t distributed stochastic neighbor embedding or t-sne while this may sound scary it's just a powerful technique to visualize high dimensional data using feature extraction t-sne will maximize the distance in two-dimensional space between observations that are most different in a high dimensional space because of this observations that are similar we'll be close to one another and may become clustered this is what happens when we apply tsne to the iris dataset we can see how to set those a species from the separate cluster while the other two are close together and therefore more similar however iris dataset only has four dimensions to start with so let's try this on a more challenging dataset our answer female body measurements dataset has 99 dimensions before we apply tsne we're going to remove all non numeric columns from the dataset by passing a list with unwanted column names to the pandas dataframe drop method t-sne doesn't work with monomer data such we could use a little trick like one hot encoding to get around this but we're using a different approach here will create a t-sne model with learning rate 50 while fitting to the dataset t-sne will try different configurations and evaluate these with an internal cost function hi learning rates will cause the algorithm to be more adventurous in the configuration it tries out while low learning rates will cost to be more conservative usually learning rates fall in the 10 to 1,000 range next we'll fit and transform the t-sne model to an emeritus set projector high dimensional data set onto an umpire a with two dimensions we'll assign these two dimensions back to original data set name enum X&Y we can now start plotting this data using C point scatterplot method on the X and y columns we just added the resulting plot shows one big cluster and in a sense this could have been expected there are no distinct groups of female body shapes with little in between instead there is a more continuous distribution of body shapes and thus one big cluster however using the categorical features we excluded from the analysis we can check if there are interesting structural patterns within this cluster the body mass index or BMI is a method to categorize people into weight groups regardless of their height I added a column BMI class to the data set with the BMI category for every person if we use this column name for the hue which is the color of the c1 scatterplot be able to see that the weight class indeed shows an interesting pattern when the 90-plus features in the dataset t-sne picked up that weight explains a lot of variants in a data set and use that to spread out points along x axis with underweight people on the left and overweight people on the right you've also added a column with hide categories to the data set if we use this hide class to control the hue of the points we'll be able to see that in the vertical Direction variance is explained by a person's height so all people are at the top of the plot and shorter people at the bottom conclusion the acini helps to visually explore dataset and identify the most important drivers of variants in body shapes now it's your turn to use Tizen e on the combined male and female"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing t-SNE and UMAP with Scikit-Learn",
        "Video Title":"StatQuest: t-SNE, Clearly Explained",
        "URL":"https:\/\/www.youtube.com\/watch?v=NEaUSP4YerM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/NEaUSP4YerM\/hqdefault.jpg",
        "ID":"NEaUSP4YerM",
        "Publish Time":"2017-09-18T14:55:30Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"I'm drawing a graph doesn't it look cool but I didn't know how until I watched a quest hello and welcome to stack quest stack quest is brought to you by the friendly folks in the genetics department at the University of North Carolina at Chapel Hill today we're going to be talking about T Snee or tis knee to be honest I don't actually know how it's pronounced but it's gonna be clearly explained I know that bet also this stack quest is by request a couple of people put it in the comments below and I got a couple of emails from other people so I'm doing it because you guys want it here goes if you're watching this stack quest chances are you've seen an example of a tea sneak graph before what tea Snee does is it takes a high dimensional data set and reduces it to a low dimensional graph that retains a lot of the original information if you're not familiar with those terms of taking a high dimensional data set and reducing it to a low dimensional graph you might want to watch the stat quest for pca because i explain what that means in that video here's a basic 2d scatter plot let's do a walk through of how tea stain would transform this graph into a flat one dimensional plot on a number line I'm going to use this super simple example to explain the concepts behind tea Snee so that when you see it applied to a much larger data set a much more complex data set you'll still know how that graph was drawn note if we just projected the data onto one of the axes we just get a big mess that doesn't preserve the original clustering if we project it on to the y axis instead of two distinct clusters we just see a mishmash and the same thing happens if we just project the data onto the x axis what T Snee does is find a way to project data into a low dimensional space in this case the one-dimensional number line so that the clustering in the high dimensional space in this case the 2-dimensional scatterplot is preserved so let's step through the basic ideas of how tasty does this we'll start with the original scatter plot then we'll put the points on the number line in a random order from here on out T sneem moves these points a little bit at a time until it has clustered them let's figure out where to move this first point should it move a little to the left or a little to the right because it is part of this cluster in the two-dimensional scatter plot it wants to move closer to these points but at the same time these points are far away in the scatter plot so they push back these points attract while these points repel in this case the attraction is strongest so the point moves a little to the right BAM now let's move this point a little bit these points attract because they are close to each other in the two-dimensional scatter plot and this point repels a little bit because it is far from the point in the two-dimensional scatter plot so it moves a little closer to the other orange points double bam at each step a point on the line is attracted to points it is near in the scatterplot and repelled by points it is far from triple bail now that we've seen what tea Snee tries to do let's dive into the nitty-gritty details of how it does what it does step 1 determine the similarity of all the points in the scatter plot for this example let's focus on determining the similarities between this point and all of the other points first measure the distance between two points then plot that distance on a normal curve that is centered on the point of interest lastly draw a line from the point to the curve the length of that line is the unscaled similarity I made that terminology up but it'll make sense in just a bit so hold on now we calculate the unscaled similarity for this pair of points now we calculate the unscaled similarity for this pair of points and now we calculate the unscaled similarity for this pair of points etc etc etc using a normal distribution means that distant points have very low similarity values and close points have high similarity values ultimately we measure the distances between all of the points and the point of interest then plot them on a normal curve and then measure the distances from the points to the curve to get the unscaled similarity scores with respect to the point of interest the next step is to scale the unscaled similarities so that they add up to 1 um why do the similarity scores need to add up to 1 it has to do with something I didn't tell you earlier and to illustrate the concept I need to add a cluster that is half as dense as the others the width of the normal curve depends on the density of data near the point of interest less dense regions have wider curves so if these points have 1\/2 the density as these points and this curve is half as wide as this curve then scaling the similarity scores will make them the same for both clusters here's an example where I've worked out the math this curve has a standard deviation equal to 1 these are the unscaled similarity values this curve has a standard deviation equal to 2 these points are twice as far from the middle the unscaled similarity values are half of the other ones to scale the similarity scores so that they sum to one you take a score and you divided by the sum of all the scores that equals the scaled score here's how the math works out when the distribution has a standard deviation equals to one we get zero point eight two and zero point one eight as the scaled similarity scores and here's the math for when everything is spread out twice as much we get zero point eight two and zero point one eight the similarity scores on top are equal to the similarity scores below they are the same that implies that the scaled similarity scores for this relatively tight cluster are the same for this relatively loose cluster the reality is a little more complicated but only slightly T Snee has a perplexity parameter equal to the expected density around each point and that comes into play but these clusters are still more similar than you might expect now back to the original scatter plot we've calculated similarity scores for this point now we do it for this point and we do it for all the points one last thing and the scatter plot will be all set with similarity scores because the width of the distribution is based on the density of the surrounding data points the similarity score for this node might not be the same as the similarity to this node so T Snee just averages the two similarity scores from the two directions no big deal ultimately you end up with a matrix of similarity scores each row and column represents the similarity scores calculated from that point of interest red equals high similarity and white equals low similarity I've drawn the similarity from a point of interest to itself as dark red however it doesn't really make sense to say that a point is similar to itself because that doesn't help the clustering so T's knee actually defines that similarity as zero hooray were done calculating similarity scores for the scatter plot now we randomly project the data onto the number line and calculate similarity scores for the points on the number line just like before that means picking a point measuring a distance and lastly drawing a line from the point to a curve however this time we're using a t-distribution a t-distribution is a lot like a normal distribution except the tea isn't as tall in the middle and the tails are taller on the ends the t-distribution is the tea in tea stay we'll talk about why the t-distribution is used in a little bit so using a t-distribution we calculate unscaled similarity scores for all the points and then scale them like before like before we end up with a matrix of similarity scores but this matrix is a mess compared to the original matrix the goal of moving this point is we want to make this row look like this row t Snee moves the points a little bit at a time edit each step it chooses a direction that makes the matrix on the left more like the matrix on the right it uses small steps because it's a little bit like a chess game and can't be solved all at once instead it goes one move at a time BAM now to finally tell you why the t-distribution is used without it the clusters would all clump up in the middle and be harder to see triple bam and now we know how teeny works i've used a really simple example here but the concepts are the exact same for more complicated data sets hooray we've made it to the end of another exciting stack quest if you like this stack quest and want to see more like it please subscribe and if you have any ideas for future stack quests just put them in the comments below until next time quest on"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing t-SNE and UMAP with Scikit-Learn",
        "Video Title":"T-SNE: Example in Scikit-Learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=yqGKdBNNLDI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/yqGKdBNNLDI\/hqdefault.jpg",
        "ID":"yqGKdBNNLDI",
        "Publish Time":"2022-11-14T16:05:10Z",
        "Channel":"Machine Learning Practice",
        "Channel ID":"UC8nFan5wplnzzsWLTjKGi0g",
        "Transcript":"we've been talking about the T sne algorithm and now it's time to look at a little bit of code all right I'm back into the same notebook that we've been working in I have restored the state to where we have our Swiss roll data set and so let's build a t s e model here and this particular one it is all uppercase this particular class does accept lots of different parameters I'm only going to use a few we've already talked about perplexity so let's start on the uh the small end of that and otherwise I'm going to use just all the default parameters the tsne class it is also one of those that does not provide an independent transform function we only have the fit transform function so I'm so I'm using it here I need an equal sign there we go when we're in the process of learning and that finished off pretty quickly so let's look at the let's look at the set of points that we embedded for this data set so there we go so one thing to notice about this particular uh embedding is that we have uh at least some amount of topology here we've got all the red points scattered out uh over here and those sort of give way to orange yellow cyan and ultimately blue what's interesting is that the Blue Points kind of split into this left hand side and this right hand side the other thing to notice is that the distribution of the points in this embedded space are very clumpy and and that's because we've used very small neighborhoods so let's start playing with that perplexity parameter in order to bring that uh to to improve uh the clumpiness of our distribution so I'm going to double that to 10. and that's probably going to take a little bit longer to execute that was not too bad so with with the 10 uh we actually have a little bit more convincing uh distribution as far as the color goes what's also nice is that there is sort of an acknowledgment of both dimensions of our original manifold so the the color encoding one and then the other one being in the width however we still have a lot of points that are very clumped up together so let's double perplexity again see if that changes anything all right here we're starting to address the clumpiness a bit there's still some fairly large areas like there's this big gap in here and a big gap in here and and again this is what this particular algorithm wants to achieve is more um one the Zero Dimensional manifolds where we have clusters so we're not going to be able to completely escape from this so I'm doubling perplexity one more time and here our distribution now now is uh a bit more convincing we still have the the Gap right there uh but uh other than that we don't have huge gaps there there are a variety of tiny gaps but uh the distribution is a lot more convincing okay so that's our Swiss roll data set let's go back to our Arrow data set now so here's our Arrow I'll execute that and now now we're back to this particular data set and the question is how well is it going to do with this switch from one dimensional manifold to two-dimensional manifold and we'll go back let's go back to the complexity of a perplexity of five and there we go that's a very interesting distribution uh We've sort of lost the the global distribution from the perspective of colors so it's interesting that we've got red here and orange and then uh and and then a bit of orange sitting out over here and then yellow sitting back over here so so clearly this algorithm has really split up the the space of points pretty dramatically and and in our original distribution the red were they were actually those points were actually connected very well to the orange so it's a bit of a surprise to have them split up in this way um likewise the the blue has a bit of an odd uh distribution to it although kind of have the far end the point of our Arrowhead sort of is sitting out over here and and then this is getting closer to uh to that one dimensional manifold um which is really this set of points and these points in here okay so let's Let's uh push perplexity up by a factor of two this should help uh with the that very substantial clustering that we're seeing there that's very interesting so so now the the red piece which is our feather of our Arrow uh actually has a reasonable representation in space right here and then this piece here that is our 1D at least part of our 1D uh manifold the there are other parts here and and here uh and here as well um and then and then the uh the head of the arrow is sort of split into two pieces that that's a very bizarre uh distribution but it is actually a little bit more convincing than the last let's double our perplexity again all right so so we actually have a bit more convincing distribution there's some gaps but most of it's mostly here so there's our our feather and uh and then the the shaft of the arrow is is this bit here uh and then this is the the head of the arrow over here and and what's nice is now with the the the red here and the cyan right in here we're actually acknowledging the two-dimensional nature of those manifolds all right one more time let's double that perplexity The Hope here is that we can end up with one uh connected representation and and we've achieved something closer to that now so there's our uh the the feather of our arrow is right there and it it acknowledges the two dimensions of the manifold and then there's the the the 1D manifold there and here is the the arrowhead itself and this Gap here is happening because uh again uh the points right along here in the original distribution are pretty spread out quite a lot more than the the rest of the uh distribution so so it's not properly identifying the fact that those are right near each other it's also interesting that it is coming to a point but it is taking its time to get to that point I'm just out of curiosity let's go up one bit more up to a perplexity of 60 see what happens with that and actually if you were to re-execute this multiple times even with the same perplexity you'll end up with different answers that is doing uh somewhat of a random search all right so that really didn't change things uh all that much uh things are a bit better connected there but uh otherwise they're they're the same all right so that's our exploration of the tsne algorithm and how it works with at least these very simple uh data sets and this is something to to be playing with as as you get into larger data sets and in particular as you're trying to visualize the data themselves and uh also your results this is a good way to try to capture your results in some low dimensional way so so that you as a human can use your own visual processing capabilities to understand what's happening"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing t-SNE and UMAP with Scikit-Learn",
        "Video Title":"t-SNE Dimensionality Reduction with Scikit-Learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=DtFQAJmlID0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/DtFQAJmlID0\/hqdefault.jpg",
        "ID":"DtFQAJmlID0",
        "Publish Time":"2022-11-21T09:10:42Z",
        "Channel":"Nono Mart\u00ednez Alonso",
        "Channel ID":"UCTn8RnMFvQWxSOZBo06e_Ew",
        "Transcript":"hi everyone it's Nono here and this is a Hands-On tutorial on dimensionality reduction we're gonna try to do that with scikit-learn and the Disney implementation that comes in this package we'll try to get the Coatings that we've obtained from another neural network put our data set inside of scikit-learn and try to reduce the dimensions of our data set into 2D so we can plot it into a 2d map or an image and we can understand the similarities or how our data set looks when you compare one input to the other or one data sample to the other here so let's get started so I'm getting material from the Hands-On machine learning with scikit-learn Keras intention of a book by orlene giron that I'm encouraging you to get or or like learn from if you want to get into machine learning specifically about chapter 17 that was Auto encoders and guns there is a lot of theory there that or or even handsome practice that we learn from there are many ways in which you can do this stacked out encoders but long story short you try to input something and get the same in the other way but on the way you're training an encoder so you're obtaining these coatings on the right and then you have another second part of the network that is a decoder which from those codings tries to regenerate the original output these are networks that are mainly used for noise removal filters not for Generation because the the output are usually noisy but if you get guns on the game and other types of Networks these are good architectures for generating images so this is what we would imagine that we can obtain this is uh Disney flattening a highly dimensional space with these libraries the Disney Library from SK learn manifold and this is really pixelated because this is a really bad image but I have the book here with me so yeah we're going to try to visualize that okay so in live stream so live 33 and 34. we did see stacked Auto encoders we made I believe two versions of a collab notebook in there we were actually having that architecture in which we were taking in an image so that would be like maybe a PNG I think that was 28 by 28 uh we were encoding that into a 30 digits right so these were the Coatings of our Network and then we were getting that back into another image that is 25 by 25 that was the the Reconstruction and this was our input so we tried to do here is to train this part which is an encoder and uh this part here that is a decoder what the encoder learns um how to do right so the encoder learns how to take an image so whatever it is and it's 28 by 28 which is 768 pixels into a 1D tensor array that has just is 1 by 30 so it has 30 digits right it's just 30 digits it's not pixels because here actually we have RGB so three channels for each of those pixels all right so we're trying to get and I'm gonna go to the next page now trying to get a network where we put an image right so we have our Network here which is a decoder and we're gonna get so this is our input and this is our codings and those Coatings that I mentioned are going to be 130 so 30 digits will then input into Disney and are going to be able to plot on a 2D image and hopefully we're gonna get clusters that belong to different classes so for example this could be type 1 or type 2. the problem is that the data set that we're going to use those sketches that I showed before those don't really have a like we don't have a classifier to classify them so we'll just see how the data samples look similarity between them so we're just going to try to do that let's just add here live 38 so this is like 38 and it's 22nd of July all right so I'm gonna read here the the comment from Ricardo so I've already used Disney Let me see oh message retracted I was gonna read it in Ricardo but you delete it before I could do that so if you want to rewrite if not it's fine up to you I was now I'm curious about what you were gonna say that you've already used a Disney okay so let's keep going we have a set of 105 images we need to recover our notebook and we need to make a notebook with Psy kit learn all right so this is a data set and this has 105 sketches from the website if you go into the live repository so this is the repository of the live streams in live stream 35 you have a typescript script that will guide you step by step on how to use an asynchronous pipeline to download those images from my RSS feed that is here but you can actually change the url of the RSS feed on a few properties to download images from another source in this case we were just getting the images from here so you can go there to take a look at that I was mentioning we have this 105 images these ones are 2500 pixels height so the purpose of having the the images down to to 28 pixels is to be able to put them into the Keras model and have the training be a bit faster so format data set train stacked Auto encoder data set conversion to codings and then our coatings we're gonna go into Disney and then plot them all right so we got Google call up here and we can now access the stacked out encoders notebooks let's see what parts of the code we need from the previous notebooks the first thing that we have is that we have the fashion memes data set and I want to do this with our own data so what I'm going to do first is I'm gonna add a text block here I'm going to say load film data set and this is like images are 20 by 28 pixels RGB then the second thing that I said is that we need to Define encoder and decoder define stack Auto coder then train model and then we'll have to reality reduction with this thing all right let's take a look at what we have so we have tensorflow 2.5.0 and later we'd have to encode entire data set we have python 3.7 we have a CPU and we don't have any debuts we can select the GPU here to make our notebook faster so this is now going to allocate it for you and initialize a new runtime so now we run this again we're going to have the same version of tensors on python so now we have a GPU device it's a Tesla T4 where do we have our data set how are we going to put our data set here and maybe the acsds that we just drag and drop because this is a really small data set the problem is that this might you know require that we do that every single time that we want to do this but I'm gonna go into this sketches folder I'm going to use this script to to see my file so it's just I go into the folder and I just call this which is just zip in the file with the same name that the folder has so maybe we just call it sketches and this is 352 KV so this is super small we're going to Simply drag and drop that into our notebook a reminder that files will get deleted when the runtime is recycled so if you close your notebook go away and then come back the data there is not going to be there so there are ways in which you can download those files from other source or upload them somewhere I have a video here on how to auto download files from Google codelab and this is something that I really like because that way you can run your notebook and then your files end up on your downloads folder and you don't have to worry about that limitation now we have this here and I guess we could unzip them load custom data sets we're going to do some preparation here we're in the content folder which is this one here so we can do and see deep sketches dot zip if we put here that this process is silent so we don't see anything except for that one thing that we can do to avoid these sort of questions is we can just say remove all the jpegs in this folder and then unzip that way everything gets removed and then zipped again we can specify here to unzip on sketches that way we unzip to a directory instead of to the main problem all right so now we have our 28 by 28 pixel sketches here which is our data set which is super small now we need to see how to load that in the same data format that we had our data set on the previous live stream so this notebook here you can see we're downloading with this function here so tf.datasets fashion Eminence load data this gives us data in a really specific format a 28 by 28 pixels and they're 55 000 so we want to get our data in the same format but in um having less less of them we only have 105 samples which is going to overfeed a lot and this is not something that I had thought of but at least we can set up this notebook so we load all of our data we have extreme full White Dream full and there's the test let's try to load our data with tensorflow let's take a look here so just to see if this works so I'm going to import pathlib and I'm going to call Path lab path sketches and then here we're gonna do data there and then our pattern here is going to be just everything here with jpeg all right let's see if that executes all right we get shapes types strings this was good before just that I don't think we even need these things if we just put here sketches but keep it for later just in case I think I was doing something okay the problem is that these are strings and they don't have a shape so we could say for image in print image so what we have here is a tensor slice data set which doesn't have any shape because if the type is string and with those train files so we can say here train files and we can say here file file we can print the file and yeah again we get this binary string here that is just a path to the string so we need a function now to decode those images right so the code jpeg we're going to define a function that is going to decode an input image and convert the compressed string to a 3D viewing a tensor we're gonna need to call on train files we'll map and then call a function that we haven't defined yet that is load image drain and numpy Lane calls we're going to use auto-tune and of course this is going to fail because that function doesn't exist so we're loading the data set and we're going to Define that load image train function load image train we get the file path and what we do is that input image is gonna read the file so the file path and that input image of the code our input image and we're just going to return that input image and we could do other processing here that I'll show here and we need now that the code input image function that I was going to Define below before so we're going to get TF image the code jpeg with that image data that we have stained and we have three channels what this is doing is that we're converting the compressed string to a 3D unit 8 tensor and now what we do is we convert two floats in the zero to one range so we use TF image convert image D type and we get a TF flow 32. now we would maybe here resize the image to the desired size which you know it's going to be here 28 By by 28 but if we do this then the dataset doesn't need to infer the image size and we can do other operations so we can do here image with an image height and anti-avs we can choose here either true or false so a hyper parameter here I will put here our settings and we'll put here image width equals 28 image height is going to also be 28. we're here so we're loading and then we resize the image so this is going to be the train data which we don't really have test data so we might need to do some split later but for now let's see what we got here so we got this train we have a parallel data set Which shapes 28 by 28 by three there's one thing that is different here from the previews so we were getting D shape right we were actually getting just black and white well I mean we do want the images to be black to be color but maybe that makes this uh problem a bit more complex a bit different than what we had before and we will have to change the the neural network architecture so we have 28 28 3. these are the shapes train if we do a train data set and we run train cache Shuffle buffer size which we need to decide hatch batch size all right so this is going to ensure that we have more data samples and we now need to Define these two hyper parameters so let's say our batches are 32 and we're going to see now if this gives us something different so repeat data set shapes we can now do for sample in train data set take 128 wallets let's do just 34 print sample shape all right so we're getting batches and so let's say we just get 32 multiplied by 3 plus 8 that is 104 right so all we can get here is these four batches and then we reset because we only have 105 samples from the data set they Shuffle and then they start again here because 105 we cannot really well 35 so our batch could be for example 35 and now maybe we still get oh because of the buffer size 35. oh we still get 34. all right let's actually leave it here we can get many different batches so we can say 10 batches and we'll have more more samples here and um we don't really need this data but but we have an RGB image one thing that we can do here is so care us to black and white RGB to grayscale so we can actually convert to grayscale so TF Keras foreign to grayscale image all right let's try that out now we do have the shape that we wanted so now these are going to be the same proportions so 28 by 28 we can bring this all to to all the other notebook let's just try to debug with this all right so we have this here okay import Maybe import numpy SMP and here we have for example X Train full shape okay so this is a 6000 of 28 by 28 so we get the first one what's the shape 28 by 28 and we printed 28 by 28 so there's a 28 items of 28 digits if we do have something that is like our data right we have here um this sample 20 28 1. so this means let's let's just print it so let's take this one batch this is the opposite of expand we have this sample now here that we've reshaped so we get 10 we just get all those all right we have a way in which we can do that we probably want to because this is returning the images and here we're decoding the input so maybe input image here we do the reshape and we say that our input image is going to be reshaped to 28 by 28. let's see if this works all right nice now we don't need to do any reshaping so we just print the sample shape our data set has a set of images that are 20 by 28 and I guess individually each of these is 32 by 2828 which we saw up there what's the shape of this 28 by 28 and the 4 is 6 000. all right awesome so we're loading our data set in a good way that we can now use it we're trying to load a data set to do a dimension ID reduction exercise with the Disney implementation that comes with scikit-learn so we're loading these images in 28 by 28 size really small to see with your eyes right these are super super small images that are pixelated and we drag and drop those to here as a zip file so we unzip those files this removes the folder and then unzips great so we load the data set here we actually don't need to do this here we can put these functions we have the data set here here we can load a set of things so if I do train data set take 10 and I get the first one let's comment this out take that this is not subscriptable all right we made that on an object that is not gonna end up anytime so maybe just make it into awesome so let's see shape convert to tensor right I don't know this this seems a bit like a hack but I might get there all I'm trying to do is get the the data set that we have here right so we have a data set and if we get the let's say the this is actually a batch on our data so for image in batch we can print the image and maybe just take one batch so we have the image shape is this 28 by 28 and we could actually plot this let's see what we can get um not sure if I can do here a prefetch so here I'm getting at least a bat so we can say can I get just the first batch all right I think this might work for now right tensorflow data set to numpy array we have the train data set we can train with that and it seems like that's gonna be okay so let's say we just load the data manually so load images manually import image so we can do import glob from glove import club we do glove sketches we get all the paths right so for file path in this we can say our image is going to be image open file path and we can print for example the image with image size so it's 28 by 28 once we have the path actually even if we don't use spill we do have this load image trained so we can do load image train and then we put here the file path and then we can say print image shape we get images are 28 by 28 well we could say okay or like let's say X strain 4 could be that and we just do X strain 4 append and this image and then we print X train for shape which I think we need to make into an MP array but all right so we do get the the shape that we wanted we don't need to print here so we're just appending we don't need pill right so this is what I wanted to do I wanted to try to do it with with the TF data library and once we have these are also black and white these are the the type that we wanted and so we could say data so this is so our data is going to be this right and then we can say okay we have data on the shape of this and because we don't have more data we're gonna have to say x strain 4 is going to be so 100 um and five well 104 I'm not sure why it is 104. we do here data and we get the first 104 multiplied by 80 so let's do 84 so we leave 20 x about it all right so extreme full is going to be just is the data and white rainfall so if we get here we get the X strain for zero they're already converted to V type that is what they're doing here to flow 32 divided by 255. that's what we were doing here so we don't need to do that so all the data and now we have extrane white train so we get here x train so we're gonna get here um minus 286 24 next chain four minus twenty and this would be X valid and we'll print here x strain x value shapes shape shape train probably the other way around all right so we got the entire data set is this shape then we have all the the data and then we have the training and the valuation data set cool so we can get the encoder decoder architecture so we're going to copy and paste here for expediency otherwise we're not going to get there we have the encoder and we have the decoder and we have the Stacked out encoder which we're gonna paste here awesome so we have our stacked out encoder and now here we defined it a rounded accuracy which is our custom metric we compiled the model with binary cross entropy and this learning rate and our custom render accuracy metric so we could run everything Above This cell so run before we have a route encoder we Define our metric compile this we get a summary of the model and we have to train now so this is the actual cell that trains and the input and the output data is the same so the goal that we're trying to reach is the same so we use the same input and output data set on one training all right so this super fast so maybe we can train for more epochs it seems we might be overfitting there because we don't really have too many samples we just have 104 images we have gone through this and this and now we need to see if we can get the Coatings so we could get a sub network of what we had before so I believe we can pass an image through our stacked encoder so we could X train zero all right so if we pass the image through the stack the encoder we're passing the first image on our data set here we're getting the Coatings which are 30 digits and what I'm not completely sure so this is after training for the epochs let's say we retrain let's say we retrain for well if I retrain now I think continue from where we were so we have the same accuracy so we might need to reset our model here and train again so you see that the accuracy is starting from or the Run starts from 53 70 and 96 we get really fast we get really high so I'm just gonna put here one so we get this to this step and I'm gonna go through all the cells again we have this model here and this was with 40 epochs we're gonna try to do it now just with one epoch all right so the encoder Network right which we defined here and we included here in the auto encoder has actually been trained and its weights have changed so when we run this feed method the outer encoder has inside the encoder and we actually have this network here that can get this image the simple image says 28 by 28 image and then generate the Coatings and what we have here in our 30 Coatings right what we talked about before so these are those 30 Coatings right we were talking about a 28 by 28 image that was passing through a network so we were training in this Loop of encoder decoder and in the middle this encoder part ends up with 28 by 28 on the left and then 30 on the right and we are actually seeing that here so if we look at the shape of X trains if we look here is 28 by 28 I put it through the encoder and as promised if we get the shape of this yeah it's 1 by 30. so it's a linear tensor that has 30 digits inside of it and you know promise achieved if we change the the input so I'm taking different images here the coatings on the bottom slightly change they change a bit because the input is a different image they're different so 173 and 152 so I'm going to go to 50 netbox and I'm going to rerun these cells and retrain right so we retrain there now our render accuracy is again at 98.16 which I believe is the same that we'll get if we go to 40 epochs and what I'm going to do is I'm going to put here 15 epochs but I expect to see something similar to below let's do actually 10 epochs and really find everything from scratch here so that's the netbox I'm going to put here to netbox and now we're gonna put Portia epochs and I do think we need a larger data set so for the epochs we run this minus one everything one thing that we can do is that if we run this by the Stacked decoder right so if I now get this output from here so let's say the coding and I run this my print Coatings you see them so now you can pass the Coatings back here and we get back so this is a generated image generated image shape right so that's again a 20 by 28 image that is black and white and if we get the first one of the batch we get an image so let's take a look at how to plot those because I have the code here so plot image we have a helper method plot image show reconstructions right we just need to put this Helper and will have to do some cleanup not gonna do it in today parade helper here so we can plot the image and I'm not sure if this changes anything but if I do here predict I get a tensor and then if I do predict I get something different but it's at the end of the day is the same result and then I get an array as well all right we wanted to plot this image right so we're going to upload the generated image let's see what that does plot is not defined all right PLT we have another dependency and then we go back and we got our plot image so it's learning something we got the the input that we're using here we can also plot it this is the input that we're inputting to the network and I think we also have something to plot the digits maybe not but digits the codings so if we plot here those codings so let's say plot digits and we do those Coatings zero and we'll probably have to put all of these dependencies in the same place all right so those are my codings so they go from there are 30 digits from zero to one and they're 30 and depending on when you load them so like at what point or after what training they they change all right we Define the model we do this definition here we train summary and then we're going to train for 10 Imports so still the model always gets to the same place right 9932 accuracy and that's 10 epox so we still have the same input we could maybe take this one or this one right these are different sketches and we passed that through this one that says there wasn't 10 epochs and now we can plot our digits it's slightly different than before we generate an image and now we predict so this is a slightly different than before and we could actually just comment this out we're gonna take another image so let's say image image 80 so let's say input is going to be X train 40 so now we just do this so you just get the input you don't need to do a lot more input and now you can choose from here with a number so we say the input is a sketch 42 we have that and this doesn't do anything is any something now we plot our digits and now we reconstruct our image which doesn't look anything like this at all so I don't really understand what's going on with with the data say it might be it might be that it's overfitting and it's just like learning something that it does something really similar for everything for all the images the the fashion companies data set has 60 000. so that's probably one of the things why it is performing so much better right so now we're going after all this effort to load our custom data set we're going to Define here their data set so that we're going to have the same extra index valid that's it I don't think we need this part we do need this one so we're gonna load that we're going to load our model we're going to Define everything the same way and we're gonna train for 10 inbox let's see how long this takes this takes a bit longer now because we have a bunch more images we have 55 000 images for for training and 5000 for valuation and uh you should be uh thinking right now the accuracy is slowly going up is not as fast as it was going up before so we can see here this now number keeps changing and that's good right this means that there is some actual learning that is happening not just that we're overfitting in a data set and learning a few things I I would be interested in seeing this network how many data points it needs to give any like some information that makes sense all right so the model is trained now we can put input 14 here that is this that looks like a shoe I also looked at the data set is inverted so that also might make it learn in a different way so we get the codings we still get oh five out of the last 10 calls triggered the function retracing is expensive repeatedly all right but we get our data and now we get our codings these Coatings make a lot more sense it's it's a bit more granular it's not just like a lot of black and we get our generated image here all right so it doesn't look anything like it but at least it's a shoe so it's learned something and and it seems like we're able to reconstruct the output from what we have let's actually just train for 10 more epochs and and see what we get just to to let you know while that's training coding is here actually represent the issue if we modify this coding we're going to get a different image that's like a puppeteer like we we have an image we're encoding it into this image and then we decode it back so if we generate images like this one we will get images like this there is a correlation of course because of the weights that the neural network has learned from the Coatings to the output it seems like we're almost done we have a bit more accuracy not too much more and we are going to use the same input I'm gonna do one thing I'm Gonna Leave This here and I'm gonna compare so how the Coatings are different they are almost the same so I say let's regenerate this just in case generate the Coatings they seem to to be almost the same let's see if the decoder has learned anything else so we have the image here we're going to predict and now so I'm going to put it here so I'm going to regenerate a new image and I can already see some changes in there I'm Gonna Save I made an errors I made a mistake this has changed right so we can compare these two images and they're different so there is something else that the network is learning while we're retraining what is funny is that I forgot to change this variable here so this should actually say input because we were continuously using X strain 0 while we were visualizing here the number 14. so if I actually do this right now you're gonna see how the codings that we get are going to be super different so look at the codings now so these Coatings here are super different right these now doing code D shoe we can see is that we go to zero and we plot that image we indeed have something super similar to that boot that we were generating before which is super funny so we're gonna go back to the shoe I'm gonna go back and generate the Coatings and now I'm going to generate that shoe and I need to predict before so predict I'm doing that shoe awesome so now our decoder knows how to generate a shoot so what this brings us or where these brings us is it's actually trained for 50 more epochs something that's going to take a bit and let's try to take the rounded accuracy app let's see how the network learns differently and later we're gonna go back we're gonna use the data set of the sketches and actually verify if there's something that is being learned because before even though we were training we were not changing the actual input that we were predicting with on the encoder the Coatings were not really changing so I do still have hope that we are going to see some some things there and that this is going to improve of it all right maybe I can yeah no I'm not gonna do that all right okay we've trained for I believe 50 more epochs right I think I don't know my be 10 plus 50 so we can see those here so the Coatings do change a bit and now when we plot them so these were the ones from before so when we plot them they're really different and let's see what the generated image looks like now so we put a block in here we need to Define this here 14 we put this here we plot our Coatings we this huh what am I doing wrong now input oh okay so same error as before so we left this on the old code so that's why we were erroring here so we get the Coatings did these look a bit more like the ones below this look back again differently and now we predict and now we plot okay so we're getting you know you see here stripe and I would say it's hard to notice which one looks more similar this is a bit more refined because we've trained for longer so I would say maybe this one so this is like uh 10 this is uh 10 plus 50 let me show you here and this is the input this is the original so this is the actual input so which one looks more similar not sure they're more or less the same so what I was mentioning before so we try what I said is that these Coatings right if we look at the Coatings what they really are they're not more than an array contained inside of an array so we could get this here and we could say my Coatings could be an MP array with this data and now we print the Coatings and now we get exactly the same but this is float32 type and we could probably do that as type so I get the Coatings this way I plot them they're the same because I just copied and then we predict we get the image right let's put that image below so those are the same image and I'm gonna bring these we'll leave them here so now what I'm gonna do is I have these Coatings here and I'm gonna copy them so I'm gonna change them them a lot so maybe we put here A4 this is going to be an eight it's gonna be -4 is going to be 13 is going to be one right so we change those Coatings and now instead of the one we have above we have the one below and when I generate the image that is what happens we've changed the things and there are some parts of the image that still look the same but by changing numbers we're changing different features on this image and one can imagine that now if I do put this one here to zero and I plot again now we have just changed slightly really something here right so instead of white we get this gray instead of this gray we get this white and now I can predict again and generate the image and now that changes back again and looks again more like a shoe just changing one number like changes the app a lot so what we're going to make sure that we don't get confused later and we just use the the actual prediction function and we're gonna go back and comment to use fashion Eminence data set instead right so we comment this out and now what we just want to go back to where we are I think everything right so we don't really need to expand twice here and we don't really need two Coatings and we don't really need three visualizations so we're gonna do is that we're gonna go back here we're gonna run restart and run all so it's gonna restart the runtime of our jupyter notebook in Cola and it's going to execute all of our cells once again and yeah hopefully we can see something with our data set so this has trained we're at the same accuracy that we were able to get before but now we have an image that if I set here 60 we can see a different input so maybe we can just merge these ones because it doesn't we don't really care that these are just one cell so we're going to remove this other two so let's see if six okay so this is different right from everything else so we're gonna go and we're gonna run the Coatings which are a bit different this doesn't seem like it's changing a lot so if we go now with 16 we have the input we're actually predicting on the input we plot the digits here so we're going to plot the digits again and they barely change so we do this and we do this yeah so this is changing but it's barely changing it's just learning like a real weird map mapping let's do here save this as extreme sketches X valid sketches equals x strain x value all right so we're going to rerun everything so we're going to retrain now and regenerate the images so it seems fairly simple so we have the scikit-learn implementation which is including a scalar manifold Disney and all we need to do is something like this more or less we're going to do the coding before we go today and let's see how this went so we have these pans and we get pens which is pretty good we have a lot of data involved in in these experiment right we could get these again like hand generated Coatings which match these ones here pretty similar and we get this which is a completely different output again let's comment that out and instead of getting an input X strain so we're going to get an input X train sketches and we're going to get the fifth sketch so now we're talking about this one and uh one thing that I mentioned before is the inversion of those so let's actually put this separate so we put other logic here current data set or Force custom loaded data set all right so we are going to load that and we're gonna plot so the input minus one this would load the original image and this one would load like a separate sketch the interesting part here is that we have images that correspond to to my drawings and not to clothing we want to see if we obtain the Coatings of one of my sketches what is the the corresponding image from on the decoder that we're going to obtain all right this one for example so I think what we need to do is say here input equals one minus input to get it negative so we don't have to do this here and now we can do this we can extract the Coatings once again and we generate the image and then we plot it all right so you can see right like it's activating that part here so it's trying to make it into a shirt and it's a good experiment it's kind of activating or encoding with one data set and then obtaining with another one good thing is if we just comment this out we still have the the proper data that we needed we just need to say okay from SK learn manifold what we saw before import Disney right so we import that library and now we say x valid because otherwise we're going to have too many we say stacked encoder predict X valid and we do Disney equals Disney so we instantiate this library and then we do x value to D equals Disney feed transform X valid compressed all right so I'm gonna run this and this is this is processing and the next thing that we're gonna do is that we'll plot make a scatter plot with the X valid to the everything and the 0 x valid to the everything on the one c y valid do we have y valid I don't know if I removed that before okay so we need that for some reason and we just need here S10 cmap top 10. yeah and these are the categories because we know the categories and we know that there are 10 different classes so scholar shape array of list of colors or color options color uh scalar array like shape option the marker size in points alright cool let's try this X valid is not defined all right cool all right so these actually matches in some ways kind of changing the axis what I have on the on the book so I think maybe if I changed foreign yeah it's changing something on the axis I don't know why but it matches what I have on the book yeah so if we didn't if we didn't really have the call so the valid so if we print here just so you see why valid and I take like maybe 10 right these are the classes of the data right so we have here x valid and if I take 10 you actually have the images right these are the images that were supposedly plotted in there and then here we have the colors if I were not to provide that we'll just get a set of of um plot right and you have to think that those have similarities among them so this is me right this is the basis of what we were looking at I want to continue further with this so continue I'm looking at this a bit more and developing this further because right now we just see like plots here we don't even see images or anything and it would be good to load some images here or like even have an interactive board where we display the the samples and what else can we do can I try once more we're gonna train for this is super fast so let's do 100 epochs and we're not gonna have X valid this time so we're just going to remove this part and then we're going to reset and run all we're going to be looking at more Dimension ID reduction algorithms all right so this goes here okay what does this mean X valid I I don't really want X valid here we want the actual thing so let's actually do here x train compressed X trained to the X strain and that should be it all right so it seems like it still is fitting some in some way these these drawings by similarity I don't know that they're that different for them to learn any other things they're spread on on Space let's say and and I really want to now see the plots in this individual sketches and see which ones they are can I provide here a like an array or something one like zero one two remember now what the shape of this was 84 with X and Y it's size 84. all right so range let's actually make a list manually so colors equal that and for e in range 84 colors append I Colors Let's see all right so we get 83 and then we can put that here colors we get 80 four colors and that plots all right that is useless but we're going to do something just at least to Cluster them so we're going to say equals zero and E plus one and then when I if I is greater than 9 I is going to be zero and J is going to go up so J plus one so we have less colors so let's see how those colors play out great we have too many colors still so we might just need let's say we just have three clusters so 84 divided by 3 that's 28 so is it 25 right we have now three colors so it means that this is telling us that drawings that are not initially together right that this is not the order they're like being clustered in different ways what do we have here visualize cutter with images let's try to just copy paste this one which you shouldn't really copy and paste code but because we're finishing up let's see if we get that stretch call all right so we got this here and we have this so plot annotation bonding box um offset image okay let's see so we can Define this so visualize scatter with images let's see oh that is awesome all right so copy pasting we got a pretty good resolution image I think and I might be able to maybe pump this up can I reshape array into shape oh reshape not rescale okay image soon let's do maybe four because this is these are too small great this is pretty awesome maybe let's just do two I don't know why it is coloring with that foreign super good all right we got it so um I don't I don't think we're gonna you know we're gonna get too far with this but long story short I think we got it we got where we wanted so it seems in some way it doesn't seem like it learned too many different features it just seems like the the darkest ones are or like the more infilled ones are on the bottom left these ones are the ones that are from some like people I'm here I think it would be a bit crazy if we do that with a segment of of the other ones so let's try to just copy this Disney sketches and we're now going to copy these and paste them I don't know if I can paste them here render Disney images of fashion mnist okay all right so how we're gonna do this we can remove this we don't really need to print this anymore we do have this put it here put this here and we do want the colors we don't really need this anymore and X train to the and extreme maybe we're gonna have more images x-valid maybe let's just pick 500 images for now we need to train the model and that's going to take a bit of time so let's put this back um restart and run off there'll be things that are not gonna work but uh we'll get there and these probably the thing that's gonna take the longest so we'll start again all right so we're running again I think we're still using a GPU and you should get a cola Pro if you're going to if you're going to be training models for a long Cola Pro it's 10 bucks a month so definitely worth it all right so we're training five a six out of ten we're almost there so we should be now done with training we are great and we probably gonna see some error below but let's see probably not I don't think this is gonna get completed this is too much to be plotted all right so I'm gonna interrupt that one because what I want to really get is here so we do that and then visualize okay and we run this helper then we run this cell once again X train to the x value to the all right whatever so we do here cmap gray run that again and we're gonna do that here cmap great R this is indeed pretty pretty cool okay what this tells me is that you know we haven't trained for long enough probably I'm gonna get more samples so I'm gonna take 2 500. so let's see maybe it's a bit too much okay that works that's a cool image right I don't think these ones are similar not sure if we're doing anything wrong here let me see if there's anything so X valid fixed valid compressed that's about it with extreme all right X valid images okay so we're x value to 500 right let's see yeah this is like looks wrong definitely okay Ah that's so much better that's awesome okay so this is this is really cool so this is awesome but just with then um then epochs this looks so good so let's parametrize this then because this maybe we can do it even better so we'll do input it's gonna be this and we'll just predict on the input and maybe it's inputs all right I'm gonna generate this just to make sure that we are getting the the same so zero zero zero one and uh what we're gonna do right now is I'm going to close this thing so comment this is being plotted which is pretty cool this has now changed I'm sure why it changed for some weird reason and we're just gonna train for longer just to see how that image actually changes how the Sorting changes so we're gonna train here for 20 epochs and when we have the model trained these will be trained for 30 epochs total and then we'll be able to get this thing let's check that the images on the the other data set at all so yes I think this was okay so we have the X train an extreme and now this should be running so this is the comparison moment so we're gonna take a look at whether training for 20 more epochs is improving anything of our result and maybe we have the access changed again I don't understand why all right so it doesn't give us the same access all the time which sucks so I cannot compare so we're sorting Shoes by similarities so now you know we could create a recommendation system just with these images where if you pick this dress you could propose a few other things that are around it right support the channel by liking the video if you're enjoying or if you just want to support the work I'm doing here and subscribe if you want to get notified when I go live next or when I upload new videos it was known about that also here thanks a lot for being there I would appreciate your comments if you have any questions any suggestions leave them on the video comments and um yeah I'll see you next time bye"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing t-SNE and UMAP with Scikit-Learn",
        "Video Title":"Visualizing High Dimension Data Using UMAP Is A Piece Of Cake Now",
        "URL":"https:\/\/www.youtube.com\/watch?v=015vL0cJfA0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/015vL0cJfA0\/hqdefault.jpg",
        "ID":"015vL0cJfA0",
        "Publish Time":"2022-08-16T16:08:51Z",
        "Channel":"Krish Naik",
        "Channel ID":"UCNU_lfiiWBdtULKOw6X0Dig",
        "Transcript":"hello all my name is krishnayak and welcome to my youtube channel so guys it is always a very difficult task to basically visualize a higher dimension data let's say if you have 100 features let's say if you have 200 features yes we know some of the dimensionality reduction techniques like pca tsme and many more right but what is the problem with this dimensionality reduction techniques you will lose some amount of information let's say if i have 100 features and if i probably apply pca then and if i try to convert that into two dimensions or three dimension because as a human being right we can only see two dimension features or three dimension features right so if i try to convert this all features or reduce this dimensionality into two features or three features you know the problem with pca or tsne techniques are that you lose good amount of information and then you probably try to work with that specific data or apply some kind of machine learning algorithms today in this video i am going to talk about a new technique again this is a dimensionality reduction technique which is called as umap uniform manifold approximation in this specific technique okay similar to tsne it will try to reduce the number of dimensions but this is far more superior than pca or tsne techniques why i'm saying you i'll show you a practical example right now and we'll try to see that how umap can be used to basically visualize higher dimensions in short when we reduce this into some smaller n number of dimensions at that point of time like suppose if i reduce it to two dimensional three dimension the amount of information that is lost is little bit less when compared to pc or tsa and again there is an amazing research paper regarding this you should definitely have a look on to that if you probably want the theoretical explanation make sure that you hit the like button at least make it to 500 i'll definitely upload the theoretical explanation about your map but today in this video i really want to show you how umap actually works now before we go ahead there is a quick announcement regarding the tech neuron offered by i neuron so in tech neuron if you don't know it is an ott platform with 200 plus courses currently it has 240 courses and along with this you know you also have all the live classes that is included so like batches whichever we run in every month that will also be included in tech neuron and currently we are launching the lifetime offer which will be for a week this is just for the on the eve of independence day we really wanted to help many people out who are not able to afford other courses so through this they will be able to get all the other courses along with that all the live sessions also the price of the course has also been reduced to 10 000 rupees and if you're not satisfied just with 10 000 what you can do is that you can use a coupon code that is crush and then you will be getting a 20 discount so in 8 000 rupees all these courses along with many services seven days you'll be having a doubt clearing session you'll be having one to one mentorship you'll be having probably uh mock interviews resume discussion you can also raise a request with respect to any kind of courses that you want so yes uh the offer just uh is uh for another four to five days please utilize this opportunity okay now let me go back over here now over here i have first of all taken an irish data set here you can see sepal width sepal length petal width petal length and i've just created a scatter matrix and here you can see how my diagram is basically generated right so this is what is the plot that you can see and obviously this is not like this is just like a two-dimensional plot right with respect to different different features you can do the similar task with the help of c bond where you just need to write snh dot pair plot with the entire df and here you can just write hui as the species so that it can actually categorize so once you execute this here you will be able to see that you will be getting a pair plot wherein it will try to show you the correlation between each and every features right specifically independent features that is what we are actually focusing on right now now as you know this will probably take some amount of time so here you can see that your graph is ready now already you know that we have four features sepal width sepal length petal width petal length right so four dimension now let's see when we reduce this number of dimension into two with the help of tsne how my data will look like so here it is project the data into 2d with tsne so here i have taken all my features and then i'm just using tsne and when i write n underscore components is equal to 2 that basically means i'm converting that four dimension into two dimensions and then i do fit transform on the feature and then i'm plotting the scattered plot uh with respect to all these projections and my x and y value right and here you can see i'm also provided my labels along with the color that i really want to specify with respect to my output feature so once i probably execute this let's see how we will be able to see it and here is how clearly we can see fine we have we have reduced this number of dimensions into two right all the four features and here by seeing this yes you can clearly separate it but what if i make or project this into three dimensions okay and obviously if you have many number of features then again some amount of information is definitely lost now let's go ahead and see projecting the data into 3d with tsne right the same technique that is dimensionality reduction technique here you can also use pcm so here what i am doing i have taken n underscore components as three that basically means it is going to convert this into three dimension and that is the reason why i have used scatter underscore 3d plot okay so once i execute it and here you will be able to see this here obviously you'll be getting a three dimension so this is how it looks like now here you can see that okay fine my data points are completely jumbled right like yes there are a lot of overlapping still we are not able to properly see this entire dimensions you know they are still overlapped data points and yes it is not that good when compared to what we really want i really want this clusters to be completely separate now let's see this technique with the help of umap okay so first of all to start with umap you have to install slash pip install umap learn so once you install this the installation will happen i have already installed it then all you have to do is that apply umap and again it is an efficient technique when compared to pca and tsne and if i give an underscore components is equal to two that basically means it's going to convert into two dimension and then i've also made sure that i convert this into three dimensions so umap2d you map 3d i do fit transform on the specific feature and i'm scat i'm just creating a scattered plot with respect to the two dimension and with respect to the three dimension and once i see this you will be able to see how beautiful diagram i'm able to get with respect to two dimension right over here all the data points are clearly separable with respect to two dimension but the most interesting thing what we saw over here with respect to three dimension now you see with respect to three dimension the magic is here now see this how well your data points are clearly clustered right with respect to the data points that i have this is definitely preserving more information when compared to pca and tsne this is the recent technique that has actually come and you can definitely use dimensionality reduction using umap right what an amazing technique again uh i hope you are able to understand this video definitely for any dimensional reduction technique try to use umap it is an amazing library altogether yes try to make the likes to 500 at least i will also upload the theoretical video uh and we'll try to understand what is the in-depth mathematical intuition behind umap and how it is basically doing this dramatically reduction such that much information is not lost so yes this was it from my side i will see you all in the next video have a great day ahead keep on rocking and yeah i will see you all in the next video bye take care"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Association Rule Mining",
        "Video Title":"Apriori Algorithm Explained | Association Rule Mining | Finding Frequent Itemset | Edureka",
        "URL":"https:\/\/www.youtube.com\/watch?v=guVvtZ7ZClw",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/guVvtZ7ZClw\/hqdefault.jpg",
        "ID":"guVvtZ7ZClw",
        "Publish Time":"2019-06-19T13:58:27Z",
        "Channel":"edureka!",
        "Channel ID":"UCkw4JCwteGrDHIsyIIKo4tQ",
        "Transcript":"[Music] hello everyone and welcome to this interesting session on a prairie and quartum now many of us have visited reading shops such as Walmart or Target for our household needs well let's say that we are planning to buy a new iPhone from Target what we would typically do is search for the model by visiting the mobile section of the store and then select the product and head towards the billing counter but in today's world the goal of the organization is to increase the revenue can this be done by just pitching one product at a time to the customer now the answer to us is clearly no hence organization began mining data relating to frequently bought items so market basket analysis is one of the key techniques used by large retailers to uncover associations between items now examples could be the customers who purchase bread have a 60% likelihood to also purchase Jam customers who purchase laptops are more likely to purchase laptop bags as well they try to find out associations between different items and products that can be sold together which gives assisting in the right product placement typically it figures out what products are being bought together and organizations can place products in a similar manner for example people who buy bread also tend to buy butter right and the marketing team at reiden stores should target customers who buy bread and butter I provide and offer to them so that they buy a third item suppose eggs so if a customer buys bread and butter and sees a discount offer on eggs he will be encouraged to spend more and buy the eggs and this is what market basket analysis is all about this is what we are going to talk about in this session which is Association rule mining and the a priori El Corte Association rule can be thought of as an if-then relationship just to elaborate on that we have come up with a rule suppose if an item a is being bought by the customer then the chances of item being picked by the customer - under the same transaction ID is found out you need to understand here that it's not a casualty rather it's a co-occurrence pattern that comes to the force now there are two elements to this rule first is if and second is that then now if is also known as antecedent this is an item or a group of items that are typically found in the item set and the later one is called the consequent this comes along as an item with an antecedent group or the group of antecedents approaches now if we look at the image here a arrow B it means that if a person buys an item a then he will also buy an item B or he will most probably buy an item B the simple example that I gave you about the bread and butter and the X is just a small example but what if you have thousands and thousands of items if you go to any professional data scientist with that data you can just imagine how much of profit you can make if the data scientist provides you with the right examples and the right placement of the items which you can do and you can get a lot of insights that is what associate rule mining is a very good algorithm which helps the business make profit so let's see how this algorithm works so Association rule mining is all about building the rules and we have just seen one rule that if you buy a then there's a slight possibility or there's a chance that you might buy B also this step of a relationship in which we can find the relationship between these two items is known as single cardinality but what if the customer who bought a and B also wants to buy C or if a customer who bought a B and C also wants to buy D then in these cases the cardinality usually increases and we can have a lot of combination around these data and if you have around 10,000 or more than 10,000 data items just imagine how many rules you're going to create for each product that is my Association rule mining has such measures so that we do not end up creating tens of thousands of rules no that is really the ebrary algorithm comes in but before we get into the ebrary algorithm let's understand what's the mattes behind it now there are three types of matrices which help to measure the Association we have support confidence and lift so support is the frequency of item a or the combination of item a or B it's basically the frequency of the items which we have bought and what are the combination of the frequency of the item we have bought so with this what we can do is filter out the items which have been bought less frequently this is one of the measures which is support now what confidence tells us so confidence gives us how often the items a and B occur together given the number of times a occur now this also helps us solve a lot of other problems because if somebody is buying a and B together and not buying see we can just rule out C at that point of time so this solves another problem is that we obviously do not need to analyze the problems which people just buy barely so what we can do is according this is we can define our minimum support and confidence and when you have set this values we can put this values in the algorithm and we can filter out the data and we can create different rules and suppose even after filtering you have like five thousand rules and for every item we create these five thousand rules so that's practically impossible so for that we need the third calculation which is the lift so lift is basically the strength of any rule now let's have a look at the denominator of the formula given here and if you see here we have the independent support values of a and B so this gives us the independent occurrence probability of a and B and obviously there's a lot of difference between this random occurrence at Association and if the denominator of the lift is more what it means is that the occurrence of randomness is more rather than the occurrence because of any Association so left is the final verdict where we know whether we have to spend time on this but rule what we have got here or not now let's have a look at a simple example of Association rule mining so suppose we have a set of items a B C D and E and a set of transactions t1 t2 t3 t4 and t5 and as you can see here we have the transactions t1 in which we have ABC t2 a CD t3 BCD t4 a de and t5 BCE now what we generally do is create some rules or Association rules such as a gives T or C gives a a give C B and C gives a what this basically means is that if a person buys a then he's most likely to buy D and if a person by C then he's most likely to buy a and if you have a look at the last one if a person by his B and C he's most likely to buy the item here as well now if we calculate the support confidence and left using these rules as you can see here in the table we have the rule and the support converse and the lift values now let's discuss about a priori so a priori algorithm uses the frequent itemsets to generate the Association rule and it is based on the concept that a subset of a frequent item set must also be a frequent item set itself now this raises the question what exactly is a frequent Adams set so our frequent Adams set is an item set whose support value is greater than the threshold value now just now we discussed that the marketing team are going to the sales have a minimum threshold value for the confidence as well as the support so frequent Adams have is that item set who support value is greater than the threshold value already specified an example if a and B is a frequent item set then a and B should also be frequent itemsets individually now let's consider the find transaction to make the things for them easier suppose we have transactions 1 2 3 4 5 and these items are there so T 1 has 1 3 and 4 T 2 has 2 3 & 5 t 3 has 1 2 3 5 t 4 2 5 and T 5 1 3 & 5 now the fourth step is to build a list of item sets of size 1 by using this transactional data and one thing to note here is that the minimum support count which is given here is 2 let's suppose it's 2 so the first step is to create item sets of size 1 and calculate their support values so as you can see here we have the table c1 in which we have the item sets 1 2 3 4 5 and the support values if you remember the formula of support it was frequency divided by the total number of occurrence so as you can see here for the item set 1 the support is 3 as you can see here that I don't set one appears in T 1 T 3 and T 5 so as you can see it's frequency is 1 2 & 3 now as you can see here the items at 4 has a support of 1 as it occurs only once in transaction 1 but the minimum support value is 2 that's why it's going to be eliminated so we have that final table which is the table F 1 in which we have the item sets 1 2 3 & 5 and we have the support values 3 3 4 & 4 now the next step is to create item sets of size 2 and calculate their support values now all the combination of the item sets in the F 1 which is the final table in which you discarded the 4 are going to be used for this iteration so we get the table C 2 so as you can see here we have 1 2 1 3 1 5 2 3 2 5 & 3 5 now if you calculate this support here again we can see that the items at 1 comma 2 has a support of 1 which is again less than the specified threshold so we're going to discard that so if we have a look at the table F 2 we have 1 comma 3 1 5 2 3 2 5 & 3 5 again we're going to move forward and create the Adams set of size 3 and calculate the support values now all the combinations are going to be used from the item set F 2 for this particular iterations now before calculating support values let's perform pruning on the data set now what is pruning now after the combinations are being made we devise C 3 item sets to check if there is another subset whose support is less than the minimum support value that is what frequent item set means so we have a look here the item sets we have is 1 2 3 1 2 1 3 2 3 4 the first one because as you can see here if we have a look at the subsets of 1 2 3 we have 1 comma 2 as well so we are going to discard this whole item set same goes for the second one we have 1 2 5 we have 1 2 in that which was discarded in the previous set or the previous step that's why we're gonna discard that also which leaves us with only 2 factors which is 1 3 5 item set and the 2 3 5 and the support for this is 2 and 2 as well now if we create the table see for using four elements we're gonna have only one item set which is 1 2 3 & 5 and if we have a look at the table here the transaction table 1 2 3 & 5 appears only 1 so the support is 1 and since see for the support of the whole table C 4 is less than 2 so we're gonna stop here and return to the previous Adam set that is 3 3 so the frequent itemsets have 1 3 5 & 2 3 5 now let's assume our minimum confidence value is 60% for that we're gonna generate all the non-empty subsets for each frequent itemsets now for I equals 1 comma 3 comma 5 which is the item set we get the subset 1 3 1 5 3 5 1 3 & 5 similarly for 2 3 5 we get 2 3 2 5 3 5 2 3 & 5 now this rules taste that for every subset s of I the output of the rule gives something like s gifts I to s that implies s recommends I of s and this is only possible if the support of I divided by the support of s is greater than equal to the minimum confidence value now applying these rules to the item set of F 3 we get Rule 1 which is 1 3 gives 1 comma 3 comma 5 and 1\/3 it means 1 & 3 gives 5 so the confidence is equal to the support of 1 comma 3 comma fired wherever the support of 1 comma 3 daddy pulse 2 by 3 which is 66% and which is greater than the 60 person so the rule 1 is selected now if we come to rule 2 which is 1 comma 5 it gives 1 comma 3 comma 5 and 1\/5 it means if we have 1 and 5 it implies we also gonna have 3 now if we calculate the confidence of this one we're going to have support 1 3 5 whereby support 1 5 which gives us 100 person which means rule 2 is selected as well but again if you have a look at rule 506 over here similarly if it select 3 gives 1 3 5 & 3 it means if we have 3 we also get 1 & 5 so the confidence for this comes at 50% which is less than the given 60% target so we're gonna reject this rule and same goes for the rule number 6 now one thing to keep in mind here is that although the rule 1 and rule 5 look a lot similar they are not so it really depends what's on the left-hand side of the arrow and what's on the right-hand side of the arrow it's the if-then possibility I'm sure you guys can understand what exactly these rules are and how to proceed with this rules so let's see how we can implement the same in Python right so for that what I'm going to do is create a new Python file and I'm going to use the Jupiter notebook you are free to use any sort of IDE I'm going to name it as a priority so the first thing what we're gonna do is we'll be using the online transactional data or ETS store for generating Association rules so firstly what we need to do is get the pandas and mlx tel libraries imported and read the file so as you can see here we are using the online retail dot xlsx format file and from ml extend we are going to import a priori and Association rules it all comes under ml extend so as you can see here we have the invoice the stock code the description the quantity the invoice date a unit price customer ID and the country now next in this step what we're going to do is do data cleanup which includes removing the species from some of the descriptions and drop the rules that do not have invoice numbers and remove the great grad transactions because that is of no use to us so as you can see here at the output in which we have like five hundred and thirty two thousand rows with eight columns so after the cleaned up we need to consolidate the items into one transaction per row with each product for the sake of keeping the data set small we are only looking at the sales for France so as you can see here we have excluded all the other seeds we are just looking at the seeds for France now there are a lot of zeros in the data but we also need to make sure any positive values are converted to one and anything less than zero is set to zero so as you can see here we are still 392 rows you're gonna encode it and see check again now that you have structured the data properly in this step what we're going to do is generate frequent itemsets that have support at least 7% now this number is chosen so that you can get close enough and generate the rules with the corresponding support confidence and left so as you can see here the minimum support is 0.7 and then what if we add another constraint on the rules such as the lift is greater than six and the conference is greater than 0.8 so as you can see here we have the left-hand side on the right-hand side of the Association rule which is the ant ascendant and the consequence we have the support we have the confidence to lift the leverage and the conviction so is that's it for this session that is how you create Association rules using the ebrary algorithm which helps a lot in the marketing business it runs on the principle of market basket analysis which is exactly what big companies like Walmart you have reliance and Target to given IKEA does it and I hope you got to know what exactly is Association rule mining what is lift confidence and support and how to create Association rules that we have any queries feel free to mention it in the comment section below till then thank you and happy learning I hope you have enjoyed listening to this video please be kind enough to like it and you can comment any of your doubts and queries and we will reply them at the earliest do look out for more videos in our playlist and subscribe to any rekha channel to learn more happy learning"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Association Rule Mining",
        "Video Title":"Apriori Algorithm (Associated Learning) - Fun and Easy Machine Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=WGlMlS_Yydk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/WGlMlS_Yydk\/hqdefault.jpg",
        "ID":"WGlMlS_Yydk",
        "Publish Time":"2017-10-31T22:17:26Z",
        "Channel":"Augmented AI",
        "Channel ID":"UCFJPdVHPZOYhSyxmX_C_Pew",
        "Transcript":"hey guys and welcome to another fun and easy machine learning tutorial on a Priory where you go grocery shopping we often have a extended list of things to buy each shopper has a distinctive list depending on one's needs and preferences a housewife might buy healthy ingredients for a family dinner while a bachelor might buy beer and chips understanding these buying patterns can help to increase sales in several ways such that if there is a pair of items that are both frequently together like bread and muck for example when you go to store would you not want the Al's to be an order in such a manner that reduces your efforts to buy things for example I would want a toothbrush toothpaste mouthwash and other dental products on a single Al's this is done in a way in which we find associations between items both x and y products can be placed on the same shelf so that buyers of one item would be promoted to buy the other promotional discounts could be applied to just one out of the two items advertisements on X could be targeted to buyers who purchase Y and X & Y could be combined into a new product such as having Y in flavors of X while we may know that certain items are frequently poured together the question is how do we uncover these associations please subscribe and click the power icon to join our notification squad in order to understand the concept better let's take a simple dataset as call it the coffee dataset consisting of a few hypothetical transactions we all try to understand this in simple English the coffee dataset consists of the items purchased from a retail store the Association rules for this dataset we can write the following Association rules Row 1 if mall is purchased then sugar is also purchased rule 2 is sugar is purchase then mark is also purchased prove 3 if moles and sugar are purchased then coffee powder is also purchased in 60% of the transactions generally Association rules are written in an if-then format we can also use the term antecedent for if left hand side and consequent for them right inside from the above rules we understand the following explicitly whenever milk is purchased sugar is also purchased or vice versa if mock and sugar are purchased then the coffee powder is also purchased for example if we see milk as a set with one item and coffee as a set with one item we'll use this to find sets with two items in the data sets such as multi and coffee and then later we'll see which products are purchased with both of these in our basket therefore now we will search for a suitable rat inside or consequent so if someone buys coffee with milk will represent it as coffee to milk where coffee becomes the left-hand side and milk the rising side when we use these to explore more key item sets we might find that coffee and milk to tea which means that people who buy coffee and milk have a possibility of buying tea as well an item set is a mathematical set of products in grocery al book eggs vegetables liquor aisle liquor red flash wine bottle of beer soda et al herbs tropical fruits roll pans fruit juices jams and practice our zero yoghurt price good the a priori algorithm is a classical algorithm in data mining that we can use for these sorts of applications ie recommend engines so it is used for mining frequent itemsets and relevant Association rules it is device to operate on a database containing a lot of transactions for instance items bought by customers in a store it is very important for effective market basket analysis and it helps the customers in purchasing the items with more ease which increases the sales of the market it has also been used in the field of health care for detection of adverse drug reactions a key concept in a priori is to assume that all subsets of a frequent itemsets must be frequent and similarly for a in frequent itemsets all its supersets must be infrequent - let's take another easy example from the supermarket sphere the example that we are considering is quite small and in practical situations terraces contains millions and billions of transactions the set of item set I which contains onions burgers potatoes smokin beer and a dataset consisting of six transactions each transaction is a tuple of zeros and ones where zero represents the essence of an item and one the presence so far so good an example for the rule in a scenario would be onion and potatoes to burger which means that if onion and potatoes are bought customers also buy a burger they are multiple rules possible from even a small data set so in order to select the interesting ones we use constraints on various measures of interest and significance we will look at some of these useful measures such as support conference left end conviction the support of an item set X is the proportion of transaction in the database in which the item X appears it signifies the popularity of an item set so support of x equals the number of transactions in which X appears over the total number of transactions in our example the support of onions equals four over six which equals zero point six six six seven if the sales of a particular item or product is a certain proportion having a meaningful effect on profits that proportion can be considered as a support threshold furthermore we can identify items that have supports values beyond the stretch hold as significant item sets conference conference of a rule is defined as follows so confidence of X to y equals the support of X Union Y over support of X it signifies the likelihood of item Y pin purchased when item X is purchased so for the rule onions was potatoes and what's the likelihood of pearls being bought so I'm sure by now you're sensing some basic theory along these lines you would be correct if you interpreted this as conditional probability ie the probability of finding the item sets Y in the transactions given the transactions already contain X if you give some important insights but it also has a major drawback it only takes into account the power polarity of the item set X and not the popularity of Y if Y is equally popular as for X then there is a high probability that transactions containing X will also contain Y that's increasing confidence to overcome this drawback there is another measure called left the left of a rule is defined as the left of X to Y which equals the sport of X Union Y over the support of X multiplied by the support of Y this signifies the likelihood of an atom Y being purchased when item X purchase while taking to account the popularity of Y in our example if the value of the left is greater than 1 it means that the item set Y is likely to be bought with item set X while the value with less than 1 implies that in essence it Y is unlikely to be bought if the item set X is bought conviction the conviction of a rule can be defined as conviction of X to y equals 1 minus the support of Y divided by 1 minus confidence enough X to Y for this rule a bug was both given that we have onions and potatoes so the conviction value of 1 point 3 2 means that the rule and in the potatoes to Burgas would be incorrect 32% more if the association between x and y was an accidental chance let us now look at their intuitive explanation of the algorithm what the help of an example we used earlier before beginning the process let us set the support tresh of to 50% meaning that those items are significant for which support is more than 50% so step 1 we create a frequency table of all the items that occur in all the transactions based on our data set so for our case we can see that onions we have frequency for potatoes 5 purpose for bulk for and br-2 step 2 we know that only those elements are significant 4 which is what is greater than or equal to the support threshold here the support edge hold is 50% hence only these items are significant which occur in more than 3 transactions and such items are onions potatoes burgers and milk step 3 the next step is to make all the possible pairs of the significant items keeping in mind that order doesn't matter which means that a B is the same as ba to do this take the first item and pair it with all the others such as and insert potatoes and then burgers and in the milk similarly considering the second item and pair it with preceding items which means potatoes and burgers and potatoes and milk we are only considering the preceding items because potatoes and onions which is the same as and in said potatoes already exists so all the pairs in our example our onions potatoes and in burgers and in smoked potatoes and burgers potatoes and milk and burgers and milk we essentially calculate the combinations of the pairs and we'll end up with six pairs step four will now calculate the occurrences of each pair in all the transactions step 5 again only those significant item sets which cross the spot Rachel are passed through these are onions potatoes and in smokers potatoes burgers and potato smoke step 6 now let's say we would like to look for a set with three items that are purchased together we'll use these items as found in step five and create a set of three items to create a set of three items another rule called self join is required it says that from the item pairs and in sp\u00edritus and in focus potatoes burgers and Pareto smoke we look for pairs with the identical first letter and so we get onions potatoes and onions burgers the skills onions potatoes and burgers and then we have potatoes burgers and potatoes and milk the skills potatoes progress and milk next we find the frequency for these two item sets and then we get three for onions potatoes hamburgers and two for potatoes percent milk so applying the threshold rule again we find that onions potatoes and progress is the only significant sit there for the set of three items that was purchased most frequently is onions potatoes and burgers so as you can see the algorithm is quite simple right here you can see a visualization to summarize the steps we took in our example earlier we first tally up our items from our dataset then we eliminate items that are below the support threshold thereafter we pay our of our items and repeat the process for the peers the same goes for three item item sets depending on the size of our dataset we can see how many items and transactions we have and then you can go beyond for item item sets to compute using the a priori algorithm let's take a look at the pros and cons of the a priori algorithm so first of all it isn't easy to implement an easy to understand algorithm it can be used on large item sets and is easy parallelized taking a look at the cons so sometimes it may need to find a large number of ten-minute rules which can be computationally expensive calculating support is also expensive because it has to go through the entire data set ok so that is it for me please don't forget to Like subscribe and share click the bell icon to see more machine learning tutorials and please support us on patreon link in the description below if you like to download the script to the video please click the link down below also and download it for free stay tuned to the next lecture where we will see how we can implement an a priori algorithm in python thank you for watching and we'll see you in the next lecture you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Association Rule Mining",
        "Video Title":"#2 Solved Example Apriori Algorithm to find Strong Association Rules Data Mining Machine Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=zi_ydmbWfAs",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/zi_ydmbWfAs\/hqdefault.jpg",
        "ID":"zi_ydmbWfAs",
        "Publish Time":"2022-01-31T13:00:24Z",
        "Channel":"Mahesh Huddar",
        "Channel ID":"UCPi23Ql765_5smMj2-r0X4g",
        "Transcript":"hi welcome back in this video i will discuss how to apply a priori algorithm to generate strong association rules for the given data set this is the solved example number two the link for other solved example is given in the description below this is the data set given to us consisting of five transaction ids and in each and every transaction the customer buys few products like bread butter and milk beer cookies diapers and so on here what we need to do is uh we need to apply a priori algorithm to generate a strong association rules given the minimum support of 40 and the minimum confidence of 70 in this case first what we need to do is we need to generate the frequent item sets once you generate the frequent item sets we can generate the association rules uh with this particular minimum confidence so first what we do is we will try to generate the frequent item sets a step by step for that reason first we need to generate the item sets and then we need to find the qualified frequent item sets that is the item set which is having minimum support so i will explain how to do it step by step in this case we have five unique products here that is bread butter milk diaper and beers so we will write those particular thing in the first column of this table as one item set then we count the number of times a particular product was bought for example bread bread was bought once twice thrice here so that is three in this case similarly butter butter was bought first time second time and then somewhere here it's a third time so it is three year similarly we need to count the number of times a particular item was bought and then we need to write in the second column that is the support count so this is a one item set table from here we need to generate the one frequent item sets for that reason first we need to do some simple calculation the minimum support given to us is 40 so the minimum support count is equivalent to minimum support that is 40 percent multiplied by item set count that is how many items are there there are five items so the answer is 200 percent that is nothing but two two should be the minimum support count then only you can say that one item set is a one frequent item set so in this case except this cookies all are one frequent item sets those i have written in this particular table that is nothing but one frequent item set now if we look at this particular one frequent item set we have five unique items again we need to generate two item sets here so these are the five items we have so two item sets are like bread butter bread diaper bread milk bread beer that's a first five combinations next one is butter diaper butter milk butter beer the next are diaper milk diaper beer and the last one is milk beer so those are the combinations i have written here once you write this particular combinations we need to find or can say that we need to count how many number of times these two items were bought together bread and butter if you see here it is bought first time in this first transaction second time it was bought and then in the fourth transaction also these two items were bought together so the answer is three in this case similarly bread diaper bread diaper was not bought here i think in the fourth transaction bread is present and diapers is present so it is bought only one time bread and milk bread and milk is present in the first one bread and milk is present in the fourth transaction so it is two similarly we have to find the support count for each and every item that is nothing but how many number of times these two items were bought together and then we need to select the two item sets which are having minimum support count of for two or more so in this case we have only four possibilities that is bread milk bread butter buttermilk and diaper beer in this case so these are the four possibilities now from this particular two frequent item sets we need to generate three frequent item sets for that reason first we need to write three item sets so first we identify what are the unique items are present here bread butter milk diaper and beer all five are unique items here from this particular five unique items we need to generate three item sets in this case i will show you one example how can you do this one is bread we can start with bread here bread butter milk bread butter diaper bread butter beer so bread and butter is over so bread milk diaper bread milk beer bread and milk is over bread diaper beer so bread butter bread milk bread diaper all are over now i will come with this one butter milk diaper butter milk beer and then butter diaper beer that's a ninth possibility and the tenth possibility is a milk diaper beer here so totally we have 10 possibilities for this particular 10 possibilities we have to again count the minimum support for example how many number of times these items were bought that is nothing but the support count bread milk butter bread milk butter is present here this is the first time bread milk butter is present one more time here so it is two similarly if you go with the second one bread butter diaper bread butter diaper is present in the fourth transaction you can see here bread butter diaper is present anywhere else it is not present so it is only one similarly we have to do it for all combinations and then write that particular support count in the second column next we have to select the three frequent item set which satisfies the minimum support of 2 in this case we have only one that is a bread butter milk with a support count of 2 in this case now coming back to the next one that is so once you find the free three frequent item set we need to check whether it is possible to write four frequent item sets in this three frequent item set we have only three items so definitely it is not possible to write four frequent item sets so we have to stop here and that is generating the frequent item sets now we need to write the association rules and select only those association rules which are strong based on the confidence percentage now uh if you want to generate the what we can say that the strong association rules we have to follow this particular procedure first we need to note what is the minimum confidence given the minimum confidence in this case is 70 percent so if you want to calculate the confidence of a particular rule let us say that x tends to y it is nothing but the number of times both the products were bought together divided by the number of times the first item was bought together what so in this case uh how many number of times x and y brought together divided by the number of time x was bought here so first i will write down the frequent item sets in this case we have four two frequent item sets are there and one three frequent item set is present so i have listed all those particular things over here next i will start with one frequent item set and then i will generate association rule so first one is this that is a bread and butter so bread and butter the rules can be bread butter or butter bread these are the two possibilities now i will check how many number of times bread and butter was bought together bread and butter was put together three times and how many number of times bread appears bread appears three times so 3 by 3 is equal to 100 it is a strong rule in this case the other side is what butter bread butter bread is bought three times and how many number of times butter appears butter was appearing three times it means it was bought three times so three by three is equal to hundred again it is a strong rule here coming back to the next one that is bread milk bread milk is the one possibility milk bread is another possibility bread milk how many number of times it was brought together bread milk is bought together two times how many number of times bread appears is it is three times so two by three 67 percent it is not a strong rule in this case other side is milk bread milk bread is again bought two times milk appears 2 times 2 by 2 is equal to 100 percent is a strong rule coming back to the next one that is uh butter milk here so buttermilk or milk butter butter milk is bought together two times butter appears three times two by three sixty seven percent it is not a strong rule milk butter is other possibility milk butter is brought together two times milk appears two times so two by two it is a strong rule in this case coming back to the next one that is diaper beer this is the next one so the first rule is diapers beer second rule is beer diapers diapers beer is bought together two times diapers appears three times 67 percent it is not a strong rule beer diapers beer diaper is bought together two times beer appears two times two by two which is 100 it's a strong rule in this case coming back to the last one we have left with only one frequent item set that is bread butter milk here we have to use a simple logic to generate the different association rules so what we do here is we will write two at a time on the left hand side or one at a time on the left hand side any one is perfectly fine so bread butter remaining is milk here so how many times all three were brought together all the time it is two only bread and butter three times so two by three 67 percent not a strong rule so other side of this one is what milk bread butter or any order you can write but in this case i have written this butter this side so it will become bread milk butter this are more one more course possibility all three were brought together two times and the bread and milk were bought together two times so that is nothing but two by two hundred percent and the next time what we can do is we can bring this particular butter they said bread this side that's one more possibility okay so milk butter bread is the one more possibility two times all three were bought milk and butter were bought together uh milk and butter two times so it will be a two by two again here strong rule strong rule now the next one is what bread butter milk so the opposite of this one so two by three it's not a strong rule next one is i can bring this particular butter this side bread this side and the last time i will bring milk this side and then the butter this side so that is one more thing these two are not a strong rules but the last one is a strong rule in this case so this is how actually you can generate the association rules and then you can check whether association rule is a strong or not by considering this particular confidence value so this is a very simple process with which you will be able to generate the frequent item sets and then you can generate the association rules and then select the association rule based on the minimum confidence in this case i hope the concept is clear if you like the video do like and share with your friends press the subscribe button for more videos press the bell icon for regular updates thank you for watching"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Association Rule Mining",
        "Video Title":"Market Basket Analysis (Association Rule Mining) With Excel",
        "URL":"https:\/\/www.youtube.com\/watch?v=aslTl6i-hpQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/aslTl6i-hpQ\/hqdefault.jpg",
        "ID":"aslTl6i-hpQ",
        "Publish Time":"2021-06-10T12:45:01Z",
        "Channel":"David Langer",
        "Channel ID":"UCRhUp6SYaJ7zme4Bjwt28DQ",
        "Transcript":"market basket analysis is a wildly useful skill for any professional it's not just for folks who work in the grocery store industry whether you work in hr finance marketing supply chain it you name it market basket analysis or association rule mining is a powerful way to discover new and interesting insights from your business data in this video i'm going to show you how you can conduct market basket analyses using excel and to prove to you that it is wildly useful for any professional i'm not going to use grocery store data i'm actually going to use a famous data set from the field of machine learning and show you how you can use market basket analysis in excel to mine association rules from the famous titanic dataset so here i am in an excel workbook and by the way you can download this excel workbook there'll be a link in the description below to a github and you can grab the file that you see in this video so i'm in excel and i've got a data set here and this is a representation of the famous titanic machine learning data set but before we get into that let's talk a little bit about what market basket analysis is market basket analysis is a technique where you take a data set and you mine from it interesting associations so for example in the classic grocery store industry example you might have an excel table where all of the columns of the table are products in a grocery store meats and cheeses and milk and cereal and who knows what and then each row in that table in excel is an individual customer who bought a bunch of items at the grocery store and checked out and those are known as market baskets so each row is a market basket in each column is a potential product and how this is represented in excel is each individual cell in the table is either a one or a zero which essentially just says did a person buy this particular product in their market basket and what can happen then is you mine that data you mine these association rules from the data and you find very interesting patterns so for example in the grocery store industry example to continue on you might find that people who buy bread also buy mayonnaise extreme at an extremely high rate those things are highly associated because in the united states anyway bread is often bought to make sandwiches and mayonnaise is a common condiment that americans put on their sandwiches so that's an association rule two products that are bought together more often than you would expect and then you do things like well maybe i'll create a coupon special where you buy a loaf of bread and you get a discount on mayonnaise and the idea being that you would sell more of both products now this this idea of mining out association rules is so powerful the market basket analysis can be used for basically any type of problem and that's why i'm using the titanic data set in this video is to prove that to you so what we have here is a table of data and what i've done is i've engineered some columns here so they can just have ones and zeros in them to tell you whether or not any individual market basket had this particular characteristic so let's talk about what this means in practice so let's take a look at this row right here row number two so each row isn't actually a market basket in this analysis it's actually a passenger in the titanic data set so this passenger was not in first class was not in second class they were in third class they were not female they were not a child they were an adult and they were traveling solo so on and so forth you can see here what i've done is i've taken characteristics of a titanic passenger and turned them into product columns in this market basket analysis now what's really really super powerful with market basket analysis is that you can actually guide the association rules so typically in a market basket analysis you just throw a bunch of data into the analysis and it just shows you interesting things that are associated together so for example bread mayonnaise or maybe meat and cheese you know things like that things that you didn't necessarily know now there's a legend that goes something around the idea that a grocery store found that beer and diapers were highly associated and then they were able to capitalize on that by making a special where they would have hey you buy some diapers and you get a discount on beer that's legend but you get the idea but you don't have to do that you can actually guide the market basket analysis you can say hey look i'm interested in this thing right here and then go mine all the association rules that are highly associated with that and it's a little bit abstract so let's cement it for this particular example in the titanic data set what we're highly interested in is knowing what patterns in the data are associated with survival not surprisingly right i mean the titanic is was a disaster so the data set is designed from a machine learning perspective to train predictive models that says look based on these characteristics of the passengers how likely are they to survive based on that and you can use association rules to actually do this so what we're going to do in this particular video is show how you can peg or fix survived and then ask the algorithm ask the market basket analysis to find all of the other things that are highly associated with survival pretty cool right what this shows you is that anything that you can think of any sort of characteristic any sort of behavior in your business data that can be modeled as a column and then have a one or a zero in it to denote presence can be analyzed using market basket analysis and these association rules are wildly useful i've personally used them to support marketing to support product management to support finance you can use this all over the place but enough of that let's go ahead and dive into actually doing a market basket analysis in excel so first up we're going to go over here and do a very simple case which is two-way lift so essentially we're just going to say we've got survived on one side and we're going to say hey hey market basket analysis go find out of all the remaining columns of data in that table that i just showed you which one is most highly associated with survival so just one column of data or one feature or one product depending on you look at it which one of those is most highly associated with survival to make this a little bit easier in excel what we're going to do is create a few pieces of data here first step we're going to have a cell that just has the total number of transactions transactions so there are 891 rows of data or 891 transactions so we just got that here and then we're going to create a lookup table because i'm going to be using vlookups in this spreadsheet to help us do market basket analysis and what you can see here is i've got all the individual features all the individual columns there are 16 of them total and you see them all listed here and then i've provided an index for each one of them which we'll see later on i'm using that with the solver we'll talk about that in a bit and then the frequency so how many folks in that 891 rows of data how many individual passengers were in first class about 24.2 percent how many were in second class 20.7 how many were female about 35 percent so on and so forth so this lookup table right here is going to be very very useful for us to actually do our market basket analysis first up we can see here that i'm going to ask the market basket analysis on the right hand side i'm going to say look i'm interested in survival i'm interested who survived in the data and then i'm going to go look at each of the individual other columns there are 15 of them total and then say which one of these is most highly associated with survival so you can see here i've got the remaining 15 features right here on the left hand side and i'm just using survived over and over again and the next step what i do is i calculate the combinations thereof so for example how many folks in the data were both in first class and survived there were 136 of them how many folks were in second class and survived there were 87 of them so on and so forth and you can see all of the calculations in here the formulas that i'm using and once again you can download this spreadsheet from my github and then you'll have it at your disposal this is the base data that we need and then after that what we do is we calculate lift and what lift is is a very powerful metric for market basket analysis in a nutshell what it tells you is based on the data that you have how many times more likely is this particular happenstance this particular association rule more likely than chance and let me just cement that with an example directly here so what we can see here is first class survival right so every member of the first class portion of the titanic data that also survived there was 136 of them and what we can see over here is a lift value of 1.64 and what that tells us is that folks that were in first class were 1.64 times more likely to survive than what we would expect by random chance which is good so basically what you're looking for is a lift greater than one any time you have a lift calculation in market basket analysis that's greater than one you have a happenstance you have an association rule that is happening more often than you would expect and those are the kinds of things that you want to drill in on so for example the best right here as you can see here in the color coding and the heat map is this particular situation right here where people that fit this association rule are 1.93 times more likely to survive so they're almost twice as likely to survive than everybody else and if we just roll over here scroll over females so if you're a female in this data set you're almost twice as likely to survive than males but we can get more complicated than this notice that we're only using a very simple association rule here we're just saying look something on the left hand side how much is it associated to something on the right hand side but we can increase the number of items we can make the association rule more complicated by allowing more columns of data on the left-hand side so what i'm going to do now is show you how you can do that with a three-way lift and this is where the solver comes in so i'm just going to go ahead and scroll down here so you can see this a little bit more so here's the problem actually no i'm going to scroll back up real quick sorry about that so when we're doing a two-way lift right we're only looking at two things simultaneously and especially if we're pegging the right hand side which is quite frankly the most interesting way to use market basket analysis in my experience when we peg the right hand side going through all the various permutations on the left hand side is actually kind of easy right because you can see here i have 16 total columns of data 16 features if i peg 1 as survived on the right hand side then i just have 15 rows because i just have one association rule for each one of the remaining 15 columns of data and that's pretty tractable so you can just create a table like this it's not very difficult and just figure it out however the real power of market basket analysis is when your association rules get more complicated when you peg the right hand side let's say and then you have two columns or three columns or four columns on the left hand side the number of combinations in that particular scenario grows exponentially so it becomes really really difficult to do this in a manual way using just a simple table like you see here and this is where the solver comes in the excel solver will then you'll set it up to actually iterate through all the different combinations and it will find you the best one let me show you what i mean by that so i'm going to go do a three-way lift here which means i'm going to have i'm going to peg survived on the right-hand side and then i'm because i'm doing a 3-way that leaves me two columns on the left-hand side and what you can see here is i've set up these two particular cells to allow the solver to iterate through all of the different possible combinations let me show you what i mean by that so right now we have one and four but let me change this i'm going to change it to three and nine and you can see here that things dynamically changed because i've got a bunch of vlookups in place like for example i've got a vlookup here so the name of the feature is based on the index not surprisingly i go look do the lookup in the lookup table same over here and i'm keeping survived pegged right it doesn't change i'm keeping that as my right hand side because i'm interested in finding out what are the other features that are most highly associated with survival and you can see here that the lift isn't so great so if you're a third class passenger traveling by yourself you are way below one you're 0.52 that's that's not good now what we can do is we can use the solver to essentially iterate through all of these like try out 3 and 12 and see what it is and then 7 and 11 and see what it is and what it'll do is it'll iterate through all these different combinations for you and find the one with the best lift and i'm going to go ahead and show you how the solver is set up in this particular scenario so i go to the data ribbon up here and i click on solver and you've got the solver configuration right here now first up you can see here that what we're optimizing for right here is the lift we're telling the solver hey we want you to maximize the value in this cell maximize the value of the cell ac31 right here right we want you to find the biggest value you can and the solver says okay cool i'll do that how do you want me to achieve that goal how do you want me to do that by changing some values and when we say is okay these two particular cells right here w28 through x28 these two cells right here go ahead and manipulate these change the values over and over and over again until you find whatever values the biggest for the lift and the solver says cool i can totally do that for you and then we say okay but we need to give you some constraints here right because the values in these two cells i'm going to move out again sorry about that the value in these two cells right here need to be constrained because they can't be a thousand or 22 000 or whatever they need to be constrained to the values in that lookup table so we're going to constrain those values like so we're going to say look w28 and x28 have to be less than or equal to 15 because remember we have 16 total features the last one is the survived column so we can only play around with columns 1 through 15 features 1 through 15. we're only using integer values because we need to make sure that the solver doesn't try to do a vlookup into that table with 1.234 because that doesn't exist so only integers 1 2 3 4 all the way up to 15. also the values have to be greater than equal to 1. so basically we're saying look solver you can play around with w 28 and x 28 change the values but they are constrained between 1 and 15 and they're integers only only whole numbers nothing with a decimal point in it and then lastly we're going to say look you can find very infrequent combinations of features that are highly predictive or highly associated with a large lift so for example maybe in the grocery store industry for example if somebody buys a bottle of dom perignon which is a very expensive champagne they are extremely likely to buy caviar right and if you're a grocery store you're like look those are two very profitable items awesome but how many people actually buy a bottle of dom perignon and caviar simultaneously in your grocery store it's probably an extremely small number of people so that rule might be very very good in terms of a very high lift score but in practice it's not very useful because you're not gonna run a special of like don't burn your own caviar because you're not gonna sell very many so this last constraint here says look you need to have at least five percent of the rows of data in the data set exhibiting this rule there we go these are our constraints next up we use the evolutionary solver algorithm this is the one you want to pick evolutionary and then lastly we're going to click options here and what we want to do is we want to set the mutation rate to 0.5 all this is the technical way of setting up the solver for it to iterate over and over and over again on all those different combinations of features to see which one has the biggest lift in terms of survival so if i click okay here i can ask the solver to find the best combination of features and notice right now i'm currently at a lift of 1.495 and i'm using the adult feature with the is triplet feature and by the way this means are there three people traveling together that's what this feature means so an adult traveling with at least two other people and that lift is 1.495 so i'm going to ask the solver to actually find which one is the best so now that i've got it all configured i just click solve and this will take a couple minutes to run so what i'll do is i'll just cut over to the final result i'm going to click solve and let's see what happens all right solvers come back with a solution it's found through all these different combinations and through all the constraints that we specified the best two features that are highly associated with survival in terms of the lift score so if i click ok you can see here that the best combination of two features are females in first class and you can see here that they are 2.5 times more likely to survive than you would expect by pure random chance and this is awesome and also notice too that this also counts for about 10 percent 10 of the data so this is good this is a good rule this is a powerful insight so this is awesome stuff to do and notice that you can scale this out i've only done this with three columns right i've pegged the right hand side for survived and i've got two predictive features two features to use for the association rules you could expand this out you could use four or five or six and all you would do is you would just expand this all out tick tick tick tick so on and so forth and just run it over and over again to find out if you could find maybe a rule that involves more features that has an even higher lift than 2.5 so this is totally doable in excel but what would be really nice is if you could do this in a lot quicker way where you could have your entire table of data like we just saw and we could just ask the market basket analysis to be like look circle through two-way three-way four-way five-way rules with these particular constraints and just find me the best and just list them all off and there is actually a very easy way to do this and let me flip over to powerpoint and i'll show you what i mean all right this is an example of what i mean this output right here comes from the r programming language and let me show you how awesome it is you can see here i've pegged survived on the right-hand side and then i've asked r to do a market basket analysis mind me all of these association rules and then list them in descending order by lift and what you can see here is i've got combinations of three features two features a single feature all that just done very quickly very easily in excel i could do this too but it would take quite a bit of work it'd be a lot of manual effort and what i can't see for example very easily in excel is hey what is the second best association rule with three features or the second best association rule with two features i can't do that very easily in excel because the solver only finds the single best thing this is way easier so much easier and here's the cool thing learning our programming if you already know excel is extremely easy and i wouldn't expect you to believe me so you can check up here and i'll have a video on my channel that you can take a look at i'll also put it in the description as well and you can see how easy it is for an excel user to learn our programming and then do market basket analyses like these and it is so much easier than doing it in excel you can totally do in excel don't get me wrong it's a fine tool but if you're interested in having more power there's nothing like our programming if you're liking this video so far would you mind helping me out and just giving me a like that will tell the youtube algorithm this is good content and then more folks that might be interested in doing market basket analysis with excel can see the video also i'll put up some more videos on my channel here just in case you're interested in learning more about our programming or some other things that you can do with data analysis okay market basket analysis with excel mining association rules is a wildly wildly useful skill for any professional until next time please stay healthy and i wish you very happy data smoothing"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing Association Rule Mining in Python",
        "Video Title":"Data Science using Python -- Association Rule Mining  (Demo using mlxtend)",
        "URL":"https:\/\/www.youtube.com\/watch?v=0yaYwDyBxFA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/0yaYwDyBxFA\/hqdefault.jpg",
        "ID":"0yaYwDyBxFA",
        "Publish Time":"2020-06-29T03:32:01Z",
        "Channel":"Sucharita Das",
        "Channel ID":"UCXKu8jcUDoeTDOhOsh1ZAWw",
        "Transcript":""
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing Association Rule Mining in Python",
        "Video Title":"How to do association rule mining using python?",
        "URL":"https:\/\/www.youtube.com\/watch?v=r-vymRtEzN8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/r-vymRtEzN8\/hqdefault.jpg",
        "ID":"r-vymRtEzN8",
        "Publish Time":"2023-06-27T21:00:21Z",
        "Channel":"KamiriTech ",
        "Channel ID":"UCPnyCM20JCaXm2omAZfpgiA",
        "Transcript":"hello and welcome to this video in this video we are going to discuss about data mining and more specifically we are going to discuss about Association rule mining using the a priori algorithm now system mining has become so common and I it is one of the most explored areas by the scientists and the reason as to why determining is so common is because it enables us to enhance patterns from data and once we unpack these patterns we can be able to generate some insights Association rules mining on the other hand is a part of determining that makes it possible for us to determine the relationships between various data sets or various transactions in now Association rules my associate rules mining is heavily used when you want to do a Market Basket analysis and what is this Market Basket analysis it is basically trying to understand items that are frequent reports together such that if a person buys this item they are required to buy this other item and if a person buys this combination of items it is highly likely that they also going to include this other item now in this case we are asking ourselves think of a store that has to organize their products on the shelves now this tool has to analyze the center and say if a customer has been buying these items together then they must be packaged or they must be put somewhere near one another in their shelves even our e-commerce store such as Amazon and others that exist globally they're using this rule so that once you buy a certain item they recommend others that go together without particular item they don't just do that from the blues they use Association rule mining now for us to be able to do this Asia rule mining we need to do uh we need to require a set of libraries and we're going to use Python Programming to do a simple basic one so that we can understand it clearing but before we begin consider subscribing to the channel like our videos share your comments in the comment section click on the notification Bell so that you are notified every time we release a new video so let's go ahead every first thing that we need is to import our body theme and in this case we are going to use the a priori algorithm if using Jupiter notebook and it is an older version of an accord distribution environment you may need to install MLF state to do so just do pip install ml extent however if you're using Google collab no need because it is highly supported so a priority algorithm is a classic algorithm that makes it possible for us to analyze our transactions and generate Association rules after importing a priority algorithm the next thing that we need to import is a library known as the solution rules this library is the one that will store or will enable our Aprilia algorithm to discover the various Association rules that we have then you need to import pandas and pandas is critical because we need to pre-process our data and convert it from just a array into a one hot encoding so once you import these three libraries you can put them in a separate cell from the entire or the rest of the code or else you can just do like me and put all of them in a single cell so the next thing is to now create our data sets you can decide to use uh just a small data like I have created here or in an ideal situation you'll be having your data set in form of a CSV in this case we are assuming you are analyzing data for a successful and in this store they have corrected their transaction for an entire period of about a month you know they have given you the work to analyze it so you'll have your data in form of a CSV we have done some previous videos on how to prepare CSV data you can watch them in this channel now in this case I have uh just worked on a small data and in this data we have transactions in form of an array the first transaction we have milk eggs and bread so we are saying this is a person who came and bought these three items second one we have milk and bypass the third one we have milk beer diapers and bread the fourth one we have X Beer deposit the fifth one we have milk eggs types now from where things we can be able to talk about a number of things here that uh bread has appeared in four of this transaction bypass has also appeared in four of these and in three both bread and Papas have appeared together we can also tell beer has appeared in two of this transactions and uh X has also appeared in three of these transactions milk appears in four of the transactions and uh you can see that these are just five transactions if you have 2000 transactions difficult for you to do those uh mental analysis so that's where now this capability of using a priority and Association rule mining comes in so the first thing that we do now from our data set is to create a one hot encoding we have discussed previously what is a one hot encoding and the way it enables us to handle our data in a more better an organized format rather than handling it in form of just an array one hot encoding is used both in uh text Data like we have here and also used in imaging center one so to create our own coding what we do is that variable known as DF then we say pt.data frame we pass our data sets then we take our one hot encoding that we have put here and we stuck it for purposes of uh making sure that these transactions are appearing rather than appearing they are appearing as form of data structure so we say a 100 underscore one other for DF is equal to one Hertz and that's what DFT start which is to put them in form of a data structure after we have done that the next thing is now to create or to apply our priority algorithm find the frequency in the transactions so we create a variable known as frequency sets you can also call this variable X or any other name that design so long as remember the rule of the tab you must follow the rules of naming identifiers that one is compulsory is equal to a priority is a algorithm that we have imported here one hot address one other cohort DF which is this variable that you have created here for on hot encoding then you have a mean support of 0.2 when we are talking about uh support we are simply asking ourselves what is the frequency with which a given I said for a relationship appears and in this case when you talk about minimum Support over 0.2 we are saying that uh we can consider anything that has a frequency of about 0.2 and the minimum support can be expressed using an expression why whereby X is considered to be an antecedent item Y is considered to be a consequence and this means all items that otherwise may not be a single item and this means if you have bought X that is the antecedents now how does it influence why that is the consequent so when you're talking about minimum support we are going to have minimum antecedent supports and minimum consequence support that is when you talk about minimum consequence supports how much does anticipating support consequence or what is the probability that given these antecedents you're going to have a consequence when you talk about minimum what is the probability that having this consequence you are likely to have this set of antecedents so I believe we have gotten that very clearly use column uh features or column names we have started as true so that you can be able to see our Forum headings then we go ahead and generate our rules so rules is equal to Association rules remember these are library that are important here then frequency which is what we have here then we use the metric known as confidence and uh the confidence is the minimum of support that we have discussed yeah and you talk over threshold over 0.5 then you go ahead and print our rules so if we run this particular server what is going to happen that is going to print uh these rules that we have over here and you can see that from just these five transactions we have been able to generate an array of 25 possible rules imagine that so there are 25 possible rules we can barely tell the rules by just looking at these five transactions so 2000 transactions how many rules do you think will be there so in this column we have an accident we have the consequence we have the antecedent support it continues are down here we have the consequence supports periods we have the overall support we have the confidence we have the lift the average and the conviction we are going to discuss all these elements in this video and um if you would like to know more about their priority you can look at the documentation of their period in this case we are going to discuss more about the application of it so antecedents we are saying that this is an item or item sets that are first Bots consequence these are items one item or item sets that are influenced by the fast item here that I've put for you the transaction x y in this case if you buy beer what is the likelihood of you buying bread and now that one would be in order would be determined by this metric that you are calling consequent supports So when you say that a consequent support of 0.8 that means that the high chances that if you buy beer you're also going to buy bread when you talk of antecedent support here now you are asking yourself if beer if you don't know if we take bread as the consequent how much does it in how much does it say that one would have bought be a previous one must have bought beer the probability is 0.4 so you can see that it is quite a low probability let's take another transaction here rate and bypass if you buy bread what is the chances that you're also going to buy diaper so it takes the consequent support we come down here and then we first note the transaction the transaction is a transaction to Identity that is and we can see that the consequent support is 0.8 so it means that there's a zero pointed probability that if I buy diverse I'm also going to buy bread what is the probability that if I have bought bread I must have bought diapers right or my decision to buy bread was it also uh means that I had bought diapers before I bought this it is also 0.8 which means that even if you interchange the items the same probability holds so it means that the uh beer and bread they are appearing together so many times in our deficit let's go to our data set and type see if this is true if you look at our data set here in three transactions both beer in place are appearing into transaction the first one here credit is appearing no both breads bread is appearing but diapers are not appearing in transduction two typers are appearing or the bridge is not appearing and you can see that means that these two have a very strong relationship in our data set and you can already see how these opposition rules mining can help us as data scientists to demystify how various items are related so if I was at your device at a store that has been stocking these items I would advise them to make sure that in their shelves they have put these items close together so that when you are buying bypass you can see bread or when you're buying bread you can see diapers right but in the case of uh beer and bread I would advise them that's good uh beer to be more prevalent then anybody who buys beer they look for bread but now if somebody has bought bread because of them being meaning that they had already bought beer are very low which means that bread is not influencing beer so much let us go ahead and look at a more uh diverse transaction or a more transaction or a transaction more items get an example of transaction number 25. Thai bus eggs and milk as the antecedents and are bred as the consequence let us first go to uh uh what we are calling the antecedent supports is 0.2 this means that you buying a bread doesn't exactly mean that you had already bought bypass eggs and milk this is actually 0.2 video but what are the chances that if you have bought diapers eggs and milk you also going to buy bread that is the concept of support transaction number 25 says that it is 0.8 0.8 this is very strong so let's go ahead now and say we are still organizing the shares overthrow it means in this case this is somebody who is checking out of the store output bread somewhere they can see it when they are checking out of the stock because they would have already bought these items and then they would remember they need a price right but if I put bread somewhere not near where they're checking out the store they may not remember it or they may not their transaction may not be triggered look at this other funny transaction here transaction 24. beer and milk how do they influence diapers and bread so when one is buying both diapers and bread does it really mean they must have bought beer and milk before the transaction is zero point but what if one has bought beer and milk what are the chances that they're also going to be diapers and bread transaction number 24. it is 0.6 is 0.6 so what why does it go below uh transaction about 25 we are now uh we can already see that transaction about 25 we have three items influencing one obvious reasons uh if you look at our data set there is a way in which these items are not appearing together right uh we have bread here but we don't have diapers we have typers here but you don't have bread which means in two transactions you are not we are not both together not must have lowered the probability as you can be able to see it over here so I believe that um this uh basic View of these particular transactions has triggered your curiosity into getting into the field of determining as well as exploring Association rules further I want now to challenge you to take a CSV data or a CSV data set of transactions in a particular store you can try and maybe and get one online or if you don't have one that can access that is already prepared for you you can do web crawling for some of these sites I've already done a video previously on web crawling here from our CSV and outside and do Association Rules by name eight or else you can go ahead and try and modify this that we have over here for instance you can go and maybe perhaps add one more transaction right here you can add another maybe one more transaction here let me just add another transaction just come and put that just paste that now here instead of talking about milk and die fast I cannot come and talk of a beer and now instead of talking via and Bypass or if I talk over beer and X let's just see what happened run it again it's the association rules now you can see transmission rules now have changed totally we had initial 25 rules now we have 11 rules right because what are priority is doing is that it is trying to assess how do these uh how are these items created how are they influencing one another like now you can see diapers and bread they are still influencing one another but now at a scale of 0.6 support rather than 0.8 that was there initially if you do a heavy transaction like die pass the next the chances of bypass the next influencing you buying a bridge is a 0.6 but you buying upgrade meaning that you had already bought bypass and X before is actually 0.33 so you can see that uh the transactions you have could have influence the exposition rules that generates and therefore I would encourage you to practice more with this one something that is going to um give you a battery of joining determining and it is going to make you appreciate the science thank you so much for watching the video and please consider subscribing to the channel for more such contents click on the notification Bell and thank you so much"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing Association Rule Mining in Python",
        "Video Title":"Apriori Algorithm and it&#39;s Implementation in Python | Association Rule Mining | ML 2023",
        "URL":"https:\/\/www.youtube.com\/watch?v=BKbsCvE7pJA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/BKbsCvE7pJA\/hqdefault.jpg",
        "ID":"BKbsCvE7pJA",
        "Publish Time":"2020-06-08T21:46:52Z",
        "Channel":"Kazi Amit Hasan",
        "Channel ID":"UCES_2FWYQbgyikzxCQ_oOVQ",
        "Transcript":"hello everyone welcome back to the channel if you're new here let me introduce myself I am Kermit Hasan currently completing my undergrad in grid well that's a quick introduction right now let's jump into the video in this video we are going to learn about a priori algorithm and we are going to solve a real-world problem by using a priori algorithm in Python well to be more specific this video is divided into two parts first Edith is the theoretical part where we are going to learn about the Apriori algorithm what things we need to learn and the basics to implement them and in the second phase we are going to implement the learnings in a data set and get some insights well so let's jump into the video well this is the first part that pro railroading basics well what is a priori algorithm well a prairie algorithm is a machine learning algorithm that is used to gain insights into the structured relationship between different items involved and it's a data mining technique that is used for mining the frequent itemsets and relevant Association rules well let me make things simpler to you well we all have done some online shopping right so when you click on a particular product that is added in your cart and if you want to check out the our cart that we can see some ads of the products that are basically as I said with our buying thing well that is a application of a priori rhythm okay so it means recomendation system recommending products based on your purchased item you can see this in different ecommerce website right well the things now the things that we need to about Aprilia rhythm before implementing them first in association rule well it defines frequent patterns and associations with our relations among the set of items as example if it went to a market to buy a keyboard they also might get a mouse right so did the seller wanted to place them one after another to get some more profit well that's application of a prairie algorithm and this is example of Association group and then our next arm is support well support refers to the default popularity of an item and can be calculated by finding the numbers of transactions containing a particular item divided by total number of transactions well let's find out the support of our keyboard well the support of keyboard is the transaction containing the keyboard divided by the total transition that's so simple right next is confidence well confidence refers to the likelihood of an item B maybe a mouse is also brought if the item a is also put so like our keyboard and mouse example the confidence of mouse and keyboard is the transaction containing both keyboard and mouse divided by transition of containing keyboard so this is the confidence and last but not the least is a lift well lift of keyboard and mouse refer to the increase in the ratio of fill of mouse win the keyboard is sold the lift of mouse and keyboard can be calculated by dividing the confidence and the support well this is the equation of calculating the lift well I'm attaching this PDF in my description box for your better understanding well now the second phase that is implementing the approver rhythm in the data set moving on so first of all we're importing the necessary libraries that is numpy Pender's and from ml extend the frequent pattern sorry importing the Apriori and association rules and the data set that I am using in this video can be found in you see I mentioned in repository I will provide the link in the description box please to check that and if you click in the link then it will redirect you to the USM attending for live a website so moving back so first of all we are loading our data then let's show it show the data well so in in the column we can see that it has invoice long stock code description quantity inverse date etc so now we want to know about the rows and columns in our data set for that list on this cell so it says that there are several columns in the data set and by running this shell that the data dot shape we can see the number of rows and columns and on the next cell we are checking whether there is a null value or this is present you know in this data set or not for that let's run this cell so it gives us it true it means there are some null values in the data set so we have to deal with them so now let's see that in which columns there are some London or would cancel null values so let's run this cell so it says that the description box or column has some null values in the customer IDs and some null values so now the second is second phase is data pre-processing part so first of all we are stripping the extracts places in the description bus then dropping the rose so with that has no invoice numbers in it and the next we are dropping all the transaction which are turned in the credit on the next cell now we want to see which countries are present in the data set so by running this cell you can see that if there are several countries like United Kingdom France Australia Netherland etc are present in the data set so now we are splitting the data according to the region of transaction so for this video I am only doing the transaction of France so for that I am selecting the country France here well after selecting dips okay so now we're applying one hot encoding to make the data much more suitable so let's run it so now suppose we want to analyze the market trend of France so for that applying one hat encoding in the France data so let's apply it and run this self to get more insight so well what this output says is that the invoice long this 70 has brought the tea towels and he or she hasn't bought other items that means zero okay so I hope you understand the data set okay so now we have to build the model for building the model I'm creating a data items or data frequent item set we're calling a priority and loading the friends data here and we are defining the minimum support is zero point one and use column names is true so now we are collecting the infrared rules in our data frame by four using that we are using the Association underscore dual function we are loading the frequent itemsets here and we are using the metric lift here and keeping the minimum threshold one so on the next line we we are going to sort them according to their confidence and leave cells so let's run this cell well now there's printer rules well so you can see that there is Association rule between the paper plates and paper cups and these are the support and confidence values and the lived values associated with them so what we can understand by this output is that it can be seen that the paper cups and plates are brought together in France this is because the French have a culture of having get together with their friends and family at least once a week so you can get these kinds of insights by using a priori algorithm so that's quite interesting right well this is all for today I hope you enjoyed this video well if you have any kind of query or questions about this video please leave a comment below and I will try to answer them accordingly and all done all the important links and the nose book and my github account profile links all will be in the description box please do check them well thank you for staying with me good bye"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Unsupervised Learning",
        "Topic":"Implementing Association Rule Mining in Python",
        "Video Title":"Data Science using Python -- Association Rule Mining  (Theory Part 2)",
        "URL":"https:\/\/www.youtube.com\/watch?v=2CIK8kK4b7w",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/2CIK8kK4b7w\/hqdefault.jpg",
        "ID":"2CIK8kK4b7w",
        "Publish Time":"2020-06-24T10:22:22Z",
        "Channel":"Sucharita Das",
        "Channel ID":"UCXKu8jcUDoeTDOhOsh1ZAWw",
        "Transcript":"hello everyone will come back so so far we have discussed about the association rule matrix okay and now we are going to discuss how these Association rule mining helps in finding the relevant item cells and which will actually help in market basket analysis okay so as a part of step one what we are doing all possible items are such generated okay so whatever list of items we have from that we are generating the all possible item sets say I have ABCD five items I have so in that case I can have item sets having all the single elements that is a b c d e those will be the item sets and then I can have item sets having the two elements that is a comma B B comma C C comma D D comma E EECOM I like that right all possible combinations then I have three elements item sets that is a comma B comma C B comma C comma Dec comma T comma he like that right all possible three element item sets then I can have four element item sets that is a b c t b c d c d e a like that and then i can have 5 element item set that is ABCDE right so all these possible items are generated from the items available then i am identifying the frequent itemsets frequent itemsets means that I am keeping one threshold value for the support okay so I can generate the support for all these item sets I can have the item sets of different lengths and the support can we calculate it based on that and suppose I am keeping a support value of 0.3 so that means that the item sets who are having the support value of more than point to it be considered and item says having support very less than point three will be not right so right item says having support value more than point three are called the frequent eye okay and it wonder stand that as there are only five items and I can have so many I possible item says and when there are thousands of items then there are many huge number of items as possible and then calculating the support for all the items as it is a really difficult right so in that situation a priori principle comes into picture so what it says it says that all subsets of frequent item says must also be frequent what does that mean that I'm saying that suppose a comma B comma C this item set is having support value of 0.35 so it is more than the threshold value so I am considering that as frequent item set so when this is a frequent item set that is ABC then obviously a comma B or B comma C or a comma C or A or B or C all these item sets will be having the support value more than this point C file so what I am saying that when there is a particular support value for item set then all the possible subsets of that item set will be having the support value more than the support value for the original item set we can have some example so from this diagram I understand that the total number of transactions have been bread is 70 plus 10 80 and total number of transactions have been so is 10 plus 4 14 and total number of transactions having both bread and so piston okay so when I am considering the support value for the items it Brit comma so then it will be something something for the value teen but when I am considering only for soap then it it will increase support will increase and when I am considering only bread hats item said the support is increasing that means I can conclude that the support value of the individual items that is bread and soap is always more than the support value of their combined item shape so that's what is being told by the Apriori principal so I can say that support value of a comma B is always more than a comma B comma C so this is called the anti monotone property of support okay and from that we can go in that just the opposite side as well that superset of infrequent item set will also be infrequent so just the opposite thing I'm saying so you understand that the support value of whoo blade is more than the support value of bread comma soap so that means that whenever I am moving to the superset then the support value will decrease right so if I'm finding that okay the support value of bread is infrequent maybe suppose the support value is not crossing the threshold so that means in any transaction where bread is present with other items the support value cannot be more than the support value of bread itself so that's why for the super says the support value we decrease will go on decreasing is why when I am finding one item set having the infrequent support value that means that I don't need to check the the super cells and when I am having a item set having the frequent item set that means it is having a support well more than threshold then I can all also consider all the subsets of that item set because they will all have the support fellow organization so that way in the both sides of that item cells I can consider them without calculating the support value right so we understand so when I have item set having a frequent itemsets so if I am getting our item set which is frequent that means all these subsets are frequent and when I am getting an infrequent item since then all is super sesor infrequent so these two principles I can consider and it will help me getting the frequent itemsets much more quickly right so based on this principles we are talking about Apriori algorithm that means actually what happens exactly what is being done so what we do first we generate on frequent items it's having only one item so maybe if there are five items then I'm generating five items each having each element and then I'm calculating their support and I'm taking only the frequent itemsets min suppose ABCDE these are five item sets so from our example so suppose a comma B comma C de they are all the individual item item sets and I am calculating their support and suppose ABCDE this four are actually frequent item set and E is not so I'm just removing E and not only that I will not consider any superset of me going forward okay so as I am writing in the second step that I am generating item set of length two so whenever I am generating length two I will I will not consider any item set of having this e element because the single item II set is not having the threshold value it is not reaching the threshold value for support so that's why it will not it will not do for the all the super sets of es value okay so I am NOT considering that then I am considering the length three item sets and again here also so in the length two item sets I had first removed not consider this element item sets and then I have I have calculated the support for the all the two element item sets and I have taken only the frequent item cells and then when I am going to the next step then I am creating this length three item sets which will be only from the length two item sets which are frequent right so at each step and what I am doing I am calculating the support I'm identifying the frequent itemsets and in the next step I am NOT considering the item sets which you are not quaint in the last item last step right and I'm not considering their super cell so that way at each level I can just you know reduce the number of items is in that way I'm continuing the process till the length of the items that reaches maximum length of that time so suppose in this example you can understand that first I will create one element item sets I will identify the frequent itemsets then I will generate two element item sets only only having the elements from the frequent itemsets then I generate their support and identify the two element frequent itemsets and based on that I create the three element item sets okay and then again I am calculating their support and then I am identifying the frequent itemsets and based on that I go to the next level and create four element items it and so on so that way after this algorithm is executed I am getting all the frequent itemsets which are having the support well more than the threshold okay so now I'm moving to the step two so in the step two what I am doing I am generating all possible rules from the frequent itemsets okay so what we are doing here our purpose is that now I got the frequent items as having the support fellow more than the threshold and now I'm checking their confidence level and here also I'm taking one threshold value for the confidence and I am going to have the frequent item says only having the confidence value more than the threshold I will consider only the record stairs item cells which are having the confidence one more than the threshold okay and I can say that confidence for this suppose X two Y I can write as support of X Y by support of it so you can you can just take back the formula for confidence and from there you can derive that I can always write that confidence for X 2 y equal to support of XY divided by support of X right so this X Y this is the list of items in the item set so suppose in my eye and said I have four elements a comma B comma C comma D so if that is the case you can understand that this thing I can write in different ways means if there are total four elements I can have three elements in the antecedent and one in the consequent or two in both antecedent and consequent or one in antecedent and she in consequence so that way I can have different combination of elements so if I write it that way you can understand that this numerator part will always be fixed because ultimately I am calculating the support for those four elements so that will be the same but what will change this denominator because you know that this support of X means it is only the antecedent so if there is one element in antecedent there is two element in 97 now there are three elements in antecedent the support is going to change and actually as and when you are increasing the items in the antecedent it will decrease the support we know from apptivity principle right so as our elements are increasing in the left part of this then this support value is going to thickness and the and as the support value this part is going to decrease that means that in this equation it will increase the confidence so I can say that increase of items in antecedent decreases support for antecedent and increases confidence right so what confidence will increase so that's why I can write this you can see here that I have written confidence of a comma B comma C 2 D is greater than equal to B comma C 2 a comedy is greater than equal to C 2 a comma B comma T so you can see here that this confidence is more when there're more elements in the antecedent part so as an as an Fane I am decreasing the elements in the antecedent and increasing it in the consequent then that confidence is also reducing so how how this process is done so rules taken with only one consequent first so first we take one consequent and we calculate the confidence and then gradually increased consequent and then we again calculate the confidence and that way I repeat until one item is left in their antecedent right so after that this process will stop but you know that suppose this this thing okay as a first step I am taking one element in the consequent and three in the antecedent okay and I'm calculating the confidence and when I have calculated the confidence I found that this confidence is less than the minimum confidence level I have said so it will anyway will not go in the frequent item set but if this is not going in the frequent item set then these calculations are not required because when I am arranging it this way the confidence will reduce so as this first one is having the confidence less than threshold so I don't need to calculate the rest part so at any step if I find that okay and the confidence is less than the minimum confidence then I don't need to go in the right set of this equation so that way I can reduce the number of calculation so after this step what I am getting that I am getting the frequent item cells which are having support value which is more than the threshold set by me and the confidence value that is also more than the threshold I have set so I have set of Association rules which satisfy both the minimum support and minimum confidence condition so once I get all these rules then I can search them for high as well so lift to make business decision so once I have this set of rules are available set of item sets are available then I can take the maximum value of lift or leverage right so because they they help us to have the close relationship between the items within the item set so I can filter out always those particular item sets and they can help us to identify that there okay which items are good together right and based on that we can take some decisions in the market basket analysis so that way Association rule mining helps in identifying the important item sets and that will decide that how the items will be will be present in the different racks or different locations in the market and which items will be close with each other like that so that is the that is the scenario here and in the next tutorial we are going to discuss the practical demo using Python library ok so in the next session we'll show that how to implement this Apriori algorithm and this Association rule mining using Python libraries so for now thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Reinforcement Learning",
        "Topic":"Markov Decision Processes (MDPs) in Reinforcement Learning",
        "Video Title":"Markov Decision Processes (MDPs) - Structuring a Reinforcement Learning Problem",
        "URL":"https:\/\/www.youtube.com\/watch?v=my207WNoeyA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/my207WNoeyA\/hqdefault.jpg",
        "ID":"my207WNoeyA",
        "Publish Time":"2018-09-20T06:34:55Z",
        "Channel":"deeplizard",
        "Channel ID":"UC4UJ26WkceqONNF5S26OiVw",
        "Transcript":"what's up guys welcome back to this series on reinforcement learning in this video we're going to discuss Markov decision processes or mdps this topic will lay the bedrock for our understanding of reinforcement learning so let's get to it [Music] Markov decision processes give us a way to formalize sequential decision making this formalization is the basis for problems that are solved with reinforcement learning to kick things off let's discuss the components involved in an NDP in an MDP we have a decision maker called an agent that interacts with the environment that it's placed in these interactions occur sequentially over time at each time step the agent will get some representation of the environment State and given this representation that agent selects an action to take the environment is then transitioned into some new state and the agent is given a reward as a consequence of its previous action so to summarize the components of an MDP include the environment the agent all the possible states of the environment all the actions that the agent can take in the environment and all the rewards that the agent can receive from taking actions in the environment this process of selecting an action from a given state transitioning to a new state and receiving a reward happens sequentially over and over again which creates something called a trajectory that shows the sequence of state actions and rewards throughout the process it's the agents goal to maximize the total amount of rewards that it receives from taking actions and given states of the environment this means that the agent wants to maximize not just the immediate reward but the cumulative rewards that it will receive over time alright let's get a bit mathy and represent an MDP with mathematical notation this will make things easier for us going forward so we're now going to repeat what we just usually discussed but in a more formal and mathematically notated way in an MDP we have a set of states big s a set of actions big a and a set of rewards big R will assume that each of these sets has a finite number of elements at each time step T that agent receives some representation of the environments state s sub T based on this state the agent selects an action a sub T and together this state in this action gives us the state action pair s T comma a tee time is then incremented to the next time step T plus 1 and the environment is transitioned into a new state represented by S sub T plus 1 at this time the agent receives a numerical reward R T plus 1 from the action taken from the previous state so generally we can kind of think of this process of receiving a reward as an arbitrary function that map's state action pairs to rewards the trajectory representing the sequential process of selecting an action from a state and then transitioning to a new state and receiving a reward can be represented like this this diagram and nicely illustrates this entire idea let's break this diagram down into steps step 1 at time T the environment is in state s T step 2 the agent observes the current state and selects action a T step 3 the environment transitions to state s T plus 1 and grants the agent reward R T plus 1 this process then starts over for the next time step T plus 1 now since the set of states and the set of rewards are finite the random variables are T and s T that represent the reward in the state at time T have well-defined probability distributions in other words all the possible values that can be assigned to RT and s t have some Associated probability these distributions depend on the preceding state in action that occurred in the previous time step t minus 1 so for example suppose s prime is a state within the set of all states and R is a reward within the set of all rewards then there is some probability that the state at time T will be S Prime and that the reward at time T will be R this probability is determined by the particular values of the preceding state and preceding action we have a bit more formal details regarding transition probabilities on the corresponding blog for this video on deep lizard comm so be sure to check that out alright we now have a formal way to model sequential decision making how do you feel about Markov decision processes so far some of this may take a bit of time to sink in but if you can understand the relationship between the agent and the environment and how they interact with each other over time then you're off to a great start it's a good idea to utilize the blog for this video to get more familiar with the mathematical notation because we'll be seeing it a lot in future videos and while you're at it check out the Deep lizard hivemind for exclusive perks and rewards like we discussed earlier MDPs are the bedrock for reinforcement learning so make sure to get comfortable with what we covered here and next time we'll build on the concept of cumulative rewards that we introduced earlier thanks for contributing to collective intelligence and alcea in the next one do they have detection the human is the question they finally killed quick finger those the counterplay comes in from open a I in fact jumping baton with the silence he's caught the trickster trading they'll get the kill they'll take down the liar arguably the pesky hero that open a I have presented he has been making a lot of plays but now finally team human are able to kill him on [Music]"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Reinforcement Learning",
        "Topic":"Markov Decision Processes (MDPs) in Reinforcement Learning",
        "Video Title":"Markov Decision Process (MDP) - 5 Minutes with Cyrill",
        "URL":"https:\/\/www.youtube.com\/watch?v=4Fqt2Nk2lhY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/4Fqt2Nk2lhY\/hqdefault.jpg",
        "ID":"4Fqt2Nk2lhY",
        "Publish Time":"2023-06-05T04:55:26Z",
        "Channel":"Cyrill Stachniss",
        "Channel ID":"UCi1TC2fLRvgBQNe-T4dp8Eg",
        "Transcript":"foreign I want to talk today about Markov decision processes or mdps mdps are mathematical framework which allows you to make decisions on uncertainty so now they do not perfectly know what the outcome of your action will be so if there are some random component to it then the mdps allows you to make optimal decisions and what the mdp does it maximizes some expected future rewards so whatever you will gain in the future will be taken into account in your decision making process and the mdp can be defined with affordable so you need to Define what are the possible States your system can be in what are the possible actions that you need to execute then you need to specify a trans so-called transition function which tells you given them in a certain State and execute an action to reach another state what the probability of this going to happen and last it specifies the reward what do I gain if I'm in certain State what's a good State what's the bad state so you can make a very simple example for example we have a mobile robot that lives in a very simple world and there is one state where charging station is where the robot gets its energy so a positive reward while there's a staircase where the robot may fall down the staircase so a place we want a white the robot to go so the robot is somewhere and wants to reach the charging station what should it do if it would perfectly know what it does So Perfect action execution we would probably navigate along the shortest path to the charging station if we however take into account that some unexpected thing may happen the robot executes a random command in a certain probability or a certain set of cases then it can happen that the robot accidentally falls down the staircase and how should we be behave in order to avoid that and what the mdp does it tells you what to do in every state a computer so-called policy which minimizes the probability of the robot falling down the staircase so the solution to an mdp is a so-called policy it's basically a recipe which tells you if you're in a certain State execute that action and this will maximize your expected future reward and they're basically two techniques or two algorithms which allow you to compute a solution to an MVP the first thing is value iteration an approach going back to belman 1957 which optimizes the very famous Belmont equation and it uses a utility of a state and you can tease a utility in what's the potential future reward that I can get in that state and it tries to compute the utility for that state and then basically does a gradient descent in this utility function and it is an iterative approach which always updates the utility of every state with a dynamic programming solution in order to compute a solution to another approach is policy iteration here you try to avoid working with the utility function and you're directly operating on a policy and iteratively updating your policy until that policy converges and with this policy you basically have a handbook or a recipe which tells you what to do in which state in order to behave as good as possible I hope that was useful and introduce you to the very basics of decision making under uncertainty we so far have only taken into account uncertainty about the action execution if we take unobservability or partial observability into the game that means we do not know in which state we are in then Things become much more complicated and we enter the world of palm DPS or partially observable Markov decision processes but those are much much harder to solve with this thank you very much for your attention thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Reinforcement Learning",
        "Topic":"Markov Decision Processes (MDPs) in Reinforcement Learning",
        "Video Title":"Markov Decision Processes - Georgia Tech - Machine Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=Jk2V9yA82YU",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Jk2V9yA82YU\/hqdefault.jpg",
        "ID":"Jk2V9yA82YU",
        "Publish Time":"2015-02-23T19:51:30Z",
        "Channel":"Udacity",
        "Channel ID":"UCBVCi5JbYmfG3q5MEuoWdOw",
        "Transcript":"So this is the framework that we're going to be using through most of the discussions that we'll be having at least on reinforcement learning. The single agent reinforcement learning, and it's called the Markov Decision Process. This should sound familiar to you, Michael. >> Well, you did say we're going to talk about decisions. >> That's true, and we need a process for making decisions. And we're going to introduce something called the Markovian property as a part of this discussion and I'll tell you exactly what that means in a moment. So, I'm just going to write out this frame work and just, and tell you what it is and what the problem it produces for us. And then we're going to start talking about solutions through the rest of the discussion. So a Markov Decision Process tries to capture worlds like this one by dividing up in the following way. We say that there are states. And states are a set of tokens that somehow represent every state, for lack of a better word, that one could be in. So, does that make sense to you, Michael? >> Yeah, I think so. >> So what would the states be in the world that we've been playing around in so far? >> So, the only thing that differs from moment to moment is where, I guess, I am. >> Mm-hm. >> Like, which grid, grid position I'm in. >> Right. >> So, I feel like each different grid position I could be in is a state, maybe there's a state for being successfully done or unsuccessfully done? >> It's possible. But let's stick with the simple one. I like that one because that's really, I think, easy to grasp. So, there are at least, of all the states one could reach, there's, well let's see there's four times three minus one, since you can never reach this state. Although we could say it is a state we just happen to never reach it. So, at most if we just think of this grid literally as a grid there are something like twelve different states. And we can represent these states as their X,Y coordinates, say. We could call this, the start state as say 1,1, which is sort of how I described it earlier. We could describe the goal state as 4,4. And say this is how we describe our states. Or frankly, it doesn't matter. We could call these states 1,2,3 up to 12. Or we could name them Fred and Marcus. It doesn't really matter. The point is that they're states, they represent something, and we have some way of knowing which state we happen to be in. Okay? >> Sure. >> Okay."
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Reinforcement Learning",
        "Topic":"Markov Decision Processes (MDPs) in Reinforcement Learning",
        "Video Title":"Markov Decision Processes - Computerphile",
        "URL":"https:\/\/www.youtube.com\/watch?v=2iF9PRriA7w",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/2iF9PRriA7w\/hqdefault.jpg",
        "ID":"2iF9PRriA7w",
        "Publish Time":"2022-10-25T17:03:08Z",
        "Channel":"Computerphile",
        "Channel ID":"UC9-y-6csu5WGm29I7JiwpnA",
        "Transcript":"today I wanted to talk a little bit about the the models and the algorithms we use for planning under uncertainty for robots so how robots make decisions when the world is against them the world's always against robots Yeah Yeah It's Tricky sure probably the best place to think about or the best place to start is a shortest path algorithm so I think that's I'm going to try and draw it and just to warm up the drawing um so you can imagine that you're starting off at home and you're trying to get to work and I should say that this is a an example of stolen from a talk I saw a few years ago and I'll give you the the acknowledgments for that later but you've got let's say the simplest shortest path problem you've only got one action so you can go by bike and that might take you 45 minutes and you can go buy car and that might take you let's say 30 minutes and you could go by train and maybe that takes you 35. this is a kind of a standard decision-making problem right we've got two states so our states are being at home or being at work and we've got three actions so we can go by bike we can go by car or we can go by train we use these kind of sort of State transition models everywhere for robotics so if I'm programming a robot to pick something up or navigate between in a map the last time we were together we're looking at robots navigating through these graphs and we we turn those graphs into these kind of decision problems as well so this is a kind of a model you could use to decide well what route do I use to get to work and in this case if we're trying to minimize time or minimize cost then we would take the the car option which is 30. I'm going to do my teaching what's the problem with this model too simple it's too simple so it doesn't reflect the fact that life is just yeah the world can be against you so the world can be uncertain so the standard thing we do in robotics is we take these what we call deterministic models so this is a model where the outcome is fully determined by the state you're in so you know where you're going to be and we can say well we can actually we can make them more complicated we want to reflect the uncertainty so I'll draw a kind of a big version First and we can think about that so we imagine we take our action which we might call car and that's still going to cost us 30 or it's going to take 30 minutes right that's my planning model this is how we're going to model the world but then what we're going to think about is actually there are going to be different outcomes so when I execute this action so when I perform that action in the world I can still only end up in one of three different states but they might be different traffic levels so it might be light medium and heavy that's kind of amazing I've got in my car I've chosen to take my car to work I've driven a little way and I hit the motorway or whatever and it's it's one of these traffic States and then I can go to work from that but what we could imagine is when we move out of those States it's going to take us a different amount of time so what we might do is we might use that to change our model so this is the basic idea and what we would do is we would assign different probabilities we might say 20 of the time or 0.2 the traffic's light 70 of the time it's medium Temps at the time or probability of 0.1 there's heavy traffic so this is reflecting some of that uncertainty some of that complexity in the world this is kind of the standard action model we'd use and when we use action models like this this is called a Markov decision process so a Markov decision process is States and actions just like a shortest path problem except now when I take my action there's a probability distribution over the the the outcomes I can reach I think we did Mark off sometimes talks about the text isn't it right yeah so Markov is a probabilistic assumption when you've got a state it doesn't need actions right it can just be State transitions excuse me State transition systems and what we do is a mark of assumption says the probability of an outcome only depends on the current state so it doesn't matter the history it doesn't matter what I was doing before I chose to leave home the only thing that determines that probability is the action I'm taking right now to be more formal you can think of that as a sort of first order Markov system and you can have a second order Markov system which says the only the current state and the previous state um determine my probabilities but in general if you think of someone like a Markov chain if you're doing text processing you can think about word generation following probability distributions and there you can think of like one step so if I give you the there's a probability of getting capped or if you if I give you I am the then you get a probability of getting something different and if you take that whole history if you make there'll be a third order Markov assumption you'd get a different probability distribution over the word you'd get next that's desperate says um right so that's our kind of action that's our action model and so we can take this action model we can stick it into the the graph we have before the process we have before let's set this up again and we'll draw that little area at the top that means this is our initial state so just like a shortest path problem we're going to start from somewhere and we typically do this in robotics because our robot has to be someone when it starts thinking and we're gonna make our whole world a bit more complex so I can either go to the railway I can go by car I can go by bike so these these actions now really are reflecting the choices I'm going to make to start with so not necessarily getting on and doing that but sort of making the decision that this is what I want to do and maybe go into that form of Transport so now car is going to take one this is me leaving the house and going to to my parking space and finding what car Railway maybe I have to walk a bit further to get to the railway so that'll be a cost of two and then we can start to put these these um actions in so actually these arrows should be dots and I can put in the the action that we had before where we have different traffic levels so light we've got medium I'm going to start abbreviating these things because my handwriting is atrocious and will take forever just watching me write words or heavy and then down at the bottom down here we're going to have work and I'm purposely doing two circles here because we cook this this state transition system we have our initial state with the Arrow coming in and I do these two rings to mean this is where we want to end up we call this an absorbing State there's no there's no way to get out of it when you're there and as this is work that's a little bit depressing but there we go and then we might have an action called Drive so our Drive action is going to be deterministic in that I can take it and it always takes me to work but I can only take it in certain States so I can only take it once I'm in my car I'm only like to drive then and we can use this now to model the fact that we might get different durations of travel so let's say the heavy traffic it takes me 70 minutes to get to work in heavy traffic 30 and medium 20 minutes in light and I think we have these probabilities of 0.2 0.7 and 0.1 so this shows me that if I go by car I get some probability of these different traffic levels and once I've experienced that traffic level there's no turning back I have to follow the car I have to follow that action choice so use the car to get to work and it might take a different amount of time we're going to say here the bike is deterministic so if I'm on a bike I'm fully in control of the duration it takes me so that will take 45 minutes so it's further by bike but there's no uncertainty and that could be interesting later in this example we can say that when I go to the railway station well maybe 90 of the time it seems quite unlikely the trait the train is there it's ready to go and then it takes me I've got an action which we'll call relax because once we're on the train we're happy and that's going to take me let's say 35 to get to work and then 10 of the time I have to wait so I'm in the waiting room and then from the waiting room I get to wait and then 90 of the time the train comes ten percent of the time I get to wait again so what's interesting here is there's a kind of notion of a Time step you can start to imagine that I'm I'm looping um I wait the train comes I wait the train doesn't come and I repeat that and of course if I get stuck in the waiting room for too long I could also choose to go home and what's nice about that is that I get to make another choice so that might cost me two my waiting maybe I wait for three minutes each time so now I've got this big graph and this is the Markov decision process this is a process because I'm going through a sequence of steps at each step I have to take a choice over what action to perform and then that evolves the state so it changes the state I'm in and we have different probabilities of reaching reaching different states that's kind of still in some sense we can think about this as the shortest path algorithm but instead of a shortest path we call this a stochastic shortest path because what we want to do is reach our goal so we're still trying to reach our goal but now we're dealing with the uncertainty as well this is a model this describes the choices your agent could have so this is the choices you might have when you're going to work we could imagine I'm building an autonomous vehicle and this is you know it's an autonomous taxi system and it has to decide how to take you to work these kind of models really capture what we the decision you know a decision-making part of artificial intelligence whether that's robotics whether that's chat Bots all sorts of stuff um so the next question is really how do we how do we solve this and there's there's two answers or there's two parts that answer the first thing is what does the solution look like the second bit is is what algorithm do we use when you say what does it look like you've been as in is it is it just a simple number or is that yeah well so in the okay what structure does the solution have so if it was the shortest path problem the the solution is a path right it's because it and that's a sequence of the actions you will take from start to finish um but in in a in an mdp in a markup decision process we can't use a path because when I take a step the world changes probabilistically stochastically so I I don't know what world I'm what state I'm going to end up in so instead of a path we have what's called a policy so policy is a lookup table that just says when you're in this state take this action and that state could be the States from the problem or we could even augment them so as the as these models get more complex we might add extra information into the state we could add the time of day we could add um you know how much time is remaining before some kind of deadline um how many times we've waited in the waiting room might have been interesting you can add extra to your state to help make decisions but in general the policy rather than just being a straight line plan a sequence of actions it's going to be a lookup table that says typically what is the optimal action so what's the best action to take to achieve a particular specification so a particular goal that you're trying to reach um when you're in this state and you can think of for mdps there's all sorts of there's a very rich space of specifications the most common one for something like this for a stochastic shortest path problem is to minimize the expected cost to goal so the the expected cost is the average cost so if I get if I'm going to work you know every day for the rest of my life and I have these these choices um then actually having the expected so the average cost kind of makes sense because I get to make this I get to execute the policy for this this problem lots and lots of times sometimes I do well sometimes I do badly but all of that kind of evens out over the time um so we could we could think about that to start with um so my goal is to get to work my cost is the time so what I want to do is minimize the cost to get get to um to get to the goal and so I need an algorithm that's going to compute the the cost of the so we think about the cost of the expected cost of the whole policy starting from the initial state which is going to be similar and it affect thinking about the average cost of a path from the start to the end or the cost of the average path not the average cost of the part the other way around because the paths change because of the probabilities the way we solve this is with something called a Bellman equation so the Bellman equation describes how good a particular action choice is so we need to think about the different action choices available to us in a state and we use those we kind of value those choices and those that we basically pick the best action which in this case will be the action with the lowest cost and we put that in the policy which is also about why we can't use things like a star or dijkstra a star and dijkstra deal with deterministic problems so they deal with State transition systems that might look a little bit like this but they assume the actions are purely deterministic so they produce past from start to finish there's no probability there's no probability yeah so they they assume that when they take an action the world changes in the wave of action behaves there are kind of analogs for some of these things in the probabilistic world then the algorithm we'll get to is is it has similarities to the way dijkstra's algorithm Works um and actually a star is a heuristic search algorithm there are also heuristic search algorithms for Markov decision processes so they're classes of algorithms that fit well together and but for this we can't take dijkstra or a star and say you know off we go if you're thinking about just the average cost then everything gets a lot easier and we what we're basically going to do is take all the the different we're going to multiply the cost by the probability of getting that cost sum them up and that tells us really how good that action is so it's just kind of like it's almost like taking something like a normal shortest path algorithm and collapsing all of the actions into an average action but we have to do that recursively because that each action takes me to another state where I can apply a different I have to make a different action choice so we have this recursive problem of my actions are only as good as the states they reach that's the kind of recursive part but I also have to think about the fact that that happens probabilistically which is where this sort of averaging comes in for the expected cost we can also think about trying to limit the probability of success or failure or the probability of certain extremes I think we can talk about that later um the algorithms are much much harder but but but super interesting and actually don't those are the things you care about when your boss says you have to be here by 10 o'clock for a meeting or your patience runs out if your commute is longer than 45 minutes so real world problems are much I think real world problems are better captured by those richer specifications I mean it looks to me like the waiting room is the sort of is the kicker here because it's potentially you know quite a bit more complicated right yeah so if you went down the train route then you've got multiple kind of nests of that right so it depends on what problem you're trying to solve so if we're solving expected value if I build a policy to only optimize the average cost of a journey then the best choice is car I think it's something like 33 or 34 for minutes which is faster than bike which is 45 and I think train comes in somewhere over 35 or over 37 on average so it's more expensive than car cheaper than bike but yeah the average thing is interesting and this is where these models become fun for Robotics and for problem solving which is your by choosing the car 10 of your Journeys take you 71 minutes right which would make you extremely late late for work if in general you should be taking 30 to 40. so you you if you only solve for average value then 10 of the time you're very late and so what you can think about in these kind of problems is different what we call either specifications or objective functions so if you're doing some optimization you typically think about the optimization the objective function the thing you're minimizing or maximizing and in when we do planning or decision making with these kind of models we often phrase it as a specification that which is a mixture of the goal you're trying to reach and the constraints you're trying to put on it yeah so for instance you're trying to do it in the shortest possible time but it should never be longer that yeah that that's a perfect example so if you try to do it in if you if you say it can never be longer than um yeah I'm going to say 60 because the mass I remember the mass of this so if you say it for 60 well you always take the bike because every everything else there's a chance the car takes 70. um there is a non-zero probability that the train also takes longer than 60. so there is a non-zero probability in the if you take the train you're waiting forever effectively um but the bike is deterministic but the bike is long so it's guaranteed to be less than 60. but it's guaranteed to always be 45 so you're always going to be later than you want to be um the the kind of optimal strategy I think to get in under 60 um guaranteed to get under 60 whilst minimizing time is to go to the train wait for three Cycles wait and then go home and get the bike so effectively you can think about the chance of getting the train within three cycles and then if you go any longer then it's going to take you longer you know you'll violate your deadline so then you go back to get the bike but that's a those kind of solution mechanisms are really I've actually been in that scene yeah exactly um it happens a lot particularly kind of post-covered where you're like well I could work from home but I should be there oh the Train's late I'm just going to go back and put my pajamas back on but you need to think about what we call an augmented State space you'd put the time that you've waited into the state and that allows your policy to include actions you know I you think about actions that I'm in the waiting room and time is now at nine minutes and those things that you do in your head going oh god I've waited this long maybe the Train's never coming you can Factor those into the the decision making as well if you know exactly where you want the robot to go you take the 3D map and then you manually annotate it with the points in this case we're doing it entirely well the graph is built autonomously using that that Lego representation so you say what we call submitted Computing in particular what we want is the"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Reinforcement Learning",
        "Topic":"Implementing MDPs with Python",
        "Video Title":"Value Iteration Algorithm - Dynamic Programming Algorithms in Python (Part 9)",
        "URL":"https:\/\/www.youtube.com\/watch?v=hUqeGLkx_zs",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/hUqeGLkx_zs\/hqdefault.jpg",
        "ID":"hUqeGLkx_zs",
        "Publish Time":"2021-04-25T14:00:12Z",
        "Channel":"Coding Perspective",
        "Channel ID":"UCvvxCVNt2EOqGB65Mq4zwSw",
        "Transcript":"hello everyone this is the knight video of dynamic prominent algorithms video series in this video i will short introduce marco decision process mdp and how to find the optimal solution by using the value iteration algorithm an mdp can be defined by set of states as set of actions a a transition function p and lastly a reward function r in this process at any iteration k we are in state s k an element of set s then we take an action a k in set a and perform it taking this action ak yields rska k reward and we move to another state sk plus 1 depending on our action this transaction happens with some probability p s k plus 1 given s k and a k our aim in this problem is to find the optimal set of actions that maximize total revenue for a given initial state to solve this problem we will use value iteration algorithm first let qsa be the expected total reward for taking action a in state s also let the spd max maximum expected to reward starting from state s v s is sometimes called a value function in s we can calculate q s a as you see rsa is the immediate reward after taking action a and the summation is the expected future reward after moving to next state s prime qsa is simply equal to the summation of these two then maximum value of qsa over a gives us a vs in the value iteration algorithm we will calculate the value functions iteratively until they converge as an initial step we set the value functions to zero then we repeat calculating vs for each state s until it converges to calculate vs we first calculate qsa as an intermediate step up for action a then take its maximum over a now let's cut this algorithm in python by writing a function called value iteration the input of this function is the set of states s as a list the set of actions a as a list the transition function p is a function and lastly reward function is a function i will later talk about how to define these inputs for a specific example we first set the value functions for each state s to 0. here i initialize the value function v as a dictionary instead of a list or an array just because we keep track of the state as a key then we have a while loop of its sorting through condition we will terminate this loop by using break once we get the convergence since we will update the value functions at each iteration i define a variable called odd wheel to store the value functions at the previous iteration we can initialize it as a copy of variable v as a termination condition we check whether the value function remains the same by comparing oddly with v to update the value function vs for uh state s at any iteration we first calculate q for each action a as you see one important point here is that we use alt v instead of v while computing the expected future reward in this sum because we update the while iterating state s then we can calculate and update the s by finding the maximum value of q over a once we attain the convergence we return value functions as an output before showing how to define these inputs it's worth mentioning that this is not the best way to code this algorithm in terms of the real-time computation performance right now i am not concerned about it because we will share a video about how to make these functions faster here i believe it's important to understand implementation of such an algorithm improving is the next step to use this function with an example let's consider a gambler's problem in this problem the gambler starts the initial fortune of size s0 and plays until the fortune reaches the value n or zero at each gamble the gambler either wins the bat with probability p or loses it with probability 1 minus p the gamble gains some reward only if the fortune reaches the value n our aim in this problem is to calculate the expected reward for any given s0 the state of the system is the current fortune so the set s is from 0 to n where 0 and n are the absorbing states the maximum possible amount of bet is the minimum of s and m minus s since the fortune cannot be less than 0 or more than n so the set a depends on the current state s and it consists of values from 0 to minimum of s and minus s in this implementation instead of restricting the set a i will define the transition function t in a way that any non-feasible transition will not be allowed so we define set a from 0 to m and transition function p as you see it returns p in case of winning 1 minus b in case of losing and 0 otherwise lastly we defined function r in a way that it returns one if s is equal to n because it's the only state that the gambler wins the game now we are ready to run our function with these inputs these inputs are totally problem specific so depending on your problem you may need to modify them before showing the outputs i will modify value iteration function so that we can obtain the optimal policy along with the value functions we initialize a dictionary called optimal policy where the keys are the states then we update this dictionary whenever we update the value function by finding the action that maximizes q and we return optimal policy as an output for the gambler's dream example let the maximum possible fortune and be equal to 10 and probability p be equal to 0.4 when we run our value iteration function for this problem the outputs look like as you see you can see how much the gambler needs to bet in each state by looking at optimum policy dictionary for example the gambler needs to bet 5 when the current fortune is 5 and 3 when the current fortune is 7. in this video i talk about how to implement value iteration algorithm i hope that it was helpful if you have any question about the problem or the algorithm please leave a comment in the next video of this dynamic chromatic video series i will talk about policy iteration algorithm and how to implement it in python"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Reinforcement Learning",
        "Topic":"Implementing MDPs with Python",
        "Video Title":"Understanding MDPs in Python - Part 1",
        "URL":"https:\/\/www.youtube.com\/watch?v=REx9z3R7_Zw",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/REx9z3R7_Zw\/hqdefault.jpg",
        "ID":"REx9z3R7_Zw",
        "Publish Time":"2024-01-06T08:49:07Z",
        "Channel":"F.",
        "Channel ID":"UCwK0IqN_oWAvMKLEVJn7fnA",
        "Transcript":""
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Reinforcement Learning",
        "Topic":"Implementing MDPs with Python",
        "Video Title":"Policy Iteration Algorithm - Dynamic Programming Algorithms in Python (Part 10)",
        "URL":"https:\/\/www.youtube.com\/watch?v=RlugupBiC6w",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/RlugupBiC6w\/hqdefault.jpg",
        "ID":"RlugupBiC6w",
        "Publish Time":"2021-05-08T14:00:10Z",
        "Channel":"Coding Perspective",
        "Channel ID":"UCvvxCVNt2EOqGB65Mq4zwSw",
        "Transcript":"Hello everyone! This is the tenth video of the dynamic programming algorithms video series. In this video, I will introduce the policy iteration algorithm and show how to code it in Python. Policy iteration is a way to find the optimal policy for a Markov decision process (MDP) problem. In the previous video of this video series, I talked about MDP and showed how to solve it by using the value iteration algorithm. Since I will use some notation and material from the previous video, I recommend you watch it by clicking the link above if are not familiar with MDP or value iteration. As a reminder, an MDP can be defined by set of states S, set of actions A, a transition function P, and lastly a reward function R. P(s prime | s, a) is the probability of moving a state s to another state s prime when action a is taken. And, R(s,a) is the reward for taking action a in state s. Our objective is to find the optimal policy that maximizes total reward where policy assigns an action to each state. To find the optimal policy, we will use the policy iteration algorithm. In the policy iteration algorithm, starting with an initial policy, we will find the optimal policy by iteratively updating it until it converges. On the other hand, in the value iteration algorithm, starting with initial value functions, we find the optimal value functions by iteratively updating them until they converge. This is the main difference between these two algorithms. The policy iteration is an iterative algorithm and consists of three steps. In the first step, we initialize a policy pi_0 with some feasible actions. Then, at any iteration i, pi_i denotes some policy with pi_i(s) being the action in state s under policy pi_i. Then, we define V^{pi_i}(s) as the expected reward or the value function in state s under the policy pi_i. We can calculate V^{pi_i}(s) by taking action pi_i(s) as you see for each state s. On the right-hand side, the first part of the summation is the immediate reward, and the second part is the expected future reward. Note that V^{pi_i} appears in both sides of the equation. Here, we have cardinality of the set S different linear equations to solve so that we can calculate the value functions under the policy pi. This is the second step and called policy evaluation as we evaluate the current policy pi_i. In the last step, we improve our current policy by using V^{pi_i}. Basically, pi_{i+1}(s) is equal to argmax of this expression. Again, the first part of the summation is the immediate reward, and the second part is the expected future reward. Until pi_i is equal to pi_{i+1}, we iterate step 2 and 3. So, we can summarize the policy iteration algorithm as you see. In step 1, we choose a policy pi_0 and set i to zero. Then, at any iteration i, we calculate the value functions V^{pi_i}(s) for each state s under policy pi_i by solving some set of linear equations. This is the policy evaluation step. After calculating these value functions, we compute pi_{i+1}(s) for each state s by using V^{pi_i}(s). And, this is the policy improvement step. We repeat these two steps until policy pi converges. Now, let\u2019s code this algorithm in Python by writing a function called \u201cpolicy_iteration\u201d. The inputs of this function is the set of states S as a list, the set of actions A as a list, the transition function P as a function, and lastly the reward function R as a function. These inputs are totally problem specific. I will later define them for a specific example. In the algorithm, we first initialize policy dictionary by setting its values to some arbitrary feasible action for each state s. Here, I choose to set them to the first element of action set A. Then, we have a while loop with a certain True condition. We will terminate it by using break once our termination condition, convergence of the policy, is satisfied. Since we will update the policy at each iteration, I define a variable called old_policy to store the policy at the previous iteration. We can initialize it as a copy of variable policy. As a termination condition, we check whether the policy remains the same by comparing the variable old_policy with the variable policy. To update the policy at any iteration, we first calculate the value functions under the current policy. Here, I use a function called policy_evaluation. This function takes the current policy along with the set of states S as inputs and then return the value functions under this policy an output. It\u2019s the policy evaluation step of the algorithm. I will show how we can write this function in a bit. After calculating the value functions, we update our current policy by using a function called policy_improvement. Different than the function policy_evaluation, it takes the value functions together with the set of states S and the set of actions A as inputs and returns the updated policy as an output. This is the policy improvement step of the algorithm. If it\u2019s the same with one in the previous iteration, we break the loop. Finally, we return policy as an output. To complete the implementation of the policy iteration algorithm, now I will show you how we can define policy_evaluation and policy_improvement functions. In policy_evaluation function, the idea is updating the value functions under the given policy until they converge. For that, we first initialize the value functions as a dictionary called V. Then, we have a while loop with a certain True condition. In this loop, we first create a copy of variable V, called oldV. We use the variable oldV while checking the convergence. Then, we update the variable V for each state s as you see. This is the expression I showed you earlier while talking about policy evaluation step. Lastly, we return variable V as an output. On the other hand, in policy_improvement function, we will find the best policy for given value functions. We first initialize a dictionary called policy. Then, for each state s, we update the variable policy. Basically we find the argmax of the expression called Q over action a. This is the expression I showed you earlier while talking about policy improvement step. Lastly, we return the variable policy as an output. Now, we are ready use the function policy_iteration with an example. I will use the same example with the previous video, a gambler\u2019s problem. So, I just copy the variables S, A, P, and R from the previous video. To learn more about this gambler\u2019s problem and how we defined these variables, please click the link above. To compare the results with the value iteration algorithm in the previous video, I set N to 10 and probability p to 0.4. Now, we are ready to run our policy_iteration function for this problem. The output optimal policy looks like as you see. You can see how much the gambler needs to bet in each state by looking at this output. For example, the gambler needs to bet 5 when the current fortune is 5 and 3 when the current fortune is 7. We obtained the same policy as we did by using the value iteration algorithm in the previous video. In this video, I talked about how to implement policy iteration algorithm. I hope that it was helpful. If you have any question about the algorithm or its implementation, please leave a comment."
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Reinforcement Learning",
        "Topic":"Implementing MDPs with Python",
        "Video Title":"Markov Decision Process (MDP) - 5 Minutes with Cyrill",
        "URL":"https:\/\/www.youtube.com\/watch?v=4Fqt2Nk2lhY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/4Fqt2Nk2lhY\/hqdefault.jpg",
        "ID":"4Fqt2Nk2lhY",
        "Publish Time":"2023-06-05T04:55:26Z",
        "Channel":"Cyrill Stachniss",
        "Channel ID":"UCi1TC2fLRvgBQNe-T4dp8Eg",
        "Transcript":"foreign I want to talk today about Markov decision processes or mdps mdps are mathematical framework which allows you to make decisions on uncertainty so now they do not perfectly know what the outcome of your action will be so if there are some random component to it then the mdps allows you to make optimal decisions and what the mdp does it maximizes some expected future rewards so whatever you will gain in the future will be taken into account in your decision making process and the mdp can be defined with affordable so you need to Define what are the possible States your system can be in what are the possible actions that you need to execute then you need to specify a trans so-called transition function which tells you given them in a certain State and execute an action to reach another state what the probability of this going to happen and last it specifies the reward what do I gain if I'm in certain State what's a good State what's the bad state so you can make a very simple example for example we have a mobile robot that lives in a very simple world and there is one state where charging station is where the robot gets its energy so a positive reward while there's a staircase where the robot may fall down the staircase so a place we want a white the robot to go so the robot is somewhere and wants to reach the charging station what should it do if it would perfectly know what it does So Perfect action execution we would probably navigate along the shortest path to the charging station if we however take into account that some unexpected thing may happen the robot executes a random command in a certain probability or a certain set of cases then it can happen that the robot accidentally falls down the staircase and how should we be behave in order to avoid that and what the mdp does it tells you what to do in every state a computer so-called policy which minimizes the probability of the robot falling down the staircase so the solution to an mdp is a so-called policy it's basically a recipe which tells you if you're in a certain State execute that action and this will maximize your expected future reward and they're basically two techniques or two algorithms which allow you to compute a solution to an MVP the first thing is value iteration an approach going back to belman 1957 which optimizes the very famous Belmont equation and it uses a utility of a state and you can tease a utility in what's the potential future reward that I can get in that state and it tries to compute the utility for that state and then basically does a gradient descent in this utility function and it is an iterative approach which always updates the utility of every state with a dynamic programming solution in order to compute a solution to another approach is policy iteration here you try to avoid working with the utility function and you're directly operating on a policy and iteratively updating your policy until that policy converges and with this policy you basically have a handbook or a recipe which tells you what to do in which state in order to behave as good as possible I hope that was useful and introduce you to the very basics of decision making under uncertainty we so far have only taken into account uncertainty about the action execution if we take unobservability or partial observability into the game that means we do not know in which state we are in then Things become much more complicated and we enter the world of palm DPS or partially observable Markov decision processes but those are much much harder to solve with this thank you very much for your attention thank you"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Reinforcement Learning",
        "Topic":"Q-Learning and Deep Q Networks (DQN) in Reinforcement Learning",
        "Video Title":"Deep Q-Networks Explained!",
        "URL":"https:\/\/www.youtube.com\/watch?v=x83WmvbRa2I",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/x83WmvbRa2I\/hqdefault.jpg",
        "ID":"x83WmvbRa2I",
        "Publish Time":"2023-11-28T15:00:49Z",
        "Channel":"CodeEmporium",
        "Channel ID":"UC5_6ZD6s8klmMu9TXEB_1IA",
        "Transcript":"greetings fellow Learners in this video we are going to talk about q-learning but let's start this with an inquisitive question can you think of a scenario in your daily life where a computer or an AI could benefit from learning on its own just like humans do now this could be anything from optimizing your morning routine to tackling problems at work in this video we're going to explore how q-learning aligns with these real world scenarios so don't be shy to share your thoughts down below we're going to divide it into three passes starting with pass one where we just start with highle definitions and then just get into more details for future passes pay attention because I'm going to be quizzing you later on let's get to it what is q-learning in simple terms well to help me explain this we have our trusty robot Frank Frank say hi he hello what a cutie Frank here is going to navigate this environment and in order to navigate this environment it uses a q table you can think of this Q table as Frank's conscience that tells Frank what to do in every situation all right I see where you are Frank so what do you want to do here my Q table is telling me to go right gotcha gotcha all right now that you're there what about now my Q table is telling me to go down you go for it buddy now this is a simple world of just nine grid squares where there's a finite number of states but what if there were a lot of States well that would mean that this Q table becomes really big too big to fit in Frank's tiny little head it would literally run out of memory so instead of having this table as a conscience it's probably better to just have a function as Frank's conscience and this function would just take in a state and an action and it would output some value that emphasizes how good that action was taken for the state this value is known as a q value by the way quiz time have you been paying attention let's quiz you to find out what role does the Q table play in Frank's decision process in the simplified world of nine squares a it directly tells Frank what action to take in each state B it serves as a backup memory for Frank in case the Q function fails C it's used for aesthetic purposes to decorate Frank's environment or D it has no impact on Frank's decision making and he relies solely on the Q function comment your answer down below and let's have a discussion now at this point if you think I deserve it and you love frank please do consider hitting that like button that'll do it for quiz time for now but keep paying attention CU I'll be back [Music] again so we replac this giant Q table with a function in the context of AI this function is typically a neural network so q-learning is to learn values in a q table deep Q learning is used to learn the parameters of a q Network or a nural network now this Q network is randomly initialized and represents Frank's conscience we have a Target Network that represents the ideal conscience and this is the same architecture as the Q Network the goal now is to compare Frank's current conscience with Frank's ideal conscience and from this comparison we can compare them get a loss and use this loss to update the parameters in the Q network with back propagation and then the cycle repeats in the next pass we're going to get into more details but for now in order to train this network we actually first need to take a step back and collect this data so let's just talk about that very briefly so this data collection first of all is done using this Q Network it's first randomly initialized and some state is chosen the Q network will generate the action to take this action is taken and then reward is gained and the agent goes to the next state so the quadruple of the state the action the reward and the next state is then taken and stored in some memory called The Experience replay buffer and we repeat this process of just choosing some State producing these quadruples and storing them so this data is now in our experience replay and then can be later used to train our Q Network which we just discussed before quiz time it's that time of video again have you been paying attention let's quiz you to find out why is experience replay considered beneficial in deep Q learning a it helps the Q Network generate actions more quickly B it stores the Q values for future reference C it breaks the temporal correlation and improves learning stability or D it ensures that the Q network is always initialized with the ideal parameters comment your answer down below and let's have a discussion that'll do it for quiz time for now but once again I will be back so pay attention [Music] now there are two phases that we discussed in past two that is the data collection phase and the training phase so let's talk about both in detail starting with the data collection phase so first we randomly initialize the Q Network we choose an initial State pass it through the Q Network and the number of neurons by the way in the output layer of this network is the number of possible actions so each neuron outputs a q value for the input State at a specific action so for example if we could only go left right up or down there would be four neurons in the output layer and each of these neurons would correspond to a q value for each of those actions so we then choose an action in an Epsilon greedy fashion we take the action in the environment or in some simulated environment and once the action is tanken Frank will get a reward and Frank will also be in a new state so now we take that quadruple that we just described that is the state the action the reward and new state and store it into the experience replay buffer now we can randomly choose a new state and repeat the process of collecting the data in the experience replay buffer and this is the data collection piece now on to training now we have this experienced replay which has a lot of data and we can use this data to train the Q Network so let's take a batch of this data now in this case just for explanatory purposes let's say the batch size is just one example so that we can see what's going on throughout we feed the current state of the quadruple into the Q Network we get the Q value corresponding to the action in the quadruple and then we also take the the same current state pass it through the target Network and we get the highest Q value there now we take the reward from the quadruple and add it to this target Q value so now we have two main values one from the target it's an ideal Q value and then we have the current Q value now we're going to compute the mean squared loss using the Q Network's Q value that represents Frank's conscience and the target final Q value that represents the optimal decision this loss is then back propagated into the Q Network and the parameters are updated so Frank learns but the target Network Remains the Same now we repeat the process for a few batches where Frank's conscience continues to learn but the target Network Still Remains the Same and then after a few batches we will update the parameters of the target Network to then match the Q Network and then continue and eventually Frank's conscience and decision making improves and he's a champ quiz time I'm back have you been paying attention let's quiz you to find out why is a target Network used in the training process of deep Q learning a to randomly initialize the Q Network B to represent Frank's current conscience C C to provide an ideal reference for parameter updates and improve learning stability or D to store experiences in the experience replay buffer comment your answer down below and let's have a discussion now that'll do it for quiz time for now and unfortunately I'm not going to be back again so before we go let's get a summary of the video a q table represents an agent's conscience the Deep Q networks combine Q learning with neural networks dqn construction has two phases a data collection phase and a training phase the data collection collects independent quadruples of experiences in data stores called The Experience replay buffer in the training phase trains the Q network using data in the this experienced replay buffer and that's all we have for today but if you want to know information more about the raw Q learning algorithm do check out my video right on screen here and as always thank you all so much for watching please consider liking the video If you do think I deserve it and I will see you in the next one bye-bye"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Reinforcement Learning",
        "Topic":"Q-Learning and Deep Q Networks (DQN) in Reinforcement Learning",
        "Video Title":"Deep Q-Learning\/Deep Q-Network (DQN) Explained | Python Pytorch Deep Reinforcement Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=EUrWGTCGzlA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/EUrWGTCGzlA\/hqdefault.jpg",
        "ID":"EUrWGTCGzlA",
        "Publish Time":"2023-12-21T00:36:40Z",
        "Channel":"Johnny Code",
        "Channel ID":"UCkHgoTyoWuz_IpVzRUkhJUg",
        "Transcript":"hey everyone welcome to my deep Q learning tutorial here's what to expect from this video since deep Q learning is a little bit complicated to explain I'm going to use the fren Lake reinforcement learning environment it's a very simple environment so I'm going to do a quick intro on how the environment works or also quickly answer the question of why we need reinforcement learning on such a simple environment next to navigate the environment we need to use the Epsilon greedy algorithm both Q learning and deep Q learning uses the same algorithm after we know how to navigate the environment we're going to take a look at the differences of the output between Q learning and deep Q learning in Q learning we're training a q table in deep Q learning we're training a deep Q Network we'll work through how the Q table is trained then we can see how it is different from training a deep Q Network in training a deep Q Network we also need a technique called EXP experience replay and after we have an idea of how deep Q learning works I'm going to walk through the code and also run it and demo it just in case you're not familiar with how this environment Works quick recap so the goal is to get the learning agent to figure out how to get to the goal on the bottom right the actions that the agent can take is left internity we're going to represent that as zero down is one right is two up is three in this case if the agent tries to go left or up it's just going to stay in place so in general if it tries to go off the grid it's going to stay in the same spot internally we're going to represent each state as this one is zero this one is 1 2 3 uh 4 5 6 all the way to 14 15 so 16 States total each attempt at navigating the map is considered one episode the episode ends if the the agent falls into one of these holes or it reaches the goal when it reaches the goal it will receive a reward of one all other states have no reward or penalty okay so that's how this environment works at this point you might be wondering why we would need reinforcement learning to solve such a simple map why not just use a pathfinding algorithm there is a Twist to this environment there is a flag called is Slippery When set to true the agent doesn't always execute the action that it intends to for example if the agent wants to go right there's only a oneir chance that it will execute this action and there's a two3 chance of it going in an adjacent Direction now with slippery turn on pathf finding algorithm will not be able to solve this in fact I'm not even sure what algorithm can that's where reinforcement learning comes in if I don't know how to solve this I can just give the agent some incentive and let it figure out how to solve this so that's where reinforcement learning shines in terms of how the agent navigates the map both deep Q learning and Q learning uses the Epsilon greedy algorithm basically we start with a variable call Epsilon equal and set it equal to one and then we generate a random number if the random number is less than Epsilon we pick a random action otherwise we pick the best action that we know of at the moment and at the end of each episode we'll decrease Epsilon by a little bit at a time so essentially we start off with 100% random exploration and then eventually near the end of training we're going to be always selecting the best action to take before we jump into the details of how the training Works between QQ learning versus deep Q learning let's take a look at what the outcome looks like for Q learning the output the output of the training is a q table which is nothing more than a two-dimensional array consisting of 16 states by four actions after training the whole table is going to be filled with Q values for example it might look something like this so in this case the agent is at state zero we look at the table and see what the maximum value is in this case it's to go right so this is the prescribed action over at Deep Q learning the output is a deep Q Network which is actually nothing more than a regular feedforward Nur Network this is actually what it looks like but I think we should switch back to the simplified view the input layer is going to have 16 nodes the output layer is going to have four nodes the way that we Center input into the input layer is like this if the agent is at state zero we're going to set the first note to one and everything else zero if the agent is at State one then the first Noe is zero and the second Noe is one and everything else is zero so this is called one hot encoding just do one more if it's at the last if we want to put in state 15 then everything all zeros and state 15 is one so that's how the input works with the input the output that gets calculated are Q values now the Q values are not going to look like what's in the Q table but it might be something similar let me throw some numbers in here okay so same thing for this particular input the best action is the highest Q value when training a neural network essentially we're trying to train the weights associated with each one of these lines in the neural network and the bias for all the hidden layers now how do we know how many hidden layers we need in this case one layer was enough but you can certainly add more if necessary and how many nodes do we need in the hidden layer I try 16 and it's able to solve the map so I'm sticking with that but you can certainly increase or decrease this number to see what happens okay so that's the differences between the output of Q learning versus deep Q learning as the agent navigates the map it is using the Q learning formula to calculate the Q values and update the Q table now the formula might look a little bit scary but it's actually not that bad let's work through some examples let's say our agent is in state 14 and it's going right to get to State 15 we're calculating Q of State 14 Action 2 which is this cell the current value of that cell is zero because we initialize the whole Q table to zero at the beginning plus the learning rate the learning rate is a hyperparameter that we can set as an example I'll just put in 0.01 times the reward we get a reward of one because we reach the goal plus the discount Factor another parameter that we set I'll just use 0.9 times the max Q value of the new state so the max Q value of State 15 since State 15 is a terminal state it will never get anything other than zeros in this table Max is zero essentially these two are gone subtracted by the same thing that we had here okay so work through the math this is just 0.01 so we get 0.01 here now let's do another one really quickly agent is here take a right Q of 13 Going to the right 13 is also all zeros this is the one we're updating here is zero plus the learning rate times a reward there is no reward plus the discount Factor times Max of the new state Max of State 14 Max of State 14 is 0.01 subtracted by again it's zero this is equal to 0.9 okay it's actually pretty straightforward now how the heck does this formula help find the PATH so if we train enough the theory is that this number is going to be really close to one and then the states next to it it's going to be 0.9 something and then the states adjacent to that it's probably going to be 0.8 [Music] something and if we keep going we can see that a path is is actually two paths are possible so mathematically this is how the path is found over at Deep Q learning the formula is going to look like this we set Q equal to the reward if the new state is a terminal State otherwise we set it to this part of the KE learning formula let's see how the formula is going be used in training for DEQ learning we actually need two neural networks let me walk through the steps the network on the left is called the policy Network this is the network that we're going to do the training on the one on the right is called the target Network the target network is the one that makes use of the DQ learning formula now let's walk through the steps of training step one is going to be creation of this policy Network step two we make a copy of the policy Network into the target Network so basically we're copying this the weights and the bias over here so both networks are identical step number three the agent navigates the map as usual let's say the agent is here in state 14 and it's going into State 15 step three is just navigation now step four we input State 14 remember how we have to encode the input it's going to look like this 0 1 2 3 12 13 State 14 15 the input is going to look like this as you know neural networks when it's created it comes with a random set of weights and bias so with this input we're actually going to get an output even though these values are pretty much meaningless so we might get some stuff that looks like I'm just putting in some random numbers okay so these Q values are meaningless and as a reminder this is the left action down right and up okay step five we do the same thing we take the exact same input State 14 and send it into the target Network which will also calculate the exact same numbers because the target network is the same as the policy Network currently step six this is where we calculate the Q value for State 14 we're taking the action of two is equal to since we're going into State 15 it is a terminal State we set it equal to 1 Step seven step seven is to set the target input State 14 output action two is this node we take the value that we calculated up on step six and we replace place the Q value in the output step number eight we take the target Q values we use it to train the policy Network so this value is the one that's really going to change as you know with neural networks it doesn't go straight to one it's going to go toward that direction so maybe maybe it'll go to 0.01 but if you repeat the training many many times it will approach One Step nine is to repeat the whole thing again of course we're not going to create the policy Network we not going to make a copy of it so what we're repeating is steps three through 8 step 10 after a certain number of steps or episodes we're going to sync the policy network with the target Network which means we're going to make them identical by copying the weight and biases from the policy over to the Target Network after syncing the networks we're going to repeat nine again and then repeat 10 until training is done okay so that's generally how deep que Learning Works um that might have been a little bit complicated so maybe rewind the video watch it again and make sure you understand what's happening to effectively train a neural network we need to randomize the training data that we send into the neural network however if you remember from steps three and four the agent takes an action and then we're sending that training data into the neur network so the question is how do we how do we randomize the order of a single sample but that's where experience replay comes in we need a step 3A where we memorize the agent's experience as the agent navigates the map rest storing the state that it was in what action it took the new state that it reached if there was a reward or not and and if the new state is a terminal state or not we take that and insert it into the memory and the memory is nothing more than a python deck how a deck works is that as the deck gets full whatever at the end gets purged we need a step 3B this is the replay step this is where we take say 30 random samples from the deck and then pass it on to step four for training so that's what experience replay is before we jump into the code if you have problems installing gymnasium especially on Windows I got a video for that I'm not going to walk through the Q learning code but if you are interested in that jump to my Q learning Code walkthrough video after watching this one also if you have zero experience with neural networks you can check out this basic tutorial on neuron networks you o need pytorch so head over to PYT to.org and get that installed the first thing that we do is create the class to represent our deep Q Network I'm calling it dqn as mentioned before a deep Q network is nothing more than a feed forward node Network so there's really nothing special about it here I'm using pretty standard way of creating a node network using pytorch if you look up any pytorch tutorial you'll probably see something just just like this so since this is not a py torch tutorial I'm not going to spend too much time explaining how py torch works for this class we have to inherit the neuron network module which requires us to implement two functions the inet function and the forward function in the init function I'm passing in the number of nodes in my input State hidden layer and output State back at our diagram we're going to have 16 input States as mentioned before I'm using 16 in the hidden layer that's something that you can adjust yourself and in the output layer four notes we declare the hidden layer which has 16 going into 16 and then the output layer 16 going into four and then the forward function X is the training data set and we're sending the training data through the neuron Network again this is pretty common pytorch code next we need a class to represent the replay memory so it's this portion that we're implementing right now in the init function we'll pass in a max length and then create the python deck in the append function we're going to append the transition the transition is this tupo here State action new state reward and terminated these sample function will return a random sample of whatever size we want from the memory and then the link function simply Returns the length of the memory the fen L dql class is where we're going to do our training we set the learning rate and the discount Factor uh those are part of the Q learning formula and these are values that you can adjust and play around with the network sync rate it's the number of steps the agent takes before syncing the policy and Target Network that's the setting for step 10 Where We sync the policy and the target Network we set the replay memory size to 1,000 and the replay memory sample size to 32 these are also numbers that you can uh play around with next is the loss function and the optimizer these two are py torch variables for the loss function I'm simply using the mean square error function for the optimizer will initialize that at a later time this actions list is to Simply map the action numbers into letters for printing in the train function we can specify how many episodes we want to train the agent whether we want to render the map on screen and what do we want to turn on the slippery flag we'll instantiate the frosen lake environment and then create a variable to store the number of states this is going to be 16 inst store the number of actions this is going to be four here we initialize Epsilon to one and we instantiate the replay memory this is going to be size of uh 1,000 now we create the policy DEQ Network this is step one create the policy Network we also create a Target Network and copy the weights and bias from the policy Network into the target Network so that is Step number two making the Target and policy Network identical before training we'll do a print out of the policy Network just so we can compare it to the end result next we initialize the optimizer that we declared earlier and we're simply using the atom Optimizer passing in the learning rate we'll use this rewards per episode list to keep track of the rewards collected per episode we'll also use this Epsilon history list to keep track of Epsilon decaying over time we'll use step count to keep track of the number of steps taken this is used to determine when to sync the policy and Target Network again so that is for step 10 the syncing next we will Loop through the number of episodes that were specified when calling the train function we'll initialize the map so the agent starts at state zero we'll initialize the terminated and truncated flag terminated is when when the agent falls into the hole or reaches the goal truncated is when the agent takes more than 200 actions on such a small map 4x4 uh this is probably never going to occur but we'll keep this anyway now we have a while loop checking for the terminated and truncated flag keep looping until these conditions are met this part is basically the Epsilon greedy algorithm if a random number is less than Epsilon we'll just pick a random action otherwise we'll use the policy Network to calculate the set of Q values take the maximum of that extract that item and that's going to be the best action so we have a function here called state to dqn input let's jump down there remember that we have to take the state and encode it that's what this function does this type of encoding here okay let's go back with P torch if we want to perform a prediction we should call this torch. no grads so that it doesn't calculate the stuff needed for training after selecting the either a random action or the best action we'll call the step function to execute the action when we take that action it's going to return the new state whether there is a reward or not whether it's a terminal state or we got truncated we take all that information and put it into our memory so that is Step 3A when we did the Epsilon greedy that was the step three after executing the action we're resetting the state equal to the new state we increment our step counter if we received an award put it on our list now we'll check the memory to see if we have enough training data to to do optimization on also we want to check if we have collected at least one reward if we haven't collected any rewards there's really no point in optimizing the network if those conditions are met we use memory. sample and we pass in uh the batch size which was 32 and we get a batch of training data out of the memory we'll pass that training data into the optimized function along with the policy Network and the target Network let's jump over to the optimize function this first line is just looking at the policy Network and getting the number of input noes we expect this to be 16 the current qist and Target qist so this is the current cus this is the target cus what I'm about to describe now is Step 3B rep playing the experience so to replay the experience We're looping through the training data inside the mini batch let's jump down here for a moment so this is Step number four we're taking the states and passing it into the policy Network to calculate the current list of Q values step number four the output is the list of Q values step number five we pass in the same thing to the Target Network step five we get the Q values out here which should be the same as the Q values from the policy Network step number six is up here so reminder of what it looks like step six is using this formula here so that's what we have here if terminated if terminated just return the reward otherwise use that that second formula so now that we have a Target we'll go to step seven step seven is taking the output of Step six and replacing the respective Q value and that's what we're doing here we're replacing the Q value of that particular action with the Target that was calculated up above step number eight step eight is to take the target values and use that to train the current Q values so that's what we have here we're using the loss function pass in the current set of Q values plus the target set of Q values and then this is just standard Pi torch code to optimize the policy Network now step nine step nine is to repeat steps 3 to 8 starting from navigation to optimizing the network basically that's just continuing this inner while loop of stepping through the states and this outer for Loop of stepping through each episode step 10 is where we sync the policy Network and the target Network and we do that down here if the number of steps taken is greater than the network sync rate that we set then we copy the policy Network into the target Network and then we reset the step counter also after each episode we should be decaying the Epsilon value after syncing the network we repeat Step n again which is repeating basically the navigation and training again so it's just going to go back up here and do this all over again all the way until we have finished the number of episodes so that was training after training we close the environment we can save the policy or the weights and bias into a file I'm hardcoding it here you can definitely make this more Dynamic here I'm creating a new graph and I'm basically graphing the rewards collected per episode also I'm graphing the Epsilon history here after graphing I'm saving the graphs into an image all right so that was the train function we have the talked about the optimize function let me fold that we already looked at that now the test function is going to run the fren Lake environment with the policy that we learned from the train function we can also pass in the number of episodes and whether we want to turn on slippery or not so the rest of the code is going to look pretty similar to what we had in the train function we instantiate the environment get the number of states and action s 16 and 4 here we declar the policy Network load from the file that we saved from training this is just P torch code to switch the policy Network to prediction mode or evaluation mode rather than training mode we'll print the train policy and then we'll Loop over the episodes we set the agent up top you've seen this before keep looping until the agent gets terminated or truncated here we're selecting the best action out of the policy Network and executing the action and then close the environment so ideally when we run this we're going to see the agent navigate the map and reach the goal however if slippery is on there is no guarantee that the agent is going to solve it in one try it might take a couple of tries for the agent to solve the map all right down at the main function we create an instance of the fen Lake dql class first first we're going to try non-slippery we'll train it for a th times or 1 th000 episodes we'll run the test four times just because the map pops up and goes away when the agent gets to the goal so um we want to run it a couple times so we actually can see it on the screen okay I'm just hitting contrl F5 all right training is done and looks like the agent is able to solve the map we also have a print out of the policy that it's learned I print it in a way that matches up with the grid let's jump back to this in a second let's go to our files so after training we have a new graph on the left side that's the number of rewards collected over the 1,000 episodes as you can see over time it's improving and finally at the end it's getting a lot of rewards on the right side we can see Epsilon decaying starting from one slowly slowly down to zero also another file that gets created is it's a binary file so we can't really display it but it contains the weights and biases of the policy Network so if we want to do the test again we don't need the training we can comment out the training line and just run this again and we can see the agent solve the map we can look at what policy the agent learned the way this is printed it matches up with the map so at state zero the best action was to go right which is State one at State one the best action is going to go right again at state two go down at St six go down again at St 10 go down again at State 14 go to the right okay so it was right right down down down right so remember earlier that there were two possible path the one that we actually learned plus the one going down because Epsilon greedy has a Randomness Factor whether the agent learns the top path or the bottom path is just somewhat random if weet train again maybe it'll go down the next time now let's turn the slippery Factor on uncomment the training line now with slippery on we expect the agent to fail a lot more often or to fall in the holes a lot more often let me just triple the number of um training and let me do 10 times for testing okay I'm running this again again with slippery Turn on there's no guarantee that after training the agent is going to be able to pass every episode let's make it come up you see it trying to go to the bottom right but because of slippery it just failed right there but there you go it was able to solve it let's look at the graph the results compared to the non-slippery surface is significantly worse also you want to be careful getting stuck in spots like these which means if I was unlucky enough to set my training episodes to maybe 2200 and it gets stuck here which means it learned a bad policy it won't be able to solve the map so in your training if you're finding that the agent is not able to solve the map when slippery is turned on it might be because of things like these okay that concludes our deep Q learning tutorial I love to hear feedback from you was the explanation easy to understand what can I improve on and what other topics are you interested in"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Reinforcement Learning",
        "Topic":"Q-Learning and Deep Q Networks (DQN) in Reinforcement Learning",
        "Video Title":"Deep Q-Learning - Combining Neural Networks and Reinforcement Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=wrBUkpiRvCA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/wrBUkpiRvCA\/hqdefault.jpg",
        "ID":"wrBUkpiRvCA",
        "Publish Time":"2018-10-27T17:58:36Z",
        "Channel":"deeplizard",
        "Channel ID":"UC4UJ26WkceqONNF5S26OiVw",
        "Transcript":"what's up guys welcome back to this series on reinforcement learning in this video we'll finally bring artificial neural networks into our discussion of reinforcement learning specifically we'll be building on the concept of cue learning we've discussed over the last few videos to introduce the concept of deep cue learning and deep cue networks this will move us into the world of deep reinforcement learning so let's get to it from everything we've discussed over the last few videos we should now be comfortable with the idea of cue learning now while it's true that the key learning algorithm that we use to play frozen lake may do a pretty decent job in relatively small state spaces its performance will drop off considerably when we work in more complex and sophisticated environments in frozen lake for example our environment was relatively simplistic with only 16 states and four actions giving us a total state action space of just sixteen by four meaning we only had 16 times four or 64 cue values to update in the cue table given the fact that these q-value updates occur in an iterative fashion we can imagine that as our state space increases in size the time it will take to traverse all those states and iteratively update all the Q values will also increase think about a video game where a player has a large environment to roam around in each state in the environment would be represented by a set of pixels and the agent may be able to take several actions from each state the iterative process of computing and updating Q values for each state action pair in a large state space like this becomes computationally inefficient and perhaps infeasible due to the computational resources and time this may take so what can we do when we want to step up our game from a simple toy environment like frozen lake to something more sophisticated well rather than using value iteration to directly compute Q values and find the optimal Q function we instead use a function approximator to estimate the optimal Q function well you know what can do a pretty darn good job at approximating functions artificial neural networks will make use of a deep neural network to estimate the Q values for each state action pair in a given environment and in turn the network will approximate the optimal Q function the act of combining Q learning with a deep neural network is called deep Q learning and a deep neural network that approximates the Q function is called a deep Q network or D queuing let's break down how exactly this integration of neural networks and Q learning works will first discuss this at a high level and then we'll get into all the nitty-gritty details suppose we have some arbitrary deep neural network that accepts States from a given environment as input for each given state input the network outputs estimated Q values for each action that can be taken from that state the objective of this network is to approximate the optimal Q function and remember that the optimal Q function will satisfy the bellman equation that we covered previously with this in mind the loss from the network is calculated by comparing the outputted Q values to the target Q values from the right-hand side of the bellman equation and as with many standard networks the objective here is to minimize this loss after the loss is calculated the weights within the network are updated via stochastic gradient descent and back propagation again just like with any other typical network this process is done over and over again for each state in the environment until we sufficiently minimize the loss and get an approximate optimal q function so take a second now to think about how we previously use the bellman equation to compute and update Q values and our Q table in order to find the optimal q function now with deep Q learning our network will make use of the bellman equation to estimate the q values to find the optimal q function so we're still solving the same general problem here just with a different algorithm rather than making use of value iteration to solve the problem we're now using a deep neural network alright we should now have a general idea about what deep Q learning is and what at a high level the deep Q network is doing now let's get a little more into the details of the network itself so we discussed earlier that the network would accept states from the environment as input thinking of the game frozen lake that we played last time we could easily represent the states of this environment using a simple coordinate system from the grid of the environment if we're in a more complex environment though like a video game for example then we'll use images as our input specifically we'll use still frames that capture States from the environment as the input to the network the standard pre-processing done on the frames usually involves converting the RGB data into grayscale data since the color in the image is probably usually not going to affect the state of the environment additionally will typically see some cropping and scaling as well to both cut out unimportant information from the frame and shrink the size of the image now actually rather than having a single frame represent a single input we usually will use a stack of a few consecutive frames to represent a single input so we'd grab say four consecutive frames from a video game we then do all the pre-processing on each of these four frames we mentioned earlier the greyscale conversion the cropping in the scaling and then we'd take the pre-processed frames and stack them on top of each other in the order of which they occurred in the game we do this because a single frame usually isn't going to be enough for our network or even for our human brains to fully understand the state of the environment for example by just looking at this single frame from the Atari game breakout we can't tell if the ball is coming down to the paddle or going up to hit the block we also don't have any indication about the speed of the ball or which direction the paddle is moving in if we look at our four consecutive frames though then we have a much better idea about the current state of the environment because we now do indeed have information about all these things that we didn't know with just a single frame so the takeaway is that a stack of frames will represent a single input which represents the state of the environment now that we know what the input is the next thing we need to address is the inner workings of the network the layers well really we're not going to see much more than we're used to seeing with any other network we've already come like seriously many deep queue networks are purely just some convolutional layers followed by some nonlinear activation function and then the convolutional layers are followed by a couple fully connected layers and that's it so the layers used in a DQ n are nothing new and nothing to be freaked out about if you do need a crash course or a refresher on convolutional neural networks or on neural networks in general then be sure to check out the deep learning fundamental series the last piece of the network to discuss is the output the output layer will be a fully connected layer and it will produce the Q value for each action that can be taken from the given state that was passed as input for example suppose in a given game the actions we can take consists of moving left moving right jumping and ducking then the output layer would consists of four nodes each representing one of the four actions the value produced from a single output node would be the Q value associated with taking the action that corresponds to that node from the state that was supplied as input to the network we won't see the output layer followed by any activation function since we want the role non transformed Q values from the network all right so now we know what DQ learning and deep Q networks are what these networks consist of and how they work I know when I was originally learning about DQ ends I was prepared to learn some type of new and mysterious neural network and was pretty surprised when I found out that really and there was nothing new about the network at all instead we were just utilizing a CNN to solve a different type of problem let me know in the comments if you're feeling the same way and stay tuned because in the next video we'll continue discussing DQ ins and dissect the training process step by step to prepare ourselves to create and train our own DQ in encode please leave a thumbs up to let us know you're learning and be sure to check out the corresponding blog to this video as well as the depot's art I've mine for exclusive perks and rewards thanks for contributing to collective intelligence and I'll see you in the next one one particular game I can show to you is this game it's an Atari game called break out those of you that are a child of the 70s or 80s may remember it very well may be new to others of you and an agent learns to play this game is by seeing the pixels on the screen then making mathematical calculations which propagate through those millions of connections layer by layer in the network producing an action which is a command on the joystick now if that action is good and results in the agent getting more points more reward then the learning algorithm reinforces that action by adjusting ever-so-slightly all of the millions of connections in that neural network we can see how the behavior of the agent changes over time with its experience of the game so at first even after a hundred games of experience it's still pretty rubbish it misses the ball all too often but if it keeps on playing after a few hours 300 games then it gets better and better at this point it's at about human ability it can return the ball keep keep alive for a long time if we let it keep on learning then after a few more hours and a lot of play and a lot of games of experience then it gets really good it gets better than a human because it learns how to how to do this trick called tunneling that is systematically send the ball to the sides of the wall so that it bounces around on top and the agent has less work and more reward the learning algorithm doesn't just work on breakout it works on most of the 57 games that we've tried it on and it achieves superhuman level of play at most of them [Music]"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Reinforcement Learning",
        "Topic":"Q-Learning and Deep Q Networks (DQN) in Reinforcement Learning",
        "Video Title":"Q Learning Explained (tutorial)",
        "URL":"https:\/\/www.youtube.com\/watch?v=aCEvtRtNO-M",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/aCEvtRtNO-M\/hqdefault.jpg",
        "ID":"aCEvtRtNO-M",
        "Publish Time":"2017-12-02T04:06:07Z",
        "Channel":"Siraj Raval",
        "Channel ID":"UCWN3xxRkmTPmbKwht9FuE5A",
        "Transcript":""
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Reinforcement Learning",
        "Topic":"Implementing Q-Learning with Python",
        "Video Title":"Q-Learning: A Complete Example in Python",
        "URL":"https:\/\/www.youtube.com\/watch?v=iKdlKYG78j4",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/iKdlKYG78j4\/hqdefault.jpg",
        "ID":"iKdlKYG78j4",
        "Publish Time":"2020-04-25T01:39:40Z",
        "Channel":"Dr. Daniel Soper",
        "Channel ID":"UCkuzGLn9nxFwlS0lY2ruuJg",
        "Transcript":"Good day everyone. This is Dr. Soper here, and today I\u2019ll be presenting a complete walkthrough of a Q-learning-based AI system in Python. The content of this lesson builds directly upon the concepts that were discussed in the previous video in this series, so if you\u2019re unfamiliar with Q-learning, I would highly recommend that you watch the previous video before watching this video. Let\u2019s begin by introducing the business problem that we\u2019ll be solving in this lesson. Imagine that a growing e-commerce company is building a new warehouse, and that the company would like all of the picking operations in the new warehouse to be performed by warehouse robots. In case you\u2019re unfamiliar, warehouse robots are autonomous ground vehicles that are designed to automatically handle many common warehouse-related tasks, such as picking. In the context of e-commerce warehousing, \u201cpicking\u201d is the task of gathering individual items from various locations in the warehouse in order to fulfill customer orders. After picking items from the shelves, the e-commerce company would like the robots to bring the items to a specific location within the warehouse where the items can be packaged for shipping. In order to ensure maximum efficiency and productivity, the robots will need to learn the shortest path between the packaging area and all of the other locations within the warehouse where the robots are allowed to travel. Our goal in this video will be to use Q-learning to accomplish this task. As a quick refresher, remember that the first task when designing a Q-learning system is to define the environment. The environment consists of: States; Actions; and Rewards. The AI agent will use the states and rewards as inputs, and will generate actions as outputs. Let\u2019s briefly discuss the states for our warehouse robot scenario. First, consider that our e-commerce company\u2019s warehouse can be represented as a diagram, as shown here. In the diagram, each black square represents an item storage location, such as a shelf or a storage bin. Each white square in the diagram is part of an aisle that the robots can use to travel throughout the warehouse. Finally, the green square in the diagram indicates the location of the item packaging area. Next, note that each of the 121 locations in the warehouse represents a state, or situation in which a robot might find itself at a particular point in time. Each state can be identified by a row and column index. The item packaging area, for example, is at location (0, 5). Note also that the black and green squares are terminal states. This means that if our AI agent decides to drive a robot into one of these areas while it is being trained, then that training episode will be finished. In the case of the green square, our AI agent will have achieved its goal, but in the case of any of the black squares, the AI agent will have failed, since it will have crashed the robot into an item storage area! This sort of failure is conceptually identical to falling down the cliff in the famous cliff-walking example that we discussed in the previous video in this series. Next, let\u2019s briefly discuss actions. The actions that are available to the AI agent in our scenario are simply to move the robot in one of four directions: Up; Right; Down; or Left. Again, the AI agent will need to learn to choose actions that will prevent the robot from crashing into the item storage areas! Finally, let\u2019s discuss the reward structure for our scenario. To help the AI agent learn, each state (or location) in the warehouse is assigned a reward value. The agent may begin at any white square, but regardless of where the agent begins, its goal is always the same: to maximize its total rewards. Note that in this scenario negative rewards (that is, punishments) are being used for all states except the goal. This encourages the AI to identify the shortest path to the goal by minimizing its punishments. You might be wondering why we don\u2019t use positive rewards for the white squares. Well, remember that the AI agent\u2019s goal is always to maximize its cumulative rewards. If we were to use positive rewards for the white squares, then the agent could simply drive around on the white squares forever and accumulate a very large cumulative reward, even if it never reaches the goal. By using negative rewards, the agent receives a small punishment for each step that it takes, and so to minimize its punishments, it will try to reach the item shipping location by taking the fewest steps possible from its current location. The result, of course, will be the shortest path from the current location to the goal. Now that we\u2019re familiar with our scenario and with the states, actions, and rewards that will define the environment, let\u2019s switch to Python, and take a look at some code! Before I begin describing the code in this notebook, I\u2019d just like you to know that a link to this notebook is available in the video description. Please feel free to download a copy of the notebook to play with, or to adapt to your own needs. I\u2019ve also included most of the information that we just discussed about the scenario and the environment in the notebook. Hopefully this will prove useful if we need a reminder from time to time as we\u2019re working through the notebook. As usual, the first thing that we\u2019ll do is import the required Python libraries. We\u2019ll just need one library for this project \u2013 numpy, which we\u2019ll use to create a few multidimensional arrays, generate some random values, and perform some other numeric tasks. Our first major task in this project (and indeed in any reinforcement learning project) is to define the environment. Let\u2019s begin with the states. Recalling that our warehouse can be represented by a grid, let\u2019s define the dimensions of that grid. In this scenario, we have 11 rows and 11 columns, yielding 121 possible states (or locations). Next, let\u2019s use numpy to define a three-dimensional array. We\u2019ll be using this array to hold all of our Q-values for the project. If you recall from the last video in this series, we\u2019ll have one Q-value for each combination of a state and an action. The first two dimensions will therefore represent the rows and columns that define our states, while the third dimension will contain one element for each possible action that the AI agent might take while in a particular state. Note that we\u2019re using numpy\u2019s \u201czeros\u201d function to initialize all of our Q-values to zero. Next, we\u2019ll define the four actions that our AI agent may take \u2013 up, right, down, and left. Based on its index location in the \u201cactions\u201d list, each action will be associated with a numeric code. 0 will mean \u201cup\u201d, 1 will mean \u201cright\u201d, 2 will mean \u201cdown\u201d, and 3 will mean \u201cleft\u201d. Now we can define the last element of our environment \u2013 the rewards. Each state in the environment has a reward value, so we can use a two-dimensional array to hold the reward values for each location within the environment. Let\u2019s begin by first creating an appropriately sized two-dimensional array, and initializing all of the array values to -100. We\u2019re beginning with all of the values set to -100 because the majority of the squares in the warehouse are black, and hence have a reward value of -100. Next, we\u2019re setting the reward for the packaging area (that is, the goal) to 100. Note that the packaging area is located at row 0, column 5. The next task is to construct a dictionary named \u201caisles\u201d whose keys will be the indexes of all of the rows in the warehouse that contain at least one white square, and whose values will be a list containing the column indexes for each white square within a particular row. This is a bit of a cumbersome task, but we need to do this in order to specify the reward values for all of the white squares in the warehouse. Finally, we can use a set of nested \u201cfor\u201d loops to iterate through our dictionary and set the reward values for all of the white squares in the warehouse to -1. We then print the results to the screen so that we can verify that the reward values were specified correctly. Congratulations! Our environment is now fully configured, and we can proceed to the second major task in our project, which is for our AI agent to learn about its environment by implementing a Q-learning model. The learning process will follow these steps: First, we\u2019ll choose a random, non-terminal state (that is, a white square) for the agent to begin a new episode. Second, the AI agent will choose an action (that is to move up, right, down, or left) for the current state. Actions will be chosen using what is known as an epsilon greedy algorithm. This algorithm will usually choose the most promising action for the AI agent, but it will occasionally choose a less promising option in order to encourage the agent to explore the environment. Third, the AI agent will perform the chosen action, and transition to the next state (that is, it will move to the next location). Fourth, the agent will receive the reward for moving to the new state, and will calculate the temporal difference value associated with that movement. The agent will then update the Q-value for the previous state and action pair. If the new (that is, the current) state is a terminal state, then we\u2019ll loop back to #1 and start a new training episode. If not, then we\u2019ll loop back to #2 and continue the current training episode. This entire process will be repeated across 1000 episodes. This will provide the AI agent sufficient opportunity to learn the shortest paths between the item packaging area and all other locations in the warehouse where the robot is allowed to travel, while simultaneously learning to avoid crashing into any of the item storage locations! Let\u2019s begin this part of the project by defining a few helper functions that will perform some useful tasks for our AI agent. This first function determines if the state at the specified row and column location is a terminal state. It figures this out simply by looking at the reward for the specified state. If the reward is equal to -1, then the state is not a terminal state (that is, it is a white square). Otherwise, the state must be a terminal state (that is, a black or green square). This next function simply chooses a random starting location for a new training episode. The function just continues choosing random row and column indexes until it identifies a non-terminal state to use as the next starting location. This next function implements the epsilon greedy algorithm in order to choose the AI agent\u2019s next action. This function works by checking whether a random number between 0 and 1 is less than the epsilon value (which in our project will be 0.9 or 90%). If so, then the algorithm will select the action with the largest Q-value for the current state. Recalling from our previous lesson that Q-values represent our current estimate of the sum of all future rewards if we were to take a particular action in a particular state, this means that our epsilon greedy algorithm will choose what appears to be the most promising action 90% of the time. For the remaining 10% of the time, it will choose a random action. This mechanism encourages the AI agent to explore its environment by occasionally choosing a less promising action. Our next function simply gets the next location in the warehouse, given the current location and the selected action. For example, if we\u2019re currently located at row index 5 and column index 3, and we want to move \u201cup\u201d, this function will return the location at row index 4, column index 3. Finally, our last helper function returns a list that contains the shortest path between the specified starting location and the item packaging location in the warehouse. After we\u2019ve trained our model, we\u2019ll be able to call this function to see what the AI agent has learned about how to move within the warehouse. Let\u2019s run this cell to make all of these functions available to our program. Now we\u2019ve reached the most exciting part of our project \u2013 the part where we\u2019ll actually train our AI agent using Q-learning! We begin by defining a few parameters that will guide the training process. First is \u201cepsilon\u201d, which will be used in conjunction with our epsilon greedy algorithm. Epsilon specifies the percentage of time when we should take the best action when we\u2019re in a state, rather than taking a random action. Next is the discount factor. This will be used when we\u2019re calculating temporal difference values to specify by how much we should discount future rewards. Finally, we specify the learning rate. This will control how quickly the AI agent learns. As noted earlier, our AI agent will learn by running through 1000 training episodes, and we\u2019re using a \u201cfor\u201d loop for that purpose. The first thing that we do at the beginning of each training episode is to call the \u201cget starting location\u201d function in order to get the location of the white square where the AI agent should begin the episode. We then set up a \u201cwhile\u201d loop, which will continue running until the AI agent moves into one of the terminal locations. Remember that the item packaging area and all of the item storage locations are terminal states, so if the agent reaches the packaging area or crashes into one of the item storage locations, the current training episode will end immediately. Each iteration of the \u201cwhile\u201d loop represents one step in the warehouse, so the first thing that we do at the beginning of each iteration is to call the \u201cget next action\u201d function to determine in which direction the AI agent should move next. Remember, actions are movements up, right, down, or left. We next store the current location, and then call the \u201cget next location\u201d function in order to move to the next location, based on the chosen action. After we arrive at the new location, we get the immediate reward for moving to that location, and then compute the temporal difference value associated with our latest move by using the temporal difference formula that we discussed in the previous video in this series. Finally, we update the Q-value for the previous state and action based on what we\u2019ve learned after moving to our current location. The new Q-value is calculated using the Bellman Equation that we discussed in our previous video. And that\u2019s it! We\u2019ve just implemented a complete Q-learning-based AI system in Python! Let\u2019s run this code cell, and train our AI agent. Now that our agent is fully trained, we can call the \u201cget shortest path\u201d function to get the shortest path that the agent identified between any white square in the warehouse and the item packaging location. Let\u2019s see a few examples. As you can see, the function gives us all of the steps on the shortest path between the starting location and the item packaging location. For the sake of clarity, let\u2019s visualize one of these paths \u2013 say the path that starts at row 9, column 5. As shown on the diagram, the AI agent has identified the shortest path between the two points, and has successfully learned to avoid crashing into any of the item storage locations. This is just one example, but if we were to check, we would find the same is true for every other possible starting location in the warehouse. Pretty awesome! Finally, it's great that our robot can automatically take the shortest path from any 'legal' location in the warehouse to the item packaging area, but what about the opposite scenario? Put differently, our robot can currently deliver an item from anywhere in the warehouse to the packaging area, but after it delivers the item, it will need to travel from the packaging area to another location in the warehouse to pick up the next item! Don't worry -- this problem is easily solved simply by reversing the order of the shortest path. Let\u2019s run the last code cell to see an example. As you can see, if we need the robot to go from the item packaging area to the location at row 5, column 2, we just need to reverse the path. Our AI system can thus direct a warehouse robot to pick an item from anywhere in the warehouse and bring that item to the packaging area, and then send the robot from the packaging area to anywhere else in the warehouse to pick the next item. This is definitely cool stuff! Well, there you have it \u2013 a complete example of Q-learning in Python, from start to finish! I hope that you\u2019re beginning to appreciate just how useful Q-learning can be, and that perhaps you've begun thinking about how you could apply Q-learning to solve other types of business problems. In the next video in this series we\u2019ll continue our adventures in cognitive computing and artificial intelligence by introducing artificial neural networks and deep Q-learning, so be sure to come back soon! Well my friends, thus ends our complete walkthrough of a Q-learning-based AI system in Python. I hope that you learned something interesting in this lesson, and until next time, have a great day."
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Reinforcement Learning",
        "Topic":"Implementing Q-Learning with Python",
        "Video Title":"Q-Learning Tutorial in Python - Reinforcement Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=MSrfaI1gGjI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/MSrfaI1gGjI\/hqdefault.jpg",
        "ID":"MSrfaI1gGjI",
        "Publish Time":"2024-09-11T19:00:12Z",
        "Channel":"NeuralNine",
        "Channel ID":"UC8wZnXYK_CGKlBcZp-GxYPA",
        "Transcript":"what is going on guys welcome back in this video today we're going to learn how to train a reinforcement learning agent in Python using Q learning so let us get right into it not [Music] a all right so we're going to do reinforcement learning in Python today to be precise we're going to implement Q learning in order to teach an agent to play a game without giving any concrete instructions now I want to show you what this looks like in the end so the end result so that you know what we're aiming for uh the game that we're going to play today or that the agent is going to play today is this Taxi Driver game so we have a taxy we have a customer spawning at one of these random locations here at one of these colored squares we have a hotel at one of the other colored squares and a taxi has to pick up the customer and bring them to the hotel now as you can see this taxi does this successfully but it doesn't do that because we hardcoded or because I hardcoded any logic into it but because of reinforcement learning or Q learning so the idea is that the taxi can in the beginning take random actions uh explore the action space in the different states it can be in and then it's rewarded for good actions and not rewarded for bad actions or maybe even punished depending on the game that you're playing and by doing that by just letting the taxi do whatever it wants and rewarding it properly it's going to learn to play the game properly so this is what we do with Q learning this is what we're going to implement from scratch in Python today all right so let's get started with the coding we're going to implement the Q learning process from scratch so we're not going to rely on any external python packages to do this for us we're only going to use numpy to work with AAS efficiently but we're going to use the open AI gym package in order to use the already existing environment there so the taxi game instead of coding it ourselves so the open aai package or the gym package will provide uh the animations and the rewards and the observation space so the environment and the current state and also the actions and so on all of this will be done using the gym package but the actual learning the actual Q learning process will be implemented from scratch using only numpy so let's get started by opening up a terminal and installing the two packages install numpy and gy and once you have them installed we can start by importing first of all random from core Python and also gym and numpy S&P all right so what we're going to do first is we're going to initialize the environment and we're going to set a couple of parameters for our Q learning process now for those of you who don't know all what reinforcement learning or Q learning is the basic idea is you have an agent in this case the taxi driver and you have an environment and the agent can interact with that environment in this case it can um Drive left right up down and it can interact with the environment by going to certain fields and if there's a customer the customer will enter the taxi and then the taxi can drive around and hopefully it ends up at the hotel location in this case it gets a reward and the idea is that instead of telling the taxi or the agent what to do with instructions we just let the agent do whatever it wants and we reward good behavior so if the agent accidentally by random Behavior ends up taking the customer and bringing them to the hotel uh we reward that with with a reward um and by doing that repeatedly we teach the taxi in all the different states what to do and how to drive uh in order to succeed at the game that's the basic idea and Q learning does that with a so-called Q table so the Q table is a table that contains values for all the actions in all the possible States so you have a certain number of states uh that you can be in this is all states that are possible in the game so all the positions of the taxi all the positions of the customer all the positions of the hotel and for each of these states you have four actions you can take up down left right and for each of these values in all of these states the Q table contains um a value that predicts how rewarding this action is now in the beginning obviously the Q table is stupid it doesn't know anything so so it's completely random and the values in there are completely random or zero depending on how you initialize it um and by doing by interacting with the environment and getting rewards and by applying a certain formula for Q learning uh we're going to update these values so that over time the agent is going to use the values in this table to make decisions and will play the game properly that's the basic idea of Q learning for this we're going to say now environment is equal to gym. make and we're going to use the taxi - V3 environment and then we're going to define a couple of parameters here the first parameter and these are parameters that we're going to use in the formula and in the training Loop uh the first parameter is going to be Alpha which could also be called The Learning rate uh it's a number between zero and one and it basically uh determines how much or to what extent new information overrides uh old information now theoretically in a deterministic environment one is the optimal number I will just go for for the sake of having some Nuance here I will go with 0.9 basically this means that new information new rewards and updates and so on are going to or new rewards are going to update the value uh way more radically than just keeping the current value so one minus Alpha is you could say the inertia keeping the already existing value and Alpha is the the uh proportion or the uh percentage of how important new information is so reward and also what you expect to get from the next state so that is our learning rate we're also going to have a gamma the gamma is a so-called discount factor it's basically how important future rewards are if you set this to one you're basically saying that the longterm rewards are as important as the immediate rewards so you have a very long-term focused model uh or agent so I'm going to go here with 0.95 then we're also going to use Epsilon Epsilon is going to be our Randomness or our exploration rate basically meaning that you have the Q table and you can either use the Q table which is going to be a deterministic result you have a certain Q table and you maximize the Q value so you want to take the action that has the maximum Q value you want to take the action that is the most promising one that would be just applying the Q table the knowledge that you already have or you could just take a random action and this has the benefit that you could end up in a space um where you end up having a reward even though you didn't necessarily know that there's going to be a reward you just took random actions and you end up um at this reward so if you set Epsilon to one It means that all your actions are going to be always random so it doesn't matter what's part of the Q table all of it will be random uh if you set Epsilon to zero never will you take a random action so all the actions are going to be based on the Q table on the knowledge in the Q table now zero makes sense if you have the perfect Q table one makes sense if you don't have any information in the Q table um what we're going to do is we're going to start at one so all the behavior in the beginning is going to be random um but we're going to Decay this Epsilon value so we're going to say epsilon Decay is going to be equal to 0.9995 and this is going to be a number that we multiply uh with Epsilon to Decay it so to make it smaller and smaller over time until we get to a certain minimum value this is going to be our Min Epsilon we always want to leave room for Randomness so even if it decays a lot we want to have at least 0.01 uh as Epsilon so at least a 1% chance of taking a random action just so we can explore the environment in the possible States a little bit more uh then we are also going to set the number of episodes this is going to be how many Epoch we basically have let's go with something like 10,000 so how many time uh how many times the agent will play the game and per episode we want to have a max number of steps so let's say this is 100 so that if the taxi just goes around in circles for example um 100 times or takes 100 steps to go around in circles then uh the episode terminates even if it doesn't end up uh getting to the desired location so we have a max number of steps and then we're also going to initialize our Q table the Q table is going to be a nump array full of zeros so it's going to be initialized with zeros uh and it's going to have the shape of environment. observation space n and and environment doaction space in the idea behind this is that you have a certain number of observations so or observation space so basically how many possible states can the game be in and how many actions can I take per state and this is what we want to model here so um for this particular example and this depends on the environment but in a taxi environment you have 25 positions because it's a 5 Time 5 grit uh so you have 25 positions for the taxi for the taxi um you have the four different colored squares where the customer can be located so the customer can be in four different um squares so that would betimes 4 and we would have uh four different locations where the hotel can be located so times four that results in I think 400 so 25 * 4 * 4 is 400 different States um but actually no this is actually not correct because uh no sorry we have five different states we have five squares for the um for the customer and we have four for the hotel because the one that the customer at is not the one that the hotel is at so we have 25 * 5 * 4 uh which is uh 500 so we have 500 different states that the game can be in and for all these 500 States we have four actions that the car can take up down left right this is our table so for each state each action will have a Q value so you take some State and you take the four actions and each of them will have a value telling you how good this action is how how much reward is being anticipated by taking this action in the beginning all of them will be zero over time we will adjust them um we're going to define a function choose action and this function is going to determine our action um given a certain state so the function takes a state and depending on the state we're going to take take a certain action now if a certain random value that we Generate random uniform a value between 0 and 1 if that value is below Epsilon we're going to take a random action so if Epsilon is one all of the values will be below Epsilon so we will take a random action which means we're going to return environment. action space sample we're going to just take any action from the action space and we're going to return it otherwise um we're going to return the best action according to the Q table so return NP arcmax um Q table uh at the current state so state with all the actions what this does is it goes to the section of the Q table where this state is relevant and it goes to all the actions and gives us the index of the action that maximizes or that has the maximum Q value so we're going to take this action uh so if it's not random we take the best action according to the table if it's random we just take any action and this is our function so let's apply this in a training loop we're going to say for episode in range uh num episodes what we're going to do is we're going to say that we want to start with an empty environment or with a with a new environment which means we're going to reset the environment uh which will give us a random starting point so environment. reset will return a state and something that we don't need um and this state now will be the input for our first decision so we're going to say done equals false and then we're going to say four step in range Max steps so we go 100 steps Max um what we do is we say that we want to take an action the action will be chosen by our function so either randomly in the beginning since Epsilon is 1 it's going to be a random action over time with the Epsilon Decay we're going to go more and more into the direction of using the Q table um but I'm going to just choose an action based on the state and this action will then be applied so now I just chose the action to actually do the action I need to do environment. step and I need to pass the action now this is going to return a bunch of values this is going to return first of all the next state so when I am in a state and I take an action I'm in a new state because now the taxi if I go right has moved one block to the right um so I have this new state and I also have a reward now most of the time this will be zero sometimes it might be a punishment depending on the game uh if you are at the goal with a passenger you're going to get a reward of 20 I think in this particular game uh and then it also returns done truncated uh and info I don't think that we're going to use info and done and truncated are just values that we're going to check for to terminate the loop if they are um true and it's also going to overwrite this one here um all right so this is how we get the next state and this is how we apply an action so the interesting part happens now the learning process is updating the Q learning table or the Q table so we're going to say here that the old value is the Q table at the specific State for the specific action that we took so we took an action in a specific State and there already is a q value for this particular action in that particular state in the beginning zero over time it changes what we're going to do now is we're going to update this by applying a formula and part of this formula will be to get the maximum Q value of the next state so this is just another variable in in the formula so what we need to do is we need to go to the next state so we need to go to the Q table next state not the current one but the one that we're now in because we took the action go there see all the uh actions I can take see all their um the Q values and take the maximum one now we're not taking the in we're not interested in which action it is we only want to know what's the most rewarding action from the next state so what reward can I expect from the next state and that is going to be our next Max and now we're going to update the Q table we're going to say that the Q table at the state that we were in for this action will now be the result of the following Formula 1 minus Alpha so this is what I initially called the inertia uh meaning we're going to keep the old values so one minus learning rate times old value so how much do you want to focus on the already existing knowledge in so it's it's going to be 1 minus Alpha and to this we're going to add the opposite Alpha times and we're going to also update this value based on the reward if we got any plus the discount Factor so how much do we want to take into account future predictions of reward gamma times the next Max so we already have a value in the beginning as I said Z we already have a certain Q value for an action in this state what we do then is we say um give me for the next state the maximum Q value I have in the table and then we update the Q value for the state that we were in and for the action that we took by keeping a part of the old value to a certain degree 1 minus Alpha and adding new stuff to it Alpha learning rate times the reward we got if we got any plus the discount Factor how much do I want to focus on not just immediate rewards but expected rewards this value times the expected reward that's what we're doing here to update the Q table and then we say state is equal to next state and if we have a done or truncated we're going to just say break out of the loop all right um so that is our training process the only thing that we don't have yet is the Epsilon Decay so outside of this uh inner full loop we want to do after each episode or at the end of each episode we want to say epsilon is going to be equal to the current Epsilon um times the Epsilon Decay but we want to also keep the minimum epsilons Epsilon so we're going to say Max Min Epsilon and this calculation so Decay Epsilon but if it is already below the minimum don't Decay it anymore use the minimum that's the basic idea all right so that is our training process and what we're going to do afterwards is we're going to use the Q table that we create or that we learn by um by going through this process by updating all these Q values we're going to initialize the environment again we're going to say environment is equal to gy. make again taxi V3 but we're going to say render mode equals human so I want to see what's happening during the training I don't need to see what's happening just do it behind the scenes but once it's done I want to see the results I want to see them in action and here I'm going to do five test runs I'm going to say four I in or actually let's say four step in range or actually let's do it properly four episode in range five so we want to do five episodes we're going to say State and something we don't need is equal to environment reset uh done equals false and then we're going to print episode episode and we're going to say four step in range Max steps we want to see how well the agent now performs so we're going to render the environment environment. render to visualize it we're going to say action is equal to NPR AR Max now here we're going to have no Randomness we're going to always go according to the Q table because we're not training anymore we're using the knowledge that we now have we're using the Q table which should now be pretty good and we're just applying it we don't need random actions now so NP Mark Max Q table for the given State um with all the actions so pick the action that results in the maximum Q value for the given state that we're currently in take that action so we're going to say again next state is equal or next state reward done truncated info is equal to environment step action then state is equal to next state if we reach done or truncated we're going to just say environment render uh finished episode episode uh with reward uh reward like this and then we're going to break out of the loop and in the end we want to close the environment so environment close like this so that is exactly what we need to do here now let me just run this to see if this actually works and to see if I didn't make any mistakes then I'm going to explain it again but there you go you can see the taxi goes to the customer and brings him back to the hotel um okay this was kind of weird now but it should still work there you go yeah seems to work quite well always picks up the customer brings them to the hotel that is what a trained model looks like or a trained agent looks like again so what's the idea we have a Q table full of zeros containing enough space for all the possible States the game can be in with all the possible actions that I can take in each state and then for each action in each state I have a certain value now you need to keep in mind that this here is not applicable to any other version of the game this is so to say hardcoded now it's not hardcoded because it learned it by reinforcement learning but this information is very very very specific or this Q table is very specific to this particular problem on this particular map uh with all the information so it just has the information for the exact thing that we're looking at here uh it cannot be generalize to any other example here so this Q table will not help you if you have a different game uh even if it's similar so it's hyper optimized for this specific game but it's one way to implement a simple reinforcement learning agent by just updating these Q values looking at the rewards looking at the expected rewards and updating them with the different factors that we defined in the beginning so this is how you can Implement Q learning in Python so that's it for today's video I hope you enjoyed it and hope you learned something if so let me know by hitting the like button and leave a comment in the comment section down below and of course don't forget to subscribe to this Channel and hit the notification Bell to not miss a single future video for free other than that thank you much for watching see you in the next video and bye"
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Reinforcement Learning",
        "Topic":"Implementing Q-Learning with Python",
        "Video Title":"Q Learning Explained (tutorial)",
        "URL":"https:\/\/www.youtube.com\/watch?v=aCEvtRtNO-M",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/aCEvtRtNO-M\/hqdefault.jpg",
        "ID":"aCEvtRtNO-M",
        "Publish Time":"2017-12-02T04:06:07Z",
        "Channel":"Siraj Raval",
        "Channel ID":"UCWN3xxRkmTPmbKwht9FuE5A",
        "Transcript":""
    },
    {
        "Domain":"Classical Machine Learning",
        "Sub Domain":"Reinforcement Learning",
        "Topic":"Implementing Q-Learning with Python",
        "Video Title":"Q Learning Explained | Reinforcement Learning Using Python | Q Learning in AI | Edureka",
        "URL":"https:\/\/www.youtube.com\/watch?v=DhdUlDIAG7Y",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/DhdUlDIAG7Y\/hqdefault.jpg",
        "ID":"DhdUlDIAG7Y",
        "Publish Time":"2019-06-12T05:37:06Z",
        "Channel":"edureka!",
        "Channel ID":"UCkw4JCwteGrDHIsyIIKo4tQ",
        "Transcript":"[Music] hello everyone and welcome to this interesting session on cue learning now let's step into the world of reinforcement learning and the beautiful branch of artificial intelligence which lets machine learn on your own in a way different from traditional machine learning throughout our lives we perform a number of actions to pursue our dreams some of them bring us good rewards others do not along the way we keep exploring different paths and figure out which action might lead to better rewards we work hard towards our dreams utilizing the feedback we get based on our actions to improve our strategies they help us determine how close we are to achieving our goals our mental states teams continuously representing this closeness in that description of how we pursue our goal in the early life we framed for ourselves our representative analogy of reinforcement learning now let me summarize the above example reformatting the main points of interest our reality contains environment in which we perform numerous actions sometimes we get good or positive rewards for some of these action in order to achieve the goals now during the entire course of life our mental and physical states evolve we strengthen our action in order to get as many rewards as possible now the key entities of interest are the environment the action reward and the state now this whole paradigm of exploring the environment and learning through actions rewards and States establishes the foundation of reinforcement learning so reinforcement learning solves a particular kind of problem where decision-making is sequential and the coal is long-term such as game playing robotics resource management or logistics now for a robot and environment is a place where it has been put to use now remember this robot is itself the agent for example an automobile factory where a robot is used to move materials from one place to another now the task we discussed just now have a property in common now these tasks involve an environment and expect the agent to learn from the environment this is where traditional machine learning fades and hence the need for reinforcement learning now it is good to have an established overview of the problem that is to be solved using the cue learning or the reinforcement learning so it helps to define the main components of a reinforcement learning solution that is the 18 environment action rewards and States so let's suppose we are to build a few autonomous robots for an automobile building Factory now these robots will help the factory personnel by conveying them the necessary parts that they would need in order to pull the car now these different parts are located at nine different positions within the factory warehouse the car part includes these chassis wheels dash board the engine and so on and the factory workers have prioritized the location that contains the body or the chassis to be deep topmost but they have provided the priorities for other locations as well which we will look into the moment but these locations within the factory looks somewhat like this so as you can see here we have l1 l2 l3 all of these stations now one thing you might notice here that there are little obstacle prison in between the locations so l6 is the top priority location that contains the chassis for repairing the car bodies now let to ask is to enable the robots so that they can find the shortest route from any given location to another location on their own now the agents in this case are the robots the environment is the automobile factory warehouse now let's talk about these states so the states are the location in which a particular robot is present in the particular instance of time which will denote its states now machines understand numbers rather than letters so let's map the location codes to number so as you can see here we have mapped location l1 to the state 0 l2 + 1 and so on we have l8 as state 7 and n line at state 8 well next what we will talk about are the actions so in our example the action will be the direct location that a robot can go from a particular location right consider a robot that is at l2 location and the direct locations to which it can move our l5 l1 and l3 now the figure here may come in handy to visualize this now as you might have already guessed the set of actions here is nothing but the set of all possible states of the robot for each location the set of actions that a robot can take will be different for example the set of actions will change if the robot is in l1 rather than l2 so if the robot is in l1 it can only go to l4 and l2 directory now that we are done with these states and the actions let's talk about the rewards so the states are basically 0 1 2 3 4 and the actions are also 0 1 2 3 4 up till 8 now the rewards now will be given to a robot if a location which is the state is directly reachable from a particular location so let's take an example suppose n line is directly reachable from L 8 right so if a robot goes from LA to n line and vice versa it will be rewarded by 1 and if a location is not directly reachable from a particular liquation we do not give any reward a reward of 0 now the reward is just a number here and nothing else it enables the robots to make sense of the movements helping them in deciding what locations are directly reachable and what are not now with this cue we can construct a reward table which contains all the reward values mapping between all possible states so as you can see here in the table the positions which are marked green have a positive reward and as you can see here we have all the possible rewards that a robot can get by moving in between the different states now comes an interesting decision now remember that the factory administrator prioritize L 6 to be the topmost so how do we incorporate this fact in the above table now this is done by associating the topmost priority location with a very high reward than the usual ones so let's put 999 in the cell L 6 comma L 6 now the table of rewards with the higher reward for the topmost location looks something like this we have now formally defined all the vital components for the solution we are aiming for the problem discussed now we will shift gears a bit and study some of the fundamental concepts that prevail in the wall of reinforcement learning and Kuehl on the first of all we'll start with the bellman equation now consider the following square of rooms which is analogous to the actual environment from our original problem but without the barriers now suppose a robot needs to go to the room marked in the green from his current position a using the specified direction now how can we enable the robot to do this programmatically one idea would be introduce some kind of a footprint which the robot will be able to follow now here a constant value is specified in each of the rooms which will come along the robots way if it follows the direction specified above now in this way if it starts at location a it will be able to scan through this constant value and will move accordingly but this will only work if the direction is prefixed and the robot always starts at the location a now consider the robot starts at this location rather than its previous one now the robot now sees footprints in two different directions it is therefore unable to decide which way to go in order to get the destination which is the green room it happens primarily because the robot does not have a way to remember the directions to proceed so our job now is to enable the robot with a memory now this is where the bellman equation comes into play so as you can see here the main reason of the bellman equation is to enable the robot with the memory that's the thing we're going to use so the equation goes something like this V of s gives maximum of a R of s comma a plus gamma of V dash where s is a particular state which is a room a is the action moving between the rooms s dash is the state to which the robot goes from s and gamma is the discount factor now we'll get into it in a moment and obviously R of s comma a is a reward function which takes a state s and action a and outputs the reward now V of s is the value of being in a particular state which is the footprint now we consider all the possible actions and take the one that you use the maximal value now there is one constraint however regarding the value footprint that is the room marked in yellow just below the green room it will always have the value of 1 to denote that is one of the nearest room adjacent to the green room now this is also to ensure that a robot gets a reward when it goes from a yellow room to the green room let's see how to make sense of the equation which we have here so let's assume a discount factor of 0.9 as a remember gamma is the discount value or the discount factor so let's take a zero point nine now for the room which is marked just below the one or the yellow room which is the astrick mark for this room what will be the V of s that is the value of being in a particular state so for this V of s would be something like maximum of a it will take zero which is the initial of our s comma a plus 0.9 which is gamma into 1 and that gives us 0.9 now here the robot will not get any reward for going to a state marked in yellow hence the are s comma a is 0 here but the robot knows the Valley opening in the yellow room hence V of s dash is 1 following this for the other states we should get 0.9 then again if we put 0.9 in this equation we get 0.8 1 then 0.72 9 and then we again reach the starting point so this is how the table looks with some value footprints computed from the bellman equation and a couple of things to notice here is that the mass function helps the robot to always choose the state that gives it the maximum value of being in that state now the discount factor gamma notifies the robot about how far it is from the destination this is typically specified by the developer of the algorithm that would be installed in the robot now the other states can also be given their respective values in a similar way so as you can see here the boxes adjacent to the green one have won and if we move away from one we get zero point nine zero point eight one zero point seven two nine and finally we read zero point six six now the robot now can proceed its way through the green room utilizing DS value footprints even if it's dropped at any arbitrary in the given location now if a robot lands up in the highlighted sky-blue area it will still find two options to choose from but eventually either of the parts will be good enough for the robot to take because of the way the value footprints are now laid out now one thing to note here is that the bellman equation is one of the key equations in the world of reinforcement learning and cue learning so if we think realistically our surroundings do not always walk in the way we expect there is always a bit of stochastic City involved in it but this applies to robot as well sometimes it might so happen that the robots machinery got corrupted sometimes the robot may come across some hindrance on its way which may not be known through it beforehand right and sometimes even if the robot knows that it needs to take the right turn it will not so how do we introduce the stochastic city in our case now here comes the Markov decision process now consider the robot is currently in the Red Room and it needs to go to the green room now let's now consider the robot has a slight chance of dis functioning and might take the left or the right or the bottom turn instead of being the upper turn in order to get to the green room from where it is now which is the red room now the question is how do we enable the robot to handle this when it is out in the given environment right now this is a situation where the decision-making regarding which turn is to be taken is partly random and partly another control of the robot now partly random because we are not sure when exactly the robot mind is functional and partly under the control of the robot because it is still making a decision of taking a turn right on its own and with the help of the program embedded into it so a Markov decision process is a discrete-time stochastic control process it provides a mathematical framework for modeling decision-making in situations where the outcomes are partly random and partly under the control of the decision maker now we need to give this concept a mathematical shape most likely an equation which then can be taken further now you might be surprised that we can do this with the help of the equation with a few minor tweaks so if we have a look at the original bellman equation V of X is equal to maximum of our s comma A plus gamma V of s - what needs to be changed in the above equation so that we can introduce some more amount of randomness here as long as we are not sure when the robot might not take the expected turn we are then also not sure in which room it might end up in which is nothing but the room it moves from its current room at this point according to the equation we are not sure of the s - which is the next state or the room but we do know all the probable turns the robot might take now in order to incorporate each of these probabilities into the above equation we need to associate a probability with each of the turns to quantify the robot if it has got any explicitness chance of taking this turn now if we do so we get PS is equal to maximum of r s comma a plus gamma into summation of s - p s comma a comma s - into V of s - now the PS a and s Dash is the probability of moving from room s to s - with the action a and the submission here is the expectation of the situation that the robot in curves which is the randomness now let's take a look at this example here so when we associate the probabilities to each of these stones we essentially mean that there is an 80% chance that the robot will take the upper turn now if you put all the required values in our equation we get V of s is equal to maximum of R of s comma ie plus gamma of 0.8 into V of room up plus 0.1 into V of room down 0.03 interim of V or from left plus 0.03 into V of room right now note that the value footprints will not change due to the fact that we are incorporating stochastically here but this time we will not calculate those value food prints instead we will let the robot to figure it out now up until this point we have not considered about rewarding the robot for its action of going into a particular room we are only reward in the robot when it gets to the destination now ideally there should be a reward for each action the robot takes to help it better assess the quality of the actions but the roars need not to be always be the same but it is much better than having some amount of reward for the actions then having no rewards at all right and this idea is known as the living penalty in reality the reward system can be very complex and particularly modeling sparse rewards is an active area of research in the domain of reinforcement learning so by now we have got the equation which we have and so what we're gonna do is now transition to Q learning so this equation gives us the value of going to a particular state taking the stochastic city of the environment into account now we have also learned very briefly about the idea of living penalty which deals with associating each move of the robot with a reward so Q learning possesses an idea of assessing the quality of an action that is taking to move to a state and rather than determining the possible value of the state which is being moved to so earlier we had instead 0.8 into V of s1 0.03 into V of s to 0.1 into V of s 3 and so on now if you incorporate the idea of assessing the quality of the action for moving to a certain state so the environment with the agent and the quality of the action will look something like this so instead of 0.8 V of s one will have Q of s 1 comma a one will have Q of s 2 comma a 2 Q of S 3 now the robot now has four different states to choose from and along with that there are four different actions also for the current state it is in so how do we calculate Q of s comma a that is the cumulative quality of the possible actions through what might take so let's break it down now from the equation V of s equals maximum of a are s comma a plus gamma some - B SAS - into V of his - if we discard the maximum function we have is FA + gamma into summation P and V now essentially in the equation that produces V of s we are considering all possible actions and all possible states from the current state that robot is in and then we are taking the maximum value caused by taking a certain action and the equation produces a value footprint which is for just one possible action in fact we can think of it as the quality of the action so Q of s comma a is equal to RS comma a plus gamma of summation P and V now that we have got an equation to quantify the quality of a particular action we are going to make a little adjustment in the equation we can now say that V of s is the maximum of all the possible values of Q of s comma a right so let's utilize this fact and replace V of s - as a function of Q so Q s comma a becomes R of s comma a plus gamma of summation P SAS - and maximum of the Q s - a - so the equation of V is now turned into an equation of Q which is the quality but why would we do that now this is done to ease our calculations because now we have only one function Q which is also the core of the dynamic programming language we have only one function Q to calculate and R of s comma a is a quantified metric which produces reward of moving to a certain state now the qualities of the actions are called the Q values and from now on we will refer to the value footprints as the Q values an important piece of the puzzle is the temporal difference now temporal difference is the component that will help the robot calculate the Q values with respect to the changes in the environment over time so consider our robot is currently in the mark state and it wants to move to the upper state one thing to note that here is that the robot already knows the q-value of making the action that is moving to the upper state and we know that the environment is stochastic in nature and the reward that the robot will get after moving to the upper state might be different from an earlier observation so how do we capture this change and the real difference we calculate the new Q s comma a with the same formula and subtract the previously known Q si from it so this will in turn give us the new Q a now the equation that we just tried gives the temporal difference in the Q values which further helps to capture the random changes in the environment which may impose now the name Q s comma a is updated as the following so Q T of s comma is equal to Q t minus 1 s comma a plus alpha T DT of a comma s now here alpha is the learning rate which controls how quickly the robot adapts to the random changes imposed by the environment the QT s comma is the current state Q value and the QT minus 1 s comma is the previously recorded Q value so if you replace the TD s comma a with its full-form equation we should get Q T of s comma is equal to Q t minus 1 of s comma T plus alpha into R of s comma a plus gamma maximum of Q s - a - minus QT minus 1 s comma a now that we have all the little pieces of Q line together let's move forward to its implementation part now this is the final equation of Q learning rate so let's see how we can implement this and obtain the best path for any robot to take now to implement the algorithm we need to understand the warehouse location and how that can be mapped to different states so let's start by recollecting the sample environment so as you can see here we have L 1 l 2 L 3 - L line and as you can see here we have certain borders also so first of all let's map each of the other locations in the warehouse two numbers are the states so that it will ease our calculations right what I'm going to do is create a new Python 3 file in the Jupiter notebook and I'll name it as q-learning [Music] numpy okay so let's define the states but before that what we need to do is import numpy because we're gonna use numpy for this purpose and let's initialize the parameters that is the gamma and alpha parameters so gamma is 0.75 which is the discount factor whereas alpha is 0.9 which is a learning rate now next what we're going to do is define the states and map it to numbers so as I mentioned earlier L 1 is 0 and till in line we have defined the states in the numerical form now the next step is to define the actions which is as mentioned above represent the transition to the next state so as you can see here we have an array of actions from 0 to 8 now what we're going to do is define the reward table so as you can see it's the same matrix that we created just now that I showed you just now now if you understood it correctly there isn't any real barrier limitation as depicted in the image for example the transition l4 to l1 is allowed but the reward will be zero to discourage that path or in tough situation what we do is add a minus one there so that it gets a negative reward when the above code snippet as you can see here we took each of the states and put ones in the respective state that are directly reachable from a certain state now if you refer to that reward table once again what we created the above added construction will be easy to understand but one thing to note here is that we did not consider the topper at a location l6 yet we would also need an inverse mapping from the states back to its original location and it will be cleaner when we reach to the utter depths of the algorithms so for that what we're going to do is have the inverse map location state the location we will take the distinct state and location and convert it back now what we'll do is we'll not define a function get optimal which is the get optimal route which will have a start location and an end location don't worry the cord is back but I'll explain you each and every bit of the code now the get optimal route function will take two arguments the starting location in the warehouse and the end location in the warehouse receptively and it will return the optimal route for reaching the end location from the starting location in the form of an ordered list containing the letters so we'll start by defining the function by initializing the Q values to be all zeros so as you can see here we have given the Q values to be 0 but before that what we need to do is copy the reward matrix to a new one so this is the rewards new and next again what we're going to do is get the ending state corresponding to the ending location and with this information automatically we'll set the priority of the given ending state with the highest one that we are not defining it now but we'll automatically set the priority of the Kuban ending state as 999 so what we're going to do is initialize the Q values to be 0 and in the Q learning process what you can see here we are taking I in range 1000 and we're going to pick up a state randomly so we're gonna use the NP dot random Rand int and for traversing through the neighbor location in the same maze we're gonna iterate through the new reward matrix and get the actions which are greater than 0 and after that what we're gonna do is pick an action randomly from the list of the playable actions in years to the next state we're gonna compute the temporal difference which is TD which is the rewards plus gamma into the queue next state and we'll take n P dot Arg max of Q of next state minus Q of the current state we gonna then update the Q values using the bellman equation as you can see here here the bellman equation and we're going to update the Q values and after that we're going to initialize the optimal route with a starting location now here we do not know or the next location yet so initialize it with the value of the starting location which again is the random location now we do not know about the exact number of iterations needed to reach to the final location hence while o will be a good choice for the iteration so we're going to fetch the starting state fetch the highest Q penetrating to the starting state we go to the index of the next state but we need the corresponding letter so we're gonna use that state to location function we just mentioned there and after that we're gonna update the starting location for the next iteration and finally we will return the root so let's take the starting location of n 9 + n location of L 1 and see what part do we actually get so as you can see here we get L 9 l 8 l 5 l2 and l1 and if you have a look at the image here we have if we start from n line to L 1 we got L 8 l 5 l 2 L 1 L 8 l 5 L 2 L 1 that would lead us the maximum value of the maximum reward for the robot so now we have come to the end of this Q learning session and I hope you got to know what exactly is Q learning with the analogy all the way starting from the number of rooms and I hope the example which I took the analogy which I took was good enough for you to understand Q learning understand the bellman equation how to make quick changes to the bellman equation and how to create the reward table the Q table and how to update the Q values using the bellman equation what does alpha do what does karma do so guys if you liked this video give it a thumbs up and if you want to see more videos like this and be technologically updated every time do subscribe to our channel there are a lot of videos regarding deep learning AI machine learning there walks all the different types of technology we even have blockchain with us so if you have any queries regarding this session please feel free to mention it in the comment section below till then thank you and happy learning I hope you have enjoyed listening to this video please be kind enough to like it and you can comment any of your doubts and queries and we will reply them at the earliest do look out for more videos in our playlist and subscribe to any rekha channel to learn more happy learning"
    }
]