[
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Perceptron and Multi-Layer Perceptron (MLP)",
        "Video Title":"What are MLPs (Multilayer Perceptrons)?",
        "URL":"https:\/\/www.youtube.com\/watch?v=7YaqzpitBXw",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/7YaqzpitBXw\/hqdefault.jpg",
        "ID":"7YaqzpitBXw",
        "Publish Time":"2022-07-11T12:00:07Z",
        "Channel":"IBM Technology",
        "Channel ID":"UCKWaEZ-_VweaEx1j62do_vQ",
        "Transcript":"You've probably heard of AI that can do really cool and interesting things like recognize objects in an image, or write stories, or play computer games. And you're probably wondering how scientists got computers to think in the way that we do. And you're probably wondering, SHOULD scientists let computers think the way that we do? Well, I can't answer that second question, but I want to talk about the first part of that question. One of the major concepts behind getting AI to think in the way that we do is the multilayer perceptron. It's a pretty long word. Don't get scared. I'll explain it using just the perceptron at first. The perceptron is heavily inspired by our own brain's most basic unit of thinking, which is the neuron. Th neuron looks something like this. It has a nucleus. It takes in inputs from other neurons. And it gives out outputs to other neurons. So this forms the output. And this forms the inputs. Neurons don't like to be alone and like to be densely connected in big groups. The neurons that are responsible for your eyes and your ability to recognize colors and objects and images in depth are a neural network that are formed of about 140 million neurons, all working together in concert to get you the images and things that you see. In the same way, a perceptron is formed of three basic components. There's the function, which is the thinking part of the perceptron. There are the inputs that come in from other perceptrons. And just like the neuron, there's also a set of outputs that go out from the perceptron. You won't be able to find a perceptron, it's just a concept. But the way that the perceptron is organized is also very similar to the way that our neurons are physically organized. Perceptron are organized in layers. And this is where the multiplayer part of \"multilayer perceptron\" comes in. They're all connected and they all feed off of each other's inputs and outputs. So this is just the basic concepts behind the multilayer perceptron. You're probably wondering how we get computers to think and how we get multilayer perceptrons to learn. Well, there's three basic parts of learning. First of all is you make an educated guess. For example, when you were learning four legged animals, you would have seen a bear and you might have called it a dog. Why would you call it a dog? Why would you guess a dog? Well, dogs have four legs and a tail, and this particular bear has four legs and a tail. Well, you were wrong, so what you have to do now is the second step of learning, which is to change. So you change your mind about what's the difference between a dog and a bear. But what about the next time when there's a horse? Neither of those answers really work, so what needs to happen is you need to repeat this process. This is basically the same way that scientists are able to train multilayer perceptrons. First of all, the multilayer perceptron gives an output based on the function, based on the inputs, it gives out an output. Very often that output is wrong and sometimes it's right, but based on that feedback, it has to change. The changing process is something called Back Propagation. Long word, I know, but very simply, it just means that the multilayer perceptron has to go back through its layers and improve itself all the way down to the input so that the next output is better. Speaking of the next output, the process of repeating is called an Epoch. Every epoch that a multilayer perceptron goes through brings it closer to the perfect output. I hope this has helped in your understanding of how AI can think and do some of the things that our brains naturally do. Thank you. Thanks so much. If you liked this video and want to see more like it, please like and subscribe. See you soon."
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Perceptron and Multi-Layer Perceptron (MLP)",
        "Video Title":"The Perceptron Explained",
        "URL":"https:\/\/www.youtube.com\/watch?v=i1G7PXZMnSc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/i1G7PXZMnSc\/hqdefault.jpg",
        "ID":"i1G7PXZMnSc",
        "Publish Time":"2023-06-24T18:52:54Z",
        "Channel":"Alice Heiman",
        "Channel ID":"UCE7fVac75YKse35UVpWP4Og",
        "Transcript":"to understand neural networks you must first understand their building block the perceptron beginning in 1957 at the Cornell aeronautical laboratory psychology researcher Frank rosenblatt built the first real-world implementation of the perceptron the Mark 1 perceptron was a machine consisting of 400 photocells that could classify images in essence the perceptron is a one layer neural network that can classify things into two parts it is inspired by biological brain units called neurons neurons receive electrical signals from several other neurons and depending on their strength the neuron can either activate or stay put similarly an artificial neuron takes input values multiplied by weights that indicates their respective importance sums them up along with a static number called bias passes them for an activation function that Maps the results an output between zero not activated and one activated an example of an activation is the step function it outputs one for all positive inputs and zero otherwise the power of the perceptron comes from its ability to automatically learn the weights and bias numbers through a step-by-step process called The perceptron Learning rule for example if you want to learn to identify breast cancer start with many examples where you know the answer then go through all the trainee examples doing the following in each round first get the prediction from the perceptron two compute the error I.E the difference between the correct output and the prediction three multiply each input value by the error and add it to the weight and four add the error to the bias to get really fancy you can multiply with an extra coefficient between 0 and 1 called learning rate which controls how fast the perceptron to updated weights you can also run all training examples multiple times and each complete pass is called Epoch in the end the perceptron should have updated its weights so they can handle new unknown instances of breast cancer and still be able to get a result in short the perception is called a linear binary classifier because it classifies things into two groups but can only do so if you can draw a straight line between them model neural networks use multiple layers of perceptrons to make more complex predictions and this is what makes a perceptron the building block of deep modern neural networks thank you for watching"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Perceptron and Multi-Layer Perceptron (MLP)",
        "Video Title":"What is Multilayer Perceptron (MLP) in Machine Learning?",
        "URL":"https:\/\/www.youtube.com\/watch?v=KQL61JP8-GM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/KQL61JP8-GM\/hqdefault.jpg",
        "ID":"KQL61JP8-GM",
        "Publish Time":"2023-10-07T11:00:29Z",
        "Channel":"Data & Analytics",
        "Channel ID":"UCIdo4tN3khKldHP3733E9JA",
        "Transcript":"ever wondered what multi-layer perceptron in machine learning is does it sound like a bunch of techno jargon well fret not by the end of this video you'll have a firm grasp on what a multi-layer perceptron or MLP really is think of MLP as a type of artificial neural network it's a little like a team with each player having a specific role the players in this case are called neurons and they're organized into layers there's definitely an order to this team the first layer is the input layer which receives all the initial information then there are one or more hidden layers that do the heavy lifting they process and transform the data finally there's the output layer that delivers the result imagine you're trying to predict the weather the temperature humidity wind speed and so forth would be your inputs the hidden layers would then crunch these numbers and the output layer would tell you if it's likely to be sunny or rainy but how do these layers communicate well they're interconnected and they pass information forward Ward through something called weights these weights are crucial since they determine how much influence one neuron has on the next and here's where the magic of learning happens these weights aren't static they're adjusted during a process called back propagation if the Network's prediction is off it goes back tweaks the weights and learns from its mistake so you see a multi-layer perceptron is like a miniature self-correcting brain that gets better with each iteration it's a powerful tool in the world world of machine learning helping to solve complex problems across various fields from Healthcare to finance to weather prediction to summarize a multi-layer perceptron is a type of artificial neural network organized into layers it takes in data processes it through one or more hidden layers using weights and delivers a result and if the result isn't quite right it adjusts its weights to learn and improve so the next time someone mentions a multi-layer perceptron you can confidently say that ah you mean that self correcting artificial brain that's revolutionizing machine learning yes I know what that is and you'll have the satisfaction of knowing you're not just spouting techno jargon but you truly understand what a multi-layer perceptron is all about"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Perceptron and Multi-Layer Perceptron (MLP)",
        "Video Title":"10.4: Neural Networks: Multilayer Perceptron Part 1 - The Nature of Code",
        "URL":"https:\/\/www.youtube.com\/watch?v=u5GAVdLQyIg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/u5GAVdLQyIg\/hqdefault.jpg",
        "ID":"u5GAVdLQyIg",
        "Publish Time":"2017-06-27T13:12:06Z",
        "Channel":"The Coding Train",
        "Channel ID":"UCvjgXvBlbQiydffZU7m1_aw",
        "Transcript":"hi again so maybe you just watched my previous videos about coding of perceptron and now I want to ask the question why is not just stop here so okay so we have this like very simple scenario right where we have a canvas and it has a whole bunch of points in that canvas or a Cartesian plane whatever we want to call it and we threw a line in between and we were trying to classify some points that are on one side of the line and some other points that are only another side of the line so that was a scenario where we had to be single perceptron the sort of like processing unit we can call it the neuron or the processor and it received inputs it had like x0 and x1 we're like the x and y coordinates of the point it also had this thing called a bias and then it generated an output each one of these inputs was connected to the processor with a wait no wait one way to or never wait wait wait and the processor creates a weighted sum of all the inputs multiplied by the weights that weighted sum is passed through an activation function to generate the output so why isn't this good now let's first think about what what's what's the limit here so the idea is that what if I want any number of inputs to generate any number of outputs that's the essence of what I want to do in a lot of different machine learning applications let's take a very classic classification algorithm which is to say okay well what if I have a handwritten digit like the number 8 and I have all of the pixels of this digit and I want those to be the inputs to this perceptron and I want the output to tell me a set of probabilities as to which digit it is so the output should look something like you know there's a point one chance it's zero there's a point two chances of one there's a point one chance' two two zero three four five six seven oh it it's like a point ninety nine chances at eight and 0.05 chance it's a ten and I don't think I got those to add up to one but you get the idea so the idea here is that we want to be able to have some type of processing unit that can take it arbitrary amount of inputs like maybe this is a 28 by 28 pixel image so there's 784 grayscale values and instead those are coming into the processor which was wait is it sudden and all this stuff when we get an output that have some arbitrary amounts of probabilities to mitla help us guess eight not that this is an eight this model why couldn't I just have a whole bunch more inputs and then a whole bunch more outputs but still have one single processing unit and the reason why I can't is a stems from an article I don't know I'm sorry a book that was published in 1969 by Marvin Minsky and Seymour Papert paper call of perceptrons you know AI luminaries here in the book perceptron Marvin Minsky and Seymour Papert point out that a simple perceptron the thing that I built in the previous two videos can only solve linearly separable problems so what does that mean anyway and why should you care about that so let's think about this this over here is a linearly separable problem meaning I need to classify this stuff and if I were to visualize all that stuff I can draw a line in between this part of the day this this stuff is to this class and this stuff that's with this class the stuff itself is separable by a line in three dimensions I could put a plane and that would be literally separable because I can kind of divide the space in half and and and understand it that way the problem is most interesting problems are not linearly separable you know there might be some Dana which clusters all here in the center that is of one class but anything outside of it is of another class and I can't draw one line to separate that stuff and you might be even thinking but that's you know still so much you could do so much with linearly separable stuff well here I'm going to show you right now a particular problem I'm looking for an eraser logging around like a crazy person I'm going to show you a particular problem called X or I'm making the case for why we need to go a step further I just had an idea let's go back to the litter I'm thinking the case for why we need to go to a close go a step further and make something called a multi-layer perceptron and I'm going to lay out that case for you right now so you might be familiar you might remember me from my videos on conditional statements and boolean expressions well in those videos I talked about operations like and and or which in computer programming syntax are often written you know double ampersand or two pipes the idea being that if I were to make a truth table true true false false so what I'm doing now is I'm showing you a truth table I have two elements I'm saying what if I say a and the B so if a is true well this makes no sense what I've drawn here because I'm losing my brain cells slowly over time with every passing any first not true false true false true hand true yields true if I am hungry and I am thirsty I shall go and have budge right true and true yields true true and false is false false and true is false false itself is false right if I have a boolean expression a and B I need both of those things to be true in order for me to get through interestingly enough this is a linearly separable problem I can draw a line right here and true is on one side and false is on the other side this means if this is a linearly separable problem which means I could create a perceptron that perceptron is going to have two inputs there are going to be boolean values true or false for a false and I could train this perceptron to give me an output which if two truths come in I should get it true if one false to the true comes David I should get a false - false coming I should get a false great or I could do the same thing what is or change in two if I'm going to do or me erase this dotted line and or now all of these become true because with an or operation A or B I only need one of these to be true in order to get true but if both are false I get false and guess what still a linearly separable problem and is literally separable or is literally separable we could have a perceptron learn to do both of those things now hold on a second there is another boolean operator which you may you might not have heard up until this video which would be really kind of exciting for me it would make me very happy if somebody watching this never heard of this before it is called x4 can you see what I'm writing near X or the X stands for exclusive exclusive its exclusive or which means it's only true if one is true and what is false it's not true both are false this or that both those things are false I'm still false but if both are true it's also false so this is exclusive or let me erase all this exclusive or I mean if one if one is true and one is false it's true if one is true is one assault is true if both are true it's false if both are false it's false this is exclusive or a very simple boolean operation however I triple dog dare with the cherry on top you to draw a single line through here to divide the false in the truth I cannot I can draw if this is not a linearly separable problem this is the point of all this like rambling I could draw two lines one here and now I have all the truths in here and the false is outside of them this means a single perceptron the simplest cannot solve cannot solve the simple operation like this so this is what Minsky and Papert talked about in the book perceptrons well this is like an interesting idea conceptually it kind of seems very exciting but if it can't solve X or what are we supposed to do in this the answer to this is a new bike I've already thought of this yourself it's not two but I kind of missed a little piece of my diagram here right let's say this is a perceptron that knows how to solve and and this is a perceptron that knows how to solve for what if I took those same inputs and sent them into both and then I got the output here so this output would give me the result of and and this output would give me the result of or well what is XOR really XOR is actually or but not and right so if I could solve something and is linearly separable not and is also linearly separable so what I want then is for both of these out but actually to go into another perceptron that would then be and so this project run can solve not and and this perceptron can solve or and those output can come into here then this would be the results of both or is true and not and is true which is actually this these are the only two things where or is too but not in but not and and so the idea here is that more complex problems that are not linearly separable can be solved by linking multiple perceptron together and this is the idea of a multi layered perceptron we have multiple layers and this is still a very simple diagram you could think of this almost as like if you were designing a circuit right if you decide what electricity should flow and because we're like a these were switches you know how could you get a bunch of outfit you have an LED turn on with exclusive or you would actually water the circuit basically in exactly this way so this is the idea here so what I am would like to do in the next so at some point I would like to make a video where I actually just kind of build take that previous perceptron example and just take a few steps farther to do exactly that but what I'm going to do actually in the next videos is diagram out this structure of a multi-layer perceptron how the inputs how the outputs work how the feed-forward algorithm works where the inputs come in get x weights get some together and generate an output and build a simple javascript library and has all the pieces of that neural network system in it okay so I hope that this video kind of gives you a nice follow-up from the perceptron in a sense of why this is important and I'm not sure if I was done yet I'm going to go check the live chat then questions are important things like this this video will be over oh yeah back so there was one question which is important like Oh what I heard somebody in the chat asked what about the hidden layer and so this is jumping ahead a little bit because I'm going to get to this in more detail in the next video there's a way that I drew this diagram is pretty awkward let me try to fix this up for a second imagine there were two inputs and I actually drew those as if they are neuron and I know I'm out of the frame but I'm still here and these inputs were connected to each of these perceptrons each was connected and each was weighted so this is actually what's now known as a 3 layer Network there is the input layer this is the hidden layer in the reason why it will actually reduce the output layer right that's obvious right this is the input those are the inputs the trues and falses this is the output layer that should give us a result are we still true or we false and then the hidden layer are the neurons that sit in between the inputs and the outputs and they're called hidden because as a kind of user of the system we don't necessarily see them a user of the system is speeding in data and looking at the output the hidden layer in a sense is where the magic happens the hidden layer is what allows one to get around this sort of linearly separable question so the war in layers the more neurons the more amount of complexity in a way that the system the more waits the more parameters that need to be tweaked and we'll see that as they start to build a neural network library the way that I want that library to be set up I want to say I want to make a network with ten inputs three outputs one hidden layer with 15 like hidden neurons something like that but there could be multiple hidden layers and eventually as I get further and further down this road if I keep going we'll see there are all sorts of other styles of how the network can be configured and set up and whether it's the output feeds back to to the input that's something called recurrent networks convolutional Network and if some this kind of like said image processing operations almost happens early on before as part of the layers so there's a a lot of stuff in the grand scheme of things to get to but this is the fundamental building blocks so okay so I'm in the next video I'm going to start building the library and to be honest I think what I need to do yeah the next video I'm going to set up the basic skeleton of the neural network library and look at all the pieces that we need and then I'm going to have to keep going and look at some matrix math that's going to be fun okay see you soon [Music]"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Implementing Neural Networks with TensorFlow\/PyTorch",
        "Video Title":"TensorFlow in 100 Seconds",
        "URL":"https:\/\/www.youtube.com\/watch?v=i8NETqtGHms",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/i8NETqtGHms\/hqdefault.jpg",
        "ID":"i8NETqtGHms",
        "Publish Time":"2022-08-03T15:23:57Z",
        "Channel":"Fireship",
        "Channel ID":"UCsBjURrPoezykLs9EqgamOA",
        "Transcript":"tensorflow an open source machine learning framework famous for powering deep neural networks with high-level code it was developed by the google brain team and first released in 2015. it's most commonly used with python but can run in other languages like javascript c plus plus and java at its core it's just a library for programming with linear algebra and statistics as you know the word tensor describes a multilinear relationship between sets of algebraic objects within a vector space aka a multi-dimensional array what makes it special is its collection of apis for data processing visualization model evaluation and deployment that make deep learning accessible to the average developer it's extremely portable and is able to run on tiny mobile cpus or microcontrollers with tensorflow lite can run in the browser with tensorflow.js while the core library can scale up to multiple gpus or run on tensor processing units ships engineered specifically to run tensorflow at a massive scale it's used in medicine for object detection and mri images by twitter to sort your timeline by tweet relevance by spotify to recommend music by paypal for fraud detection in addition to many other applications like self-driving cars natural language processing and so on to build your own neural network right now create a python file and install tensorflow next we'll need some data like fashion mnist which we can automatically import the goal is to train a model that can predict the clothing type of each image tensorflow has a subclassing api for expert users but also integrates with the beginner-friendly keras library which has a sequential api that can easily build neural networks layer by layer we start with a flattened layer that takes the 28 by 28 pixel image as an input and converts it into a one-dimensional array this input layer is then fed into a dense layer with 128 fully connected neurons or nodes you can think of each node like its own linear regression as each data point flows through it it'll try to guess the output and gradually update a mapping of weights to determine the importance of a given variable in this case it uses a rectified linear activation function that will output the input if a certain threshold is met otherwise it will just output zero and the behavior of this layer can be customized by tuning as hyperparameters finally we have our output layer which is also dense but is limited to 10 nodes which corresponds to the total number of clothing types in the data set now we can compile the model and tell it to optimize a certain loss function like sparse categorical cross entropy as we train the model for multiple epochs its accuracy should gradually improve the end result is a model that makes a prediction with the likelihood that an image is a certain type of clothing congratulations you just built a neural network this has been tensorflow in 100 seconds hit the like button if you want to see more short videos like this thanks for watching and i will see you in the next one"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Implementing Neural Networks with TensorFlow\/PyTorch",
        "Video Title":"PyTorch in 100 Seconds",
        "URL":"https:\/\/www.youtube.com\/watch?v=ORMx45xqWkA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ORMx45xqWkA\/hqdefault.jpg",
        "ID":"ORMx45xqWkA",
        "Publish Time":"2023-03-20T15:11:37Z",
        "Channel":"Fireship",
        "Channel ID":"UCsBjURrPoezykLs9EqgamOA",
        "Transcript":"pie torch an open source deep learning framework used to build some of the world's most famous artificial intelligence products it was created at The Meta AI research lab in 2016 but is actually derived from the Lua based torch library that dates back to 2002. fundamentally it's a library for programming with tensors which are basically just multi-dimensional arrays that represent data and parameters in deep neural networks sounds complicated but its focused on usability will have you training machine learning models with just a few lines of python in addition it facilitates high performance parallel Computing on a GPU thanks to nvidia's Cuda platform developers love prototyping with it because it supports a dynamic computational graph allowing models to be optimized at runtime it does this by constructing a directed acyclic graph consisting of functions that keeps track of all the executed operations on the tensors allowing you to change the shape size and operations after every iteration if needed pytorch has been used to train models for computer vision AI like Tesla autopilot image generators like stable diffusion and speech recognition models like open AI whisper just to name a few to get started install Pi torque and optionally Cuda if you want to accelerate Computing on your GPU now import it into a python file or notebook like I mentioned a tensor is similar to a multi-dimensional array create a 2d array or Matrix with python then use torch to convert it into a tensor now we can run all kinds of computations on it like we might convert all these integers into random floating points we can also perform linear algebra by taking multiple tensors and multiplying them together what you came here to do though is build a deep neural network like an image classifier to handle that we can define a new class that inherits from the neural network module class inside the Constructor we can build it out layer by layer the flattened layer will take a multi-dimensional input like an image and convert it to one dimension from there sequential is used to create a container of layers that the data will flow through each layer has multiple nodes where each node is like its own mini statistical model as each data point flows through it it'll try to guess the output and gradually update a mapping of weights to determine in the importance of a given variable linear is a fully connected layer that takes the flat and 28 by 28 image and transforms it to an output of 512. this layer is followed by a non-linear activation function when activated it means that feature might be important and outputs the node otherwise it just outputs zero and finally we finish with a fully connected layer that outputs the 10 labels the model is trying to predict with these pieces in place that next step is to define a forward method that describes the flow of data and now instantiate the model to a GPU and pass it some input data this will automatically call its forward method for training and prediction congratulations you just built a neural network this has been pytorch in 100 seconds thanks for watching and I will see you in the next one"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Implementing Neural Networks with TensorFlow\/PyTorch",
        "Video Title":"PyTorch or Tensorflow? Which Should YOU Learn!",
        "URL":"https:\/\/www.youtube.com\/watch?v=po5qTPKBrRc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/po5qTPKBrRc\/hqdefault.jpg",
        "ID":"po5qTPKBrRc",
        "Publish Time":"2023-01-31T00:36:57Z",
        "Channel":"Nicholas Renotte",
        "Channel ID":"UCHXa4OpASJEwrHrLeIzw7Yg",
        "Transcript":"if you've been floating around the machine learning space you've probably come across the words tensorflow or Pi torch at one point or another and you've probably thought to yourself which deep learning framework should I go on ahead and learn I first started out by learning tensorflow and building up a bunch of neural networks to get me to the states that I could build pretty much whatever it is that I needed but if I were to have to start out again I'd probably do exactly the same as what I did I'd pick the framework that my potential future employer uses in my particular case at that time that was tensorflow that being said learning doesn't stop there go out and learn other Frameworks right now I'm learning pytorch and Jacks just to ensure that I've got them up my sleeve because you never know when you might need it"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Implementing Neural Networks with TensorFlow\/PyTorch",
        "Video Title":"PyTorch vs. TensorFlow",
        "URL":"https:\/\/www.youtube.com\/watch?v=fSgNjOvXTCs",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/fSgNjOvXTCs\/hqdefault.jpg",
        "ID":"fSgNjOvXTCs",
        "Publish Time":"2024-09-27T12:13:36Z",
        "Channel":"Plivo",
        "Channel ID":"UCNL8MQasO7O-q_g_8X6CI2g",
        "Transcript":"pytorch or tensorflow both are leading deep learning Frameworks and they both have pros and cons so let's get into it pytorch developed by meta aai dominates research with 60% of published papers using it as of June of 2024 and people love its Dynamic computational graph intuitive model building and seamless integration with python tools like numpy the fact that it's pythonic also helps to make it more useful for a wider audience which means that it has an excellent ecosystem of models and libraries for pie torch users to tap into tensor flow on the other hand developed by Google is generally considered to be the industry standard for production environments it offers robust deployment capabilities through tensorflow serving and strong cloud service integration too tensorflow's comprehensive tooling includes tensor board for visualization tensorflow light for mobile deployment and tensorflow.js for web deployment it also supports Hardware acceleration through gpus and tpus ensuring high performance in large scale environments"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Activation Functions in Deep Learning",
        "Video Title":"Activation Functions In Neural Networks Explained | Deep Learning Tutorial",
        "URL":"https:\/\/www.youtube.com\/watch?v=Fu273ovPBmQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Fu273ovPBmQ\/hqdefault.jpg",
        "ID":"Fu273ovPBmQ",
        "Publish Time":"2021-12-06T13:00:06Z",
        "Channel":"AssemblyAI",
        "Channel ID":"UCtatfZMf-8EkIwASXM4ts0A",
        "Transcript":"in this video we are going to learn about actuation functions we go over the definition of actuation functions why they are used then we have a look at different kinds of actuation functions and at the end i also show you how to use them in your code and don't worry because deep learning frameworks like pytorch and tensorflow make it extremely easy to apply them this video is part of the deep learning explained series by assembly ai which is a company that creates a state-of-the-art speech-to-text api and if you want to use assembly ai for free then grab your api token using the link in the description below and now let's get started so what are activation functions and why do we need them actuation functions apply a non-linear transformation and decide whether a neuron should be activated or not now let's take a step back and see what this means in a previous video we learned how neural networks work in a neural network we have the input layer where we accept an input and an output layer that gives the actual prediction or the outcome of the network and in between we have the hidden layers all of these layers consist of neurons and at each neuron we apply a linear transformation it multiplies the input with some weights and maybe adds a bias now this is fine as long as we have a simple problem like this where we can model the predictions with a linear function but let's say we have a more complex problem one thing we can do is of course add more layers to our network but here's a big problem without activation functions we only get linear transformations after each other so our whole network is basically just a stacked linear regression model that is not able to learn complex patterns and this is exactly why actuation functions come into play so after each layer we want to apply an activation function this applies a non-linear transformation and helps our network to solve complex tasks now let's have a look at different kinds of actuation functions there are many different actuation functions you can choose so we take a look at the most popular ones we'll have a look at the step function sigmoid hyperbolic tangent value leaky value and the softmax the step function will just output 1 if our input is greater than a threshold and 0 otherwise this perfectly demonstrates the underlying concept that the activation function decides if a neuron will be activated or not if the input is greater than the threshold the neuron is actuated and otherwise not while this transformation should be easy to understand the step function is actually a little bit too simple and not used in practice a very popular choice in practice is the sigmoid function the formula is 1 over 1 plus e to the minus x this outputs a probability between 0 and 1. if the input is a very negative number then sigmoid outputs a number close to 0 and for a very positive number sigmoid transforms it to a number close to 1 and for numbers close to 0 we have this rising curve between 0 and 1. this again means that the more positive the input number is the more our neuron will be activated the sigmoid function is sometimes used in hidden layers but most of the time it is used in the last layer for binary classification problems until now we have only seen activation functions that output numbers between 0 and 1 but this is not a requirement for actuation functions so in the next examples you will see transformations that can output numbers also in a different range the hyperbolic tangent is a common choice for hidden layers it is basically a scaled and shifted sigmoid function that outputs a number between -1 and plus 1. value is probably the most popular choice in hidden layers the formula is rather simple it just takes the maximum of 0 and the input x so if the input is negative it outputs 0 and if the input is positive it simply returns this output without modification it does not look that fancy but it can actually improve the learning of our neural network a lot so the rule of thumb is that if you are not sure which actuation function you should use in your hidden layers then just use value there is only one problem that sometimes happens during training this is the so-called dying value problem after many training iterations our neuron can reach a dead state where it only outputs 0 for any given input which means there will be no more updates for your weights so to avoid this problem you can use a slightly adapted function which is the leaky value the leaky value is the same as the regular value for positive numbers here it just returns the input but for negative numbers it does not simply return 0 but it applies a small scaling factor a times x a is usually very small for example 0.001 so the output is close to zero but it avoids that the neuron will be completely dead so this is also a very good choice for hidden layers so whenever you notice that your weights won't update during training then try using leaky value instead of the normal value and the last function i want to show you is the softmax function the softmax squashes the input numbers to output numbers between 0 and 1 so that you will get a probability value at the end so the higher the raw input number the higher will be the probability value this is usually used in the last layer in multi-class classification problems after applying the softmax in the end you then decide for the class with the highest probability now that we've seen different actuation functions in theory let's have a look at how we can use them in tensorflow and pytorch it is quite easy with both frameworks in tensorflow i recommend using the keras api with this we have two options for each layer we can specify the optional argument actuation and then just use the name of the actuation function or we just leave this actuation argument away and create the layer ourself all the functions i just showed you are available as a layer in tensorflow.keras.layers in pytorch we also find all actuation functions as a layer under torch.nn in our init function of the neural network we can create instances of the actuation function layers and then in the forward pass we call these layers or as a second option we can use the functions directly in the forward pass by using the functions defined in torch.nn.functional and that's basically all we have to do to use actuation functions in our code alright so i hope you now have a clear understanding of what actuation functions are and how you can use them and if you have any questions let me know in the comments and also if you enjoyed this video then please hit the like button and consider subscribing to the channel for more content like this and before you leave don't forget to grab your free api token using the link in the description below and then i hope to see you in the next video bye"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Activation Functions in Deep Learning",
        "Video Title":"Activation Functions - EXPLAINED!",
        "URL":"https:\/\/www.youtube.com\/watch?v=s-V7gKrsels",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/s-V7gKrsels\/hqdefault.jpg",
        "ID":"s-V7gKrsels",
        "Publish Time":"2020-03-19T02:03:08Z",
        "Channel":"CodeEmporium",
        "Channel ID":"UC5_6ZD6s8klmMu9TXEB_1IA",
        "Transcript":"all right I'm going to explain this in multiple passes the first pass will be a high-level overview of the what's where's and how's of activation functions and the second pass I'll fill in the gaps with more details and we'll walk through a few examples exciting stuff coming up so let's jump into it case 1 we have this neural network here with a single hidden layer nothing fancy take the inputs multiply it with the weights add the bias term and just pass it to the next layer this neural network can learn how to classify data that is separable by a line nice but what if data is slightly more complicated I got this I got this yeah wait no no no why whoa not so easy anymore is it our network is too simple to capture patterns in the data so instead of just passing the data through the next layer let's pass it through a non-linear function oh yeah the network was able to classify this data with a quadratic curve this function that we passed the data through is called an activation function this function could be a sigmoid and arctan a reloj leaky relu a parametric relu swift l lu max out and the list goes on any one of these functions would have worked here but what if the data was even more complicated when data gets complicated well we add more layers and let's say that we use a sigmoid function for this and see how that turns out ok nicer a bit better wait wait why even though the network is looking at more examples it's not learning anything this is the vanishing gradient problem it happens because of the sigmoid function it squeezes information during the back propagation step the gradients become smaller and smaller until eventually they vanish no gradients means no learning a remedy to this is to use an activation function that doesn't squeeze information like relu let's use reloj and every neuron and see what happens okay nicer oh yeah don't town ah-hah relu works well in practice and this is also why we use it almost everywhere today but in a random case or in a few random cases we may get this yes yes okay why do I still have this problem looks like we're running into a similar problem of no learning like we did before this is the dying reloj problem it's similar to the vanishing gradient problem in that the problem was caused then because of sigmoids squeezing the input now the problem is caused by relic ompletely blocking inputs less than zero so a solution is to introduce some activation even in the negative case Allu and rekey relu have handled this pretty well and so the day is saved but what about the activation of the output neurons well that depends on the problem we're solving in classification we use the softmax output the number of neurons in this layer is the number of classes in the classification and the values represent the probability of belonging to a particular class now in the case of regression problems we need a real number output so we don't tend to use activation functions at all if the network spits out a single real number then we only use one output neuron if it spits out two real numbers we use two output neurons with no activation and I think you know where this is going again now this is the first pass over the activation functions we went over four cases and we took a look at the softmax activation for the output layer now let's go through these cases again and add some more details back to case one the simple neural network with one hidden layer and no activation the output is softmax activated this leads to a line separator how is this the case well the output neuron can be written as W 2h plus b2 where W and B are the weights and biases and H is the output of the hidden layer but we know that H is W 1 X plus b1 and so we can substitute that in the output equation and then we just expand the brackets and looky looky what we have here the output is a linear equation of the input this output is passed through the softmax activation since there's two neurons if we call the two neurons oh 1 and O 2 then the output activation of the first could be written as e to the oh one divided by the sum of e to the oh 1 plus e to the Oh 2 we can divide both the numerator and denominator by e to the power of 0 1 we can then substitute the value of the output equations then we expand the terms and well we can see that u 1 minus u 2 can be written as sum you zero just another constant and v1 minus v2 could be written as just some v-0 which is a constant again and in the end it essentially boils down to a sigmoid function if we use these values to plot a boundary it's a straight line basically it boils down to logistic regression this animation captures it pretty well and for a more psychedelic visual on logistic regression I have an entire video on it so you can check that out now case two we have data that can't be separated by a line and what we did before was we added a sigmoid activation the output is now more complicated and we get better fitting decision boundaries cool case three even more complicated data and for this we increase the number of layers but with functions like sigmoid that squeeze data this leads to the vanishing gradient problem why does this happen let's look at the sigmoid function at points with large positive or negative value the grading comes really close to zero this isn't good and since in neural networks these gradients are back propagated if you look at this neuron it's in a later layer and say that this neuron has a gradient that is near zero the neuron in a layer before it will have an even lower gradient this low value propagates backwards until finally it becomes zero once the gradient becomes zero the neuron is useless and there is no learning this is the vanishing gradient problem and I think you can see why this is a problem to solve this we need to understand the root cause it is the squeezing nature of the sigmoid function what I mean by this is that the sigmoid takes a real number and it squishes it between some small fixed range 0 & 1 this squeezing nature of functions creates small gradients and we see the same an arctan function which squeezes real numbers between negative 1 and positive 1 the solution to this is to use a function that doesn't squeeze values like relu and that's why we used it but in case for we saw a situation where we might run into the dying relu problem why is this the case well let's take a look at the revenue for positive inputs it lets information pass through it unfiltered for negative inputs it's completely blocked and that is the problem during training there may be a time where the bias may become very negative so WX plus B is negative for most neurons most neurons are off during the forward step and hence most neurons are also off during the backward step and what's worse is that if newer inputs are not positive enough to overcome the bias these neurons remain dead dead neurons don't learn to solve this we make sure that learning happens with negative inputs using leaky relu or ello activations who so once again the day is saved I hope this video helps you understand more about activation functions check out my other content and I will see you very soon bye bye you"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Activation Functions in Deep Learning",
        "Video Title":"What is Activation function in Neural Network ? Types of Activation Function in Neural Network",
        "URL":"https:\/\/www.youtube.com\/watch?v=Y9qdKsOHRjA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Y9qdKsOHRjA\/hqdefault.jpg",
        "ID":"Y9qdKsOHRjA",
        "Publish Time":"2021-05-02T16:41:35Z",
        "Channel":"Learn With Jay",
        "Channel ID":"UCJFAF6IsaMkzHBDdfriY-yQ",
        "Transcript":"what's going on everyone this is jay patel and in this video we will be talking about activation functions in neural network we will see why do we need activation functions what are the different types of activation functions and which functions to use and when also if you are new to this channel then consider subscribing because i regularly upload new video tutorials like these on machine learning so hit the red subscribe button and also hit the bell icon so that you get notified and for now let's get started let's first see why do we need activation functions and for that let's look at the equations of the forward propagation now in these equations if we skip the step of passing the z1 into the activation functions then e1 will directly be given by z1 and if we calculate a2 this way then a2 will be given by this and we can expand this and it will become this and here w2 multiplied by w1 can be written in the form of w dash while w2 multiplied by p1 plus b2 can be written in the form of b dash so a2 has now the similar structure as a1 and if we continue doing this for the further layers then a3 can be given by w double dash multiplied by a0 plus b double dash so no matter how many layers we use we will be getting the similar kind of equations and thus we will not be able to take the advantage or the benefit of using the multiple layers and it will become as just using one layer now without using any activation functions all these become linear so we need to use a non-linear activation functions to take the benefit of the different hidden layers and the function must be non-linear also the real-world data is non-linear and thus learning from the complex relationship between the data will be better suited with the help of the non-linear activation functions that's why we need to use an activation functions now let's look at what are the different types of activation functions and let's first start with the most famous which is the sigmoid function the sigmoid function is an s-shaped curve which looks something like this and it takes any value between zero to one and for the given zero input value it gives 0.5 as the output value now as it only takes the value between 0 to 1 it can be interpreted as a probability if we use this in the output layer so sigmoid function is most commonly used at the output neuron in the binary classification let's say if we are classifying apples versus oranges and the output value is greater than 0.5 then we can classify it as an apple and if the output value is less than 0.5 then we can classify it as an orange thus sigmoid function is very helpful and very suitable for using it at the output layer for the binary classification but using sigmoid functions in the hidden layer has some drawbacks let's look at these drawbacks as we discuss the next activation function which is tan hit function now 10 inch function or the hyperbolic tangent function is a similar to sigmoid function just that it is stretched between -1 to 1 instead of 0 to 1 and because of this it has some advantages over sigmoid function when we use it in the hidden layer in the back provocation or gradient descent we need to use the derivative of cost function with respect to w and the cost function depends on the activation function so we need to take the derivative of the activation functions as well so if we see the maximum value of the derivative of sigmoid function is only 0.25 while that of 10h is up to 1. thus using 10h activation function we can train our model much faster as the derivative value is higher it will converge much faster also in 10 h function the average of the data will be close to zero and if we use 10h function in the hidden layers then it works much better than the sigmoid function it is because of the reason that the output of the hidden layers which is also the input to the next layer will be having the values whose average will be close to zero or we can say that the data is normalized or centered around zero and when we pass a normalized data to the next layer it makes the training much more easier thus we use 10 h functions in the hidden layers and not the sigmoid functions now both these tan edge functions and sigmoid functions also has some disadvantage when we take the derivative of both these functions while doing the gradient descent we see that for a very high value of x or for very low value of x the derivative becomes very small thus the learning will be very slow now this is called vanishing gradient problem when the learning becomes very slow or the change or the training of the data does not take place effectively now this problem can be overcome by using another type of activation function which is called a relu function or a rectified linear unit function now it is a linear function where we rectify the input which is less than 0 so the graph of red will look something like this and the equation can be given by this so for every x less than or equal to 0 it gives 0 as the output and for every x greater than 0 it gives same x as the output now as the values of this derivative is same for all x greater than 0 it overcomes the vanishing gradient problem also its derivative value is 1 which was the maximum value of the derivative of 10 h function so the learning can also be very fast with the help of brand new function also note one more thing here this function is not entirely linear as it gives zero as the output for all x less than zero while it gives linear output for all x greater than zero so it can be also said as a piecewise linear function and this actually makes it even better as it can take the advantage of both linear and non-linear property the linearity of the function can help to work on the vanishing gradient problem and also makes the training faster and because of the non-linearity we can take the advantage or the benefits of using multiple hidden layers in our neural network now there are other variation of relu also available and they are leaky relu and the exponential linear unit or elu in leaky value instead of defining 0 we define it with the help of a very small linear component of x it can be given by 0.01 multiplied by x and in elu we define it with the exponential component instead of the 0. both of these leaky value and elu works well and can be used but nowadays most commonly value is only used now let's look at the other type of activation function which is the soft mac function we saw that while doing the binary classification we use sigmoid function at the output neuron but what if we are solving the multi-class classification problem what kind of activation function will we use then in this case we can't use sigmoid function as it is just not suited for this purpose and so for that we use a different kind of activation functions which we call as a soft max function let's say we are making the predictions for the four different classes and our final output values without the activation function comes out to be seven point five one and three i'm just assuming a hypothetical data here so from this data it is clear that our winner should be class 1 which is 7 in the output neuron but these output values can be made much better by scaling them into the probabilities this can be achieved by calculating the exponent of each value in the list and dividing it by the sum of the exponent values and if we do that we get these values and the summation of these values is also equal to one thus now we have converted the our given output values in the form of probabilities and and we will just classify the highest probability now note one thing here we could have directly divide the values in the list with their sum and still it could have been interpreted as the probability but taking the exponent makes it much better as the exponent function makes the larger value much larger and smaller value much smaller thus it enhanced the final output prediction and we can be more sure of our final classification as you can see with using exponent the output probability is 0.97 while without exponent is just only 0.6 so the equation of the soft max activation function is given by this well this equation is for every eighth neuron so we will be calculating it for every eighth neuron in the output layer while doing the multi-class classification now thus so far we have discussed about the activation functions for classification problems but what if our final output prediction is linear and it can take any value like making the house price prediction where the price of the house can range from any number between 0 to 1 million or maybe more in this case it is not suitable for using any kind of non-linear activation function in the output neuron and just in the output neuron we will be using the activation functions in the hidden layers but we won't be using any activation functions in the output neuron thus for the output neuron we will be just taking the linear output without activation function for linear regression okay so let's summarize everything we have learned so far for binary classification we use the sigmoid function in the output neuron because it can be interpreted as the probability but we don't use sigmoid function in the hidden layers because the 10h and the relu function always outperforms the sigmoid function so for hidden layers we either use 10h function or relu function now both these 10h and the relu function had one disadvantage which was punishing gradient problem and it was that for every very large or very small value of x the derivative becomes very small and the learning can become very slow and this can lead our model to not train effectively so it can be overcome with the help of a relu function which was a piecewise linear function and it has the advantage of both the linear and the non-linear property there are other variations of value functions also available such as leaky value and the exponential linear unit and while solving the multi-class classification problem we use soft max function in the output layers and when solving the linear regression problem we skip using any kind of activation function or non-linear activation function and directly pass z as our output now note one more thing here there are other types of activation functions also available but these are the most commonly used and there's no fixed way that we must use either relu or the 10 h function in the hidden layers it depends on the application we are trying to build and it also depends on the input data so if you want you can try using both 10h and the relu activation function and see which function works better i hope you found this video valuable and if you are new to this channel then don't forget to check my other videos in the neural network series you can find its link in the description box or by clicking at the i button in the upper right corner now in the next video we will continue our discussion on the neural network and we will derive the equation for the back propagation so i see you in the next one"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Activation Functions in Deep Learning",
        "Video Title":"Activation Functions in a Neural Network explained",
        "URL":"https:\/\/www.youtube.com\/watch?v=m0pIlLfpXWE",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/m0pIlLfpXWE\/hqdefault.jpg",
        "ID":"m0pIlLfpXWE",
        "Publish Time":"2017-11-22T23:24:22Z",
        "Channel":"deeplizard",
        "Channel ID":"UC4UJ26WkceqONNF5S26OiVw",
        "Transcript":""
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Backpropagation Algorithm for Training Neural Networks",
        "Video Title":"What is Back Propagation",
        "URL":"https:\/\/www.youtube.com\/watch?v=S5AGN9XfPK4",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/S5AGN9XfPK4\/hqdefault.jpg",
        "ID":"S5AGN9XfPK4",
        "Publish Time":"2023-06-22T11:00:23Z",
        "Channel":"IBM Technology",
        "Channel ID":"UCKWaEZ-_VweaEx1j62do_vQ",
        "Transcript":"We're going to take a look at back propagation. It's central to the functioning of neural networks, helping them to learn and adapt. And we're going to cover it in simple but instructive terms. So even if your only knowledge of neural networks is \"Isn't that something to do with chatGPT?\" Well, we've got you covered. Now, a neural network fundamentally comprises multiple layers of neurons interconnected by weights. So I'm going to draw some neurons here, and I'm organizing them in layers. And these neurons are also known as nodes. Now, the layers here are categorized. So that's let's do that, the categorization. We have a layer here called the input layer. These two layers in the middle here are the hidden layer and the layer on the end here, that is the output layer. And these neurons are all interconnected with each other across the layers. So each neuron is connected to each other neuron in the next layer. So you can see that here. Okay, so now we have our basic neural network. And during a process called forward propagation, the input data traverses through these layers where the weights, biases and activation functions transform the data until an output is produced. So, let's define those terms. Weights, what is that when we're talking about a neural network? Well, the weights define the strength of the connections between each of the neurons. Then we have the activation function, and the activation function is applied to the weighted sum of the inputs at each neuron to introduce non-linearity into the network, and that allows it to make complex relationships. And that's really where we can use activation functions. Commonly, you'll see activation functions used such as sigmoid, for example. And then finally, biases. So biases really are the additional parameter that shift the activation function to the left or the right, and that aids the network's flexibility. So, consider a single training instance with its associated input data. Now, this data propagates forward through the network, causing every neutron to calculate a weighted sum of the inputs, which is then passed through its activation function. And the final result is the network's output. Great! So where does back propagation come in? Well, the initial output might not be accurate. The network needs to learn from its mistakes and adjust its weights to improve. And back propagation is essentially an algorithm used to train neural networks, applying the principle of error correction. So, after forward propagation, the output error, which is the difference between the network's output and the actual output, is computed. Now that's something called a loss function. And the error is distributed back through the network, providing each neuron in the network a measure of its contribution to total error. Using these measures, back propagation adjusts the weights and the biases of the network to minimize that error. And the objective here is to improve the accuracy of the network's output during subsequent forward propagation. It's a process of optimization, often employing a technique known as gradient descent. Now, gradient descent, that's the topic of a whole video of its own, but essentially, gradient descent is an algorithm used to find the optimal weights and biases that minimize the lost function. It iteratively adjusts the weights and biases in the direction that reduces the error most rapidly. And that means the steepest descent. Now, back propagation is widely used in many neural networks. So let's consider a speech recognition system. We provide as input a spoken word, and it outputs a written transcript of that word. Now, if during training our spoken inputs, it turns out that it doesn't match the written outputs, then back propagation may be able to help. Look, I speak with a British accent, but I've lived in the US for years. But when locals here ask for my name-- Martin --they often hear it as something different entirely, like Marvin or Morton or Mark. If this neural network had made the same mistake, we'd calculate the error by using the loss function to quantify the difference between the predicted output \"Marvin\" and the actual output \"Martin\". We'd compute the gradient of the loss function with respect to the weight and biases in the network and update the weighting biases in the network accordingly. Then we'd undergo multiple iterations of forward propagation and back propagation, tinkering with those weights and biases until we reach convergence-- a time where the network could reliably translate Martin into M-A-R-T-I-N. This is can't be applied to people, can it? Well, but anyway, let's just talk about one more thing with back propagation, and that's the distinction between static and recurrent back propagation networks. Let's start with static. So static back propagation is employed in a feed-forward neural networks where the data moves in a single direction from input layer to output layer. Some example use cases of this, well, we can think of OCR, or optical character recognition, where the goal is to identify and classify the letters and numbers in a given image. Another common example is with spam detection, and here we are looking to use a neural network to learn from features such as the emails, content and the sender's email address to classify an email as spam or not spam. Now back propagation can also be applied to recurrent neural networks as well, or RNNs. Now these networks have loops, and this type of back propagation is slightly more complex given the recursive nature of these networks. Now, some use cases? If we think about sentiment analysis, that's a common use case for this. And that's a good example of where RNNs are used to analyze the sentiment of a piece of text, like a customer product review. Another good example is time series prediction. So predicting things like stock prices or weather patterns. Ultimately, back propagation is the backbone of the learning in neural networks. It tests for errors, working its way back from the output layer to the input layer, adjusting the weights as it goes with the goal to minimize future errors. Errors like how to pronounce Martin in a passible American accent. MART-EN... MAR-EN... MART-ENNE."
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Backpropagation Algorithm for Training Neural Networks",
        "Video Title":"Backpropagation, step-by-step | DL3",
        "URL":"https:\/\/www.youtube.com\/watch?v=Ilg3gGewQ5U",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Ilg3gGewQ5U\/hqdefault.jpg",
        "ID":"Ilg3gGewQ5U",
        "Publish Time":"2017-11-03T14:03:37Z",
        "Channel":"3Blue1Brown",
        "Channel ID":"UCYO_jab_esuFRV4b17AJtAw",
        "Transcript":"Here, we tackle backpropagation, the core algorithm behind how neural networks learn. After a quick recap for where we are, the first thing I'll do is an intuitive walkthrough for what the algorithm is actually doing, without any reference to the formulas. Then, for those of you who do want to dive into the math, the next video goes into the calculus underlying all this. If you watched the last two videos, or if you're just jumping in with the appropriate background, you know what a neural network is, and how it feeds forward information. Here, we're doing the classic example of recognizing handwritten digits whose pixel values get fed into the first layer of the network with 784 neurons, and I've been showing a network with two hidden layers having just 16 neurons each, and an output layer of 10 neurons, indicating which digit the network is choosing as its answer. I'm also expecting you to understand gradient descent, as described in the last video, and how what we mean by learning is that we want to find which weights and biases minimize a certain cost function. As a quick reminder, for the cost of a single training example, you take the output the network gives, along with the output you wanted it to give, and add up the squares of the differences between each component. Doing this for all of your tens of thousands of training examples and averaging the results, this gives you the total cost of the network. And as if that's not enough to think about, as described in the last video, the thing that we're looking for is the negative gradient of this cost function, which tells you how you need to change all of the weights and biases, all of these connections, so as to most efficiently decrease the cost. Backpropagation, the topic of this video, is an algorithm for computing that crazy complicated gradient. And the one idea from the last video that I really want you to hold firmly in your mind right now is that because thinking of the gradient vector as a direction in 13,000 dimensions is, to put it lightly, beyond the scope of our imaginations, there's another way you can think about it. The magnitude of each component here is telling you how sensitive the cost function is to each weight and bias. For example, let's say you go through the process I'm about to describe, and you compute the negative gradient, and the component associated with the weight on this edge here comes out to be 3.2, while the component associated with this edge here comes out as 0.1. The way you would interpret that is that the cost of the function is 32 times more sensitive to changes in that first weight, so if you were to wiggle that value just a little bit, it's going to cause some change to the cost, and that change is 32 times greater than what the same wiggle to that second weight would give. Personally, when I was first learning about backpropagation, I think the most confusing aspect was just the notation and the index chasing of it all. But once you unwrap what each part of this algorithm is really doing, each individual effect it's having is actually pretty intuitive, it's just that there's a lot of little adjustments getting layered on top of each other. So I'm going to start things off here with a complete disregard for the notation, and just step through the effects each training example has on the weights and biases. Because the cost function involves averaging a certain cost per example over all the tens of thousands of training examples, the way we adjust the weights and biases for a single gradient descent step also depends on every single example. Or rather, in principle it should, but for computational efficiency we'll do a little trick later to keep you from needing to hit every single example for every step. In other cases, right now, all we're going to do is focus our attention on one single example, this image of a 2. What effect should this one training example have on how the weights and biases get adjusted? Let's say we're at a point where the network is not well trained yet, so the activations in the output are going to look pretty random, maybe something like 0.5, 0.8, 0.2, on and on. We can't directly change those activations, we only have influence on the weights and biases. But it's helpful to keep track of which adjustments we wish should take place to that output layer. And since we want it to classify the image as a 2, we want that third value to get nudged up while all the others get nudged down. Moreover, the sizes of these nudges should be proportional to how far away each current value is from its target value. For example, the increase to that number 2 neuron's activation is in a sense more important than the decrease to the number 8 neuron, which is already pretty close to where it should be. So zooming in further, let's focus just on this one neuron, the one whose activation we wish to increase. Remember, that activation is defined as a certain weighted sum of all the activations in the previous layer, plus a bias, which is all then plugged into something like the sigmoid squishification function, or a ReLU. So there are three different avenues that can team up together to help increase that activation. You can increase the bias, you can increase the weights, and you can change the activations from the previous layer. Focusing on how the weights should be adjusted, notice how the weights actually have differing levels of influence. The connections with the brightest neurons from the preceding layer have the biggest effect since those weights are multiplied by larger activation values. So if you were to increase one of those weights, it actually has a stronger influence on the ultimate cost function than increasing the weights of connections with dimmer neurons, at least as far as this one training example is concerned. Remember, when we talk about gradient descent, we don't just care about whether each component should get nudged up or down, we care about which ones give you the most bang for your buck. This, by the way, is at least somewhat reminiscent of a theory in neuroscience for how biological networks of neurons learn, Hebbian theory, often summed up in the phrase, neurons that fire together wire together. Here, the biggest increases to weights, the biggest strengthening of connections, happens between neurons which are the most active, and the ones which we wish to become more active. In a sense, the neurons that are firing while seeing a 2 get more strongly linked to those firing when thinking about a 2. To be clear, I'm not in a position to make statements one way or another about whether artificial networks of neurons behave anything like biological brains, and this fires together wire together idea comes with a couple meaningful asterisks, but taken as a very loose analogy, I find it interesting to note. Anyway, the third way we can help increase this neuron's activation is by changing all the activations in the previous layer. Namely, if everything connected to that digit 2 neuron with a positive weight got brighter, and if everything connected with a negative weight got dimmer, then that digit 2 neuron would become more active. And similar to the weight changes, you're going to get the most bang for your buck by seeking changes that are proportional to the size of the corresponding weights. Now of course, we cannot directly influence those activations, we only have control over the weights and biases. But just as with the last layer, it's helpful to keep a note of what those desired changes are. But keep in mind, zooming out one step here, this is only what that digit 2 output neuron wants. Remember, we also want all the other neurons in the last layer to become less active, and each of those other output neurons has its own thoughts about what should happen to that second to last layer. So, the desire of this digit 2 neuron is added together with the desires of all the other output neurons for what should happen to this second to last layer, again in proportion to the corresponding weights, and in proportion to how much each of those neurons needs to change. This right here is where the idea of propagating backwards comes in. By adding together all these desired effects, you basically get a list of nudges that you want to happen to this second to last layer. And once you have those, you can recursively apply the same process to the relevant weights and biases that determine those values, repeating the same process I just walked through and moving backwards through the network. And zooming out a bit further, remember that this is all just how a single training example wishes to nudge each one of those weights and biases. If we only listened to what that 2 wanted, the network would ultimately be incentivized just to classify all images as a 2. So what you do is go through this same backprop routine for every other training example, recording how each of them would like to change the weights and biases, and average together those desired changes. This collection here of the averaged nudges to each weight and bias is, loosely speaking, the negative gradient of the cost function referenced in the last video, or at least something proportional to it. I say loosely speaking only because I have yet to get quantitatively precise about those nudges, but if you understood every change I just referenced, why some are proportionally bigger than others, and how they all need to be added together, you understand the mechanics for what backpropagation is actually doing. By the way, in practice, it takes computers an extremely long time to add up the influence of every training example every gradient descent step. So here's what's commonly done instead. You randomly shuffle your training data and then divide it into a whole bunch of mini-batches, let's say each one having 100 training examples. Then you compute a step according to the mini-batch. It's not going to be the actual gradient of the cost function, which depends on all of the training data, not this tiny subset, so it's not the most efficient step downhill, but each mini-batch does give you a pretty good approximation, and more importantly, it gives you a significant computational speedup. If you were to plot the trajectory of your network under the relevant cost surface, it would be a little more like a drunk man stumbling aimlessly down a hill but taking quick steps, rather than a carefully calculating man determining the exact downhill direction of each step before taking a very slow and careful step in that direction. This technique is referred to as stochastic gradient descent. There's a lot going on here, so let's just sum it up for ourselves, shall we? Backpropagation is the algorithm for determining how a single training example would like to nudge the weights and biases, not just in terms of whether they should go up or down, but in terms of what relative proportions to those changes cause the most rapid decrease to the cost. A true gradient descent step would involve doing this for all your tens of thousands of training examples and averaging the desired changes you get. But that's computationally slow, so instead you randomly subdivide the data into mini-batches and compute each step with respect to a mini-batch. Repeatedly going through all of the mini-batches and making these adjustments, you will converge towards a local minimum of the cost function, which is to say your network will end up doing a really good job on the training examples. So with all of that said, every line of code that would go into implementing backprop actually corresponds with something you have now seen, at least in informal terms. But sometimes knowing what the math does is only half the battle, and just representing the damn thing is where it gets all muddled and confusing. So for those of you who do want to go deeper, the next video goes through the same ideas that were just presented here, but in terms of the underlying calculus, which should hopefully make it a little more familiar as you see the topic in other resources. Before that, one thing worth emphasizing is that for this algorithm to work, and this goes for all sorts of machine learning beyond just neural networks, you need a lot of training data. In our case, one thing that makes handwritten digits such a nice example is that there exists the MNIST database, with so many examples that have been labeled by humans. So a common challenge that those of you working in machine learning will be familiar with is just getting the labeled training data you actually need, whether that's having people label tens of thousands of images, or whatever other data type you might be dealing with."
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Backpropagation Algorithm for Training Neural Networks",
        "Video Title":"Backpropagation in Neural Networks | Back Propagation Algorithm with Examples | Simplilearn",
        "URL":"https:\/\/www.youtube.com\/watch?v=ayOOMlgb320",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ayOOMlgb320\/hqdefault.jpg",
        "ID":"ayOOMlgb320",
        "Publish Time":"2022-11-07T11:56:18Z",
        "Channel":"Simplilearn",
        "Channel ID":"UCsvqVGtbbyHaMoevxPAq9Fg",
        "Transcript":"foreign you might have thought though we have a forward propagation method why is there need for back propagation why is it important to learn back propagation well I guarantee you that after watching this video you will understand why back propagation is needed and why it is applied everywhere hey everyone I hope you all are doing well and in this video we will be looking in detail about back propagation back propagation can be called the building block of a neural network and you'll understand why after watching the complete video but before we get started consider subscribing to Simply learns YouTube channel and hit that Bell icon and that way you'll be the first to get notified when we post similar content now let's move forward and look at what the agenda is for today first we'll look at what is back propagation and after that what is back propagation in a neural network next how does back propagation in a neural network work and further we will understand benefits of back propagation and finally applications but before moving forward let me ask you a question which among the following is not a neural network input layer output layer propagation layer hidden layer please leave your answer in the comments section below and stay tuned to find the answer so what is a back propagation algorithm back propagation is an algorithm which is created to test errors which will travel back from input nodes to Output nodes it is applied to improve accuracy in Data Mining and machine learning coming to back propagation in neural networks what is back propagation in neural networks well the concept of back propagation in neural networks was first introduced in the 1960s an artificial neural network is made up of Bunches of connected input and output units Each of which is connected by a software program and has a certain weight this kind of network is based on biological neural networks which contain neurons coupled to one another across different network levels in this instance neurons are shown as nodes well now that we've understood what the back propagation algorithm is we'll come to the next topic the working of the back propagation algorithm so the back propagation algorithm is applied to reduce cost function and to reduce errors we have a sample network with two hidden layers and a single input layer where data passes in and this data is finally received by the output layer through all these neural networks whenever we pass the data to the input layer it will pass through the neural network until it reaches the output layer each model receives its input from the previous layer and the previous layer output is Multiplied with weight which will give the activation function or a and the result of this is passed as an input for the next layer and this process continues to happen until we reach the output layer of each neural network and this process is referred to as forward propagation so after reaching the output layer we get the resulting output for the given model from the input output with the highest activation will be considered as the suitable output match for the corresponding input loss is calculated on what the model has predicted as input and what the actual input is now here back propagation is used to calculate the gradient of the loss function in back propagation we are coming back from the network and now we have the output generated by the given input now gradient descent looks at the output layer and gradient descent understands that the value of one output increases and the other will decrease we know that the output is derived from the previous layer's output multiplied by the weight of the network back propagation is the tool that the neural network considers in order to calculate the gradient of the loss function it is calculated by taking the derivative of loss function by weight and now we have the loss function calculated from the previous layer now gradient descent will start calculating the value through the previous network using back propagation with the aim of reducing loss we know that the value is coming from the weighted sum of the previous Network being multiplied by the output of the previous layer and this process is repeated until we update the value of the previous sum so as we can see we are going backwards and from this we can increase the value of the correct output node and decrease the value of the incorrect input node thus it will reduce the loss and the activation function which has the highest value should increase and the lower value will decrease do you remember the question that I asked earlier and which among the following is not a neural network well I hope you got the answer by now the answer for the question is propagation layer and to do this we need to change the output of the previous layer we cannot directly change it so we're going to change its previous Network and why do we need to choose back propagation well below are some of the important factors on why we need to choose back propagation so back propagation is quick easy and simple to implement it is a versatile method because it doesn't need prior Network knowledge and only has input numbers as parameters to tune and there is no need to make any special note of the characteristics of the function that must be taught because it is a common way that typically Works effectively and the final topic is applications of back propagation it can be used in the field of speech recognition and it can also be used to recognize voice and signature the neural network has received training to pronounce each letter in a word and a sentence and the neural network has received training therefore back propagation finds itself in almost every field where neural networks are used now in case of any questions please mention them in the comments section below and I hope you enjoyed watching this video Happy learning [Music] hi there if you like this video subscribe to the simply learn YouTube channel and click here to watch similar videos to nerd up and get certified click here"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Backpropagation Algorithm for Training Neural Networks",
        "Video Title":"Backpropagation in Neural Network Explained Deep Learning | Artificial Intelligence #backpropagation",
        "URL":"https:\/\/www.youtube.com\/watch?v=NHKht6p9X1Q",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/NHKht6p9X1Q\/hqdefault.jpg",
        "ID":"NHKht6p9X1Q",
        "Publish Time":"2023-09-01T03:00:10Z",
        "Channel":"UncomplicatingTech",
        "Channel ID":"UCkZEo1r6Hsc5yIlW3jfW0KQ",
        "Transcript":""
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Loss Functions in Machine Learning",
        "Video Title":"133 - What are Loss functions in machine learning?",
        "URL":"https:\/\/www.youtube.com\/watch?v=-qT8fJTP3Ks",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/-qT8fJTP3Ks\/hqdefault.jpg",
        "ID":"-qT8fJTP3Ks",
        "Publish Time":"2020-06-15T07:00:00Z",
        "Channel":"DigitalSreeni",
        "Channel ID":"UC34rW-HtPJulxr5wp2Xa04w",
        "Transcript":"hey guys you're watching Python tutorial videos on my youtube channel Python from across the base in the last tutorial we looked at activation functions so let's continue this learning about the parameters of deep learning by talking about loss function and the last function where do you find it well when you compile this model one of the things that you actually include is the last function and the other two are optimizer and metrics okay so what is a loss function now let me use an example here but first a loss function is also referred to as a cost function or error function okay and it quantifies as the name suggests the error between the output okay and a target value okay if you say that okay with the training data for example you know the answer is let's say 10 for whatever this regression problem and you get a value of nine point seven when you do the training and then when you use and when you compare these two then that is what an error function basically is okay it quantifies the error between the output and the target value okay and the example I just gave you is the quantity right nine point seven versus ten so what is this nine point seven in ten well let's actually use this as an example let's say a linear regression as an example you have a whole bunch of data points and how do you fit this to a straight line if I give this to I would say 100 of you then you'll probably come up with slightly different variation but what math is going on here in your mind what's going on is hey I want to make this line as perfect as possible for each of these so this is probably not what you're going to do this is probably what you're going to do why because this line seems to go through you know all of these points and the compromise between the you know the the error here is is okay with this blue line compared to the red if it is the red line then the distance between this point and the point on the line is very far away you though it looks very good for these three data points it's not looking good for this one yeah in fact even for this one it looks good the blue line actually is even though the blue line is not as good as red line for this data point or for this data point in fact the blue line is a good average of all of this so what math is going on in your mind when you're actually when you're actually doing this well this is basically you're looking at the distance between the actual data points versus the data points that lie on this line the line is again that the the prediction that the machine learning model actually gave us in this case linear regression yeah and the error in this case if you just look at one data point and if you say this one is x one y one coordinate and the point on this line representing this this data point or the closest is x2 y2 okay then what is the error y2 minus y1 right this is this is the error right there so the error is y2 minus y1 and the line can be on the other side if this if the data point lies on the other side then your y2 minus y1 can be a negative value yeah or y1 minus y2 can be a negative value so when you are looking at error you typically square it so you don't it doesn't matter if it is on the positive side or on the negative side when you square it ok so that's where the squared error is coming from and now it's just for one data point now you need to do that for every data point which means you have to take the mean or average of all of these data points this is where the term mean squared error comes from okay so when you do linear regression all it's doing is it's trying to minimize this distance between the point and the line for all of these data points meaning you're looking at the mean squared error so for linear regression why am I talking about linear regression because for linear regression mean squared error is a great loss function that we are trying to minimize the goal is to find the position of this line where the mean square error gives us a minimal value so during the Train process it changes the line okay and what changes the line that's where the optimizers come into picture and we'll talk about that in the next tutorial but then the line is changed and then the mean square error is calculated and it finds them the position where the mean squared error is the minimum now when it comes to the common loss functions in machine learning there are two types of problems right that we talked about I should have animated this slide but please focus on the left hand side there is a regression problem which is what is the value you are trying to predict a value what is going to be the stock price like five days from now based on the trend this is a regression problem classification is is this a cat or a dog or a tiger or is this a cancer cell versus healthy cell and so on so regression for regression problems the most common the most common loss function is mean squared error and I just explained what that is okay and the other ones are mean absolute error and mean bias error and there are a few others but this is the primary one for classification problems again you may have binary classification or multi class process of classification either way cross-entropy seems to be the most common one that people use and and again if you want to know why this is out of the scope of this tutorial okay I'm focusing this tutorial for non mathematical you know people in general who want to use machine learning as a tool but if you really want to do some research find the right material on YouTube dig deep into these topics okay so for classification binary and categorical across entropy and the binary is just a special case of categorical cross entropy and again they're used for classification problems in general now there are others that are support vector machine lost like SVM loss and a few others I found a great blog that talks about a whole bunch of these and here is the link and I'll try to provide this as part of the description but please go ahead and have look at it so I found this I mean I hope you found this tutorial to be useful and in the next tutorial let's actually talk about optimizers which is again one of the three things that we define when we actually do model compiled so let's meet again in the next tutorial thank you very much"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Loss Functions in Machine Learning",
        "Video Title":"Loss Functions - EXPLAINED!",
        "URL":"https:\/\/www.youtube.com\/watch?v=QBbC3Cjsnjg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/QBbC3Cjsnjg\/hqdefault.jpg",
        "ID":"QBbC3Cjsnjg",
        "Publish Time":"2020-01-20T15:00:06Z",
        "Channel":"CodeEmporium",
        "Channel ID":"UC5_6ZD6s8klmMu9TXEB_1IA",
        "Transcript":"what is the best loss function the age-old question in machine learning will we solve this problem today nope but we will talk about some loss functions their pros and cons and even discuss a recent paper on adaptive loss functions so that would mean that you don't need to keep trying out different losses to find the best one that suits your needs fun stuff ok first let's focus on regression I've got this data set here it looks like a line can fit this data I'll train a linear regression model on it and I choose to use the squared loss to minimize it the l2 loss my curve looks something like this okay looks pretty good and it's also pretty simple but if I introduce some outliers in this data my model responds by freaking the hell out and trying to fit those data points better this happens because that square term scales the errors by these outliers so the model really wants to get these obscure points right I'll just change the loss function to the absolute difference my model now treats the outliers like any other data point so it won't go out of its way for outliers if it means compromising the rest of the model this might lead to poor predictions from time to time but if you really don't care about the extreme cases this will do support vector regression uses this by the way the advantage of the squared error is the ease with which we can compute the gradient for machine learning during gradient descent this gradient is not as simple in the absolute error case because of the points of discontinuity the mean absolute error isn't optimized through gradient descent but it's optimized by computing sub gradients instead it adds a bit more complexity and I'll add some reading material in the description down below we got two losses one that loves outliers and another that ignores them if you think that one doesn't work you'll just use the other and that might be fine in most cases but consider this our data is about like 70% in one direction and 30% in the other direction technically this data does not have any outliers but our absolute loss may treat the 30% data as outliers and ignore it altogether while the squared loss will try to capture those 30% both decisions can lead to poor model performance how do we compromise we can do so by using the pseudo Hueber loss this is the best of both losses if a data point has a relatively low error we take the squared loss if the data point is an outlier we take the absolute loss the result is that it reduces the effects of outliers on the model while still being at different Schabel and as such it's slightly more complex the main problem here is that we have an extra hyper parameter play with these are the most popular regression based losses that you see in built-in regressors now for classification losses in classification our outputs are obviously the class but more precisely it's the list of probabilities of belonging to different classes and we just choose a class with the highest probability cuz duh this list is a probability distribution we compare this to the ground truth and how we compare it depends on the losses we use so cross-entropy loss entropy has its roots in information theory so I'll explain it from that perspective so say that there's this weather station and it sends you a weather forecast at the beginning of each day and it tells you what weather it is on that day using some n bits of information in its best case say this information can be packed in as low as 3 bits on average 2 bits for sunny four bits for a rainy day three bits for a partly cloudy day and so on the entropy of a distribution is the average number of bits required to convey a piece of information like today's weather in this case so the entropy in this example is three three bits but the tower isn't perfect it's designed by engineers who have flaws themselves there is some wastage and it is found that the tower actually sends you five bits on average this is cross entropy we are comparing the true average and the satellites current average entropy is three bits but cross entropies five bits this means that we could have had a system that tells us the weather with just three bits but we have a system currently that is our satellite that is using five bits to do the same thing ideally we want these numbers to be much closer to each other this two-bit difference is known as the KL or the Colback Liebherr divergence this little satellite is actually similar to a model that we trained to predict the weather in machine learning as a classification problem and so in many classification problems cross entropy and KL divergence are often used as loss functions to minimize another loss is the hinge loss typically used in support vector machines for classification tasks minimizing this we get a boundary that splits the data well and is as far away from every data point as possible that is it maximizes the minimum margin from the data points this loss penalizes data points even if they are correctly labeled if they lie in this margin I've made several overly mathematical videos on kernels and SVM's check it out if you want to lower your self-esteem I'm gonna wrap up this video with a paper discussion we've taken a rough look at six common losses for classification and regression but there are far more some better suited for certain problems we have a set of points we want to fit a regression line through squared loss does it decently well we had outliers and try to fit it again doesn't look too great anymore so we try the pseudo Hueber loss and this gives us better results but I'm not satisfied yet so let's try some other losses so we have the Welsh loss results our trash giveme McClair loss it fits this data better now cauchy's loss this fits the data even better I like this it's nice that I found the loss function I liked but I found this by trial and error is there a way that it could have just used a loss function without trial and error and somehow arrived at the actual minimum that I wanted turns out that all these losses that I mentioned can be generalized to this equation by setting different values for alpha which is a shape parameter how do we add alpha into the mix though maximum likelihood estimation we maximize the likelihood of the probability distribution or minimize the negative log likelihood so it becomes an adaptive loss this technique is typically used to derive losses mathematically and this actually leads to some interesting results here are some examples of images when we let a variational auto encoder to determine a loss and generate images they aren't half-bad the idea of an adaptive loss sounds like an amazing idea hope you all have a better idea behind loss functions the differences between them and a sprinkle of research on adaptive loss functions and so we can avoid trial and error to determine the most appropriate losses I have resources in the description below if you like these videos please subscribe to keep the lights up in my little apartment and I will see you soon bye"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Loss Functions in Machine Learning",
        "Video Title":"What is a Loss Function? Understanding How AI Models Learn",
        "URL":"https:\/\/www.youtube.com\/watch?v=v_ueBW_5dLg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/v_ueBW_5dLg\/hqdefault.jpg",
        "ID":"v_ueBW_5dLg",
        "Publish Time":"2025-01-06T12:01:19Z",
        "Channel":"IBM Technology",
        "Channel ID":"UCKWaEZ-_VweaEx1j62do_vQ",
        "Transcript":"How good is an AI model at forecasting? We can put an actual number on it. In machine learning a loss function tracks the degree of error in the output from an AI model, and it does this by quantifying the difference or the loss between a predicted value. So let's say that that is five, the model gave us five, as the output and then comparing that to the actual value. So maybe the model gave us ten and we call that the ground truth. Now, if the model's predictions are accurate, then the difference between these two numbers, the loss, in effect, is comparatively small. If it's predictions are inaccurate, let's say it came back with an output of one instead of five, then the loss is larger. So let me give you an example of how we can use this. Now, I have for a colleague who built an AI model to forecast how many views his videos would receive on YouTube. He fed the model YouTube titles and then the model forecast how many views that video would receive in its first week. Here they are. Little bit vain, if you ask me. But it wasn't me. It was my colleague. Now, how well did the model do? Well, when comparing the model forecasts to the actual number of real YouTube views, the model wasn't getting too close. The model predicted that the cold brew video would bomb, and that pour over guide video would be a big hit. Just wasn't the case, though. Now, this is a hard problem to solve and clearly this model needs some adjustments and that's where loss functions can help. Loss functions let us define how well a model is doing mathematically. And if we can calculate loss, we can then adjust model parameters and see if that increases loss, meaning it's made it worse, or if it decreases loss, meaning it's made it better. And at some point we can say that a machine learning model has been sufficiently trained. When loss has been minimized below some predefined threshold. Now at a high level, we can divide loss functions into two types, regression loss functions and then classification loss functions. And let's start. With regression, which measures errors in predictions involving continuous values. Predictions like the price of a house or the temperature for a given day or well, the views for a YouTube video. Now, in these cases the loss function measures how far off the model's predictions are from the actual continuous target values. Now, regression loss must be sensitive to two things, basically whether the forecast is correct or not. But also the degree to which it diverges from the ground truth. And there are multiple ways to calculate regression loss functions. Now, the most common of those is called MSE or mean squared error. Now, as its name suggests, MSE is calculated as the average of the squared difference between the predicted value and the true value across all training examples. And squaring the error means the MSE gives large mistakes a disproportionately heavy impact on overall loss, which strongly punishes outliers. So that's MSE. MAE or mean absolute error measures the average absolute difference between the predicted value and MAE and is less sensitive to outliers compared to MSE as it doesn't square the errors. So how do you decide which regression loss function to pick? Well, if your ground truth data has relatively few extreme outliers with minimal deviation. Like, I don't know, the temperature ranges in the month of July in the southern US, which, trust me, is basically always hot. Well then MSE is a particularly useful option for you as you want to heavily penalize predictions that are far off from the actual values. MAE is a better option when data does contain more outliers. And we don't want those outliers to overly influence the model. Forecasting demand for a product. That's a good example where occasional surges in sales shouldn't overly skew the model. But there is a third choice. The third choice is called huber loss. Now, hubar loss is a compromise. It's a compromise between MSE and MAE. It behaves like MSE for small errors and MAE for large errors, which makes it useful when you want the benefits of penalizing large errors but not too harshly. Now I've calculated the lost functions for the YouTube example. This is the MAE value summing up the absolute differences, meaning on average the predictions were off by about 16,000 views per video. The MSE lost function, that's over 400 million. It skyrockets and that's due to the squaring of large errors, and the huber loss. That also indicates poor predictions, but provides a more balanced perspective, penalizing large errors less severely than MSI. But look, these numbers don't mean a whole lot on their own. We want to adjust the model's parameters, generate new forecasts and see where we move the needle on loss. But before we get to how to do that, let's talk about the other type of loss function classification. Unlike regression loss functions which deal with predicting continuous numerical values, classification loss functions, well, they're focused on determining the accuracy of categorical predictions. Is an email spam or not spam? Are these plants classified into their correct species based on their features? So the loss function in classification tasks measures how well the predicted probabilities or labels match the actual categories. Now cross entropy loss is one way of doing this, and it's the most widely used loss function for classification tasks. Now, what is entropy? It's a measure of uncertainty within a system. So if you're flipping a coin, there are only two possible outcomes heads or tails. The uncertainty is pretty low. So low entropy. Running a six sided die means there's more uncertainty about which of these six possible numbers will come up. The entropy is higher. Now cross entropy loss measures how uncertain the model's predictions are compared to the actual outcomes. In supervised learning, model predictions are compared to the ground truth classifications provided by data tables. Those ground truth labels are certain, and so they have low or in fact no entropy. As such, we can measure the loss in terms of the difference in certainty we'd have using the ground truth labels to the certainty of the labels predicted by the model. Now, an alternative to this is called hinge loss instead. Now, this is commonly used in support of vector machines and hence loss encourages the model to make both correct predictions and to do so with a certain level of confidence. It's all about measuring that level of confidence, and it focuses on maximizing the margin between classes with the goal that the model is not just correct, but it's confidently correct by a specified margin. And this makes the hinge loss particularly useful in binary classification tasks where the distinction between classes needs to be as clear and as far apart as possible. So we've calculated our loss function. Great, but what can we do with that information? Now remember that the primary reason for calculating the loss function is to guide the model's learning process. The last function provides a numeric value that indicates how far off the model's predictions are from the actual results. And by analyzing this loss, we can adjust the model's parameters typically through a process called optimization. In essence, the loss function acts as a feedback mechanism, telling the model how well it's performing and where it needs to improve. The lower the loss, the better the model's predictions align with the true outcomes. Now, after adjusting the YouTube prediction model, we get a new set of forecasts and we can now compare the loss functions between the two models, and in all three cases, the loss function is now lower, indicating less loss with the greatest effect on MSE, mean squared error. As the model reduced the large prediction error for the poorer the video. Now that's lost function as an evaluation metric, but it can also be used as inputs into an algorithm that actually influences the model parameters. To minimize loss, for example, by using gradient descent. And that works by calculating the gradient or the slope of a loss function with respect to each parameter. Using the gradient of the loss function Optimization algorithms determine which direction to step the model in order to move down the gradient and therefore reduce loss. The model learns by updating the weight and bias terms until the loss function has been sufficiently minimized. So that's loss function. It's both a scorekeeper that measures how well your model is performing, and a guide that directs the model's learning process, and a thanks to lost function. My a, my colleague, can keep tweaking his YouTube AI model to minimize the loss and teach that model to make better predictions."
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Loss Functions in Machine Learning",
        "Video Title":"Loss Functions Explained | Machine Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=gm1E8Y99WFA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/gm1E8Y99WFA\/hqdefault.jpg",
        "ID":"gm1E8Y99WFA",
        "Publish Time":"2024-05-12T05:42:28Z",
        "Channel":"Alice Heiman",
        "Channel ID":"UCE7fVac75YKse35UVpWP4Og",
        "Transcript":"hello everyone today I'm going to go through what are loss functions in machine learning so we use loss to train machine learning models and the loss function explains the difference between the current and The Wanted model output or the model prediction and the goal is to make this difference between the current and The Wanted model output to be as small as possible and this is why we want to minimize the loss the entire purpose of every single node in a Neal network is working to minimize the loss and over time this is how the model will get better because it will make its prediction and then it will compare that prediction to The Wanted prediction and then we make tweaks to lower the difference between the current and to wanted so how does this work well back propagation is the algorithm we use to kind of update the model throughout training and back propagation asks one question over and over it says to every weight and every parameter in the network what happens to the loss it would tweak this par parameter a tiny bit and then we want to tweak it in such a way that the loss goes down for each training iteration so really asking how do we tweak every single number in this neural network so that the loss gets lower and this is really what we're optimizing when we're training a noral network and this is also why the loss function and the choice of loss function is so important because it really defines the goal of what our neural network is doing and the loss is computed at the last step as we can see on the right and then we compare against that last step throughout the entire network in what is known as back propagation so for example we have an for example the um data points in blue here and let's say we want to fit a line to these Blue Points well this line is kind of a function with two learnable parameters or the MX plus b or w1x minus B here um and what we want to do is want to minimize the loss we want to minimize the difference between our predicted line and these real data points and in the beginning the loss will be very high meaning that we have a large difference in the predicted and wanted outcomes but over time we can update our line to be closer to the real ones so that the difference is smaller and eventually we're getting to a line where the loss is not really improving that much and here we can kind of say that we have trained a model and it's as good as it's going to get and when it comes to loss functions there are two main types of loss functions we use progression and for classification problems so regression problems is like the one we mentioned before on the other slide this is when we want to predict a continuous variable it can range any VAR any value between for example zero and one or zero and 100 and want to predict a number on that uh number scale and the most common one is called the mean squared error Ms e or the L2 loss and really what we're doing is we're taking all of these differences between our prediction and the data points we add them up and we also make sure to square them so that um if we were having like a negative and a positive difference that we don't cancel each other out so we Square them and then we divide by n and that will be the number of the samples that we use so this is the complete formula is we're saying hey sum up so from one to n so all our samples and we want to make take a difference between the true label the correct label and the predict so y IUS y i but we square that to make sure that we are not um letting the positive and negative differences cancel each other out and then we take the average and the Really pros of this one is that it's simple and efficient and you also have one unique uh minimum which you can kind of optimize towards but the concept that is not robust to outliers and this is because of the squared term because we are squaring the distance if you have one data point which is really really really off that distance is really going to affect the loss um which makes it not that robust to outliers it can kind of explode the loss and is also scale dependent so another one you can use if the data contains many outliers is to un the mean absolute error or the Huber loss and also to make its scale independent you can also consider using a root mean squared error and these are just variations of this one so for example for the mean absolute error instead of squaring we're doing just an absolute value so we're not um having the problem of negative and positive differences but we're not really squaring the distan as making it more abust to outliers which can be both a pro uh and a con depending on if you want to penalize or not the outliers is also known as L1 loss however if you take absolute difference is not going to be differenti uh differentiable at all points because you kind of got to have this little um Edge um at zero and the hu loss is kind of saying well I kind of want a little bit of both then so I wanted to um be working with the square differences very close to the predictions but then as I'm going further away um we can use um use the absolute error instead so here the Huber loss is kind of splitting it up and also making sure that we have a differentiable um when we're getting closer to zero um and it also makes it more robust to outliers um the only errors or cons here is that with this split you're increasing the complexity it can introduce bugs when you're coding it up and you also have this extra parameter called Delta which is your threshold for what should be considered a small error so you have another kind of hyperparameter that you need to tune and need to find a good value for next up is classification so classification is all about things into categories and these can be everything from two categories to thousands of categories and one of the most common things that unify the classification loss functions is what is known as cross entropy and cross entropy really means the negative log loss but what does this mean the simplest kind is called the binary cross entropy loss or BCE and this is when we want to classify things into two categories thus the binary and again the loss here is we want to try and penalize wrong predictions and we want to assign large values to wrong predictions and small values to correct predictions so because we have two categories we really only have two scenarios if the correct label is one so let's say here we want to classify if something is water or land we want to penalize predictions that are close to zero so if we represent Water by one and land by zero we can see on the left here that we have one example where we predicted um something that was um land but we predicted it um to be water or or vice versa and if the correct label is zero we want to penalize predictions close to one and how does this work well we use what we known as the negative log loss and on the left here we can see um the negative log curve and if the correct label is one we're going to call that y1 is equal to 1 then we want to see the loss curve is going to be the negative log of the probability of us or the model predicting a one so if the model predicts one that mean it's a good prediction and if we look at the graph we can see that indeed negative log of one is zero so we made a perfect prediction so we're not going to add any penalty for that prediction however if the model predicts zero when it should have predicted one we can see that the negative log goes all the way up to Infinity almost so it's going to add a lot of penalty to the loss function making the model really want to change this prediction and similarly we have the other case when the correct label is zero and now we really want to use negative log one minus our predict um our predicted outcome because in this case let's say the correct label is zero and we predict zero then netive log of 1 - 0 is going to be negative log of 1 which again gives us zero meaning it's a good prediction and a low penalty but if we predict one when the correct label is zero we're going to get closer to that log of negative log of zero again making that a high penalty and putting all of this together we have this formula right here so it just takes these two parts to say when the correct label is one in the blue box we're going to keep the log of the probability we're protecting it to be one and then in The Orange Box if the correct label is one that other term is just going to go away and then we're going to add that loss function together and then for the next sample perhaps the correct label is zero then the blue box would go away and The Orange Box would stay and here we're am getting the value which is log one minus the probability that we talked about in the last slide we add all of those up and then take the average and that's what you see here so we summon them all up and then one over n for the number of samples that we have and the pro of this it's quite simple and efficient it's differentiable but it can have problems with class imbalance I where maybe one of the two categories are just much much larger than the other for example in many maybe disease studies um for a very uncommon disease you know maybe 95% of cases will be healthy cases and only 5% or much much less will be the actual C States and in those cases you could consider using a weighted binary cross eny law where you can give more weight to these um smaller classes although they are um smaller in number and if you don't just want to classify things into two things there's a kind of a generalization of the binary cross entropy called categorical cross entropy and this is where you can classify things into many categories and what we can see here is that we do two summations so we add in an inner summation with a number of categories but again we're kind of saying multiply the correct label by our predicted label the log and what special about the categorical cross entropy is that we must um kind of use a specific format for a correct labeling called a one hot encoding and that just means that we have um our correct label with zero is a one and all the other that's not the right um categories you'll be zero so to represent the second category we would use 0 1 0 and again here if the data contains class imbalance we can use focal loss instead and in if we don't want to use a one hot encoding but rather use an integer like category one Category 2 Category 3 we can use the sparse categorical cross entry be loss and just a final note on the difference between a performance metric and loss is that we use the loss again to train a machine learning model and the performance metrics are used to to evaluate the model so the performance metrics are used after model training to evaluate compare and test models against a task at hand and different models can use different loss functions but the end goal is to perform well on these performance metrics so you might evaluate the U models using the same metrics but they might have been trained with different loss functions so we can kind of say that the loss is a memes to an end and the loss just quantifies the road to get there and the performance metrics is then telling us how well we actually ended up at I hope this video helped you understand loss in a better way and here are my references and with that thank you for watching"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Implementing Custom Loss Functions with Python",
        "Video Title":"TIPS &amp; TRICKS - Deep Learning: How to create custom loss function?",
        "URL":"https:\/\/www.youtube.com\/watch?v=99ohAVk2uRk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/99ohAVk2uRk\/hqdefault.jpg",
        "ID":"99ohAVk2uRk",
        "Publish Time":"2021-11-22T12:00:07Z",
        "Channel":"eMaster Class Academy",
        "Channel ID":"UCtfTf1nNJQ4PbUDqj-Q48rw",
        "Transcript":"[Music] in some cases we might also want to create our custom loss functions in order to better fit our deep learning problems and tensorflow carriers also allow us to create some custom losses and that is a callable function that returns in the rates of losses that can be positive to the compiled functions assay loss let's take a look on the example so first thing first we just import the library that include the sequential model the dance layer and just rename the tensorflow as the tf and numpy as the np and then we just create the input and output value and we create a sequential models with two layers now a custom loss functions can be created by defining a functions that take the true value and predict the value as required parameters in that case we define a function this is a custom noise function and we take two argument that is the true value and predict the value and the function should return and a weight of losses and that is then a way of the losses say for example we just create a mean square error function this is a custom loss function so we take the square difference here and then we calculate all we try so we reduce the dimensions and take the mean and now we will return you the mse here the functions can then pass to a compound as a loss functions we just use the loss equals to these defined custom loss functions and of course we pass it on with another parameters that is the atoms optimizer and if we compile and if we run it fit the models and just to give you a comparison between the custom mse and building nfc you can take a look on the differences and actually they are the same and you can see they provide you the same loss value the reason is because the custom msd is just uh sorry the custom loss function is just a mse and of course we can also use the built-in mse in a nutshell if you want to create a custom loss functions then you can just define a function and take the true value and predict the value as the functions input and then remembers you need to return in a ways of losses as the outputs from these functions so that we can assign these laws we can assign these a rate of losses equals to these loss functions or loss value parameters"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Implementing Custom Loss Functions with Python",
        "Video Title":"How can you implement custom loss functions in PyTorch",
        "URL":"https:\/\/www.youtube.com\/watch?v=fQxoVTvaDPo",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/fQxoVTvaDPo\/hqdefault.jpg",
        "ID":"fQxoVTvaDPo",
        "Publish Time":"2025-01-04T18:30:35Z",
        "Channel":"CodeToday",
        "Channel ID":"UCPOS3ewc3d21e6I5H9DOITw",
        "Transcript":"how can you implement custom loss functions in pytorch you can Implement a custom loss function by defining a python function that takes in the predicted outputs and the ground truth and Returns the computed loss alternatively you can create a class that inherits from torch NN module and implement the forward method example python class custom loss torch NN modules depth forward self outputs targets return torch mean outputs targets two"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Implementing Custom Loss Functions with Python",
        "Video Title":"Deep Learning Techniques with Custom Loss Functions and Sequence Models | 100 Days of Python: Day 62",
        "URL":"https:\/\/www.youtube.com\/watch?v=qrXwWXZpfBg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/qrXwWXZpfBg\/hqdefault.jpg",
        "ID":"qrXwWXZpfBg",
        "Publish Time":"2024-08-21T03:00:36Z",
        "Channel":"Code with me",
        "Channel ID":"UCrUoAOP0krZHdXdwsfR0oRg",
        "Transcript":"[Music] welcome back pythonistas it's day 62 of our 100 days of python series and we're diving deeper into the world of deep learning today we'll explore custom loss functions and metrics powerful regularization techniques like Dropout and batch normalization and we'll introduce sequen to sequence models and attention mechanisms this is where deep learning gets really exciting so let's Jump Right In deep learning framework works like tensorflow and pytorch come with several built-in loss functions but sometimes you need something custom to fit your unique problem let's create a custom loss function in tensor floater carrers and use it in a model here we defined a custom loss function using tent tolow operations then used it to compile a simple neural network model custom loss functions give you the flex ability to tailor your model's training to your specific needs over is a common problem in deep learning where your model performs well on training data but poorly on unseen data to combat this we can use regularization techniques like Dropout and batch normalization Dropout randomly deactivates a percentage of neurons during training to prevent overfitting while batch normalization standardizes layer inputs improving stability and performance these techniques are essential for building robust deep learning models now let's explore sequence to sequence models which are vital for tasks like machine translation and text summarization we'll also introduce attention mechanisms a game-changing Innovation that allows models to focus on relevant parts of the [Music] input this is a basic sequence to sequence model often used for translating sequences from one domain to another like languages the next level is adding attention which allows the model to focus on the most relevant parts of the input [Music] sequence that's it for day 62 today we explored some Advanced deep learning techniques like custom loss functions regularization and sequence to sequence models understanding these Concepts is crucial for tackling more complex problems with deep learning if you enjoyed this video don't forget to like subscribe and hit the Bell icon for more python insights I'll see you tomorrow for day 63 fre until then Happy coding"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Implementing Custom Loss Functions with Python",
        "Video Title":"Using a custom loss function",
        "URL":"https:\/\/www.youtube.com\/watch?v=Hm8_PgVTFuc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Hm8_PgVTFuc\/hqdefault.jpg",
        "ID":"Hm8_PgVTFuc",
        "Publish Time":"2021-11-15T13:30:56Z",
        "Channel":"HuggingFace",
        "Channel ID":"UCHlNU7kIZhRgSbhHvFoy72w",
        "Transcript":"In this video we take a look at setting up a custom loss function for training. In the default loss functions all samples such as these code snippets are treated the same irrespective of their content, but there are scenarios where you it could make sense to weight the samples differently. If for example one sample contains a lot of tokens that or of interest to us or it has a favourable diversity of tokens. We can also think of other heuristics we can implement with pattern matching or other rules. For each sample we get a loss value during training and we can combine that loss with a weight. Then we can for example create a weighted sum to get the final loss for a batch. Let\u2019s have a look at a specific example: we want to setup a language model that helps us autocomplete complete common data science code. For that task we would like to weight samples stronger where tokens related to the data science stack, such as pd or np, occur more frequently. Here you see a loss function that does exactly that for causal language modeling. It takes the models it takes the model\u2019s inputs and predicted logits as well as the key tokens as input. First the inputs and logits are aligned, then the loss per sample is calculate followed by the weights. Finally the loss and weights are combined and returned. This is a pretty big function so let\u2019s take a closer look at the loss and weight blocks. During the calculation of the standard loss the logits and labels are flattened over the batch. With the view we unflatten the tensor to get a matrix with a row for each sample in the batch and a column for each position in the sequence of the samples. We don\u2019t need the loss per position so we average the loss over all positions for each sample. For the weights we use boolean logic to get a tensor with 1s where a keyword occurred and 0s where not. This tensor has an additional dimension as the loss tensor we just saw because we get the information for each keyword in a separate matrix. Only want to know how many times keywords occurred per sample so we can sum over all keywords and all positions per sample. Now we are almost there, we only need to combine the loss with the weight per sample. We do this with element wise multiplication and then average over all samples in the batch. In the end we have exactly one loss value for the whole batch. And this is the whole necessary logic to create a custom weighted loss. Let\u2019s see how we can make use of that custom loss with Accelerate and the Trainer In Accelerate we just pass the input_ids to the models to get the logits and can then call the custom loss function. After that we continue with the normal training loop by for example calling backward. For the Trainer we can overwrite the compute loss function of the standard trainer. We just need to make sure that that we return the loss and the model outputs in the same format. With that you can integrate your own awesome loss function with both the trainer and accelerates."
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Vanishing and Exploding Gradient Problems in Deep Learning",
        "Video Title":"Vanishing and exploding gradients | Deep Learning Tutorial 35 (Tensorflow, Keras &amp; Python)",
        "URL":"https:\/\/www.youtube.com\/watch?v=qowp6SQ9_Oo",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/qowp6SQ9_Oo\/hqdefault.jpg",
        "ID":"qowp6SQ9_Oo",
        "Publish Time":"2021-01-23T14:30:12Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"vanishing gradient is a common problem encountered during neural network training in this video we will look into how this problem affects regular artificial neural network and rnn here i have a simple neural network that tries to predict if a person will buy insurance or not based on factors such as age, education income and so on when you think about hidden layers these hidden layers are extracting some features for example age and education might affect awareness income and saving might affect the affordability when you train this neural network on a sample data where the age is 29 the income is 150k and so on during training we will have a forward pass in which these values will be passed and initially we will initialize this weight w1 to w10 with some random values or maybe with zero values and then in the end we compare the actual output with predicted output and we compute the loss then we do backward propagation to update these weights in backward propagation for example to update w9 we will subtract a small factor from w9 which will be learning rate learning rate is like small number point zero zero one, zero zero two whatever and the gradient the gradient is the most important part what gradient tells you here is it's represented as a derivative of loss compared to w9 which means how much loss is changing for a given change in w9 so this is a guessing work you're trying to optimize your loss or minimize your loss and you want to see how much w9 is changed so that that results into certain amount of change in loss so that you can reduce that loss so you can think of this as how much awareness contributes to a final output let's say awareness contributes to only 57 20 percent to the final output affordability contributes 80 percent to the final output then you know this w9.. this chain.. this gradient will be less here okay and now when we think about w1 you want to update w9, w10 all the weights so when you further back propagate the equation here will be the gradient equation would be derivative of loss with respect to w1 and that will be basically you know if you have seen my chain rule tutorial it will be your derivative of loss compared to awareness and derivative of awareness compared to w1 which means how much awareness changes for a given change in w1 so you can see that as you increase number of layers in the neural network these multiplications will increase so here for our case you know we had like two multiplication we had a multiplication of two factors like we know derivative one and derivative 2. and if these numbers happen to be small let's say both d 1 and d 2 are smaller then the resulting gradient will be even more smaller it is a simple math if you multiply couple of small numbers you get even a bigger small number not a bigger small number smaller small number maybe so now what happens is when you have a very low gradient like a very small gradient your learning process becomes slow so your equation for w one new will be w one o minus learning rate into gradient learning rate is less than point zero zero one then point zero zero one into point zero zero one is very very small number so your w1 is hardly changing so during your training process now your weights especially the weights in the earlier layers are changing by a very small amount and that affects your learning process and this concept is called vanishing gradients when you have a big neural network in the earlier network the gradient effect will be very small that's why it's called vanishing gradient it's not good for the neural network training because your weights are hardly changing and you're not really learning anything you know it's like a dumb student in the class whom you're teaching so many things and he's hardly learning anything on the other hand if the d1 and d2 the values of individual derivatives are bigger then the resulting number will be even more bigger so again a simple math when you have a product of multiple numbers and individual numbers either all of them or some of them are big your overall product will be very big and this is called exploding gradients now when you think about deep neural network you know which has n number of layers your gradient becomes even more smaller so vanishing gradient problem is more prominent in deep neural network now let's talk about variation gradient problem in rnn so there is exploding gradient problem also but vanishing gradient problem is something that affects more so let's say you have these two statements so here i am doing an autocomplete nlp task you know in google when you in gmail when you type some line it tries to auto complete let's say my statement is today due to my current job situation in family condition i when i type i or when i say condition comma my google let's say gmail tries to auto complete i need to take a loan whatever okay so these are the two statements now watch the words which are highlighted in yellow here if i want to use need or head depends on my first first few words in the statement because see all these words due to my current job situation they are all same so you can see that in english language often the wording that we put in the end they they have you know the wording which are in the beginning they have an impact to the words in the end you decide the end words based on very initial few words so if you're training an rnn like this and if you don't know how this works i highly recommend you watch my what is under rnn video in the same series because watching that video is very important otherwise you will be confused because this is an unrolling of layers neural network layers in time so it's not like we have six layers of neural network there is just one layer but this is a time stamp t1 t2 t3 so this we have unrolled into time for that reason you want to understand this convention and for that you watch my video of what is rnn so here i'm saying today due to whatever my whole sentence you know i have daughter here and then my neural network tries to predict i need to take a loan so the word need this is derived based on this first word today if i had last month then i would put i had so you can see that in rnn since we are feeding these words in a statement one by one uh the vanishing gradient problem becomes more prominent because you know all these activation that you are passing so you are passing activation a1, a2, a3 so by the time you are at a4 already the effect of a1 is kind of reduced and if your english statement is very long by the time you reach here the effect of today has significantly reduced and you will have a hard time predicting this need word so traditional rnns are said to have a very short memory so maybe when you have today maybe on due or two maybe until here uh there is an impact of today but as you pass through more words the impact of the earlier words reduces and that's why we say it has a short memory and this this shows the impact of vanishing gradients because as you back propagate right as you back propagate the as you back propagate what will happen is the vanishing gradient problem will arise here as you know these earlier layers will hardly change their weight based on the need let's say you're feeding this exact same statement and here i am finding need okay and my actual output was something else so there was a bigger error now that's bigger error when i back propagate by the time it comes here it it it will have resulted into vanishing gradients and we have two solutions for it which is gru and lstm and these are like special types of rnns or neural networks which addresses the problem of short-term memory so we'll look into this in the future videos i hope you like this particular video if you do please give it a thumbs up and watch my complete deep learning tutorial series i have a complete machine learning tutorial series as well as deep learning tutorial series with theory, exercise, code and everything so please check it out thank you"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Vanishing and Exploding Gradient Problems in Deep Learning",
        "Video Title":"Vanishing &amp; Exploding Gradient explained | A problem resulting from backpropagation",
        "URL":"https:\/\/www.youtube.com\/watch?v=qO_NLVjD6zE",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/qO_NLVjD6zE\/hqdefault.jpg",
        "ID":"qO_NLVjD6zE",
        "Publish Time":"2018-03-23T17:55:14Z",
        "Channel":"deeplizard",
        "Channel ID":"UC4UJ26WkceqONNF5S26OiVw",
        "Transcript":"hey what's going on everyone in this video we're going to discuss a problem that creeps up time and time again during the training process of an artificial neural network this is the problem of unstable gradients and is most popularly referred to as the vanishing gradient problem so let's get to it [Music] all right what do we really already know about gradients as it pertains to neural networks well for one when we use the word gradient by itself we're typically referring to the gradient of the loss function with respect to the weights in the network we also know how this gradient is calculated using back propagation which we covered in our earlier videos dedicated solely to back prop and finally as we saw in our video that demonstrates how a neural network learns we know what to do with this gradient after it's calculated we update our weights with it well we don't per se but stochastic gradient descent does with the goal in mind to find the most optimal weight for each connection that will minimize the total loss of the network so with this understanding we're now going to talk about the vanishing gradient problem we're first going to answer well what the heck is the vanishing gradient problem anyway here we'll cover the idea conceptually we'll then move our discussion to talking about how this problem occurs then with the understanding that we'll have developed up to this point we'll discuss the problem of exploding gradients which we'll see is actually very similar to the vanishing gradient problem and so we'll be able to take what we learned about that problem and apply it to this new one so what is the vanishing gradient problem well first talk about this kind of generally and then we'll get into the details in just a few moments in general this is a problem that causes major difficulty when training a neural network more specifically this is a problem that involves the weights in earlier layers of the network recall that during training stochastic gradient descent or SGD works to calculate the gradient of the loss with respect to weights in the network now sometimes and we'll speak more about why this is in a bit the gradient with respect to weights in earlier layers of the network becomes really small like vanishingly small hence vanishing gradient okay what's the big deal with a small gradient well once SGD calculates this gradient with respect to a particular weight it uses this value to update that weight so the weight gets updated in some way that is proportional to the gradient if the gradient is vanishingly small then this update is in turn going to be vanishingly small as well so if this new updated value of the weight has just barely moved from its original value then it's not really doing much for the network this change is not going to carry through the network very well to help reduce the loss because it's barely changed at all from where it was before the update occurred therefore this weight becomes kind of stuck never really updating enough to even get close to its optimal value which has implications for the remainder of the network to the right of this one weight and impairs the ability of the network to learn so now that we know what this problem is how exactly does this problem occur we know from what we learned about back propagation that the gradient of the loss with respect to any given weight is going to be the product of some derivatives that depend on components that reside later in the network so given this the earlier in the network a weight lives the more terms will be needed in the product that we just mentioned to get the gradient of the loss with respect to this weight what happens if the terms in this product or at least some of them are small and by small we mean less than one small well the product of a bunch of numbers less than one is going to give us an even smaller number right okay cool so as we mentioned earlier we now take this result the small number and update our weight with it recall that we do this update by first multiplying this number by our learning rate which itself is a small number usually ranging between point zero one and point zero zero zero one so now the result of this product is an even smaller number then we subtract this number from the weight and the final result of this difference is going to be the value of the updated weight now you can think about if the gradient that we obtained with respect to this weight was already really small ie vanishing then by the time we multiply it by the learning rate the product is going to be even smaller and so when we subtract this teeny tiny number from the weight it's just barely going to move the weight at all so essentially the weight gets into this kind of stuck State not moving not quote learning and therefore not really helping to meet the overall objective of minimizing the loss of the network and we can see why earlier weights are subject to this problem because as we said that earlier in the network the weight resides the more terms are going to be included in the product to calculate the gradient and the more terms were multiplying together that are less than one the quicker the gradient is going to vanish so now let's talk about this problem in the opposite direction not a gradient that vanishes but rather a gradient that explodes well think about the conversation we just had about how the vanishing gradient problem occurs with weights early in the network due to a product of at least some relatively small values now think about calculating the gradient with respect to the same weight but instead of really small terms what if they were large and by large we mean greater than one well if we multiply a bunch of terms together that are all greater than one we're going to get something greater than one and perhaps even a lot greater than one the same argument holds here that we discussed about the vanishing gradient where the early in the network a wait lives the more terms will be needed in the product we just mentioned and so the more of these larger value terms we have being multiplied together the larger the gradient is going to be thus essentially exploding in size and with this gradient we go through the same process to proportionally update our weight with it that we talked about earlier but this time instead of barely moving our weight with this update we're going to greatly move it so much so perhaps that the optimal value for this weight won't ever be achieved because the proportion to which the weight becomes updated with each epoch is just too large and continues to move further and further away from its optimal value so a main takeaway that we should be able to gain from this discussion is that the problem of vanishing gradients and exploding gradients is actually a more general problem of unstable gradients this problem was actually a huge barrier to training neural networks in the past and now we can see why that is in a later video we'll talk about techniques that have been developed to combat against this problem for now let's take our discussion to the comments let me know what you're thinking thanks for watching see you next time [Music] you"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Vanishing and Exploding Gradient Problems in Deep Learning",
        "Video Title":"Exploding Gradient and Vanishing Gradient problem in deep neural network|Deep learning tutorial",
        "URL":"https:\/\/www.youtube.com\/watch?v=IBODsB4q8cQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/IBODsB4q8cQ\/hqdefault.jpg",
        "ID":"IBODsB4q8cQ",
        "Publish Time":"2020-05-26T14:27:19Z",
        "Channel":"Unfold Data Science",
        "Channel ID":"UCh8IuVJvRdporrHi-I9H7Vw",
        "Transcript":"welcome to unfold data science friends this is aman here and i am a data scientist in this picture that you can see in front of you i have a neural network where i have input layer output layer and three hidden layers okay i am calling this as a deep neural network now in general a deep neural network has multiple hidden layers but to keep it very simple i am just keeping three hidden layers we will try to understand two of the very important concept from deep learning point of view and also interview point of view using this network okay one is known as exploding gradient problem okay and other is known as vanishing gradient problem these are the must know concept if you want to understand how to initialize the weights in neural network so in neural network first we have to initialize the weights and then through back propagation the weight updation happens right that is what we discussed in last video now what are these exploding gradient and vanishing gradient problem we will take an example and try to understand so can you recollect what is the formula for gradient descent so if i have to optimize weight w 1 1 then how gradient descent will work is w 1 1 old is equal to w 1 1 nu is equal to w 1 1 old minus eta which is the learning rate multiplied by doe of loss function with respect to w 1 1 this is how gradient descent works i have explained that in a separate video you can see the link okay now with this formula all the weight updation happens in neural network okay so if you see this carefully the new weight w11 nu will vary from w11 old based on this particular input in the equation correct so if w one one old is let's say 1.5 then from this 1.5 how much shift will happen either forward shift or backward shift will depend on what is the output of this term here in this term there are two components one is called learning rate we keep the learning rate normally in the range of zero point one to 0.001 and other is called the gradient so this gradient creates those two problems of exploding and vanishing gradient let's try to understand how so if you can recollect little bit about how gradients work in my last video i was talking about something known as chain rule in mathematics correct chain rule right in this chain rule what we do what we did is if we have to compute derivative of loss with respect to w 1 1 then we had multiple terms here correct so these are different different derivatives actually term 1 term 2 term 3 because we do not have any direct relation of loss with w 1 1 hence we try to create a chain of different derivatives and try to find out the values and compute this term okay now little bit of mathematical background here what happens when a positive number greater than 1 is multiplied with other positive number greater than 1 for example when you multiply 2 with 2 you get 4 as an output right which is larger than these two individual numbers but what happens when you multiply 2 decimal numbers 0.1 into 0.2 the result we get is 0.02 which is smaller than both these numbers on this particular concept the problem of vanishing and exploding gradient works okay so in this multiplication of chain rule what will happen when all these numbers are a decimal number less than one what will happen is the final value will become lesser than all these three numbers and what will happen when all these three numbers are greater than one the final value will become too high too high then all these numbers correct and when you plug this derivative in that gradient descent formula the gradient will never optimize the value will never optimize okay let's try to understand with this diagram and data so what are the inputs here x1 and x2 okay so let us create a matrix call as input matrix x1 and x2 what is the weight here let us call this as weight at first level so i am calling it weight at level 1 means at this layer okay so what is the weight w11 0 0 and 1 2 okay now what goes as input to this node here it will be nothing but a multiplication of x and w for now we are not considering this b term the bias term for simplicity so input to this will be just a multiplication of x and w right from here an activation function will get applied on this correct so let us write activation of this now this will go as an input to this this node here correct this node here again one activation function will get applied correct so let us write another activation function and this will get multiplied with weight at this location right weight at this layer or to say in particular weight at this node so what will be the weight at this node let us say w21 so this gets multiplied with w21 and similarly as you keep moving forward you will see that multiple weights are getting involved right so when we come back and try to optimize w11 in that derivative chain rule we will see that the effect of too many weights are coming into picture now what happens when your weights are smaller here we have in this w we have w 1 1 and w 1 2 both here we have w 2 1 so when the weights are smaller then their multiplication will become more small when the weights are larger then their multiplication will explode will become too large and what will happen when we try to optimize this the bigger number will go as input so w nu is equal to w old minus eta of your derivative loss by w let us see for simplicity now when this term is too high which is the problem of exploding gradient what will happen is this w nu will keep oscillating between minimum value so it will not never come to minimum value for example so let's see minimum value is here then w old will be once here w new will come here and then again it might go here it might come back here because we are not giving this w an opportunity to stop here okay this problem when your gradient is too high is called exploding gradient problem because the step size is very big from here it will come here from here it will come here like this and when this derivative becomes low then what happens is there will not be too much difference between w nu and w old because this term itself is very small that problem is called vanishing gradient problem okay now the whole thing boils to weight initialization if you see so what weight we should give we are saying that if it's a positive number greater than one it will explode if it's a negative number uh less than one uh if it's a positive number between zero to one then it will shrink the another problem will happen where it will just shrink so what should be the optimized weight the optimized there are different ways to give the optimal weight there are different ways to initialize the neural network which i will discuss in my next video for now if you have any doubts on this topic what is a vanishing gradient and what is exploding gradient problem write mean comment i will definitely respond to you i'll see you all in the next video till then stay safe and take care you"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Vanishing and Exploding Gradient Problems in Deep Learning",
        "Video Title":"Vanishing\/Exploding Gradients (C2W1L10)",
        "URL":"https:\/\/www.youtube.com\/watch?v=qhXZsFVxGKo",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/qhXZsFVxGKo\/hqdefault.jpg",
        "ID":"qhXZsFVxGKo",
        "Publish Time":"2017-08-25T20:22:47Z",
        "Channel":"DeepLearningAI",
        "Channel ID":"UCcIXc5mJsHVYTZR1maL5l9w",
        "Transcript":"one of the problems with training your network especially very deep neural networks is that are vanishing and exploding gradients what that means is that when you're training a very deep network you're derivatives or your slopes can sometimes get you to very very big or very very small maybe even exponentially small and this makes training difficult in this video you see what this problem of exploding or that vanishing gradients really means as well as how you can use careful choices of the random way the initialization to significantly reduce this problem less your training very deep neural network like this the same space on this slide I've drawn it as if you have only two hidden units per layer but it could be more as well but this neural network will have parameters W 1 W 2 W 3 and so on up to WL for the sake of simplicity let's say we're using an activation function G of Z equals Z so a linear activation function and let's ignore be the set B of l equals 0 so in that case you can show that the output Y will be W L times W 0 minus 1 times WL minus 2 dot dot down to w3 W 2 W 1 times X that means if you want to just check my math W 1 times X is going to be Z 1 right because B where is equal to 0 so Z 1 is equal to I guess W 1 times X and then plug V which is 0 but then a 1 is equal to G of Z 1 but because you use a linear activation function this is just equal to Z 1 so this first term W 1 X is equal to a 1 and then by still everything you can figure out that W 2 times W 1 times X is equal to a 2 because that's going to be G of Z 2 is going to be G of W 2 times a 1 which implies that in here so this thing is going to be equal to a two and then yo this thing is going to be a three and so on until the products all these matrices gives you the Y hat not Y now let's say that each of your weight matrices WL is equal to let's say is just a little bit larger than one time's the identity so it's one point five one point five zero zero right technically the last one has different dimensions so maybe this is just the rest of these void matrices then Y hat will be you know ignoring those last ones different dimension will be this one point five zero zero one point five matrix to the power of L minus one times X because if we assume that each one of these matrices you know is equal to this thing is really one point five times the identity matrix then you end up with this calculation and so Y hat will be essentially one point five to the power of L mm mm minus one times X and if L is large for very deep neural network Y has will be very large in fact this grows exponentially it grows like one point five to the number of layers and so if you have a very deep neural network the value of y will explode now conversely if we replace this with zero point five so something less than one then this becomes zero point five to the power of L where this matrix um becomes zero point five to the o minus one times X we can ignoring WL but so each of your matrices are less than one then if let's say X 1 X 2 where 1 1 then the activations would be 1\/2 1\/2 1\/4 1\/4 1\/8 1\/8 and so on until this becomes a right 1 over 2 to the L so the activation values will decrease exponentially as a function of the deaf as a function the number of layers elves in network so they be very deep network these activations end up decreasing exponentially so the intuition I hope you can take away from this is that if the weights W if they're all you know just a little bit bigger than one I'll just work with bigger then the identity matrix then with a very deep network the activations can explode and if W is you know just a little bit that's the identity right so this was maybe is 0.9 0.9 right then if a very deep network the activations will decrease exponentially and even though I went through this argument in terms of activations increasing or decreasing exponentially as a function of level the similar argument can be used to show that the derivatives or the gradients you compete with we understand will also increase exponentially or decrease exponentially as a function of the number of layers where some of the modern neural networks you actually have l equals hundred and fifty Microsoft basically got great results of encountering 52 layer in your network but whether such a deep neural network if your activations your gradient increase or decrease exponentially as a function of L then these values could get really big or really small and this makes training difficult especially if your gradients are exponentially small in elm then you know gradient descents will take tiny little steps they'll take a long time for gradient descent to learn anything to summarize you've seen how deep networks suffer from the problems of vanishing or exploding gradients in fact for a long time this problem was a huge barrier to training deep neural networks it turns out there's a partial solution that doesn't completely solve this problem but that helps a lot which is careful choice of how you initialize the weights to see that let's go on to the next video"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Gradient Stabilization Techniques",
        "Video Title":"Gravity Gradient Stabilisation",
        "URL":"https:\/\/www.youtube.com\/watch?v=-WsuNSuIhG0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/-WsuNSuIhG0\/hqdefault.jpg",
        "ID":"-WsuNSuIhG0",
        "Publish Time":"2014-01-03T19:33:45Z",
        "Channel":"HOCgaming",
        "Channel ID":"UC3igMkdtWOF4p_tOkLsaGBw",
        "Transcript":""
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Gradient Stabilization Techniques",
        "Video Title":"The Unreasonable Effectiveness of Stochastic Gradient Descent (in 3 minutes)",
        "URL":"https:\/\/www.youtube.com\/watch?v=UmathvAKj80",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/UmathvAKj80\/hqdefault.jpg",
        "ID":"UmathvAKj80",
        "Publish Time":"2021-10-22T12:30:16Z",
        "Channel":"Visually Explained",
        "Channel ID":"UCoTo2gtN527CXhe7jbP6hUg",
        "Transcript":"a big problem with gradient descent in machine learning is that it is slow at each iteration you need to compute the gradient of a loss function that has as many terms as samples in your training data set think millions or even billions of terms bill gates once said i will always choose a lazy person to do a difficult job because a lazy person will find an easy way to do it well stochastic gradient descent is a lazy person approach to address gradient descents problems stochastic gradient descent works exactly the same as gradient descent except that instead of computing the gradient of the whole loss function you just pick a constant number of terms and take the gradient of that stochastic gradient descent picks these terms exactly as a lazy person would in a completely random fashion in summary at each iteration stochastic gradient descent picks a random batch of data points uses that batch to compute a noisy gradient approximation and uses that approximation to perform a descent step with this approach not only the time per iteration is basically a constant the memory needed to compute the gradient is also a constant so while each iteration of the stochastic version of gradient descent is less accurate than that of the vanilla version the fact that each iteration is cheap allows us to do more of them what is surprising is that in practice stochastic gradient descent not only catches up with the vanilla gradient descent but it often finds better solutions and this should come off as a complete surprise if anything using an inaccurate method should lead to sub-optimal solutions in the last few years there has been an explosion of publications that tried to give partial answers to this phenomena let's go through some of them one common explanation is that in subtle points where vanilla gradient descent would get stuck the extra noise in stochastic gradient descent increases your chances of escaping another explanation has to do with the geometry of local minima when a local minimum is narrow changing the entries of our solution ever so slightly deteriorates performance a lot this indicates that a wide local minimum will generalize better to unseen data the noise in stochastic gradient descent will help you again escape narrow minima more easily but my favorite explanation is this one if you look at one iteration of stochastic gradient descent and you perform this change of variables then you can rewrite the iteration of stochastic gradient descent as follows taking expectations with respect to the noise leads to this or in other terms stochastic gradient descent is just gradient descent in disguise applied to a smooth version of f to conclude this video let me just point out that stochastic gradient descent is by no means a miracle solution it still suffers from some of the same problems of gradient descent it is just that empirically people have realized that it tends to give better results for a wide variety of machine learning applications and the moral of the story is that fast and sloppy is often better than slow and perfect i know you come for math content but you live with life lessons this was stochastic gradient descent in 3 minutes if you liked the video like and subscribe and see you next time [Music] [Music] you"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Gradient Stabilization Techniques",
        "Video Title":"Real-time Proximal Gradient Method - inverted pendulum stabilization",
        "URL":"https:\/\/www.youtube.com\/watch?v=9mA5GPTmSVM",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/9mA5GPTmSVM\/hqdefault.jpg",
        "ID":"9mA5GPTmSVM",
        "Publish Time":"2018-04-26T11:57:52Z",
        "Channel":"MECO Research Team, KU Leuven",
        "Channel ID":"UCx6CCM-bq-Iz7n2uzx7JI2w",
        "Transcript":""
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Gradient Stabilization Techniques",
        "Video Title":"Maryam Fazel (UW): &quot;Gradient based methods for linear system control&quot;",
        "URL":"https:\/\/www.youtube.com\/watch?v=5hMhGM0NOHI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/5hMhGM0NOHI\/hqdefault.jpg",
        "ID":"5hMhGM0NOHI",
        "Publish Time":"2019-06-04T12:57:32Z",
        "Channel":"MIT Institute for Data, Systems, and Society",
        "Channel ID":"UCSlhlkPaq3RL1CP0Tv1xWBQ",
        "Transcript":"[Pablo Parillo]: Okay, so why don't we get started. It's a pleasure today right now to have a Maryam as one of our speakers. I think Maryam is well known to many of you, because of her work mostly on convex optimization and control, now machine learning and algorithms. So, Maryam is a professor in the University of Washington in the EE Department but she also has joint affiliations with math, computer science, and statistics. Let me just make this brief - let's welcome Maryam. okay hi good morning everyone *applause* Maryam Fazel: Hi, good morning everyone. Thanks very much for the introduction, and it's a real pleasure to be here to see such great interest in this topic of machine learning and control theory intersection. So, I'll talk about gradient based methods for control and learning of dynamical systems. It is a joint work. I'll mention a few different works, but mainly joint work with Rong Ge who is at Duke University, and my colleagues at University of Washington, Sham Kakade, Mehan Mesbahi, and the student Jingjing Bu who works with Mehran and me. Okay, so general motivation for looking at gradient based design of control - looking at control of dynamical systems but trying to analyze gradient-based algorithms is because these kind of gradient based algorithms are used very commonly in practice they are generally very popular and seem to work well in very complicated situations. Including things like game playing, all the way from AlphaGo to Atari, as well as robotics manipulation and various robotics tasks, but often these approaches lack statistical and computational guarantees even when the dynamics is very simple, like linear dynamical systems. So, in this talk, we will try to look at - what do I mean by gradient descent, by the way, is we would like to do update on the policy in order to improve some kind of a cost for the controller. So the control policy needs to get updated and the control policy update uses gradient descent. So in this talk, we will be looking at studying direct policy updates for the linear quadratic regulator problem. This is basically the simplest control problem, well studied in control theory, but we're going to take a different approach to it. So, we will be looking at updating the gradient or updating the policy by taking the gradient of a cost function of this policy. So what is linear quadratic control? This is a very brief review of a topic that is very common and well understood in control theory. So, the dynamics of the system is linear, and the cost will be quadratic cost function of the state and the input. So here's the dynamic state xt denotes the dynamics at this state at time t. xt plus one is Ax t plus Bu t, this is a discrete-time, linear dynamical system. We are going are going to be interested for now in discrete-time, what continuous-time systems can also be handled, and we will be looking at an infinite horizon control problem. Meaning that we would like to choose a sequence of inputs u 0 u 1, all the way to - all times, in order to minimize a total quadratic cost function, which is the summation of xt transpose qx t plus ut transpose rut, where the matrices r and q are cost matrices that are given to us. So, this is a quadratic cost of state and input, and we would like to minimize it. So we would like to, for example, make the state be small or track a desired state that cannot be captured in this, and we would like the control effort to be small. For example measured in some weighted L2 norm, and that can be captured with the R. So, this problem is well-studied and in the traditional sense. If you know the a matrix and the b matrix which has determined the dynamics of the system, the solution is given by - is known to be given by solving the Riccati equation, discrete Riccati equation, algebraic Riccati equation, which looks like this. So, you actually need to solve for an auxiliary variable matrix P that satisfies this equation. The equation is a nonlinear equation, so as you see, for example, it's p on this side, there's p on this side, appearing here inside the inverse. So, it's a nonlinear equation, but it can be solved. There are linear algebra methods to solve it, both directly and iteratively and once this problem is solved, once you find the P that solves this, then you can find the optimal control by simply plugging in this formula. So, in this problem, the optimal control input is actually a static state feedback, meaning that there exists a matrix - fixed matrix K - that at every point in time your state needs to just be multiplied by this k matrix. So k star will be the optimal controller that actually minimizes this LQR cost among all possible controllers and it has a closed form formula that is given like this in terms of the p that solves the Riccati equation. However - so, this is a classical theory and it's a cornerstone of model-based control essentially since the 60's, going back to Kalman, and there's a lot of literature on it. There's a recent resurgence and interest in studying this problem from different points of view, mainly motivated by robotics application, which are interested in the policy updates as I was mentioning, and the classical version which goes through building this p matrix has an extensive theory, and it has computational guarantees. So, for example, to solve the Riccati equation, there are various methods and they have computational iteration guarantees. This goes back to work in 60's and 70's, but one issue for us is that these methods would need to solve for the matrix p first and then after that you can obtain the k star. So our goal is actually to revisit this classical problem but look at it differently. So, we would like to consider methods that would update the policy, meaning the k. K captures the the control policy as a function of state, right? So this gain k, we would like to update it by - we would like to look at algorithms that update this k directly by using a cost function c of k to which we have access in different ways. For example, we can have a gradient oracle, so we can have the gradient be evaluated, or we have a function value evaluated, or we have a zeroth-order oracle. So, either of these, and then the question is - so we start with looking at gradient. Actually, main focus of the talk is when gradient is available. So, the first question is, if I do a gradient descent on the cost induced by the LQR problem, does it converge? Does it converge to the well-known optimal controller k star? And then the second question is, what if gradients are not available? Can anything be said if we sample the function value? Sorry. Okay, so this - why is this problem challenging? So, the main challenge is that actually the cost as a function of k - so cost as a function of policy - is actually not convex, and therefore doing gradient descent directly on a non-convex function can be horrible, and it generally does not have guarantees. *technical difficulties* it's not moving forward... Oh, sorry about that. Maybe in the meantime, I will continue. So the challenge is that the cost as a function of k is not a convex function, and we are going to actually look at what that cost looks like. The, basically, the landscape of the cost function optimization landscape. If I wanted to minimize ck as a function of k. So while it is not convex, it has actually a lot of nice properties. One of the the main properties is that we can show that the function is actually gradient dominated, which is an inequality that says that the change in the function value is controlled by the norm of the gradient, and this is a very nice property. The one challenge about this function is that, if you know control theory, you know that the set of controllers that stabilize it in your dynamical system itself is a non convex set, and it's a very complicated set, but this function, if we define it correctly, it's going to actually be something that is, that will actually have its domain be the set of stabilizing controllers, and it has a coercive property, meaning the function value actually blows up to infinity as you approach the boundary. And in this way, if you are actually starting from a particular k star, the soft level set of the function - soft level set meaning k such that the set of ck less than the value ck0. That's set is actually a compact set. So we can use these properties to actually - even though the function is not everywhere smooth - thank you - we can use those properties to be able to guarantee - to show the gradient domination. So that's a nice property that the function has. So actually, before we look at the function, very quickly - I'll go a little bit fast - there's a large literature, of course, on connecting learning and control. They are, there's a variety of different lines of work, a lot of them are about controlling a system while learning it. A lot of them are about learning a model first and then using it to to design the controller in the traditional way. Perhaps among this list, one of the closest to us is actually the last one. The last is a paper that's trying to design structured controllers, so controllers where you want to have constraints on the k matrix, for example, sparsity pattern. Certain KIJ's in that k matrix need to be 0, and that's a known hard problem. They are solving this problem by taking gradient descent on the controller k, and it works very well in practice, but it doesn't really have any theory about optimality or about even rates of convergence. So, we will talk a little bit about the use of this gradient-based methods for structured control design as well, but let me move a bit quicker. So, why is this interesting? Because the gradient descent directly on k, first of all, allows us to impose additional constraints on k, for example, the structure constraints as they were saying. Structured controller design problem arises, for example, if you have a network system and you would like the controller to have a restriction about which actuators can have access to what information and things like that - so, for distributed control, for example. And that the formulation is a natural one to view the cost as a function of k, and also that this way of looking at things helps us to understand the sample-based approach or derivative-free approach to try to solve the problem. So, here's the problem. We will be looking at the LQR problem, but we will be - actually, one thing we will change is that, let's consider the initial state x0 to be random chosen from some distribution d, and we assume for simplicity that the dynamics are noiseless, although that's not a big assumption. Then we look at what the cost function c of k is as a function of k. For that, we define two matrices - sigma k, it's the state covariance matrix for infinite horizon, and sigma 0 is the covariance matrix of the initial state. So now, suppose that the algorithms that we will be looking at have - are of this kind. They have access to either one of these different oracles. So perhaps A and B matrices are not known explicitly, but we have some way of getting the gradients of the cost function as a function of k. So getting the exact gradient often does need to know A and B, but suppose they're - all that we need is a gradient oracle. So, if you get a gradient oracle, then we will be updating the policy like that with a fixed step size move in the direction of negative gradient. Another algorithm that is popular is the natural gradient descent, and this algorithm takes a weighted version of the gradient, weighted by the inverse of the covariance of the states, and it's essentially like gradient descent in a different - in a Riemannian manifold, or different way, the geometry. Or we can have a zeroth order or a code that gives you function values of c of k sampled at different ki's. So, as I mentioned, the c of k is a non-convex function of k and that's the main difficulty. We can even explicitly write it out. It's a big mess, it looks like this. So, by the way, here I'm taking expected value of the LQR cost over the random initial state, because we started with random initialization, and I'll have actually a comment about why we picked random initial state shortly, and in that case, the objective would look like this. The cost function would look something like this. So, the key thing is that when you write it as a function of k, k appears here and here and this is a Kronecker product and it's all inside an inverse, and then k appears quadratically here. Very messy. It's easy to see this is not convex. In fact, if the size of the state is anything equal to or bigger than three, we can easily construct a counterexample. You can think of the counterexample being, for example, that I have a controller that stabilizes the system, and another controller - so k1 and another controller k2 - that stabilizes the system, but if you take a convex combination of these, the controller may not be even stabilizing. So that means that the cost can blow up in a convex combination. So, that is a counterexample. To the convexity is also not quasiconvex or star convex, it doesn't have any of the nice convexity properties. However, we can actually write down what the stationary points of this function are, or the optimization problem which is when the gradient of the function equals zero, what can happen? So it's actually interesting that once we study the function, we can show at the points that the gradient are zero, either k is optimal or that this controller k corresponds to a ranked efficient state covariance matrix. So, if I want to guarantee optimality, I just have to rule out the second possibility and that's actually pretty easy to rule out, especially if you just start with a random initial state that has a full rank covariance matrix. So if sigma 0 is full rank, then of course all the all the future ck's just add to that full rank matrix, will remain full rank, and so we will never be in the second situation and k will be optimal. So, in a way, just ensuring full rankness of the covariance of initial state is enough. We can also try to look at this problem in a slightly different way, by looking at a convex LMI formulation, just to take the set of stabilizing controllers, write it in a convex way and then try to argue the same thing. You can do that as well, but the proof is actually no simpler. The direct proof is much shorter. However, that LMI formulation could be useful for other purposes, I'll mention something in the end. So, what if the initial state isn't random? Well, we picked at random for a reason, because if you look at the quadratic cost as a function of state, this actually depends on x naught and if k is stabilizing, then of course ck is always bounded, but if k is not stabilizing, then whether ck blows up or not depends on what initial state you took. So, because we're interested in an initial state independent version of the problem, we can do the following. We can either take x naught to be random, like we did or we can, for example, say that we defined the cost to be the average of a cost evaluated from a bunch of different x naughts that are linearly independent and span the space. So, that also does the job. So, making the cost independent of initial condition, in either of these ways works, and that in this case, if you define it this way, you can also have all the same properties that we have. In particular, the function is coercive. We can show it has compact sublevel sets, as I was pointing out and that further we can show that it is gradient dominated. So, let's actually go back to the random setting and the random initial condition. Here's the main theorem. We can show that gradient descent on the cost with exact gradients converges, and we can give a concrete rate of convergence in terms of problem parameters. So, for the natural policy gradient descent, that is the one that had the normalization with the state covariance matrix. The number of iteration - so, to get epsilon close to the optimal controller, so after n iterations, we get epsilon close to the optimal controller where n has to be this big. So we see that the dependence on epsilon here is log one over epsilon, which is the the basis of linear rate, and the dependence on initial distance to optimality, very natural. The coefficients that appear in the front - this first one has a sort of a meaning of a condition number it's the maximum singular value of the optimal state covariance matrix divided by the minimum singular value of the initial covariance, and these other parameters have to do with the matrices a, b, r and our choice of k zero, the initial controller. And the only condition we need for this theorem is that the initial k or k0 that we pick is a stabilizing controller. For gradient descent, a similar rate also holds, still also a linear rate - log one over epsilon. The dependence on parameters is a bigger... terms a little bit messier, but it's all polynomial in terms of all the problem parameters. So, good. Okay, even though the problem is non-convex, we have a rate of convergence guarantee. One use of this framework is actually, if you want to do structure controller design, which is by itself a very hard problem, an historically difficult problem that has been studied a lot. So in this problem, typically, you have the same problem that you want to find optimal k, but subject to constraints that certain kij's are 0 if the edge (i, j) is not allowed. So, if you have a distributed controller, if you want to design a distributed controller, and that there is a lot of work in this topic, but some of the recent work are mentioned here. There is a condition called quadratic invariance, under which this problem allows a convex formulation, but in general it's a hard problem and as I mentioned the work of Martensson and Rantzer actually uses gradient descent on the policy to empirically solve this problem and gets very good performance, but no theory. So for this problem, in general, we can actually show a rate of convergence of one over one over k, but only to a stationary point. In general, it's a hard problem. However, under quadratic invariance, we can - we are working on it now, we were finishing it up, that we can show that there is a global convergence to the optimal controller. So - but what, there's still more interesting cases of - special cases of structured controller to be studied. So the goal is to, if you update the controller, given that empirically it works well in practice can we show anything about it in theory? Now then, I'll just mention very briefly about the case that - suppose your a and b matrices are not known, so you don't know the model, and you have only an approximate oracle for the gradient. So you cannot have an exact gradient of the cost with respect to the controller, but you have an estimate of it. How would this work? Then your algorithms would look like this - you have an estimate of the gradient appears here in gradient descent in, for example, national policy gradient you would have to also estimate the state covariance matrix. But so, suppose this could be useful for cases where we have other ways of evaluating the approximate gradients and trajectory covariances. So what happens in this case? In this case, we can do the following. We can try to obtain, for example - so it's also useful if we can actually obtain samples of the cost function instead of even noisy approximation of gradient, we are only restricted to getting values of the cost function evaluated at certain ki's. This will be similar to derivative-free or zeroth order optimization. So if we actually try to use that kind of approach, saying that we have a way of evaluating c of k at a particular k, how do I use that? I can take my current controller and perturb it by noise, and generate a few random k's around that point, evaluate the c at those and use those to proceed, but then if I want to do that how much noise do I need to add? What is the length of rollouts I need to do? So in this approach, you would actually try to run your system either using a simulator or the physical system for a limited period of time to obtain sample trajectories, and then use that to estimate, for example, sigma k and ck. So, the question would be - what can we guarantee? So this would lead us to an algorithm, for example, of this kind, that is a very, very simple sort of derivative-free algorithm that only works based on function values. It is extremely simple. This is for - in order to give a first cut type of theoretical result, this is the simplest algorithm you can imagine. So, in this algorithm, we would have - suppose you start at sum k0 and you will be at that particular k0, you perturb the controller by adding noise to it, where this noise comes, for example, uniformly from a sphere or from a ball, and then you obtain estimates of the cost function. So then, you run this controller for a certain length of time, starting from the initial condition, from x0, and then obtain the samples of the cost function along the trajectory, as well as the trajectory itself. So, based on that very simple, very limited observation, you get an estimate of the cost and an estimate of the covariance, and then you can try to estimate the kind of approximate the gradient in this very simple way, and covariance in this very simple, simple way. So these are again, really simple estimates. They have very high variance, that's true, but no result exists for this problem, so you want to at least see - does this work? So, yeah, we can actually give a global convergence rate in this case, as well. So, the theorem would look something like this. The details aren't shown, but just, again we assume that k0 is such that the initial, the initial k0 is stabilizing, so the initial ck0 is bounded. It's a finite value. We do need that assumption, if there are ways to estimate a stabilizing controller. That might actually be an issue if you don't know the model, but suppose you can. Then, if the step size, if a fixed step size is chosen appropriately small enough, then in a number of iterations and number of rollouts and length of rollouts that are all polynomial in log one over epsilon and in all the other problem dependent parameters, we do converge to the optimal controller k star. We get epsilon close to optimal controller k star. The number of samples we need for this to work is polynomial in one over epsilon, so not optimal, but still one can characterize it, and it has polynomial dependence. It also is polynomially depending on the distance from the initial to optimality and again, everything else, all the other problem parameters. So, it's nice to have a way to combine results from zeroth order optimization with this particular problem, and be able to say that if you do gradient descent, even if your gradients are extremely noisy by function evaluation, it still does converge to the optimal controller. Again, it doesn't mean this is the best way to do it. Maybe for a linear system, it's much better to learn a model first and then do control, but if you are going down this approach of 'I want to do the pure model-free,' at least this gives you some way of understanding what's going on with these methods, and that the convergence to global optimal and controller in the linear case is by itself interesting. Okay, so yeah, so we studied a gradient descent and it's variance for the LQR problem. The updates are directly on the policy. The cost function over the set of stabilizing controllers has nice properties including coerciveness despite non-convexity. I didn't go into the details but we have actually a paper that will be soon on the archive that works out these details. So, thank you very much and I'll take any questions *applause* [Pablo Parillo]: We have time for a few questions, so... [Audience Question 1]: Okay, so in the case of structure control design, you mention a qi property and then I think in that case, the operon control it is a dynamic one, so your current transition the case is static. So how do you deal with these dynamic controller? [Maryam Fazel]: Yes, it's not the traditional quadratic invariants. In fact, it's true the quadratic invariant, if you want to say - or the optimal control over the set of all controllers is a dynamic controller in your problem formation, you're only looking at static controllers. So it's not exactly the quadratic invariance condition. Something similar to that. [Audience Question 2]: Have you by any chance considered cases when you have state or control constraints? [Maryam Fazel]: I'm sorry? [Audience Question 2]: A case with state or control constraints in addition to... [Maryam Fazel]: Ah, I see. I see. No, we have not actually. That's a good question. The control is itself penalized in the objective, so at least there's a quadratic penalty on the control, but if you wanted to penalize it in other ways, then a slightly different formulation. So, we didn't look at that. Good question. [Audience Question 3]: Just, you mentioned in your future work, extending to other costs, maybe could you speak to what other types of costs do you think you could analyze? [Maryam Fazel]: There are many different control problems with different costs. So, for example, h infinity control, which is a very different cost function and it's for useful robustness. You want to protect against the worst case adversarial noise. That's very interesting but is, of course, a much harder problem because it's not even differentiable, nonsmooth problem, etc. So we have some partial results along that direction but yeah, I'll leave it at that. So work will be forthcoming. [Audience Question 4]: So, thank you very much for the nice talk. So, in the beginning of the talk, you talked about the similarities of the continuous time, discrete-time systems, and that using your method you can actually handle continuous time systems. I don't think that's that easy for the continuous time systems because it's the Hamiltonians, they depend on the dynamics, although in discrete-time systems you don't have this, so how do you handle this one? [Maryam Fazel]: Yeah, so our work is for discrete time system. We're actually more interested in discrete time but for continuous time, you're right. The problem is actually different, but you can look at gradient flows and gradients those actually behave nicely. So, some things are more complicated about the continuous time, but some things are actually simpler. The Lyapunov equation, for example, is simpler. The Riccati is simple, etc. So there are some nice properties. You can still show that the cost as a function of controller has nice properties, but yeah, there's differences. [Pablo Parillo]: Great. Thank you for a very nice talk. [Maryam Fazel]: Thank you. *applause*"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Normalization Techniques: Batch, Layer, Instance, Group",
        "Video Title":"All About Normalizations! - Batch, Layer, Instance and Group Norm",
        "URL":"https:\/\/www.youtube.com\/watch?v=1JmZ5idFcVI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/1JmZ5idFcVI\/hqdefault.jpg",
        "ID":"1JmZ5idFcVI",
        "Publish Time":"2022-03-15T15:00:01Z",
        "Channel":"ChiDotPhi",
        "Channel ID":"UCo8HIr2vN5PzV8yB8ehoiRw",
        "Transcript":"hey guys welcome to this video um it's been a while since i've made a video that is not aligned with the cds so i've been making videos about pie torch i have to make videos about halite but it's been a while since i just made a video about you know a topic that i found interesting and today is that so today i will talk about or work through and demonstrate all the different kinds of normalizations there are uh in deep learning and this is pretty fascinating so um and while preparing for this video or while trying out a bunch of things i realized um some differences in behavior in how all these frameworks implement batch normalization and um other normalizations so i hope it's a good learning experience or it's a good revision for all of you all as well so let's begin so first of all why do you need normalization so without normalization either your activations would grow very large or or would go down so either they were you know um uh you know go towards infinity they'll keep on adding up or multiplying so if positive numbers keep on multiplying they might grow larger or they might grow very small so which is bad neural networks are bad whenever the values are very small or the values are very large so neural networks tend to work very well um between this minus one to one or zero to one regime because that's where the most of that's where the weights are initialized and so we want to restrict our activations between that small regime and um this is the way you do it is you know once you have your activations of out of a layer you rescale your activations to put them in uh to normalize them between zero to one or you know minus one to one depending your uh depending on your application so there are various methods of doing these so by the way i assume that you you know of patch norm or you've heard of and you uh you know know why we need it so that's why i just blazed through that explanation this is more about you know how do you play with the implementations and what kind of advanced normalizations are so let's actually jump in these are the more famous kind of implementations so we have here batch norm layer norm instance norm and group norm so this this figure i got from the group normalization paper um so let's see how each of them work so if i look at oh before i do that by the way i am drinking my own beer i wish i had a glass but then you could see the color of the beer but um it's so this is something i brewed couple of weeks back it's um it's somewhere between a hazy ipa and a hoppy heaven bison does that make sense all right um good so let's assume um we have an a tensor nchw where n is your patch dimension so this axis corresponds to the batch dimension this x axis corresponds to your features channels and um height and width have been rolled in this axis so you know you flattened height and width out because you can't um imagine a 4d figure like you know on a 2d plane okay so with that um what batch normalized normalization does is it finds the statistics across each channel so i have a batch of n so for i will look at all the channel values across this batch and compute the statistics the mean and standard deviation along this so for each channel i will find the mean and standard deviation so i'll find like two numbers the mean and the and the variance and then my activation will be rescaled accord excuse me according to those for the second channel i would do the same for the third channel i would do the same so for each channel i run go across the height with and the n and i compute the statistics that's how batch normalization works now layer norm on the other hand computes the statistics for each sample so let's say you have n data points so you have the batch size of n so for each of the data point it will compute the statistics and then compute the uh then the values would be rescaled according to the mean and standard deviation of that sample itself so within itself now you would need learn norm in a situation where you can't do batch norms so in when you get sequential data in rn and lstms and those kind of networks when you get sequential data that's where layer normalization works well so again for each sample you will normalize so each sample would will get its own statistics then comes instant snore now instance norm is each channel and each sample so for each channel and for each sample you will compute the statistics so for um for n equals 1 c equals 1 i get some statistics and the same for n equals 1 c equals 2 and so on so for each of your filter or each of your channel you will get the mean and standard deviation and then you will rescale your output like that in group norm you will compute statistics according to groups now groups are made of multiple channels stacked together so if you look at layer norm layer norm was for all of the channels group norm is channels belonging to a particular group so here group one is three these three channels so that's why the normalization is like this so um yeah so this is a rough overview of these normalizations now let's look at how the equations look like so let me again go back to batch normalization and let's look at the figure again so for batch normalization you will compute statistics for each channel so for each channel i will um you know check out all the h w and n values and then compute statistics so given an input of dimensions r uh nc wh i compute the mean for each channel by summing or aggregating all the values so x i c j k represents uh the value of the tensor at the i at c at j gate location so i sum over all the other dimensions and then so i get per channel mean and because this is the number of elements i have n times w times h so um so i compute mean per channel and then i compute my standard deviation per channel so value minus the mean square and then i divide by the number notice it's the number it's not the number minus one so the vessels correction isn't um i don't remember if the bessel's correction is over n or over n minus one but then you know uh there's no minus one here and this distinction will become kind of important um and now so now once you have obtained the corrected values now the output for each channel so this should be x cap c would be x for each value in a particular channel would be the value the current value minus the mean of the channel and divided by the standard deviation of the channel rather the variance of the channel this epsilon has been added for numerical stability you don't want division by zero so this is how you normalize um your activations and this is how you compute the statistics again batch normalization is with respect to um all the uh to per channel so batch normalization is a per channel thing all right so um well i forgot to add this bit here so i just added it now now once you have the new activations these normalized activations you scale and shift them so basically the final output is this normalized value times gamma plus beta so now the scaling and shifting are learnable parameters of your network so these this is like the bias and this is like the weight um they are needed to you know a network might prefer having values between -2 or minus 2n minus 1 or whatever it might prefer so this gives the network the freedom to scale my distribution so this will scale and move my distribution so that's why there's just added freedom um to the network um right so this is how batch normalization works now let's look at layer normalization so for normalizations i mentioned that you look at each each batch and you compute statistics within a batch and then that's what you use for computing the activations so the mean and the standard deviation for each batch are computed by aggregating the values of the batch together so and they're divided by the elements which is cwh so you compute the mean of the batch and then the standard deviation of the batch and then the same you use the same formula for normalizing your data and then finally scaling and same again you have the weight and bias these are again the learnable weights so all these layers or all these normalization techniques have two learnables two sets of learnables um right so layer norm you compute across each um each sample right and also notice that um here also the division doesn't have a minus one so it's over n you know it's the whole sample and population variance uh distinction now we look at instance normalization um instance normalization was per channel and per batch so for for a particular batch or for a particular sample i look at a channel and compute statistics with respect to it so again here you can see the mean for a particular sample in the batch and for a particular channel is computed by aggregating the values of height and width for that there should be the second letter should be a c so it should be nc and then jk same way you compute the standard deviation and then rescale the value again you get some weight and bias again so here your means are computed for each channel and each element of the batch that's why it's an instance finally we so you see like um layer normalization and um instance normalizations and then group normalizations are like you know one kind of the same thing so um instance is at one end of the spectrum layer is the other end of spectrum and group norm is in the middle so um i i feel like these are like kind of more or less the same thing um while batch is something completely different because there you're doing each per each channel so you're looking across the batch to reduce the variance um right so for group normalization um the thing is the thing in group normalization is that you've broken your channels or you've segregated your channels into groups so um nchw becomes n times g times c prime times w h where g times c prime is equal to c so what that means is um the number of groups times the number of channels per group is equal to my original number of channels so i have so here i let's say i have three channels in a group and then i'll have two groups so two times three is six and that's the total number of channels so um in this case you would normalize per batch and per group in instance it was per batch and per channel so ng so um yeah so you would sum over the remaining dimensions same thing for standard deviation and then you rescale your score and then finally scale and shift so this is how this is kind of how all the different normalizations work now that you've seen the theory or seen a lightning introduction of the theory let's jump into some coding um let's let's see it in practice the framework doesn't matter so i'll use pi torch but i just want to show you how each of these things look like so let me jump into a collab notebook okay so i have pulled up google collab over here and now let's look at the documentation for batch norm it says 2d over here because you could have like a 3d version of patch norm as well and a 1d version of it as well so the batch norm 2d requires a number of features as input because remember patch numbers per channel so you need number of channels as input this epsilon is the new medical stability factor momentum is something that i will mention in a bit track running stats you kind of because um okay so batch norm has two modes training and evaluation you'll see this this in action so in training mode the statistics are different and then because while training you will pass multiple batches right so you kind of want to keep a running mean and running statistics so so this option will uh by default it's true so it'll keep track of run-ins running statistics and then finally when you are in evaluation mode the final statistics will be used the final you know after the running statistics whatever they've accumulated they will be used in computing um the activations now momentum is how you momentum tells you how you want to keep the running statistics so here you can see the new mean would be one minus momentum times old momentum plus the momentum times the new thing so it's a way of keeping the running mean and um a fine basically tells you whether uh you want the learnable parameters so when i said learnable parameters in the slice they were gamma and beta they were the weight and bias or the scaling and shift factors right so um okay so let's let's jump into it so let me um import torch let me import numpy let me see if you can see all right okay good now let me create um dummy input and i will create an input of a very specific value so that it's easy for us to handle now you see i'm creating a tensor of dimension 2 2 2 2. so n is 2 c is 2 w is 2 h is 2. and then what i'm going to do is for no the first channel is going to be um identity thus so first channel of the first sample second channel is going to be a torch i'll make it two times identity now what am i doing right now the first channel of the second sample i want it to be same and then the second channel of oh sorry now it makes sense so this is um first sample first channel first sample second channel second sample first channel second sample second channel and i'll do three and then let's output this so good so you can see the so this is the first sample it has two channels of two by two two by two and then this is the second sample which has two channels two by two two by two awesome now let's create a batch normalization layer so torch dot nn dot batch norm 2d and let me specify so how many channels we you want two channels um i want momentum to be one because you know i don't want to compute that that you know the that running statistics i want to be uh done with momentum one so in this case uh the aggregation so when i set set momentum to be one the aggregation would be so the new value would be completely x of t not the right thing to do but then uh in in our small example i want to set momentum to be one just to show you guys um but in general keep momentum low like 0.1 and um what else um yeah that's this is pretty much it now what i'm gonna do is i've created this layer um let's pass um the input through it and see what we get we get something now let's see if we can make sense of this right if if so what bash numb would have done it would have computed um the mean and standard deviation of the first channel so this and this first channel across all the samples and then for the second channel across all the samples and then computed statistics so let's see what batch normally what does the batch norm layer contain so let's check out its state dick so it has weights and biases now the weight is a two dimension one one because uh there are two channels right so that's why one one bias by default it start off starts off as zero now we have also a running mean and a running variance now okay so this is the mean this is for the first channel this is the variance for the second channel now um let's check if the values are correct right let's let let's try to reproduce this particular one um i go so basically i have to do the out the original value minus the mean divided by the variance um right right i just realized that i think in the previous slides i um uh i switched standard deviation variance so standard deviation is the square root of variance so um apologies for that i just realized by looking at this um all right so so yeah um let's jump in um okay so right so we will reproduce this particular we'll try to see this particular value now the output is so the original x is zero so i get zero minus mean now mean for the first channel is 0.75 and then i need to divide this by the square root of the variance and then the variance is this particular thing then let's see oh why i should do import numpy as np and then this should work so i get negative eight point six six huh which is not equal to negative zero point uh whatever nine zero four five so what's going on here um before i tell you that let's look at another different thing we will look at um a different mode of batch normalization which is so this is the training mode now we will look at the evaluation mode for batch normalization all right so i will set bn.eval and once i have that i will pass in the input again all right so let's run that and look what we have here so first of all the numbers are completely different so if i go up here it's some values but now it's something completely different so what's what's going on um and now you see it's negative eight four six one which is kind of closer to what we had so um oh i just realized i made some error um if i choose the variance let me put correct variance and um yeah so our values completely match so from the running mean if i used the statistics from the running mean i completely i'm able to i'm i am able to reproduce this interesting so what has happened is um right so in evaluation mode um you're not training anymore so any value so running mean keeps a tr keeps the track of um uh you know over the course of the training what's the mean that has been observed and over the course of the training what's the standard deviation or variance that was observed variance i need to be um to be uh careful so um now with those things in hand you will compute the outputs that's the evaluation mode so with the final running mean and the final variance that's what you'll use to compute the output however when you are doing training you will compute the output this output is based on the statistics of the batch that was given it does not depend on the running statistics because the running statistics indicate the entire training that has been done till this point but this output is based on the statistics of um of uh the current batch that was processed so okay so that's the difference now why are these values different because um right now i have passed only one batch so for one batch they should match right so what's what's going on now that now the the key difference is um when you do in training mode patch normalization is computed the the variance is computed by dividing by n where in um so when you do that so let's see uh so let's compute the variance right um what i'm going to do so i'll compute the variance of the two channels right so um let's see online variance calculator right so i will input the values of the two channels so i had four zeros two ones and two twos and now we want the population variance so let's calculate um right so one one 2 2 right so the variance came out to be 0.829 okay let's go back this is different from the running variance mean doesn't matter because mean is always divided by the number of samples but the running variance is now different because running variance was computed using n minus 1. um and now when i perform the computation um sorry let me see i uh computed for this for this um give me one sec to figure out what oh yeah i just realized um sorry the variance is 0.6875 so i need to divide by um square root of 0.6875 right now when i do it i get negative 0.9045 which is exactly this so that's the difference to keep in mind in training mode um you only use statistics of the batch that is given to you and the variance is computed uh without the missile correction however in the evaluation mode um you take the take into account the vessel correction and then you also do it on the running mean and running variances so this is how and that's why in the formulas that i showed you there was no n minus 1 for any of the formulas now batch norm is the only tricky bit layer normalization and um instance normalization they uh they kind of work um so there so batch norm has two different behaviors training and evaluation learn all my instance norm don't have that it's because they don't work across batches so even group norm all these three they don't so they they just have one mode of execution and then we'll now look into that so let's look at layer norm so i will create a layer normalization layer oh before that let's first look into the documentation all right um so the layer normalization takes the uh as input normalized shape so basically this means the shape that um you need to normalize by so here you you specify the dimensions of the shape um of the blue block so basically chw the dimensions across which you'll compute the normalization features and then element or let me explain this again so normalize shape is so you give it the shape across which you will compute your statistics so in this nchw array layer norm was for each sample you will compute the statistics within sample so you will aggregate all the values of the sample so we for each sample the dimensions would be c times h times w so that's why um epsilon is the same element wise a fine um basically again tells you whether you want learnables or not and that's pretty much it all right so let's uh let's jump into layer normalization and um let's do layer uh sorry torch.nn uh learn arm and then i will pass my chw dimension is two comma two comma two and um that's pretty much it now let's pass the input and we get the values so um i kind of want to i want to show you um so learn normalization statistics are computed per batch so let me work um let me just you know work out one of these to show you that this is actually working so remember the mean and standard deviations were computed per sample so what i'm going to do i'm going to compute the mean how do you how do i do this means across the last three dimension does this not work minus 1 minus 2 minus 3 it should work right give me a second to oh it's ah it's it's them right so this computes the mean across my two samples so the mean of the first sample is this mean of the second sample is this let's also compute the standard deviation um i need to remember to divide by standard deviation of the square root because again minus 1 minus 2 minus 3. all right so the standard deviation that we get is 0.8864 so now let's look at this value so in the input there was a 0 here so if i do 0 minus mean divided by standard deviation it i get um maybe one sec right um hang on what's this shouldn't be um am i doing the right computation uh so um i need to compute the population statistics it's right it's yeah i should get this particular value um mmm oh oh it's not oh i see sorry um i know that i wrote okay so this standard deviation is um it's biased rather um so we need to remember we need we need to compute the population variances so i need to specify um the unbiased thing to be false right right right right yeah so so the division by n and n minus one is the key uh differentiating factor and remember for all the normalizations it's um it's the so for layer norm for group norm from instance norm the variance is divided by n for batch norm um for one mod it's n for the other mod it's n minus one um all right so it's eight divided by eight two nine two when i do divided by eight two nine two i get uh negative zero point four and this is exactly the value awesome so we know how layer normalization works where you compute the statistics within um within a sample finally let's look at uh instance normalization i won't look into group norm because kind of works the same way but then let's just do instance normalization so um again let's do okay so for instance normalization we have to specify the number of features so for an nchw tensor that would be the value of c the number of features that's why c a fine again whether you want learnables or not track running stats momentum all the jazz so again number of features it should be c and the expected tensor input is nchw or chw all right so um how do you instance norm okay so um i'll just call it layer torch instance norm 2d and the number of features are two because that's the number of dimensions that i'm using and then i i'll show you what momentum 0.1 does this time um all right so now let me compute the input right so [Music] actually um this the momentum wouldn't affect but i'll show you momentum later let me keep momentum one because i think uh yeah it should be indifference of uh independent of momentum but then what what instance normalization is doing for each for each matrix because for each remember it's for each sample and each channel so for each let me show you the input as well so for each of these it'll compute the statistics and it'll normalize now the way our matrices are there are like you know one zero zero one two zero zero two and so on um their normalizations would become something like this between minus one and one they they have a pretty neat shape i can i can show it to you so let's look at this one so here the mean is three so again we go to our variance calculator and i pass in and i compute so what's my mean it's 1.5 and my standard deviation is 1.5 so when i do 3 minus 1.5 i get 1.5 divided by 1.5 is 1. for the zero element i do 0 minus 1.5 divided by 1.5 i get minus 1. so um so instance norm pretty much works pretty straightforwardly so for each of these small matrices we compute for each of the each of my features in each of the samples i compute the statistics and that's what i um do my um normalization with now i just want to show you the state dict and um sorry what is layer oh are the loanables disabled by default oh i see so by default there are no learnables let me change that so oh let me enable that so notice that this layer does not keep a running statistics the same goes with layer norm as well because run in statistics is not required for um for these layers however you can turn it on by this option track running stats which is by default false in batch norm that's true by default and then the same way group normalization also works let me finally just show you the effect of momentum so in batch norm if i set momentum to be 0.1 press enter and then i compute the values the these values don't change much but let me show you the running mean so now the running mean is a decimal place further away so it's 0.075 instead of 0.75 and the reason is this equation so the new momentum is 0.9 times the old momentum which was zero plus 0.1 times the new uh new mean i said momentum but mean so momentum times mean 0.1 times whatever the mean is that gets you that extra decimal place or extra zero after the decimal so wow this was a long one but this is what i wanted to convey through this whole exercise you know how these normalization layers work and the inner workings of them so i hope this was useful in some fashion or the other i i mean i'm sure like most of you know how the batch normal all these layers work but i hope this was a good revision of sorts because we worked through an example to see how the the normalizations are achieved before i end i want to open the group normalization paper and i apologize for the confusion between standard deviation and variance i don't know what i was thinking right so this is the paper that introduced group normalization now it has some interesting research which i just wanted to point out so yeah this is where i took the figure from um right so here they do training they do a comparison of training error and validation error on um on resnet 50 and here you can see a comparison of the performances of group norm batch norm layer norm instance now instance norm does the worst batch norm and group norm kind of do uh roughly this same performance even validation batch norm and group norm kind of give a better performance than land on an instance norm um here so check out this paper it has a you know good comparison of uh it also depends on what batch size you use so i remember beyond a certain batch size or so there's a region in which group normalization is the best and beyond that um patch norm becomes the best let me see if that's that graph is here right is this this one um can't tell but then you know anyway check out the paper and then you can figure out which one you want to use it's it's all empirical anyway so you know try it out all right i hope oh wow this was a 40-minute video but i hope this was useful and um i will keep on making videos like this when i get time alright see you guys next time bye"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Normalization Techniques: Batch, Layer, Instance, Group",
        "Video Title":"Batch Normalization - EXPLAINED!",
        "URL":"https:\/\/www.youtube.com\/watch?v=DtEq44FTPM4",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/DtEq44FTPM4\/hqdefault.jpg",
        "ID":"DtEq44FTPM4",
        "Publish Time":"2020-03-09T14:00:03Z",
        "Channel":"CodeEmporium",
        "Channel ID":"UC5_6ZD6s8klmMu9TXEB_1IA",
        "Transcript":"let's take an interesting problem of an NBA predictor determine whether a person will make it to the NBA based on their height and age okay Allen Iverson this guy is gonna make it to the NBA got you okay Yanis on to takubo this guy also makes it to the NBA okay let me process this okay LeBron James this dude makes it to the NBA okay let me process that Peter this guy doesn't make it to the NBA okay let me process this even though it's obvious and cool okay then now for another 2000 examples so this neural network is learning and it will learn correctly minimizing the loss but training seems pretty slow let's normalize their input data this makes sure that the agent height are between the same small fixed range and let's see what happens Allen Iverson makes it okay I got it Yanis makes it okay got you LeBron makes it okay Peter Griffith doesn't make it okay I need another 50 examples now this Network learned much faster reaching the same loss minimum so if normalizing the input actually works and why not normalize the activations in every layer with batch normalization ai makes it Yanis makes it LeBron makes it Peter doesn't make it I'm done time to test this is even faster during training time and it's called batch normalization because we normalize the values with respect to the batch of inputs this is the overarching principle but I'm going to explain this in detail with the math later on but first let's ease into the technical stuff why are we doing batch normalization well there are three main reasons it increases the speed of training like we've seen it decreases the importance of weight initialization and it regularizes the model a little bit so let's go through each one batch normalization increases the speed of training how is this the case well first up how does normalization in general speed up training in our NBA model we had two features height and age feeding these values directly to our network will make our loss look like this it's elongated this is because the height has a small range of like 0 to 2.5 meters and age has a larger range up to like 150 years small variations in height can greatly change the laws so in order to effectively learn with gradient descent we need a small learning rate to ensure we don't overshoot the minimum normalizing data helps to subtracting the mean and dividing by standard deviation what this our data will have a zero mean and standard deviation of 1 and our cost function looks more symmetric we can now use much larger learning rates getting to the minimum faster hence normalization speeds of training that said we could also use adaptive learning rate optimizers like Adam where we have 1 learning rate for age and another learning rate for height however it's still better practice to normalize your data before feeding it into your network so yeah normalization speeds up training your free network small caveat to this though this contour plot you see how the lines are closer to each other in some cases and further away in other cases the closer lines indicates steeper edges while the lines that are further away indicate a flat area both of these make learning very difficult because they're harder to traverse faster normalization has the effect of smoothing out this terrain making the concentric rings more evenly spread and this makes it much easier to traverse this effect of loss smoothing is the main reason why batch normalization works fast and work so well now from the next point vast normalization allows suboptimal starts in the loss function without batch normalization we start somewhere out here it might take us a hundred iterations or so to get to the minimum but if we start somewhere further out it might take us a thousand iterations to get to the minimum we can't easily define the initial value since its range is so large but look at the new loss function we can now randomly sample a number between like negative 1 and positive 1 for an initial start to a feature and no matter where we start we will end up at the minimum in a similar number of iterations so Bachelor Malaysian helps make initial weights less important batch normalization acts as a regularizer when you think of a regularization in neural Nets you think of dropout multiplying the activations of neurons with either 0 or 1 to randomly turn off neurons with batch normalization 2 there is an element of randomness the mean invariants for every neuron activation these values are highly dependent on the batch a batch composed of random samples and in that regard Bachelor realization does induce some regularization despite this we still use batch normalization with dropout for better results so these are the three main reasons why batch normalization helps model performance some of these reasons and the overview of batch normalization will probably become clearer when we get into the details so let's do that these are the equations in the original paper for batch normalization let's pick these apart we train the network with mini batch gradient descent this means the parameters are updated only after we see a batch of some M samples let's focus on this neuron assume the batch size is 3 so that means we update the parameters after seeing 3 samples pass in the first sample the neuron has an activation of for the second sample now that activation of the neuron is 7 and the third sample the activation is 5 the mean activation for this batch is 5 point 3 3 pretty easy math and the variance is one point five five five five five five you get it now we normalize the activation values by subtracting the mean and dividing by the standard deviation and we have this noise parameter in the denominator in case the variance becomes zero in other words the activation values of the batch now have a zero mean and a unit standard deviation this speeds up training but we can't just leave things like this the mean and variance are heavily dependent on the samples of the batch and so we calibrate the mean and the variance by introducing to learn about parameters for each neuron gamma and beta training over multiple batches gamma should approximate to the true mean of the neuron activation and beta should approximate to the true variance of thinner on activation and so we get activations that not only speed up execution but they also give us good results hope all of that stuff was clear but the question why does batch normalization work is still not completely understood even today the initial paper said that it was because of its effect on reducing something called internal covariant shift this has been debunked in more recent papers stating that it's mostly because of the smoother terrain of the loss function which I mentioned previously but still the research is ongoing even then I hope this video helped you get some clarity on the matter and I'll include some interesting reads in the description down below hopefully the main paper is now more accessible click on my other videos for some amazing explanations subscribe and I'll see you soon bye you"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Normalization Techniques: Batch, Layer, Instance, Group",
        "Video Title":"Batch normalization | What it is and how to implement it",
        "URL":"https:\/\/www.youtube.com\/watch?v=yXOMHOpbon8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/yXOMHOpbon8\/hqdefault.jpg",
        "ID":"yXOMHOpbon8",
        "Publish Time":"2021-11-05T17:00:42Z",
        "Channel":"AssemblyAI",
        "Channel ID":"UCtatfZMf-8EkIwASXM4ts0A",
        "Transcript":"wouldn't it be amazing to have a way of dealing with the unstable gradients problem in our neural networks while also making the network train a little bit faster and also maybe even dealing with the overfitting problem at the same time well if you want that you're in the right place because today we're talking about batch normalization this video is part of the deep learning explained series by assembly ai a company building a state-of-the-art speech to text api and if you want a part of it you can go get their free api token using the link in the description we will first talk about how batch normalization works and how everything works under the hood and then we will talk about some benefits of batch normalization and why we would choose to use it and lastly i will show you how to implement it in python code using keras the first thing that i want to clarify is the definition of normalization so you might have heard in a lot of different places that you need to normalize your input before you feed it to your neural network or you need to standardize your input before you fit it to neural network these terms sometimes are used interchangeably and they're not really strictly defined so but let's define them here just so you know what i mean when i say normalization or standardization normalization is collapsing the input range that you have to be between 0 and 1 whereas standardization is changing your value so that their mean equals to 0 and the variance or standard deviation equals to 1. so on like a little visual what it would look like is let's say if we had some values that go from 0 to 100 let's say they were 20 70 and 90 after we normalize them they're going to be between 0 and 1 so then they'll probably be 0.2 0.7 and 0.9 still keeping the ratio that they had to each other whereas for standardization if we had something similar at the end we're going to change these values so that when we put them in a distribution their mean is going to be at zero and the most amount of values are going to be between -1 and 1 and then as you go further in the distribution you're going to see less and less of these values so why do we need any sort of normalization for our neural networks to begin with so let's take this example let's say we have a neural network and we want to feed data into it our two features are a number of phones that were ever owned by someone and the number of um or the amount of money they've withdrawn from the atm today so as you can see they have very different ranges one of them goes from two to twenty four and the other one goes from zero to a thousand so what's going to happen if you feed your network the unnormalized version of this data is that you're going to make it very hard for your network to learn the optimal weight values that minimizes the cost or the error and in turn you will also cause your network to have weights that are very different than each other so the weight that we are going to multiply the number of phones with is going to be very different than the number of then the weight that we're going to multiply the input that is the amount of money that was withdrawn from the bank account so in turn you might cause your network to also be unstable and in the end have a network that has a vanishing or the exploding gradients problem so what we do to overcome this problem normally is that first of all of course we normalize our inputs and we also try to use the correct weight initial initialization technique and also the correct activation function that goes with this weight initialization technique but even if you do everything correctly unstable gradients problem might come back later in the training but there is one solution that could save the day and that's batch normalization so what we do with batch normalization is instead of only normalizing our inputs and then feeding the data into our network we normalize all the outputs of all the layers in our network so in this diagram you can see we have our network and in each in between each layer we have a batch normalization layer so what the it does is basically normalize our data and do a little bit more of a small trick on top of it and then feed the data or feed the output from the previous layer to the next layer so let's see how that works in this small example let's say we have six data points they go from three to twenty four and we have three five eight nine eleven and twenty four what bash normalization does at first is to standardize them based on what we were talking in the first lesson you can call it normalization two but what it does is to make sure that their mean is zero and their variance is one so it recalculates them and puts them in the correct place but after it does that this is not the end of what batch normalization does it also scales and offsets these values by some amount that is going to be determined based on the training process so as you can see here and this is kind of like the last step the formula from the last step of batch normalization we have the values that have been changed already that have been standardized and on top of these values we multiply them by some value which is called the scale and we also add another value to them which is called the offset these two values are basically trainable parameters we're not going to determine them they're not hyper parameters or anything we're not going to determine them before the training starts these are going to be learned like any other parameter in the network like the weights and the biases so what it would look like if i scale this value these values that i have right now if i multiply them by two if i scale them by two where this is going to basically be multiplying by them by two and what else you can do is to offset them if you want to offset them by 0.5 and then it's going to look like this it's basically going to be sliding them a little bit on the axis that they're on so this is what bash normalization does to kind of find a good transformation a transformation that works for these data points to help the network overcome the unstable gradients problem and in turn it actually makes it train a little bit faster well you might say how does that work there are so many extra calculations that we need to do in between the hidden layers how do we end up having a network that trains faster and you're right what happens is when you're training a network that has batch normalization the epochs take longer every epoch takes a bit longer than it would have if there were no batch normalization but in the end batch normalization helps us achieve the same accuracy that we did without having bash normalization with less epoch so at the end the amount of time that we add because of patch normalization is much less than the amount of time that we save because we added batch normalization and not surprisingly when you can train your network with fewer epochs to achieve the same accuracy that you did without batch normalization you can of course train it a little bit more and maybe even achieve better performance and on top of that because this is a normalization layer if you'd like you do not have to separately normalize your or standardize your inputs before feeding it to your neural network but you can just have a batch normalization layer before your first layer and then effectively your impulse will be normalized so you can keep everything in one neat package so that's another advantage of using mesh normalization and lastly it was seen that batch normalization actually reduces the need for doing regularization if you remember regularization is something we did to deal with overfitting but with batch normalization you don't even have to do that anymore but of course you might need to try this out for your own network and then see if that's actually the case or not but it was shown that it is actually one of the other benefits of batch normalization so that was all that i want to say in terms of what how batch normalization works and the benefits of batch normalization now let's see how we can implement it using keras and python i will show you how it works using the mnist dataset so really classic example of handwritten digits here i'm just importing the libraries that i need and the dataset from keras and this is what the data points look like so this is one example of data it is a 28 to 28 image so that means there are 784 pixels each of these pixels have a value so this this value goes from 0 to 255 and the lower the value the darker the pixel and the higher the value the lighter the pixel so if you look at this example probably this one this dark one here is around like 200 whereas this one is probably 70 and the actual fully white ones are going to be 255. so what we want to do before we feed this data set to our network is to normalize it one way of doing it is basically just dividing all the training values to 200 255 and then effectively you're going to have a network or a data set in your hand that goes where all the values go from zero to one and later you feed that data to your network that you created here and train it as you wish let's look at what our network looks like we basically have one flattened layer that takes a 28 to 28 matrix and then flattens it to be one long list of 784 values and then we have two hidden layers one with 300 neurons the other with 100 neurons and a output layer with 10 neurons so what if i wanted to have batch normalization in here well it's very simple actually all you have to do for batch normalization is to add one layer and this is one of the predetermined or predefined keras layers and that is called batch normalization you just need to put it in between two layers where you want it to be so i can also put it here after the second hidden layer and now my network has batch normalization but as i said if you need to normalize your data and if you're doing it manually you can exchange that to use instead batch normalization and how you're going to do that is basically before you feed the data into the hidden layers you just need to have a batch normalization layer so by doing this after i flatten my input i am putting it through batch normalization so the values that are going to be fed to the first dense layer are going to be normalized so i do not have to do this anymore so this is one advantage of using batch normalization everything in one place you don't have to worry about manually normalizing separately there is one other detail that you should pay attention to while you're implementing bios normalization and that is deciding to put batch normalization before or after an activation function the authors of the original bios normalization paper spoke favorably about this technique of using bash normalization before the activation function but this is something that you might want to try out and decide for yourself if it works for your specific system and specific problem but i'll show you how to do that if you wanted to so basically when you have a dense layer the activation function is already included in here we specify that it needs to be the really activation function but if you wanted to you can have your activation function as a separate layer so if i did this that means then whatever was outputted from the batch normalization layer will be fed through an activation function then i would not need to have an activation function anymore in this layer and i can do that for the second hidden layer too then i would be taking the output of a layer through batch normalization first and then through activation and this is something that people argue that can work and might be better for your network but there is one other detail that we should look into here and that is to usage of bias so if you remember what happens in a dense or layer or a hidden layer is that we get some sort of input from the previous layer right let's call it x and we have our weights so we multiply the input with the weights and then we add a bias to it so when we have an activation function also already built in what we do is we put these values through an activation function and that is the output of our dense layer so if we strip the activation function out of this that means what is going to be fed to the batch normalization layer is going to be the this is going to be this value but what if we what do we do with batch normalization we normalize the values and then we scale them and then we offset them if you remember and of setting is basically the same as bias you just add one value to it so at the end you do not really need your biases anymore you can just train the offset values to find its optimal value inside your neural network rather than all having a bias and an offset so that kind of like helps you have a lower amount of parameters and also helps you train your network attach faster so then all you have to do is inside your dense layer you just say use bias false because you don't want to use any bias here but that's it when it comes to implementing batch normalization it's very simple it's just one extra layer that you can add if you're using keras to build your network uh just realize that you can use it as a normalization layer without the separate manual normalization that you need to do and also make sure that you decide if you want to use it before or after the activation function of your layers thanks for watching and i hope you enjoyed this video if you liked this video don't forget to give us a like and maybe even subscribe because we're going to be here every single week if you have any questions or comments i would love to see that in the comment section also if you'd like to integrate speech to text capabilities to your own projects you can go grab the free api token from assembly ai using the link in the description but for now have a nice day and i'll see you around"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Neural Networks in Deep Learning",
        "Topic":"Normalization Techniques: Batch, Layer, Instance, Group",
        "Video Title":"Lecture 49  Layer, Instance, Group Normalization",
        "URL":"https:\/\/www.youtube.com\/watch?v=NE61nLoM-Fo",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/NE61nLoM-Fo\/hqdefault.jpg",
        "ID":"NE61nLoM-Fo",
        "Publish Time":"2019-11-22T16:52:20Z",
        "Channel":"Santanu Kundu",
        "Channel ID":"UCowYWEkj5XrBbmIsD1QXhDQ",
        "Transcript":"hello welcome back to the nptl certification course on deep learning so last few classes we are talking about the normalization techniques and we have said that these normalization techniques takes care of the covariant shift which is a problem while training off the deep neural network as with CO as the coent Shi is nothing but the change of the distrib bution of the data even if the data belongs to the same class and as the classifi the basically learn from the data distribution so if the data belonging to the same class changes distribution from batch to batch basically the classifier gets confused that what to learn or what is the boundary between different classes so when you talk about this normalization techniques the normalization techniques basically tries to reparameterize the data or repar parameterize the distribution with uh having some standard deviation say gamma and the mean beta where this gamma and beta they are tuned during the back propagation learning of the neural network and this ensures that even if there is a covariate shift of the training date data uh from batch to batch but the covariant shift will be minimal or the shift of distribution from one batch to another batch will be minimal as a result the different layers of the neural network will learn independently of other layers the training will be much more stable and the training will be much more faster so that is the advantage of employing normalization techniques along with your uh gradient descent procedure so that makes gradient descent procedure much more efficient much more fast so the particular normalization techniques that we have discussed in your previous class is the batch normalization technique and we have also seen that uh what is the performance of batch normalization techniques that is with an example network if you employ batch normalization if you and if you don't employ batch normalization then we have seen that without batch normalization the number of iterations required to reach Say More than 70% of the validation accuracy if you don't use batch normalization is more than 30 million in fact it is almost 36 million iterations are required to reach an validation of accuracy More than 70% and this was done this experiment was carried out on uh a particular architecture uh which is the Inception architecture and then we have seen that uh when you use batch normalization uh the number of iterations required to attain the similar performance of More than 70% of validation accuracy it is drastically reduced and in fact from 30 6 million from above 30 million it becomes uh less than 1 million or so so this batch normalization techniques improves the training uh the learning rate uh much more faster and uh you can train the neural network in uh very very less number of iterations so in today's lecture we will talk about the other normalization techniques like we'll talk about layer normalization we'll talk about instance normalization and we'll talk about group normalization now one of the problems with batch normalizations uh operation we'll see later is that the performance highly depends on batch size if you reduce the batch size then uh the performance degrades whereas if the batch size is more the performance is quite acceptable and and the other uh disadvantage with batch normalization is that you cannot normalize until and unless the entire batch is operated on because when you compute the mean and standard deviation you are computed the mean and standard deviation over the number of channels over the features in the same channel computed by the same um convolution canel but on different examples in the same back so as a result I cannot normalize the data until and unless the entire batch is processed and that becomes a difficulty because in some other kind of network for example in recurrent neural network or RNN such b normalization techniques cannot be employed similarly in gradient descent in instead of batch stochastic gradient descent if we simply use stochastic gradient descent where the weight updation is done sample-wise with every training sample you compute the error you compute the error gradient and accordingly you can normalize uh or you can update the weight vectors there your operation has to be done sample by sample so it cannot be done in batch normalization because in batch normalization you have to wait until unless the entire batch of the training samples is operated on so these are the problems which are addressed in uh what is known as uh layer normalization and this can also be addressed in instance normalization so let us see that what this layer normalization is so as we have seen that in case of batch normalization when you go for normalization what you do is you take this training s uh this feature from one batch The Identical feature from another batch The Identical feature map from another batch and so on and then you compute the mean and standard deviation with respect to these feature Maps whereas in case of layer normalization what you do is you take the features or the feature Maps computed by the different care naels convolution care naels over a single example so these are the feature maps that you get from say example one or training uh data one this you get get from tning example two this you are getting from tning example 3 and so on and when you compute mean and standard deviation the mean and standard deviation are computed over these features feature Maps computed from example one similarly these feature Maps computed from example two another mean and standard deviation will be computed here another mean and standard deviation will be computed here another mean and standard deviation will be computed so as a result as we have seen in case of batch normalization that the dimensionality of the mean vector or the dimensionality of the standard deviation Vector is same as the number of channels because for every channel you compute one mean and one standard deviation whereas in case of layer normalization because the mean and standard deviation are computed from all the features all the feature Maps or all the channels else coming out of a single example so here the dimensionality of the MU vector or the dimensionality of Sigma Vector the standard deviation Vector that will be same as the number of channels that you have right so using the same example or example case that we were discussing previously so here what we had is n is the number of training samples per batch and C is the number of channels so here what we are doing is for every training example I'm Computing mean and standard deviation and this computation is done by considering the features of all the channels we are computed from the same example so mathematically it will be put like this that mu n will be computed as these other features features from example n and uh you have Channel I I Channel and J K represents this is the index of the feature in that particular nth example I channel so the do way you compute me is just x n i j k take the summation over i j and K and then you divide that with C into W into H that is the number of total number of data you have within that uh particular layer so this is how you compute mu n similarly you compute Sigma n\u00b2 which is uh the variance over the same data over the same layer and your normalization will be X hat will be x - mu n upon square root of epson + my n sare so as we told before that this Epsilon is a very small positive constant and it is introduced to ensure that I don't come across any situation like division by zero which makes your division unstable so this is how your layer normalization works that in case of layer normalization the way you compute mean and standard deviation is that you get all the channels from a particular tring example and over those channels you compute mean and standard deviation and using those mean and standard deviation you normalize your data that is exact so again over here as in case of batch normalization I have to go for reparameterization that means I have to compute say Yi which is same as some gamma * x i hat plus beta where this G GMA and beta are now new parameters telling you what is the distribution of the data and this gamma and beta are tunable and they can be tuned in the same gradient descent procedure along with the weight vectors of the neural network uh as we have discussed in case of batch normalization technique so you find that in this layer normalization in case of batch normalization I have to wait for all the batches I mean all the samples training samples belonging to the batches to be processed before I can compare the mean and standard deviation of a particular channel so here I'll compute mean and standard deviation of a particular Channel when it is batch normalization but in case of layer normalization I have to compute the mean and standard deviation of the channels for a particular training example so here as I'm not waiting I don't have to wait for processing of all the samples all the training samples in the batch so this can be uh utilized in other uh kind of operations where your batch processing is difficult like in case of recurrent neural network or even simple stochastic gradient descent where the network is tained with every training example right not over batches so this is what is layer normal the other kind of normalization techniques that we have is what is instance normalization so even in case of layer normalization you find that you are Computing mean and standard deviation by considering all the channels that you get out of a training example in case of instance normalization I'll not even consider all the channels of a training example but for every individual channel will be normalized with respect to that Channel's mean and standard deviation so the operation is something like this so here you find that for every channel over here you compute one mean and standard deviation for this channel for this channel you compute another mean standard deviation for this channel you compute another mean and standard deviation similarly what all other cases so for every channel you compute one mean and standard Dev iation so in case of layer normalization all of them taken together gives you one mean and one standard deviation whereas in case of instance normalization for every individual Channel you are Computing mean and standard deviation and with respect to that mean and standard deviation you are normalizing the features in that particular channel so mathematically as we have seen before this operation can be put like this that again I have n number of samples in a batch I have c number of channels and every channel is of width W byh so you find that now mean and standard deviation of every channel is computed over the channel itself right so here if I consider say for example n is equal to 3 and C = 2 that means for third example and second Channel I take third example over here and second channel so I compute the mean and standard deviation for this particular Channel only and this is the expression for that right similarly I compared the variance for with the features within this channel only I don't consider the features belonging to other channels for computing this mean and standard deviation and with respect to that you normalize the features in that particular Channel only so for standard for normalization of the features belonging to different channels I have to compute mean and standard deviation pertaining to that particular Channel only I don't consider any other channel to compute that mean and standard deviation and with respect to that you are normalizing the features and again here you find that the same reparameterization is applicable that is here again I have to compute Yi which is equal to uh gamma * x i hat plus beta where this gamma and beta will be tuned using the back propagation learning algorithm along with other parameters of the network so this is what is instance normalization and we have also talked about other normalization techniques that is known as group normalization so what you do in case of group normalization in case of group normalization the different channels are grouped into different groups say as over here maybe channel number one channel number one two they are put in one group then three and four they'll be put another group five six they'll be put in another group and so on and for each such group of an example you compute the corresponding mean and corresponding standard deviation so similarly for this group you compute a mean and you comput a standard deviation and when you normalize the data you normalize the features belonging to that group only with respect to the corresponding mean and standard deviation so when I compute this mean and standard deviation for this group let me call it a group G5 and this is a group G4 and mean and standard deviation is computed from the features or from the channels in G5 only so when I go for normalization with these feature with this mean and standard deviation we will normalize the features of the channels belonging to group five we'll not use this mean and standard deviation to normalize the features or the channels belonging to group four so again mathematically this group normalization can be put like this so here you find the that U if every channel or the number of groups that you have is G and say C Prime is the number of Channel per group that means your total number of channels now becomes c equal to g into C Prime where G is the total number of groups and C Prime is the number of channels per group so that's that is what has been shown over here uh pictorially so this is what so this is one group this is another group this is another group this is another group and so on so you find that this group every group consists of a number of particular a number of channels corresponding to a particular example and here again when you compute the mean and standard deviation you compute the mean and standard deviation for that group only okay so here you have uh mean and standard deviation for a particular training example and then a particular group as has been shown over here pictorially and then you normalized the features within the corresponding group following the same procedure with respect to the mean that you have computed over the group and with respect to the standard deviation that you have computed over the group of course here again the same process of reparameterization that is uh projecting back the normalized data as y i is equal to gamma * XI hat + beta that is also applicable in this case and this gamma and beta will be updated or tuned using the back propagation learning algorithm along with the parameters of the weight vectors of the network so these are the different normalization techniques that can be employed to take care of the co-variate shift problem now let us see that uh how these uh this is another way of representing what is back NM what is layer Norm what is instance norm and what is group Norm I mean the same figure but put in a three-dimensional form where actually W and H has been put as one dimension in earlier discussion what we have done is we have put it in two Dimension assuming that every cell within that two-dimensional array is a Matrix of w so it depicts the same thing so now to look at the performance of these different normalization techniques you'll find that uh there is an output which is taken from this uh particular paper as given over here by U ukine and climing he so they have given the relative performance of these different normalization techniques uh both on the training data as well as on the validation data so what has been plotted is the training error as well as the validation error so on the left hand side and this was experiment was done on ret 5 and the data set is same as image net and the batch size that was taken was 30 2 that is 32 examples training examples in a batch okay so the training error for different normalization techniques has been shown over here so here you find that during training it is the group normalization techniques which performs uh best among all of them in this particular uh experiment and uh the instance normalization which is over here that performs the work first and uh the batch normalization and layer normalization the performance is somewhere in between when it goes to validation error so validation process is when you train a neural network or train a deep neural network your first step is you have to train the neural network using the training data so all the data set you have to uh put into three different partitions one partition is the training data set another partition is validation data set and another partition is test data set so you train that all these data sets are mutually exclusive so you have to train your network using the training data set and then you see the performance of the trained network using the validation data set so there you get what is known as validation error and it is expected that a network which is properly trained and performance valid ated should perform okay should give uh a satisfactory performance on the tested data set as well so here what is shown is the training performance with respect to training error and the performance with respect to validation error so when it comes to validation error here you find that the batch normalization so far as this particular experimental result is concerned the batch normalization has performed the best group normalization is is uh slightly worse than batch uh batch normalization instance normalization as in case of training error it is performing worst layer normalization is somewhere in between though you find that here your group normalization performs worse that than batch normalization but the group normal gives you other Advantage as has been uh experimented so here you find that the batch normalization versus group normalization for different batch size okay uh so when you perform only batch normalization you find that uh as you vary the batch size the error of percentage varies right so when your batch size is only two uh I mean the batch size has varied from 30 222 and you find that U the error or the batch normalization performs best as we have seen before when your batch size is 32 right when the batch size is two the performance is worse that is the error rate has increased so here you find that what this diagram depicts is and these are the different errors that you get with different EPO that is the stages of training so what this diagram depicts is that the performance of the network with batch normalization depends upon the batch size or the error rate depends upon the batch size on the right hand side what is shown is if I use group normalization then of course as desired we as the training uh proceeds with the number of epo your error rate reduces but the error rate is more or less independent of the batch size that you're using so if your batch size is the two or batch size is four or 8 or even 32 the error date is more or less than so this shows that your group normalization is is independent of the batch size the performance of the group nomation is independent of the batch size whereas performance of the batch normalization depends upon the size of the batch that is also shown with the help of uh another plot so as shown over here you'll find that here you vary the batch size and for different batch size of compute the error so as this diagram again shows that when you go for Group normalization which is this red plot with different batch size the error that remain remains more or less same but when you go for batch batch normalization when your batch size is quite large 32 or so performance of the batch normalization is better than group normalization but as the batch size reduces the performance of batch normal becomes worse than group normal and the performance becomes even worse as you reduce the batch size so these are the performance of uh different normalization techniques that you have discussed and you also find that this uh when you go for normalization the normalization technique not only takes care of the co-variate shift of the training data it also imposes some sort of regularization regularization ation which prevents the network from overfitting we'll discuss about that in details later uh but just how this nor relation techniques prevents uh the overfitting or it imposes some sort of regularization on the network is that we have seen when we have talked about D noising Auto encoder that in order to regularize uh the network or prevent the network from over fitting what you do is you input a noisy data during the training operation here when you go for batch normalization you are Computing the mean and standard deviation over a batch of data which is not the actual mean of mean and standard deviation over the inter set of data or in other words you can say that the mean and standard deviations that you are Computing those are actually noisy mean and standard deviation and because of this noise you are training your neural network with the noisy data considering that the mean and standard deviation that you have computed is noisy so this introduces some sort of regularization as a result the network does not overfit onto the training data so you find that this is a short of side effect I mean normalization is not really meant for regularization but it imposes it gives some short of regularization uh so with this we'll stop here today we'll continue with other topics from our next class thank you"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"Fundamentals of Convolution Operations",
        "Video Title":"But what is a convolution?",
        "URL":"https:\/\/www.youtube.com\/watch?v=KuXjwB4LzSA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/KuXjwB4LzSA\/hqdefault.jpg",
        "ID":"KuXjwB4LzSA",
        "Publish Time":"2022-11-18T16:00:39Z",
        "Channel":"3Blue1Brown",
        "Channel ID":"UCYO_jab_esuFRV4b17AJtAw",
        "Transcript":"Suppose I give you two different lists of numbers, or maybe two different functions, and I ask you to think of all the ways you might combine those two lists to get a new list of numbers, or combine the two functions to get a new function. Maybe one simple way that comes to mind is to simply add them together term by term. Likewise with the functions, you can add all the corresponding outputs. In a similar vein, you could also multiply the two lists term by term and do the same thing with the functions. But there's another kind of combination just as fundamental as both of those, but a lot less commonly discussed, known as a convolution. But unlike the previous two cases, it's not something that's merely inherited from an operation you can do to numbers. It's something genuinely new for the context of lists of numbers or combining functions. They show up all over the place, they are ubiquitous in image processing, it's a core construct in the theory of probability, they're used a lot in solving differential equations, and one context where you've almost certainly seen it, if not by this name, is multiplying two polynomials together. As someone in the business of visual explanations, this is an especially great topic, because the formulaic definition in isolation and without context can look kind of intimidating, but if we take the time to really unpack what it's saying, and before that actually motivate why you would want something like this, it's an incredibly beautiful operation. And I have to admit, I actually learned a little something while putting together the visuals for this project. In the case of convolving two different functions, I was trying to think of different ways you might picture what that could mean, and with one of them I had a little bit of an aha moment for why it is that normal distributions play the role that they do in probability, why it's such a natural shape for a function. But I'm getting ahead of myself, there's a lot of setup for that one. In this video, our primary focus is just going to be on the discrete case, and in particular building up to a very unexpected but very clever algorithm for computing these. And I'll pull out the discussion for the continuous case into a second part. It's very tempting to open up with the image processing examples, since they're visually the most intriguing, but there are a couple bits of finickiness that make the image processing case less representative of convolutions overall, so instead let's kick things off with probability, and in particular one of the simplest examples that I'm sure everyone here has thought about at some point in their life, which is rolling a pair of dice and figuring out the chances of seeing various different sums. And you might say, not a problem, not a problem. Each of your two dice has six different possible outcomes, which gives us a total of 36 distinct possible pairs of outcomes, and if we just look through them all we can count up how many pairs have a given sum. And arranging all the pairs in a grid like this, one pretty nice thing is that all of the pairs that have a constant sum are visible along one of these different diagonals. So simply counting how many exist on each of those diagonals will tell you how likely you are to see a particular sum. And I'd say, very good, very good, but can you think of any other ways that you might visualize the same question? Other images that can come to mind to think of all the distinct pairs that have a given sum? And maybe one of you raises your hand and says, yeah, I've got one. Let's say you picture these two different sets of possibilities each in a row, but you flip around that second row. That way all of the different pairs which add up to seven line up vertically like this. And if we slide that bottom row all the way to the right, then the unique pair that adds up to two, the snake eyes, are the only ones that align, and if I schlunk that over one unit to the right, the pairs which align are the two different pairs that add up to three. And in general, different offset values of this lower array, which remember I had to flip around first, reveal all the distinct pairs that have a given sum. As far as probability questions go, this still isn't especially interesting because all we're doing is counting how many outcomes there are in each of these categories. But that is with the implicit assumption that there's an equal chance for each of these faces to come up. But what if I told you I have a special set of dice that's not uniform? Maybe the blue die has its own set of numbers describing the probabilities for each face coming up, and the red die has its own unique distinct set of numbers. In that case, if you wanted to figure out, say, the probability of seeing a 2, you would multiply the probability that the blue die is a 1 times the probability that the red die is a 1. And for the chances of seeing a 3, you look at the two distinct pairs where that's possible, and again multiply the corresponding probabilities and then add those two products together. Similarly, the chances of seeing a 4 involves multiplying together three different pairs of possibilities and adding them all together. And in the spirit of setting up some formulas, let's name these top probabilities a 1, a 2, a 3, and so on, and name the bottom ones b 1, b 2, b 3, and so on. And in general, this process where we're taking two different arrays of numbers, flipping the second one around, and then lining them up at various different offset values, taking a bunch of pairwise products and adding them up, that's one of the fundamental ways to think about what a convolution is. So just to spell it out a little more exactly, through this process, we just generated probabilities for seeing 2, 3, 4, on and on up to 12, and we got them by mixing together one list of values, a, and another list of values, b. In the lingo, we'd say the convolution of those two sequences gives us this new sequence, the new sequence of 11 values, each of which looks like some sum of pairwise products. If you prefer, another way you could think about the same operation is to first create a table of all the pairwise products, and then add up along all these diagonals. Again, that's a way of mixing together these two sequences of numbers to get us a new sequence of 11 numbers. It's the same operation as the sliding windows thought, just another perspective. Putting a little notation to it, here's how you might see it written down. The convolution of a and b, denoted with this little asterisk, is a new list, and the nth element of that list looks like a sum, and that sum goes over all different pairs of indices, i and j, so that the sum of those indices is equal to n. It's kind of a mouthful, but for example, if n was 6, the pairs we're going over are 1 and 5, 2 and 4, 3 and 3, 4 and 2, 5 and 1, all the different pairs that add up to 6. But honestly, however you write it down, the notation is secondary in importance to the visual you might hold in your head for the process. Here, maybe it helps to do a super simple example, where I might ask you what's the convolution of the list 1, 2, 3 with the list 4, 5, 6. You might picture taking both of these lists, flipping around that second one, and then starting with its lid all the way over to the left. Then the pair of values which align are 1 and 4, multiply them together, and that gives us our first term of our output. Slide that bottom array one unit to the right, the pairs which align are 1, 5 and 2 and 4, multiply those pairs, add them together, and that gives us 13, the next entry in our output. Slide things over once more, and we'll take 1 times 6 plus 2 times 5 plus 3 times 4, which happens to be 28. One more slide and we get 2 times 6 plus 3 times 5, and that gives us 27. And finally the last term will look like 3 times 6. If you'd like, you can pull up whatever your favorite programming language is and your favorite library that includes various numerical operations, and you can confirm I'm not lying to you. If you take the convolution of 1, 2, 3 against 4, 5, 6, this is indeed the result that you'll get. We've seen one case where this is a natural and desirable operation, adding up to probability distributions, and another common example would be a moving average. Imagine you have some long list of numbers, and you take another smaller list of numbers that all add up to 1. In this case, I just have a little list of 5 values and they're all equal to 1 5th. Then if we do this sliding window convolution process, and kind of close our eyes and sweep under the rug what happens at the very beginning of it, once our smaller list of values entirely overlaps with the bigger one, think about what each term in this convolution really means. At each iteration, what you're doing is multiplying each of the values from your data by 1 5th and adding them all together, which is to say you're taking an average of your data inside this little window. Overall, the process gives you a smoothed out version of the original data, and you could modify this starting with a different little list of numbers, and as long as that little list all adds up to 1, you can still interpret it as a moving average. In the example shown here, that moving average would be giving more weight towards the central value. This also results in a smoothed out version of the data. If you do kind of a two-dimensional analog of this, it gives you a fun algorithm for blurring a given image. And I should say the animations I'm about to show are modified from something I originally made for part of a set of lectures I did with the Julia lab at MIT for a certain open courseware class that included an image processing unit. There we did a little bit more to dive into the code behind all of this, so if you're curious, I'll leave you some links. But focusing back on this blurring example, what's going on is I've got this little 3x3 grid of values that's marching along our original image, and if we zoom in, each one of those values is 1 9th, and what I'm doing at each iteration is multiplying each of those values by the corresponding pixel that it sits on top of. And of course in computer science, we think of colors as little vectors of three values, representing the red, green, and blue components. When I multiply all these little values by 1 9th and I add them together, it gives us an average along each color channel, and the corresponding pixel for the image on the right is defined to be that sum. The overall effect, as we do this for every single pixel on the image, is that each one kind of bleeds into all of its neighbors, which gives us a blurrier version than the original. In the lingo, we'd say that the image on the right is a convolution of our original image with a little grid of values. Or more technically, maybe I should say that it's the convolution with a 180 degree rotated version of that little grid of values. Not that it matters when the grid is symmetric, but it's just worth keeping in mind that the definition of a convolution, as inherited from the pure math context, should always invite you to think about flipping around that second array. If we modify this slightly, we can get a much more elegant blurring effect by choosing a different grid of values. In this case, I have a little 5x5 grid, but the distinction is not so much its size. If we zoom in, we notice that the value in the middle is a lot bigger than the value towards the edges. And where this is coming from is they're all sampled from a bell curve, known as a Gaussian distribution. That way, when we multiply all of these values by the corresponding pixel that they're sitting on top of, we're giving a lot more weight to that central pixel, and much less towards the ones out at the edge. And just as before, the corresponding pixel on the right is defined to be this sum. As we do this process for every single pixel, it gives a blurring effect, which much more authentically simulates the notion of putting your lens out of focus, or something like that. But blurring is far from the only thing that you can do with this idea. For instance, take a look at this little grid of values, which involves some positive numbers on the left, and some negative numbers on the right, which I'll color with blue and red respectively. Take a moment to see if you can predict and understand what effect this will have on the final image. So in this case, I'll just be thinking of the image as grayscale instead of colored, so each of the pixels is just represented by one number instead of three. And one thing worth noticing is that as we do this convolution, it's possible to get negative values. For example, at this point here, if we zoom in, the left half of our little grid sits entirely on top of black pixels, which would have a value of zero, but the right half of negative values all sit on top of white pixels, which would have a value of one. So when we multiply corresponding terms and add them together, the results will be very negative. And the way I'm displaying this with the image on the right is to color negative values red and positive values blue. Another thing to notice is that when you're on a patch that's all the same color, everything goes to zero, since the sum of the values in our little grid is zero. This is very different from the previous two examples where the sum of our little grid was one, which let us interpret it as a moving average and hence a blur. All in all, this little process basically detects wherever there's variation in the pixel value as you move from left to right, and so it gives you a kind of way to pick up on all the vertical edges from your image. And similarly, if we rotated that grid around so that it varies as you move from the top to the bottom, this will be picking up on all the horizontal edges, which in the case of our little pie creature image does result in some pretty demonic eyes. This smaller grid, by the way, is often called a kernel, and the beauty here is how just by choosing a different kernel, you can get different image processing effects, not just blurring your edge detection, but also things like sharpening. For those of you who have heard of a convolutional neural network, the idea there is to use data to figure out what the kernels should be in the first place, as determined by whatever the neural network wants to detect. Another thing I should maybe bring up is the length of the output. For something like the moving average example, you might only want to think about the terms when both of the windows fully align with each other. Or in the image processing example, maybe you want the final output to have the same size as the original. Now, convolutions as a pure math operation always produce an array that's bigger than the two arrays that you started with, at least assuming one of them doesn't have a length of one. Just know that in certain computer science contexts, you often want to deliberately truncate that output. Another thing worth highlighting is that in the computer science context, this notion of flipping around that kernel before you let it march across the original often feels really weird and just uncalled for, but again, note that that's what's inherited from the pure math context, where like we saw with the probabilities, it's an incredibly natural thing to do. And actually, I can show you one more pure math example where even the programmers should care about this one, because it opens the doors for a much faster algorithm to compute all of these. To set up what I mean by faster here, let me go back and pull up some Python again, and I'm going to create two different relatively big arrays. Each one will have a hundred thousand random elements in it, and I'm going to assess the runtime, of the convolve function from the NumPy library. And in this case, it runs it for multiple different iterations, tries to find an average, and it looks like, on this computer at least, it averages at 4.87 seconds. By contrast, if I use a different function from the SciPy library called fftConvolve, which is the same thing just implemented differently, that only takes 4.3 milliseconds on average, so three orders of magnitude improvement. And again, even though it flies under a different name, it's giving the same output that the other convolve function does, it's just doing something to go about it in a cleverer way. Remember how with the probability example, I said another way you could think about the convolution was to create this table of all the pairwise products, and then add up those pairwise products along the diagonals. There's of course nothing specific to probability. Any time you're convolving two different lists of numbers, you can think about it this way. Create this kind of multiplication table with all pairwise products, and then each sum along the diagonal corresponds to one of your final outputs. One context where this view is especially natural is when you multiply together two polynomials. For example, let me take the little grid we already have and replace the top terms with 1, 2x, and 3x squared, and replace the other terms with 4, 5x, and 6x squared. Now, think about what it means when we're creating all of these different pairwise products between the two lists. What you're doing is essentially expanding out the full product of the two polynomials I have written down, and then when you add up along the diagonal, that corresponds to collecting all like terms. Which is pretty neat. Expanding a polynomial and collecting like terms is exactly the same process as a convolution. But this allows us to do something that's pretty cool, because think about what we're saying here. We're saying if you take two different functions and you multiply them together, which is a simple pointwise operation, that's the same thing as if you had first extracted the coefficients from each one of those, assuming they're polynomials, and then taken a convolution of those two lists of coefficients. What makes that so interesting is that convolutions feel, in principle, a lot more complicated than simple multiplication. And I don't just mean conceptually they're harder to think about. I mean, computationally, it requires more steps to perform a convolution than it does to perform a pointwise product of two different lists. For example, let's say I gave you two really big polynomials, say each one with a hundred different coefficients. Then if the way you multiply them was to expand out this product, you know, filling in this entire 100 by 100 grid of pairwise products, that would require you to perform 10,000 different products. And then, when you're collecting all the like terms along the diagonals, that's another set of around 10,000 operations. More generally, in the lingo, we'd say the algorithm is O of n squared, meaning for two lists of size n, the way that the number of operations scales is in proportion to the square of n. On the other hand, if I think of two polynomials in terms of their outputs, for example, sampling their values at some handful of inputs, then multiplying them only requires as many operations as the number of samples, since again, it's a pointwise operation. And with polynomials, you only need finitely many samples to be able to recover the coefficients. For example, two outputs are enough to uniquely specify a linear polynomial, three outputs would be enough to uniquely specify a quadratic polynomial, and in general, if you know n distinct outputs, that's enough to uniquely specify a polynomial that has n different coefficients. Or, if you prefer, we could phrase this in the language of systems of equations. Imagine I tell you I have some polynomial, but I don't tell you what the coefficients are. Those are a mystery to you. In our example, you might think of this as the product that we're trying to figure out. And then, suppose I say, I'll just tell you what the outputs of this polynomial would be if you inputted various different inputs, like 0, 1, 2, 3, on and on, and I give you enough so that you have as many equations as you have unknowns. It even happens to be a linear system of equations, so that's nice, and in principle, at least, this should be enough to recover the coefficients. So the rough algorithm outline then would be, whenever you want to convolve two lists of numbers, you treat them like they're coefficients of two polynomials, you sample those polynomials at enough outputs, multiply those samples point-wise, and then solve this system to recover the coefficients as a sneaky backdoor way to find the convolution. And, as I've stated it so far, at least, some of you could rightfully complain, grant, that is an idiotic plan, because, for one thing, just calculating all these samples for one of the polynomials we know already takes on the order of n-squared operations. Not to mention, solving that system is certainly going to be computationally as difficult as just doing the convolution in the first place. So, like, sure, we have this connection between multiplication and convolutions, but all of the complexity happens in translating from one viewpoint to the other. But there is a trick, and those of you who know about Fourier transforms and the FFT algorithm might see where this is going. If you're unfamiliar with those topics, what I'm about to say might seem completely out of the blue. Just know that there are certain paths you could have walked in math that make this more of an expected step. Basically, the idea is that we have a freedom of choice here. If instead of evaluating at some arbitrary set of inputs, like 0, 1, 2, 3, on and on, you choose to evaluate on a very specially selected set of complex numbers, specifically the ones that sit evenly spaced on the unit circle, what are known as the roots of unity, this gives us a friendlier system. The basic idea is that by finding a number where taking its powers falls into this cycling pattern, it means that the system we generate is going to have a lot of redundancy in the different terms that you're calculating, and by being clever about how you leverage that redundancy, you can save yourself a lot of work. This set of outputs that I've written has a special name. It's called the discrete Fourier transform of the coefficients, and if you want to learn more, I actually did another lecture for that same Julia MIT class all about discrete Fourier transforms, and there's also a really excellent video on the channel Reducible talking about the fast Fourier transform, which is an algorithm for computing these more quickly. Also Veritasium recently did a really good video on FFTs, so you've got lots of options. And that fast algorithm really is the point for us. Again, because of all this redundancy, there exists a method to go from the coefficients to all of these outputs, where instead of doing on the order of n squared operations, you do on the order of n times the log of n operations, which is much much better as you scale to big lists. And, importantly, this FFT algorithm goes both ways. It also lets you go from the outputs to the coefficients. So, bringing it all together, let's look back at our algorithm outline. Now we can say, whenever you're given two long lists of numbers, and you want to take their convolution, first compute the fast Fourier transform of each one of them, which, in the back of your mind, you can just think of as treating them like they're the coefficients of a polynomial, and evaluating it at a very specially selected set of points. Then, multiply together the two results that you just got, point-wise, which is nice and fast, and then do an inverse fast Fourier transform, and what that gives you is the sneaky backdoor way to compute the convolution that we were looking for. But this time, it only involves O of n log n operations. That's really cool to me. This very specific context where convolutions show up, multiplying two polynomials, opens the doors for an algorithm that's relevant everywhere else where convolutions might come up. If you want to add probability distributions, do some large image processing, whatever it might be, and I just think that's such a good example of why you should be excited when you see some operation or concept in math show up in a lot of seemingly unrelated areas. If you want a little homework, here's something that's fun to think about. Explain why when you multiply two different numbers, just ordinary multiplication the way we all learn in elementary school, what you're doing is basically a convolution between the digits of those numbers. There's some added steps with carries and the like, but the core step is a convolution. In light of the existence of a fast algorithm, what that means is if you have two very large integers, then there exists a way to find their product that's faster than the method we learn in elementary school, that instead of requiring O of n squared operations, only requires O of n log n, which doesn't even feel like it should be possible. The catch is that before this is actually useful in practice, your numbers would have to be absolutely monstrous. But still, it's cool that such an algorithm exists. And next up, we'll turn our attention to the continuous case with a special focus on probability distributions."
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"Fundamentals of Convolution Operations",
        "Video Title":"What are Convolutional Neural Networks (CNNs)?",
        "URL":"https:\/\/www.youtube.com\/watch?v=QzY57FaENXg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/QzY57FaENXg\/hqdefault.jpg",
        "ID":"QzY57FaENXg",
        "Publish Time":"2021-10-06T19:48:21Z",
        "Channel":"IBM Technology",
        "Channel ID":"UCKWaEZ-_VweaEx1j62do_vQ",
        "Transcript":"OK, pop quiz. What am I drawing? I'm going to make three predictions here. Firstly. You think at your house, you'd be right? Secondly, that that just came pretty easily to you, it was effortless. And thirdly, you're thinking that I'm not much of an artist and you'd be right on all counts there. But how can we look at this set of geometric shapes and think, Oh, how? If you live in a house, I bet it looks nothing like this. Well, that ability to perform object identification that comes so easily to us does not come so easily to a computer, but that is where we can apply something called convolutional neural networks to the problem. Now, a convolutional neural network or a. See, and and. Is a area of deep learning that specializes in pattern recognition. My name is Martin Keane, and I work in the IBM garage at IBM. Now let's take a look at how CNN works at a high level. Well, let's break it down. CNN convolutional neural network Well, let's start with the artificial neural network part. This is a standard network that consists of multiple layers that are interconnected, and each layer receives some input. Transforms that input to something else and passes an output to the next layer, that's how neural networks work and see an end is a particular part of the neural network or a section of layers that say it's these three layers here and within these layers, we have something called filters. And it's the filters that perform the pattern recognition that CNN is so good at. So let's apply this to our house example now. If this house were an actual image, it would be a series of pixels, just like any image. And if we zoom in on a particular part of this house, let's say we zoom in around here, then we would get, well, the window. And what we're saying here is that a window consists of some perfectly straight lines. Almost perfectly straight lines. But, you know, a window doesn't need to look like that window could equally look like this, and we would still say it was a window. The cool thing about CNN is that using filters. CNN could also say that these two objects represent the same thing. The way they do that, then, is through the application of these filters. So let's take a look at how that works. Now, a filter is basically just a three by three block. And within that block, we can specify a pattern to look for. So we could say, let's look for. Pattern like this, a right angle in our image. So what we do is we take this filter and it's a three by three block here. We will analyze the equivalent three by three block up here as well. So. We'll look at first of all, these first. Group of three by three pixels, and we will see how close are they to the filter shape? And we'll get that numeric score, then we will move across one, come to the right and look at the next three by three block of pixels and score how close they are to the filter shape. And we will continue to slide over or vote over all of these pixel layers until we have not every three by three block. Now, that's just for one filter. But what that will give us is an array of numbers that say how closely and the image matches filter, but we can add more filters so we could add another three by three filter here. And perhaps this one looks for a shape like this. And we could add a third filter here, and perhaps this looks for a different kind of right angle shape. If we take the numeric arrays from all of these filters and combine them together in a process called pooling, then we have a much better understanding of what is contained within this series of pixels. Now that's just the first layer of the CNN. And as we go deeper into the neural network, the filters become more abstract all they can do more. So the second layer of filters perhaps can perform tasks like basic object recognition. So we can have filters here that might be able to recognize the presence of a window or the presence of a door or the presence of a roof. And as we go deeper into the sea and into the next leg, well, maybe these filters can perform even more abstract tasks, like being able to determine whether we're looking at a house or we're looking at an apartment or whether we're looking at a skyscraper. So you can see the application of these filters increases as we go through the network and can perform more and more tasks. And that's a very high level basic overview of what CNN is. It has a ton of business applications. Think of OCR, for example, for understanding handwritten documents. Think of visual recognition and facial detection and visual search. Think of medical imagery and looking at that and determining what is being shown in an imaging scan. And even think of the fact that we can apply a CNN to perform object identification for. Body drawn houses, if you have any questions, please drop us a line below, and if you want to see more videos like this in the future, please like and subscribe. Thanks for watching."
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"Fundamentals of Convolution Operations",
        "Video Title":"2D Convolution Explained: Fundamental Operation in Computer Vision",
        "URL":"https:\/\/www.youtube.com\/watch?v=yb2tPt0QVPY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/yb2tPt0QVPY\/hqdefault.jpg",
        "ID":"yb2tPt0QVPY",
        "Publish Time":"2023-05-15T14:03:42Z",
        "Channel":"LearnOpenCV",
        "Channel ID":"UCeoXXZ2u_cUQ6CJ-oXpss9w",
        "Transcript":"Hi everybody, in this video we're going to cover one of the most fundamental operations used in Computer Vision which is 2D Convolution. Hi there and Welcome to LearnOpenCV. The convolution operation starts with the concept of a filter which is also referred to as a Kernel, usually three by three or sometimes five by five which is used to process an input image and transform that image to something that might be more useful for downstream processing. In this example we show how a six by six input is convolved with a three by three Kernel. Here we use a well-known Kernel often used in image processing called a Sobel Kernel which is designed to detect vertical edges. The Convolution operation consists of placing the filter over a portion of the input and multiplying the elements of the filter with the corresponding elements of the input. The resulting value is a single number representing the output of the convolution operation for a given filter location. Sliding the filter one pixel at a time corresponds to a stride of one. The filter location is shown in dark blue and this region is also referred to as the Receptive Field. Notice that the center of the filter is indicated in green the convolution operation performed at each filter location is simply the dot product of the filter values with the corresponding values in the receptive field in the input data. A sample calculation is provided for the first two filter locations so you can confirm your understanding of the operation. Notice that at each filter location, the operation produces a single number placed at the corresponding location in the output. In other words the location and the output corresponds to the center of the receptive field overlaid on the input slice. Notice that the output from the convolution operation has a smaller spatial size than the input slice. This has to do with how the filter is placed over the input data so that it does not extend beyond the boundary of the input. However padding techniques are often used to pad the input image so that the output is the same size as the input. Using a stride larger than one will also reduce the size of the output from the convolution operation. Before we continue, we wanted to give you a more concrete example of how specific Kernels can detect basic structures in an image This is a really nice website that allows you to interactively experiment with various Kernels using a pixelated input image so that you can appreciate the computations involved in the convolution operation and the associated output shown to the right. As you can see when the Kernel moves from left to right across the image it's able to detect vertical edges in the input image. Other Kernels can also be used to detect different types of edges for example horizontal edges or diagonal edges. So that's an example of how 2D Convolution is used in image processing but this concept has been extended and generalized for use in Neural Networks and specifically Convolutional Neural Networks in which there are special layers within those networks that perform the convolution operation on not just the input images but intermediate data throughout the network. In a Convolutional Neural Network, there's one important distinction and that is that the values of the Kernel are learned by the network during the training process. So rather than trying to engineer what Kernel to use for a specific problem, a CNN will automatically learn those values during the training process, but regardless of this distinction the convolution operation itself is the same. So let's have a look at this in more detail. In this example here we're showing how a single filter is used to process a grayscale image as the input to a Convolutional Neural Network and notice that the weights in the filter which are the elements of the Kernel are not fixed but are shown as variable meaning that those values are learned during the training process and therefore, CNNs have the ability to detect many different types of features automatically. In other words the network is incentivized to come up with Kernels that extract meaningful features whatever those may be. So if you're interested in learning more about Convolutional Neural Networks, we have an introductory video on that topic that we've provided a link to here, which is part of our TensorFlow and Keras Getting Started Series. So you might want to take a look at that to get a more comprehensive understanding of how the convolution operation is used within Neural Networks. So that's all we wanted to cover in this video on the convolution operation. If you found this video helpful, please LIKE and SHARE the video and SUBSCRIBE to our Channel to get more content on Computer Vision, Deep Learning, and AI."
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"Fundamentals of Convolution Operations",
        "Video Title":"Tutorial 21- What is Convolution operation in CNN?",
        "URL":"https:\/\/www.youtube.com\/watch?v=Etksi-F5ug8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Etksi-F5ug8\/hqdefault.jpg",
        "ID":"Etksi-F5ug8",
        "Publish Time":"2019-08-20T15:49:46Z",
        "Channel":"Krish Naik",
        "Channel ID":"UCNU_lfiiWBdtULKOw6X0Dig",
        "Transcript":""
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"CNN Building Blocks: Pooling, Stride, Padding",
        "Video Title":"Convolutional Neural Networks | CNN | Kernel | Stride | Padding | Pooling | Flatten | Formula",
        "URL":"https:\/\/www.youtube.com\/watch?v=Y1qxI-Df4Lk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Y1qxI-Df4Lk\/hqdefault.jpg",
        "ID":"Y1qxI-Df4Lk",
        "Publish Time":"2020-06-26T17:15:37Z",
        "Channel":"Binod Suman Academy",
        "Channel ID":"UCIZ3CSNL-8mBHfFZLic1-aQ",
        "Transcript":""
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"CNN Building Blocks: Pooling, Stride, Padding",
        "Video Title":"Module 3 video 3 CNN Building blocks, Padding, Stride, Pooling, Channels",
        "URL":"https:\/\/www.youtube.com\/watch?v=sA53swEwpoA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/sA53swEwpoA\/hqdefault.jpg",
        "ID":"sA53swEwpoA",
        "Publish Time":"2021-10-10T09:15:09Z",
        "Channel":"Vipul Mishra",
        "Channel ID":"UCvnJSBrGc-8b558x-WbnErw",
        "Transcript":"hello everyone so in the last video we have seen the first building block of convolutional neural network that is the convolution operation now in this video we will see other important building block of convolution neural network such as padding channels stride so we'll see all these building blocks in this video so let's go into the detail so the first building block is the padding so what is the important part here so suppose we'll have this green part as input so you can say the input is 5 cross 5 so you can say 1 2 3 4 5 and 5 in this side so 5 cross 5 input that is there now we have used one padding padding means you are added extra zeros all around your original input then you applied this kernel that is three cross three kernel once you applied this kernel what you will get as a output you will get again five cross five matrix now you can say why this padding is important so as we have seen in the previous convolution video if we will not apply this padding then the output size will reduce down and suppose we have very depth network or we have multi-layer network and we have not applied any padding then at the end will not get any output because everything will be absorbed by the convolution operation so that is one reason why you want to preserve the size of the input in the output as well then you will apply the padding that is one very important reason now there is another reason as well while we are using the padding so to understand that reason let's go into the next video now just watch how many times will visit your kernel in this place so you can say your first place how many times the colonel will visit in second place and how many times colonel visit in third place we just watch this and count it so we have seen in the first place your kernel is visited only once now in the second place your kernel is visited four times but in the third place your kernel is visited nine times so the important point to notice whatever it is in the center your kernel will extract features from that point more exhaustively or you can see more features will be extracted from the center as compared into the corner but in a particular image there is a possibility your important features are in the corner as well so in order to preserve these corners or you can say in order to extract these corner features you are adding the padding here right so that if you add a padding on the top of it it will visit at least four times in the corner as well so let's take one example suppose we are making application for attendance in a classroom so the person who are in the corner they also need to be mark as a present but if we are not applying the padding then it is possible that they will not mark as a present because your kernel is not visited that place so in that way you can say writing is important while you have important information in the corner so strike that information writing is important right so once you do the padding how exactly the output will be appear so the output will be appear in the form of this so you can say the size of output will be n plus 2 f minus f plus 1 into n plus 2 p minus f plus 1 so here p is the padding how much padding we have applied so if you applied padding here 1 so based on that you will get the output here now while you are doing the coding that will see in the exercise as well if you want to apply the padding you will use the same as a keyword by adding equal to same so it will system your program will automatically recognize okay you want to apply the padding so you'll apply the appropriate padding in the input so that you will get the same size of output here and if you are saying you do not want to apply the padding so you will say padding equal to valid so it will not apply any padding over there so in that way you can utilize the patterns first thing you utilize the padding for preserving the size and second whenever you have important information in the corner then padding is really helpful for you now the next important building block is the stride now in case of stride so previously in the previous slide we have seen the jump is only one time here right so in suppose i have a very big image in hand in that case if you jump one time only while you are doing the convolution then it will take longer to compute or to go through the entire image so it is possible that instead of jumping one you are jumping more and then you apply the convolution operation that is the jump is controlled by the stride so in general we have a stride one then one jump in the horizontal then one jump into the vertical direction but if you want to jump more then accordingly you will apply the stride so in this case suppose stride is three so in that case what will happen in the first part your kernel will come into this point then in the next part it will jump three times so once it jump three times your kernel will come here right then similarly in the vertical direction it will jump three times the kernel will come here and then next the kernel will come here since in that way your output will have four values so you can say your output has minus 5 then 8 then minus 3 and minus 16. so that is the way how you can control the jump so in order to control the jump you are using using the stride so once you applied this stride your formula to compute the final output value will change and it will become floor value of n plus 2 p minus f divided by s plus 1 cross same n plus 2 p minus f divided by s plus 1 so this will give you the final output dimension what will be the dimension then fourth building block is the channel so what we are looking from the channel so you can see as we have seen if you have a kernel and here we have three channels so one is the red channel second is the green channel and third is the blue channel now your kernel or you can see your filter will also have three channels so what it will do the first channel will go on the top of here the second channel will go on the green and third channel will go on the blue then first channel will multiply here second general multiply in the green third channel multiply into the blue and then sum all those 27 multiplications and you'll get one value here then similarly it will move one direction in the in the horizontal side and then again the top corner will go to the first second canal will go to the second third kernel will go to the third or third channel will go to the third and then you will get the second value so in that way you will get all the values now suppose instead of extracting so this kernel will extract one type of feature now i want to extract multiple variety of features then what we need to do we have to take multiple kernels here but every kernel have similar channels as the input so in this example we have three channel input so in that case your kernel will also have three channel right now suppose we have in in in our example we have n channel as input then this kernel will also have n channel to take care those inputs right now this will produce so one kernel will produce one channel output so this one channel is looking for one kind of feature now what i want i want to extract multiple features from this input so then what we will do we will add more kernels here so this kernel will apply here and you will get one feature then second kernel will apply here you will get second feature so it means once you apply two kernels your output have two channels so in that way you can see the channel concept so important part whatever is here so you can say this is n value so this will also have n value right and if you have number two here so then the output will be four cross four cross two two means one is coming from first kernel and second is coming from second kernel so you have two channel as a output similarly if you have n kernels here so then what will happen every kernel have n depth and that so in this case the output will have 4 cross 4 cross m because you have m such kernels that is extracting m different variety of features so that is the way how you can see the channel and kernel concept right so in in so you can say more descriptive way you have one input here you have two kernels here then you will apply the activation function at the bias and you will get the final value now last and important concept is the pooling so we have seen like we have a bigger value or we can say bigger set of output produced here now what pooling is looking pooling is looking for most important feature from a bigger pool so you can say from a bigger pool i want to select one value that is the concept of pooling so suppose here you have pulling function the size is 2 cross 2 so what will happen it will come here and 2 cross 2 and will pull the biggest value and once you pull the biggest value it will say it is the max pool so what concept is saying that is saying like you are picking the most important features from one set and how you define this most important features which have the highest activation value means for a particular input if your impact is high it means your activation is also high so then you are picking that value in the next layer of output similarly if you have a stride two so your pool will come here and then you will pick the biggest value then again stride two you will pick here you will get the biggest value then again straight here to two straight strike two so pull from here now it is possible like instead of two stride if you have one star then what will happen then you will have more numbers here right so in that case what you what will be there so let me erase suppose you have stride equal to 1 so then and your pole size is 2 cross 2 so you will first pick this your the value will be 8 then pick this then your value will be 9 then you will pick this your value will be again nine so the value will be eight nine nine now in the second you will pick this so value will be nine again you will pick this the value will be nine and again you will pick this will be another nine now in the third and last you will pick this value will be nine then you will pick this value will be nine and once you'll be this value will be four so in that way once a stride is 1 your value will be this from the pooling so pulling is also traverse in the same way as kernels are moving but instead of performing any multiplication addition it will just pick the maximum value in case of max pool but so if there is another variety of pooling that is the average pooling so instead of picking maximum you are taking the average of all the values here and you are picking one value so that is known as the average pulley so these are the most important building blocks of any convolution neural network so you can say these big networks are made from these building blocks so in the next video we'll talk about how we can make bigger network from these building blocks thank you"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"CNN Building Blocks: Pooling, Stride, Padding",
        "Video Title":"No More Strided Convolutions or Pooling: A New CNN Building Block for Low-Res Images &amp; Small Objects",
        "URL":"https:\/\/www.youtube.com\/watch?v=UlJQZFMdUEk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/UlJQZFMdUEk\/hqdefault.jpg",
        "ID":"UlJQZFMdUEk",
        "Publish Time":"2022-10-28T18:19:50Z",
        "Channel":"Tony T. Luo",
        "Channel ID":"UCCYPFDdVLhXwrBRUnjU2BEA",
        "Transcript":"hi everyone I'm Raja I'm doing my Master's in computer science and then basically in this talk I'll be presenting about our work on uh titled no more uh stated convolutions or polling in new uh building block for lower resolution images and small objects so this work is done along with my uh along with Thai Leo all paper can be found in using the QR code and then all our code is uh on GitHub repository it's like so the outline of this talk is first I'll be talking about what is the motivation behind our work and the next thing is what are the key observations and also uh our new building block called SPD con and then use how to use our SPD Con in to computer vision representative tasks like image classification and object detection and then we'll be going over training and then experiments and and conclusion so the motivation of our work is we all know that the CNN classification accuracy drops significantly when we train and then test it on Lower resolution images and then the second motivation is the object detection models suffer large MEP laws on small objects so like when we look at state of the art architectures like YOLO V5 and efficient debt so we see that there is a large gap between AP that is computed on large objects with ap that is computed on small objects and then the the right bottom figure if you see the The Mask rcnn model basically failed to detect this extremely small objects like these birds and then like our the key observations that lead to build and use even CNN build blog are the the usage of the stated convolutions and the polling basically sacrifices the feature map granularity when we do the down sampling and then the second observation is in most of the common uh settings of CNN architectures where we use three by three kernel with side equal to two there is an asymmetric sampling between even and odd rows or columns and then that results in a biased feature learning and also say that we use a 2x2 filter with stride equal to 2 then every pixel uh in feature map is sample only one time and then that can lead to that can lead object detection models you know not to detect the very small objects so these two are the key observations and then so that is the reason why like we built a new uh building block called spdcon basically this we wanted to eliminate the usage of cited convolutions and pooling and then replace with our building block called spdcon so the our uh build SPD con basically comprises of a space to depth layer and followed by a non-stated convolutional layer so space to depth is basically it is proposed before uh to uh the transform area image basically let's say that we have a three Channel image of resolution height by width so we we store all this information along the channel Dimension by reducing the image size but it method we use the two down sample like we use the same to down sample in the future map so let's say that we have a feature map of resolution s by S by C1 number of channels so after applying SPD layer we bring down the spatial resolution to S by scale s by scale but increasing the channel Dimension by a factor of scale over two so after that we apply a non-stated convolutional layer with filter size of three by three uh and unstyle equal to 1. so this is the building block uh in the in the indep in a pictorial way so let's say that we wanted to reduce a feature map resolution of s by S of C1 into s by 2 by S by 2 by C2 so we apply space to depth transformation to get four of feature Maps after that we concatenate these four feature Maps along the channel dimension and then in the next step we apply a convolutional filter of size three by three and then stride equal to one so this is the the architecture for our blog and then this block is used uh to uh to reduce spatial resolution by a factor of two and then to use our SPD con so let's say that we wanted to use it in object detection models so we took YOLO V5 architecture the state of the art architecture and replaced all these type 2 convolutions with our with our SPD con block similarly for image classification we took the resonate 18 and the resonate 50 architecture and we replaced all the strike to convolutions with the SVD con block so this is the detailed architecture after replacing the strike two convolutions with the with SPD layer so the YOLO V5 architecture consists of five type convolutions in the backbone and then two strike two convolutions in the neck architecture so we replace the strike two convolutions with our SPD and then con block so this is the architecture obtained after incorporating our the blog similarly we take resonate 18 and resonate 50 architecture and we replace the stripe to convolutions with SPD con block so in the register in the recent so in the skip connections as well there are just type two convolutions so we replace with the SPD Con in this connections as well so these are the uh this is the training setup so we follow standard trading setup for object detection like we use the Coco 2017 data set that is containing uh 118k trading images and 5K validation images under for Ticket test images and then for hybrid parameters we directly adopted from YOLO V5 paper you will be file repository without retuning and to compute the evaluation metric we use both validation and then test deficit and the different ioe threshold thresholds and also different object skills small medium and large so uh like similarly YOLO V5 had like uh it's four scale of models Nano small model media model and large models so we are we also use the same depth factor and with factor to obtain YOLO V5 SPD and S and M and L so if you look at these uh plots in the bottom so we see that uh we out using the SPD block so we out uh outperform both APS the outperform Nano small and medium models on APS similarly when we compute APS on Large Scale Models the our approach of using SPD outperforms the state of the art architectures and then these are all the results that are evaluated on validation data set and test div data set if you look at the performance on APS so we there is a consistent Improvement of like approximately 13 percent on the Nano model under 11 on small model and 8.61 medium and then 1.8 on launch model so there is a consistent Improvement on APS and also similarly the same uh is observed on the test data set as well this is a consistent Improvement of accuracy on AP small then these are some of the visual results so if we look at this uh the left top figure so here the these are the predictions made by the YOLO V5 model and the right one is a prediction made by YOLO V5 with SPD block if you see that this occluded giraffe is missed by YOLO V5 M prediction but but with SPD we are able to detect the object similarly in the bottom two figures so there is a phase that is not detected by YOLO V5 and also to paint uh objects here similarly for image classification training we took the the lower resolution images like tiny imagenet data set that is containing 500 training 50 validation and 50 tests for each class and we also took C for 10 data set containing 50 000 training and 10 000 test images and then for tuning the hyper parameters we use a random search approach and then the learning rate Decay is directly adopted from original people so similarly on the image classification we see the Improvement in accuracy so so resonant 18 after replacing with our SPD block was able to have a good Improvement in accuracy similarly the resonant 50 SPD on c410 has a Improvement in accuracy so these are the visual results on the image classification tasks so these are eight images that are misclassified by resonate 18 model but they are correctly classified by resonate 18 SPD model so basically this is the this is the two label the green one and then the blue one is the prediction that is made by resonate 18 SPD and then the red one is the prediction made by resonant 18 architecture so to conclude our work so we basically identified a defect 2 and a common design in CNN architecture that is the usage of cited convolutions and polling and to eliminate that we proposed a new building block called spdcon to replace them and we do down sampling feature Maps while retaining all the granularity of feature Maps and our proposed approach is a generic and unified that it can be applicable to any CNN architectures where we use static convolutions or pooling and we showed its Superior performance on two computer vision tasks like object detection and image classification thanks that's all thanks for the presentation there is one question in the audience [Music] hello so have you tested your convolution for aggression in the average Precision of actual big objects on on big big objects yeah is there a regression of the metrics for the big objects because you apparently improved toward the small object detection but yes so in uh when we tested on the large objects APL as well so performance has improved a bit yes improved as well nice yeah thank you for the presentation"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"CNN Building Blocks: Pooling, Stride, Padding",
        "Video Title":"Operations in Convolutional Neural Networks | Convolution, Pooling and Fully Connected Layer",
        "URL":"https:\/\/www.youtube.com\/watch?v=9zzZZjYkLV4",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/9zzZZjYkLV4\/hqdefault.jpg",
        "ID":"9zzZZjYkLV4",
        "Publish Time":"2023-10-24T06:30:03Z",
        "Channel":"UncomplicatingTech",
        "Channel ID":"UCkZEo1r6Hsc5yIlW3jfW0KQ",
        "Transcript":""
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"Popular CNN Architectures: AlexNet, ResNet, EfficientNet",
        "Video Title":"ResNet (actually) explained in under 10 minutes",
        "URL":"https:\/\/www.youtube.com\/watch?v=o_3mboe1jYI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/o_3mboe1jYI\/hqdefault.jpg",
        "ID":"o_3mboe1jYI",
        "Publish Time":"2022-10-24T03:54:18Z",
        "Channel":"rupert ai",
        "Channel ID":"UCgLgHT0PrS6EcsI37XPWHHw",
        "Transcript":"foreign [Music] I want you to imagine approximating a function parameterized by a deep neural network in this example we're going to pass our Network an up sample low resolution input image and pass it through each of the network layers we want the network to Output the input image but now in a high resolution a task commonly known as super resolution unfortunately in practice after training our Network on high and low resolution image pairs somehow our network is spitting out images they're even worse than our input after putting all your effort into a beautifully deep architecture you are horrified to see that instead of going down your training loss shoots endlessly upwards your classmates and colleagues can't help but laugh and seemingly counter-intuitive because now the model has more parameters now how can we address this and get your loss going in the right direction [Applause] this problem partly comes down to the fact that we have an input signal that is being lost the deeper and deeper we go into our Network as the signals pass through each of the non-linear functions at each layer look at what can happen to a training signal even after being passed through a single relay function the most popular activation function for neural networks essentially you ask in the network to do two things one is to retain the input signal and the second is to find out what needs to be added to the input image to transform it from a low to high resolution image instead let's look at the problem from a different angle let's first minus the low and high resolution image from one another this gives us what is known as a residual image or the difference between the two images now let's reshift this equation to get our intended output on the right hand side now given we already have the low resolution image at training time let's now just get our Network to learn the only bit we actually care about the residual framing the problem in this way makes the Network's life easier as it doesn't need to retain the entire input signal this was the same intuition that inspired the authors from the 2015 paper deep residual learning for image recognition this paper is now considered seminal in relation to deep learning with over 130 000 citations it is rare to run into a model architecture in deep learning today that doesn't utilize the contributions from this paper and some fashion in the previous example I gave you an easy and intuitive introduction to residuals let's have another look at the layers of a neural network I chose to present residual connections to you using the example of super resolution as it can be visualized very easily by simply adding the input onto the output we can instead learn the mapping to the residual image as you can see here however this approach I've shown you so far has two major problems when generalizing to other tasks the first problem is where we have a task where the input and outputs don't share the same dimensionality for example an image classification where you take an image input and map it to a single class label how would you meaningfully add the inputs and outputs in this scenario the second problem is how the input signal is propagated throughout the network let's consider the midpoint of our super resolution Network at this point no matter what our input or output is it is still easy for the network to lose the training signal this signal is an important piece of information that would be useful for the network to have access to in order to remedy both these problems we can add what are known as residual connections all the way along our Network this not only boosts input signals all the way along the network but also makes it easier to submit inputs and outputs as feature dimensionality is adjusted on the go we can now also view the network as a series of residual blocks instead of a series of independent layers most importantly now the network has the option to not fully utilize all the blocks since it is easy for each block to Output the identity function and take no penalty in relation to the loss function this opens the doors to training extremely deep Networks now let's have a deeper look at the main idea I introduced here the residual block foreign so what exactly was the resnet block they proposed in the original paper let's go through it step by step firstly we pass our inputs through a 3X3 convolutional layer with a stride of one and padding one these parameters mean that our output features will have the same dimensionality as our input we then apply batch Norm to renormalize these features and pass them through an activation function such as relu we then pass the features through a second convolutional layer exactly the same as the first and again followed by a batch Norm at this stage we just have a normal vanilla neural network so let's now add a residual connection we can do this by simply adding the Block's inputs onto the current set of features we do this element wise as our inputs and features share the same dimensionality remember this is only because we have carefully chosen our convolutional parameters however for tasks such as image classification we do actually want to reduce our dimensionality throughout the network more on that in a moment finally we pass our features through a final activation function now that is essentially it it really is quite a simple idea now let's have a quick look at the official Pi torch code implementation for a resnet blocks forward pass and consolidate what we've just learned we start with an input tensor X and save a copy of this as our identity function we can use later we then pass our input through a set of convolutional batch norm and activation layers down sampling the features if required more on that at a moment when we discussed Dimension matching we can then simply add our saved identity features to our current set of features in the network this is done element wise finally we pass this through a final activation function and return this as the output of our residual block note that some of these choices are arbitrary such as applying the activation function after adding the identity function this is simply done because this is what the authors found to give the best results when performing a residual connection we must ensure that the dimensions match such that we can do element wise Edition in the original paper they choose to reduce dimensionality every few residual blocks as their end goal is image classification where you go from a high dimensional input to a low dimensional output the authors decided to reduce Dimensions by halving the height and the width of their current set of features to keep the computational requirements of each part of the network consistent they also increase the number of channels every time they half the height and the width this leaves us with potentially two scenarios of mismatched dimensions firstly where the height and the width don't match and secondly where the channels don't match could be either one of these or a combination of the two let's have another look at the resonate block and understand how the network can downsample features let's have a look at the first convolution which I told you earlier had a stride of one and padding of one to keep input feature dimensionality the same as the output the authors propose to down samples features directly by occasionally altering this convolutional layer to have a stride of two this produces features with half the height and half the width when the authors down sampled in this fashion they also double the number of convolutional filters which in turn doubles the number of channels in the output features this is where we have a problem with Dimension matching as our input that is sent through our residual connection does not have the same dimensions as the features coming through our Network let's now have a look at our input features coming through our residual connection and see what options are available to us when addressing this Dimension mismatch the authors proposed two solutions firstly they propose to match the number of feature channels by zero padding this option has the benefit of introducing no new parameters into the model this is simply done by filling out half the features with zeros although no parameters are added we are now wasting computation on meaningless features full of zeros the second solution is to match the number of channels by passing over our input features with a one by one convolution this of course adds extra parameters that means that our output features only contain real information so for example if our input features have three channels we would now have six one by one convolutional filters to double the number of channels in our output space for both options a stride of two is again used this means that our output feature maps have half the width half the height and double the channels this means that they exactly match our current features in the network essentially these two options are very similar they both skip over every other pixel in the input features the main difference is whether we Zero part our output or use the one by one convolutional option to match the number of channels the authors found that the one by one convolutional option led to the best results hey guys it's Rupert thanks for watching the video I hope you now have an intuitive understanding of residual networks the main idea is that you can now train your networks deeper and deeper whilst keeping training stable please don't forget to hit that like And subscribe button for more machine learning videos and let me know in the comment section down below what you want to see in the next video"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"Popular CNN Architectures: AlexNet, ResNet, EfficientNet",
        "Video Title":"Convolutional Neural Networks-AlexNet,VGG-16,Google net,Mobile net,ResNet,SquuezeNet, LeNet5#cnn#ai",
        "URL":"https:\/\/www.youtube.com\/watch?v=UfLPGzNc1Z8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/UfLPGzNc1Z8\/hqdefault.jpg",
        "ID":"UfLPGzNc1Z8",
        "Publish Time":"2024-07-20T09:57:34Z",
        "Channel":"Data Science Made Easy",
        "Channel ID":"UCZ65jHnuZd8QxiutPthY-zQ",
        "Transcript":"in the world of artificial intelligence convolutional neural networks have revolutionized the way we perceive and interact with the world around us and these powerful models have paved the way for breakthrough Innovations in various industries from media to law entertainment to retail so today we will take a deep dive into the most influential and widely used CNN architectures which include alexnet w16 Google net mobile net dress net squeez net and leet 5 taking the real world applications so so let's get started with alexnet taking an example from the law sector so basically this alexnet is a type of neural network or a machine learning algorithm used for image recognition and it was first introduced in 2012 and was a breakthrough in the field of deep learning this neural network is like a brain with many layers that process information to make a decision so this alexnet had five layers and was able to recognize images of different classes such as dogs cats and words by learning patterns in the images in the legal domain alexnet could be used to analyze images of legal documents such as contracts or code transcripts for example alexnet could be used to classify different types of legal documents such as contracts Deeds or Wills by recognizing patterns in the text and layout of the document and this could help legal professionals quickly identify the type of document they are dealing with and streamline their workflow to summarize this we can say Alex net is a type of neural network used for image recognition it can be used in the legal domain to analyze images of legal documents and classify them into different categories next comes rest net and tourism now this res net is a type of neural network that is used for image recognition it is designed to overcome the vanishing gradient problem which occurs when the gradient of the loss function becomes very small as it propagates through many layers in the deep neural network and this can make it difficult for the network to learn from the data and make accurate predictions now resnet uses a technique called residual connection to bypass some layers and allow the gradient to flow more easily through the network this allows rest net to have many more layers than traditional neural networks while still being able to learn from the data effectively now in the tourism domain rest net could be used to analyze images of tourist destination such as beaches mountains or historical sites for example rest net could be used to classify images of different tourist destination into different categories such as beaches mountains or cities and this could help travel companies and tourism boards to Target the marketing efforts more effectively and make personalized recommendations to Travelers based on their interest and preferences so to summarize this we can say resnet is a type of neural network used for image recognition it is designed to overcome the venish ing gradient problem by using residual Connections in the tourism domain it can be used to classify images of tourist destinations and help travel companies and tourism boards to improve their marketing efforts next comes v-16 and entertainment now this vj1 16 is a type of convolutional neural network used for image recognition it was developed by the visual geometry group at the University of Oxford in 2014 now this CN n is similar to a regular neural network but it is specifically designed to work with images it uses a series of convolutional layers to process and analyze the image making it more efficient at recognizing patterns and features within the images now this vg6 has 16 layers which means it can analyze an image in more detail and make more accurate predictions it has been widely used in various image recognition tasks such as image classification object detection and image segmentation now in the entertainment domain V16 could be used to analyze images of movies TV shows or music artist for example V1 16 could be used to classify images of different movies into different categories such as action drama or comedy this could help entertainment companies and streamline platforms to recommend movies to users based on their interest and preferences so to summarize this we can say V16 is a type of convolutional neural network used for image recognition and it can be used in the entertainment domain to analyze images of movies TV shows or music artists and make personalized recommendations to users based on their Interest next comes Google net and retain now this Google net is a type of convolutional neural network that was introduced in 2014 it is designed to be efficient and accurate making it well suited for applications in a variety of applications now the key innov of Google net is the use of inception modules now an Inception module consists of several parallel convolutional layers with different kernel sizes and strides now this allow the network to process information at multiple levels of granularity and special Direction Additionally the Inception module uses one cross one convolutions to reduce the dimensionality of input before passing it through the larger convolutions which which helps to reduce the number of parameters and computations now in the retail domain Google net could be used to analyze images of product for example Google net could be used to classify images of product into different categories such as clothing Electronics or HomeGoods this could help retailers to quickly and accurately categorize products and improve the accuracy of search results for customers additionally Google net could be used to perform object detection and tracking which could be useful for tasks such as automatically tracking in entry levels or monitoring score occupancy so to summarize as we can say Google net is a convolutional neural network that uses Inception modules to process information at multiple levels of granularity and special resolution it can be used in the retail domain to classify products improve search results and monitor inventory levels now next comes squeez net and cooking now the squeez net is a type of convolutional neural network fact was introduced in 2016 it is designed to be lightweight and efficient making it well suited for use in Mobile and embedded system now the key innovation of squeeze net is the use of Novel building block called the fire module now this fire module consists of three parallel convolutional layers A squeeze layer an expand layer and a projection layer now the squeeze layer applies a small number of filters to the input image reducing the number of channels expand layer then increase the number of channels using a set of 1 cross one and 3 cross three convolutions finally the projection layer projects the output of expand layer back to the original number of channels now in the cooking domain squeez net could be used to analyze images of recipes or dishes for example squeez net could be used to classify images of different recipes into different categories such as vegetarian vegan or glutenfree now this could help cooking Enthusiast and re websites to recommend recipes to users based on their dietary preferences and help them find the perfect recipe for their needs so to summarize this we can say squeez net is a lightweight and efficient convolutional neural network that uses the fire module building block and it can be used in the cooking domain to classify images of recipes or dishes and make personalized recommendations to users based on their dietary preferences now next comes mobile net and HR now this mobile net is a type of convolutional neural network that was introduced in 2017 it is designed to be lightweight and efficient making it well suited for use in Mobile and embedded system now the key innovation of mobile net is the use of depthwise separable convolutions a depthwise convolution applies a single filter to each Channel while a pointwise convolution then applies a one cross one convolution to combine the results and this allows for significant reduction in the number of parameters and computations while still maintaining high accuracy now in the HR domain mobile net could be used to analyze images of job applicants or employees for example mobile net could be used to classify images of resumes into different categories such as technical or non-technical roles and this could help HR professionals to quickly and accurately identify the qualifications and skills of job candidates and make more informed hiring decision so to summarize this we can say mobile net is a lightweight and efficient convolutional neural network that uses depthwise separable convolutions it can be used in the HR domain to classify images of job applicants and help HR professionals make more inform hiring decision and then next comes lanet 5 and TV media now this lanet 5 is a type of convolutional neural network that was developed for image classification task it has five layers including including two convolutional layers two pulling layers and one fully connected layer now in the TV media domain linet 5 could be used to classify images of TV shows or movies into different categories such as drama comedy action or thriller and this could help in organizing a large collection of TV shows or movies making it easier for users to find what they're looking for additionally lanet 5 could be used to generate tags or keywords for images which could be useful for search and recommendation system for example example if you have a large collection of TV shows and you could use lanet 5 to classify the images of show posters into different geners and this would allow you to easily filter and search your collection based on the Jenner you could also use lanet 5 to automatically generate tags or keywords for the show posters which could be used to create personalized recommendations based on the users viewing history and preferences to uh so to summarize this we can say lenet 5 is a classical convolutional neural network that consist of convolutional pulling and fully connected layers it can be used in the TV media domain for tasks such as classifying images of TV shows and generating tags or keywords for image based recommendations thank you"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"Popular CNN Architectures: AlexNet, ResNet, EfficientNet",
        "Video Title":"What is ResNet? (with 3D Visualizations)",
        "URL":"https:\/\/www.youtube.com\/watch?v=nc7FzLiB_AY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/nc7FzLiB_AY\/hqdefault.jpg",
        "ID":"nc7FzLiB_AY",
        "Publish Time":"2022-03-12T16:45:03Z",
        "Channel":"Prof. Ryan Ahmed",
        "Channel ID":"UC76VWNgXnU6z0RSPGwSkNIg",
        "Transcript":"hello everyone and welcome to task number nine in this task i'm going to show you guys super powerful type of convolutional neural networks is known as resnets or residual neural networks please note that resnets are super powerful and it actually won the imagenet challenge back in 2015 and is able to achieve incredible performance compared to human performance on classifying images which is super powerful if you guys remember in the previous task we have been able to visualize our learnet convolutional neura network in tensorspace.js what i wanted to show you guys today is i wanted to show you resnets what they are and please note it's quite complex and that's why i'm going to show you guys kind of the architecture from a very high level and then i'm going to show you as well the visualization behind the scenes in tensorspace.js and that should simply is going to conclude our today's activity and of course we're going to have a final project in task number 10 where you guys are going to take a brand new network and you will be able to visualize it as well in tensorspace.js okay so the question is what is resnets and why they are called resnets or residual neural networks so resnet is a powerful convolutional neural network that won the imagenet challenge back in 2015 and if you guys remember imagenet is an open source repository of images contains like 14 plus million images and it contains over 21 000 categories and resnet was actually able to achieve pretty powerful performance like around 3.57 error which is pretty incredible and the main key point behind resnet is right now we are essentially able to stack many many layers maybe around 152 layers approximately within the resnet and were able to do that and also overcome a problem known as the vanishing gradient problem basically before resnets whenever we stack additional layers on top of our just the basic let's say lynette or alexnet when we stack additional layers what happen is is that a problem known as vanishing gradient problem tend to occur basically we use the gradient to try to go back to the network and update its weights and one of the challenges when you stack all these different layers on top of each other the gradient becomes very very small and essentially the network performance becomes super poor extremely poor and you are not able to train the network anymore if you have a very deep convolutional neural network and the key i would say take away that i want you guys to get out of resnets because again it's it's a complex essentially topic but the overall idea is we use what we call it skip connection basically we feed in the input here let's say to this layer and we also do a skip connection we feed in the exact same input we skip the convolution layer here and then we pass it along to the next layer and by doing that we are able to overcome the vanishing gradient issues and we are able to stack hundreds of layers on top of each other so i have couple of resources available for you guys here if you wanted to dig a little bit deeper so for example this is the first resource basically it states the understanding and coding of resnets in keras and again this course we're not going to be doing any coding in it at all i just want to show you guys for example couple of performance let's say of various networks so if you guys remember here i have let's say a couple of networks such as alexnet for example and it only has eight layers we also have vgg another network has around 19 layers and what you guys see here is resnet has 152 layers were actually able to stack many layers on top of each other and were able to achieve massive improvement like the error was again around 3.57 on imagenet dataset which is super powerful and there is also a couple of description here so resnet's short for residual neural network is a classic neural network that is primarily used as backbone for many computer vision tasks and the model has won the 2015 imagenet challenge and the main breakthrough within resnet is you are able to train extremely deep neural networks with over 150 plus layers and before resnets we weren't able to actually do that and that's why the limitations here that we had because of the vanishing gradient problem that is that i described earlier okay and you guys can go ahead of course and read a little bit more about the skip connection the overall idea is we feed in the input to the convolution layer and we skip this layer here and then we feed it into the next layer and i'm going to show you guys when we cover the visualization as well how we do that skip connection and if you guys wanted to kind of open and explore the structure of the network here is the structure of the network um what you guys see again we have many layers 152 layers and please know guys that this is super powerful this is essentially like you're building like like a brain like a mini human brain that is able to classify images able to recognize objects which is super powerful and this is simply the skip connection that i'm talking about so you have a bunch of convolutional layers and then here you are skipping the or feeding in the input and you're feeding in as is to here to this layer and then your feeding is keeping it again and skipping it again and by doing that you are overcoming the vanishing gradient problem when you train your convolution on your network okay all right so what i wanted to show you guys next is let's go ahead and head to tensorspace.js and i wanted to show you guys the residual neural network so if you guys see here we have learnet here we have the resnet 50. so if you click on it what you guys see is right now it's around 98 megabytes approximately and we are right now um loading the resnet 50 pre-trained model and what we're going to be able to do is i wanted to show you guys all the different layers and i wanted to also show you guys the feature of the skip connection i wanted to show it to you visually and that should simply conclude our today's task and um in the next task we are going to have a final project what i'm going to ask you guys to explore a brand new network and here we go what you guys see here is basically i have my residual neural network or resnet and what you guys see is resnet here has many many layers of course it's a lot more complex compared to learnet which only had five layers before and if you open let's say if you zoom in a little bit and you open for example one of the layers here let's say the first layer what you guys see is i have a ton of feature extraction layers right they have tons of convolutions what you guys see here and then if you guys hover on top of this red here uh layer you guys will notice that there is a skip connection that's essentially the skip connection i'm talking about you are skipping the input here from the this layer these two layers and you're skipping them and that's what helps us or empower us to train a very deep convolutional neural network and without having the vanishing gradient issue or vanishing crating problem and of course if you try to expand all these different layers a little bit here we go you can essentially expand upon this layer this is another convolution layer another convolution layer pretty pretty amazing this is essentially again like you know like a like a brain that is able to do human level tasks which is pretty incredible and this is simply the skip connection that i'm referring to and here we go if you open let's say another layer another convolution layer and here again another skip connection that's what we got here and what you guys see is all these different layers again it's a massive brain essentially and when you get closer to the end that's where you have your outputs and in the output essentially it's i have the dense fully connected artificial neural network at the end so if you guys see here essentially these are the output layers so if you try to expand upon it again i have the skip connection i have the last layer and then i have dense fully connected at the end here again pretty intuitive pretty amazing again it's a lot more complicated of course compared to alexnet but these are essentially the different feature extraction layers and convolution layers within our residual neural network or resnet okay all right so that's simply all i have for today's task i hope you guys enjoyed it and uh basically in the next task in task number 10 we i'm going to cover our final project so i'm going to introduce you guys to a brand new convolutional neura network i'm going to ask you guys to explore this type of convolutional neural network and also visualize it in tensorspace.js please stay tuned best of luck and i'll see you guys in the next task"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"Popular CNN Architectures: AlexNet, ResNet, EfficientNet",
        "Video Title":"VGGNET Architecture In-depth Discussion Along With Code -Deep Learning Advanced CNN",
        "URL":"https:\/\/www.youtube.com\/watch?v=mRVTKrbRYi0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/mRVTKrbRYi0\/hqdefault.jpg",
        "ID":"mRVTKrbRYi0",
        "Publish Time":"2020-09-12T14:30:14Z",
        "Channel":"Krish Naik",
        "Channel ID":"UCNU_lfiiWBdtULKOw6X0Dig",
        "Transcript":"[Music] hello all my name is krishnak and welcome to my vlogging channel so guys we'll continue the discussion with respect to advanced convolution neural network architecture and in this particular video we are going to discuss about the vgd16 architecture in short i'll also be discussing about vg16 and vgd19 that are two variation with respect to vgg net you know and in my previous video if you remember we had already discussed about alex net architecture itself now this particular architecture and this particular model that we usually develop in cnn is very very efficient and i have seen according to my experience right if i compare some advanced architectures like resnet even vg16 has performed better than resonant in some of the scenarios so it is very very important that you understand the whole architecture and how it works we'll also try to compare what is the basic difference between vg16 and alexnet and then we'll try to see that why vg16 performs better what are the disadvantages in alex neta that we are trying to overcome in with the help of vg16 so all these things will be covered in this particular video if you're new to this particular channel guys please do subscribe the channel and press the bell notification icon because i will be definitely uploading videos every day and i upload nowadays i'm making the speed to two to three videos every day itself so i hope you'll like it so yes please do subscribe the channel okay now let us go ahead and try to discuss about the vgc 16 architecture now to begin with guys let me just consider this suppose this is your whole architecture the source of this particular image is from http researchgate.net okay so the whole credit goes to researchget.net and over here you will be able to see that the image is getting passed through a convolution layer so this convolution layer the count that i would like to keep is two because there are two convolution layer right one thing that you need to note over here is that the image size that is going 224 cross 224 cross 64. this basically means this is your height and width and uh sorry the image size will be 224 cross 224 cross three right three is basically a rgb channel then when it is getting passed through this convolution layer we are getting this don't worry i'll just discuss about what is the kernel size what is the filter size how many number of filters are being used in my next diagram okay but in short we'll try to understand so first of all we have two convolution layer then we have one max pooling layer okay one max pooling layer then again we have two convolution layers okay then again one max pooling layer now in this particular scenario right i have three convolution layer right then we have max pooling layer as one then again three convolution layer then again i have one max pooling layer then this is my three convolution layer then again one max pooling then i have fully connected layer funny connected layer and my output is thousand why thousand because we this whole model is basically trained on image net data set right so where you have thousand categories and there are millions of images and definitely this uh uh this has actually won so somewhere around i think it was in 2013 or 14 this whole paper had actually come we'll also see the research paper don't worry about it now with respect to this i will i can basically write initially we use two convolution layer then one max pooling then two convolution layer then uh we had one max pooling and three convolution layer one max pooling then three convolutional layer one max pooling three convolution layer one max pooling i guess i have written three one three one three one perfect and then i have one fully connected fully connected and finally fully connected which is my output layer of thousand categories so this is the overall architecture one two important things that you have to see in this architecture you are providing your image of 224 comma 224 comma 3 this is based on the architecture but you can always change your image whenever you are implementing this you can give 50 cross 50 cross 3 it depends on the image quality that you actually have right so uh now this was a basic architecture still i did not decide like how did we get these values how did we get these values that i'm going to discuss over here in my next architecture in my next diagram basically and you'll also see that what is the filter size that we will be using okay that filter size i really want to mention it over here so that you will be able to do the calculation and remember guys this is a very very simple model okay very very simple model very easy to understand model you can also remember it if you just know this particular patterns the main thing in alex net what is the main problem in alex net you will be seeing that in alex net if i take this particular example uh you you you see that sometime it is using uh 227 tokar's 227 cross 3 sometimes it uses convolution filter or filter size 11 cross 11 and stride is 4 over here then you have suddenly 96 kernel then suddenly in max pooling you have 3 cross 3 then again your convolution layer says 5 cross 5 so here you see a lot of variations right and it is very very difficult for people to just remember all these things right and also becomes difficult to understand if there are so many changes in this so what they have done is that in order to overcome this right they have come up with vg16 and in vg-16 it is very very simple okay i'll just mention you about vg16 in vg16 it says that all the convolution layer okay all the convolution layer okay in this remember the filter size is three cross three okay and then here you basically have a stride of one and your padding will be same what does this basically mean this basically means that when i'm passing through this convolution layer right this image is 224 cross 224 cross 3 right now whenever i pass through through this convolution layer i need to get the same output with respect to the previous image only this will change right this is the number of kernels but here in short you are applying a three cross three filter okay and the number of filters that you are using is 64 that you need to remember so when you apply this you will be able to get this but whenever you do max pooling in max pooling because there are only two things that are being used right convolution layer conversion and layer with value and max pooling layer with ray loop right in conversion you have the filter size you have the filter size of 2 cross 2 okay and your stride is actually 2 okay so this is the property of convolution layer this is the property of max pooling layer so every time whenever we use stride of 2 here you can see that this 224 when we are passing through the convolution layer sorry max pooling layer of this one the output that you will be seeing will be the half of this okay half of this so how do i calculate you just have to use this formula right uh 9 n minus or i can also write n plus 2 p minus f divided by uh the stride right plus 1 just try to use this formula and this i have easily explained in my alex net architecture right in this particular case what is my n n is nothing but 224 padding is almost same nothing is there then i will be using minus f what is my filter size my filter size is 2 divided by what is the stride stride is again 2 so it is nothing but 222 divided by 2 right if i divide this this is nothing but 1 1 1 this plus 1 is still there so i'll add this plus 1 and i'll get 1 1 2 so that is the reason why i'm getting 1 1 2 over here right so that is how each and every competition is done now you see that in this convolution layer again 3 cross 3 is there stride is 1 but padding is same so even though we pass one one two images like this we'll be getting one one two images itself right and in this particular case one two eight is basically the kernel that is getting passed over here again the filter size with three cross three only the number of number of filters are getting changed right so after this if we are again passing through the max pooling layer again when i apply this particular formula it will be one one two then for that we will be getting output as 556 cross 56. now again over here you can see that number of filters is basically changing initially we had filters like 64. here we had 128 here we had 256 right here we had 256. now similarly in this max pooling again this 56 will get reduced to 28 right and here number of kernels is 5 to l again whenever we are applying max pooling with this filter size 2 cross 2 and stride is equal to 2 you will be seeing that your image size is getting reduced by half okay this is getting reduced by half and in this particular case the number of kernels are not changing again when you apply max pooling so you can see that this divided by 2 i i'll not put this divided by 2 but instead i'll apply this particular formula and then from 14 i'll be getting 7 cross 7 and this will basically be my kernel size right and this is pretty much simple fully connected layer if you multiply all these things 7 cross 7 cross 5 12 we are going to get 1 cross 1 cross 4 0 9 6 and finally you will be able to see that in this also you have 4 0 9 6 in this also you have 4 zero nine six and finally the number of output is basically based on the output of the image net right this is basically my image net so this is the output this is the thousand outputs that you are getting but two important things if anybody ask you what are the advantages when compared to vg60 which i am going to discuss sorry when compared to alex net and the second question is that what is the main thing you'll be saying that in each and every convolution layer the filter size is three cross three the stride is one and padding is same but in the case of mass max pooling layer you have filter size as 2 cross 2 and the stride is equal to 2. so every time when you pass through a max pooling layer the image size will get reduced by half okay and that is how the whole architecture actually deals with right and this is pretty much important now what i'm going to explain you that how uh how it is better than the previous architecture of lx net first of all guys there are many layers okay in this particular thing we have 16 layers okay in vgd 19 will be having 19 layers so can again there'll be a slight difference with respect to uh how the architecture will behave because i'll just show you in the coding you'll be able to see everything in the coding like how the architecture looks like you know when we'll be seeing the model summary we'll be able to see this let me drop this quickly because i really have some more things to explain because you need to understand after alex and trust me guys alex net architecture was the architecture from where this advanced deep learning cnn came people started experimenting with different different convolutions uh max pulling layers and all okay now let me just go to the next slide now in this this slide you will be able to see that we are passing three cross three convolution layer 64 three cross three the same thing whatever i have explained the same thing finally you will be getting uh the thousand outputs uh this is the architecture and remember in vgg net architecture you have two variations one is vgg vgg 16 and the other one is vgd19 again some architecture chain so this vg19 may perform better than vg16 but you know just just a small accuracy gap will be there not not much now let's compare with alexnet and vggnet now we saw that in alex net sometimes you use 11 cross 11 sometimes you use 5 cross try some suddenly you'll apply padding is equal to some value suddenly you'll say stride is equal to 4 right so this this is really really difficult randomly because they have experimented with all these things they have put a lot of efforts in experimenting so they are basically used but with respect to vgd net you have understood yes my convolution layer right it will be having three cross three filter size it will be having padding is equal to same and it will be having stride is equal to one right and by applying this hole and similarly with max pooling layer right we did the same thing you had to cross through you had stride is equal to two right this is the thing and one more thing that you see that here the layers are only seven okay these are really seven and if you know guys if you're using relu even you create a deep deep convolution neural network your really will take care of the vanishing gradient problem right so it that i have already explained in my complete deep learning playlist so this is also a deep uh convolutional neural network so you will be able to extract more parameters from the images right and remember the fast last layer is also called as soft max because that softmax will actually be giving you the thousand categories if you have two binary categories you basically use sigmoid but in this particular case it is soft max so if an interviewer asks what is the difference between vgd16 vgnet and alexnet you should say that okay lx net has less number of layers when compared to vgnet and you know that both this layer uses relu now even though we create a deep convolution neural network definitely there is no chance of a vanishing gradient problem because we are using relu right because of the vanishing gradient problem we sometimes use relu right to overcome those right and then you can actually explain about this and you can also say one point in alex net they have randomly not randomly basically after experimenting they have selected some pixel filter size right what if if you have a general architecture and we know that and it is also very very easy to remember all these things right so these are the some of the things that you can compare with respect to lx net and vgnet now once you have understood this architecture the next thing that you need to understand definitely is coding right unless and until i don't show you coding then everybody will shout at me so it is better that i show you the coding part also okay and coding part is very very simple guys you just have to believe in keras that's it okay you just have to believe in keras so i'll just try to show you so i'll minimize this now this is the research paper guys very deep convolution neural network for large scale image recognition now remember coding is pretty much simple one is that you can you can uh write with your own okay you can write the whole by just seeing the architecture it's also showing you in the alex net how you can write your own similarly you can write it but this all models are already presented care as you can reuse the weight you can see you have exception v3 169 16 vg 19 resonate and all so through keras if i go to vg16 this is what you just have to use you know in order to download the weights of vg60 so let me go over here now in this example what i'm actually going to do is that guys i am going to solve a problem statement which is called as cotton disease okay so i have these four categories now remember uh i think i've also taken this in live class but i really want to show you this particular example because it will be very very important to you so these are my images right this is these are actually all my images you can see over here with respect to different different categories and it not be this problem take simple cats and dog data set from kaggle okay and you can basically uh implement your own so this is the image classification disease image leaves classification that i'm actually going to do over here remember the output is having four categories now how do you download the weights i'm actually going to show you i'll go going to show you for vg 16 and vg 19 using keras again this material will be present in the github now don't worry about this particular code guys this code basically says that what memory fraction of gpu you have to use because i if i if i show you i have a gpu which is called as nvidia titan rtx so let me just show you nvidia dash smi okay so if you see over here we'll be seeing that i have titan rtx and uh you know all the other things what is my cuda version everything 24 gb uh vram i actually have how much is being accessed and all these things are available over here now first thing i'll just execute this this is just to allocate how much memory uh i want from the gpu to be used and then you'll be see able to see and remember guys this tensorflow version right this tensorflow tensorflow version right if i print it tf dot underscore underscore version underscore underscore right if i execute this you'll be able to see that i'm having 2.2.0 okay so that is the reason if you have less than 2.0 just remove this tensorflow from starting okay start from keras dot layers and all okay so here i'm putting input lambda dense flatten then i have model then you can see that again just if you go and see over here from tf.keras.application.vg16 so similarly i've written away from keras.application.vg16 import vg610 right so i've done this you can also use the image data generator for data augmentation and finally you also have to use some sequential model now once i execute this no module keras where is keras which line is this okay so i made a blunder over here so i showed you if your tensorflow is less than uh 2.0 at that time that particular thing will work so i've executed this right then i am given the image data set as 224 comma 224 why did i give it because in vg16 you'll be expecting as 224 comma 224 as a good practice i'm giving that same size again you can give your own size okay then i'm giving my training path and my test path so this is my train and test now this is the most important statement okay i'm just going to write vgg16 library okay as shown below to this and this okay now when i'm using i have imported this vg16 right so here i'll be using this vg16 the input image shape is nothing but image size plus 3 so this basically says that 224 comma 224 comma 3 3 is my rgb channel always remember if you really want to reuse the weights this weights parameter should be given as image net please do remember this this is very very important to understand and then we have include top is include underscore top is equal to false okay this basically says that remove the first and the last layer because we definitely know that my first layer will definitely have 224 comma 224 from the weights itself but i sometimes people want to put their own input image size right apart from the last layer which will definitely be thousand categories but my problem statement in this case has four categories suppose if you are really developing your problem statement you want to develop a classifier for cats and dogs you may be requiring two categories right so at that time you have to put two categories two dense layers in the output so i have removed this so here you can see that it is downloading the model of vg16 quickly okay it has downloaded it from this uh url itself right so that is where your model is basically done now this fun this code is very very important in this code i have basically said that for layers in vdg 16 dot layers layer dot renewable is equal to false remember we are using the existing weights of image net so we need not retrain the weights that is pretty much important right only the training which should happen in the last layer not in this middle layers okay so for this you can write like this and you can basically make the layer strainable as false now how many number of output classes i have written a generic code guys so that you get a number of output classes so we have used a glob function so here you see that once i execute this i will be getting the folders okay and if i execute it over here what is in my folders my folders will give me the path of everything now if i go and calculate the length of folders in this list it will basically give me a four output right then obviously you know that from the architecture from the architecture we saw that we have to flatten this layer after this right so flattening the layer is required so for flattening the layer what i'll do is that i will be using sorry this i had written vg16 okay you just have to write v16 dot output once you execute this and then you can see the length of folders okay uh yes vg16 dot output length of folders here also i try to see the length of folders right then uh you can see over here i've used the length of folders as my output layer because the length of folder is nothing but four that basically means four output categories is put inside the dense layer and the activation is basically softmax function if you don't remember guys here is my activation softmax function that i have applied okay so this will get applied in the last layer which is pretty much important and i think i've taken all these things in my live sessions also and finally you get your input and finally you create your model as this where your inputs is nothing but vg16.input your output is nothing but whatever dense layer you created in this prediction is basically output so execute this then once you see your model summary so this is your whole model summary so after executing this code you can actually see your model dot summary remember to check the last layer that is having four nodes okay after this you can do compile and these all are simple operations you can read the data you can basically read your training data this is basically your data augmentation techniques that you want to apply remember data augmentation should not be applied to test data okay and this code you can see in the github right i've already given in the description so uh this is basically your training data you're reading your test data you're basically reading and then you can actually start running this here i've given as epochs as 20. if you see after the end of training i'm able to get somewhere around 69 i just did 20 if you increase it to 30 and 40 you'll be able to get this this is how your graph was right it is in increasing order and then you can also save this model so instead of writing resonant because i don't know why i have written this resonant but this should be vg60 vg16 model you can save it in the h5 file it will be getting saved in the same location now you can take your own image example guys just follow the steps i think you will be able to understand each and everything right so uh okay now one more thing about this i told you about vg16 now there is one more architecture of vgg 19 okay so in order to use vg19 just remove this instead of writing vg16 just make it vg19 and just make it vg19 so once you executed it you'll be able to see it'll work absolutely fine over here instead of vg16 just write it as vg19 change this variable name over here make your layer trainable as false and put all the information that you want to put in this right based on the changes that you have actually done that's it that is how vg 19 will actually work and this is a very very common architecture itself so i hope you like this particular video please do subscribe the channel if you are not already subscribed and guys these are important to understand these are some advanced cnn techniques uh in the upcoming videos you'll be seeing that i'll be discussing about resnet inception mobile net and there are many things you can see over here resonate different different versions are there like we discussed about vcd60 and vg19 i missed about exception exception also i'll try to include we have mobile net mobile and v2 lot of this transfer learning techniques is there we'll discuss this also so i hope you like this particular video please do subscribe the channel i'll see you all in the next video have a great day thank you all bye"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"Implementing CNNs with TensorFlow\/PyTorch",
        "Video Title":"PyTorch or Tensorflow? Which Should YOU Learn!",
        "URL":"https:\/\/www.youtube.com\/watch?v=po5qTPKBrRc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/po5qTPKBrRc\/hqdefault.jpg",
        "ID":"po5qTPKBrRc",
        "Publish Time":"2023-01-31T00:36:57Z",
        "Channel":"Nicholas Renotte",
        "Channel ID":"UCHXa4OpASJEwrHrLeIzw7Yg",
        "Transcript":"if you've been floating around the machine learning space you've probably come across the words tensorflow or Pi torch at one point or another and you've probably thought to yourself which deep learning framework should I go on ahead and learn I first started out by learning tensorflow and building up a bunch of neural networks to get me to the states that I could build pretty much whatever it is that I needed but if I were to have to start out again I'd probably do exactly the same as what I did I'd pick the framework that my potential future employer uses in my particular case at that time that was tensorflow that being said learning doesn't stop there go out and learn other Frameworks right now I'm learning pytorch and Jacks just to ensure that I've got them up my sleeve because you never know when you might need it"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"Implementing CNNs with TensorFlow\/PyTorch",
        "Video Title":"PyTorch Tutorial 14 - Convolutional Neural Network (CNN)",
        "URL":"https:\/\/www.youtube.com\/watch?v=pDdP0TFzsoQ",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/pDdP0TFzsoQ\/hqdefault.jpg",
        "ID":"pDdP0TFzsoQ",
        "Publish Time":"2020-02-07T15:32:59Z",
        "Channel":"Patrick Loeber",
        "Channel ID":"UCbXgNpp0jedKWcQiULLbDTA",
        "Transcript":"hi everybody welcome to a new PI torch tutorial today we are implementing a convolutional neural network and do image classification based on the SyFy 10 dataset the cipher 10 is a very popular image data set with 10 different classes like we have airplanes cars birds cats and other classes and this data set is available directly in PI tersh so you will create a convolutional neural net that can classify these images so now let's talk about convolutional neural networks very briefly I will not go into too much detail now because this tutorial should be focused on the PI torch implementation but I will provide further links in the description if you want to learn more in detail so convolutional neural nets or confidence are similar to ordinary neural networks they are made up of neurons that have learn about Sande biases and the main difference now is that convolutional nets mainly work on image data and apply the so called convolutional filters so a typical confident architecture looks like this so we have our image and then we have different convolutional layers and optional activation functions followed by so-called pooling layers and these layers are used to automatically learn some features from the images and then at the end we have a one or more fully connected layers for the actual classification tasks so yeah this is a typical architecture of a C and n and these convolutional filters now they work by applying a filter kernel to our image so we put the filter at the first position position in our image so this is the filter here and this is the input image so we put it at the first position the rat position and then we compute the output value by multiplying and summing up all the values and then we write the value into the output image so here at the red and then we slide our filter to the next position so the green position then if you can see this here and then we do the same thing and the same filter operation and then we slide our filter over the whole image until we are done so this is how convolutional filters work and now with this transform our resulting image may have a smaller size because our filter does not fit in the corners here except if we use a technique that is called padding but we will not cover this here in this lecture so getting the correct size is an important step that we will see later in practice and now let's also talk about pooling layers briefly so pooling layers are more specific in this case the max pooling max pooling is used to down sample an image by applying a maximum filter to separations so here we have a filter of size two by two and then we look at the two by two sub regions in our original image and we write the maximum value of this region into the output image so max pooling is used to reduce the computational costs by reducing the size of the image so this reduces the number of parameters that our model has to learn and it also helps to avoid overfitting by providing an abstracted form of the input so yeah these are all the concepts we must know and again please check out the provided links if you want to learn more and now enough of the theory and let's get to the code so here I already wrote the most things that we need so we import the things that we need then we make sure that we also have the GPU support then we define the hyper parameters and if you don't know how I structure my pie chart files then please also watch the previous tutorials because there I already explained all of these steps so then first of all we load the data set and here as I said the cypher 10 data set is already available in pie charts so we can use it for from the pie chart data sets module then we define our pie chart data sets and the pie chart data loader so then we can do automatically batch optimization and batch training then I defined the classes and hard-coded them here and then here now we have to implement the convolutional net and then as always we typically we create our modeled and we create our loss and the optimizer so in this case as this is a multi-class classification problem we use the cross entropy loss and then as optimizer we used a stochastic gradient descent which has to optimize the model parameters and it gets the defined learning rate and then we have the typical training loop which does the batch optimization so we loop over the number of epochs and then we loop over the training loader so we get all the different batches and then here again we have to push the images and the labels to the device to get the GPU support then we to do our typical forward pass and create the loss and then we do the backward pass where we must not forget to call to empty the gradients first you with the zero crap then we call the backward function and optimize a step and then print some information then when we are done we evaluate the model and as always we wrap this in a with torch dot no gret argument or statement so because we don't need the the backward propagation here in the gradient calculations and then we calculate the accuracy so we calculate the accuracy of the total network and we are lady accuracy for each single class so yeah so this is the script you can also find this on my github so please check that out there and now the only thing that is missing now is to implement the convolutional net so for this we define a class confident which must inherit an N dot module and as always we have to define or implement the init function and the forward function for the forward pass so now let's write some code here so for this we have a look at the architecture again so here first we have a convolutional layer and then followed by a real activation function then we apply a max pooling then we have a second convolutional layer with a real function and a max pooling and then we have three different fully connected layers and then at the very end we have the softmax and the cross entropy so the softmax is already included in the cross entropy loss here so we don't need to care about this so yeah so let's set up or create all these layers so let's say self conf one equals and here we get the first convolutional layer by we get this by saying n n dot conf 2d and now we have to specify the sizes so the input channel size now is three because our images have three color channels so that's why the input channel size is a 3 and then let's say the output channel size is 6 and the kernel size is 5 so 5 times 5 and now let's define a pooling layer self pool equals n n dot max pool 2d with a kernel size of 2 and a stright of 2 so this is in as in the image that we have seen so our kernel size a size two by two and after each operation we shifted to pixels to the right so that's why the stride is two and then let's define the second convolutional layer so self-conscious and now the input channel size must be equal to the last output channel size so here we say six and as output let's say 16 and kernel size is still 5 and so now we have our convolutional layers and now let's set up the fully connected layer by saying self dot FC 1 equals n n dot linear and now here as an input size so first I will write this for you so this is 16 times 5 times 5 and as output size I will simply say I will say 100 so you can try out a different one here and I will explain in a second why this is 16 times 5 times 5 then let's set up the next fully collected layer so this has 120 input features and let's say 84 output features and then let's use a next of final fully connected layer so we have FC 1 FC 2 and FC 3 and this is an input size of 84 and the output size must be 10 because we have 10 different classes so you can change the 120 here and also the 84 but this must be fixed and also the 10 must be fixed so now let's have a look at why this is this must be this number so here I have a little script that does exactly the same thing so now let me change the number of epochs oh yeah is for so here I have the same thing in the beginning I load the data set and let's also print or plot some images and then here I have the same layers so here I have the first convolutional layer and the pooling layer and the second convolutional layer and first of all let's run this and plot the images so let's say Python C and n test dot pi and I've already downloaded it's a prince yeah it's very blur but I think you can see this this is a horse and maybe a bird and another horse and yeah I don't recognize this actually let's run run this again see some better pictures maybe so you got still very blurred so I think this is a deer a car a frock and a ship so yeah so let's see how the sizes looks so first we just print images touch shapes so this is 4 by 3 by 32 by 32 and this is because our batch size is 4 and then we have three different color channels and then our images have size 32 by 32 so now let's apply the first convolutional layer so we say x equals quant 1 and this will get the images and now let's print the next size after this operation so let's don't oh sorry I don't want to plot this anymore so now we have the next size so this is 4 by 6 by 28 by 28 and so the 6 now we have 6 output Chen as we specified here and then the image size is 28 by 28 because as I said the resulting image may be smaller because our filter doesn't fit in the corners here and the formula to calculate the output size is this so this is the input width minus the filter size plus 2 times padding's in this case we don't have padding and then divide it by the stright and then plus 1 so in this example we have an input size 5x5 a filter size 3x3 padding is zero and stride is 1 so then we have the output size is 5 minus 3 plus 1 so this is 2 then divided by 1 still 2 and then plus 1 so that's why here our output image is 3 by 3 and now we have to apply the same formula in our case so we have 32 minus the filter size so minus 5 so this is 27 plus 0 still 27 divided by 1 still 27 and then plus 1 so that's why it's 28 so here we have 28 by 28 then let's apply the next layer so the next operation is the pooling layer so let's save this and run this so now our size is 4 by 6 by 14 by 14 so this is because as in the example our pooling layer with a kernel size 2 by 2 and a stride of 2 will reduce the images by a factor of 2 so yeah and now let's apply the second convolutional layer so let's print the size after this operation so clear this first and run this and then again we would have to apply the formula as I just showed you to reduce the size so here PI torch can figure this out for us so the size is 4 by 16 and this is because the next channel output size and that we specified is 16 and then the resulting image is 10 by 10 and then we apply another pooling operation that will again reduce the size by a factor of two so this is why now we see that the final size after both convolutional layers and the pooling layers is 4 by 16 by 5 by 5 so and now if we have a look again so now after these convolutional layers now when we put them into our classification layers we want to flatten the size so we want to flatten our 3d 10 0 to a 1 D 10 ZOA and now this is why now if we have a look at the size now the input size of the first linear layer is exactly this that we have here so 16 times 5 times 5 so this is very important to get the correct size here but now we know why this is so this must be 16 times 5 times 5 and now we have the correct sizes so now we have all the layers defined and now we have to apply them in the forward pass so we say x equals and now let's apply the first convolutional layer which gets x and then after that we apply an activation function so we can do this by calling F so I imported torch and and functional as F and then I can call F dot riilu and then put in this as the argument and then after the activation function so by the way the activation function does not change the size so now we apply the first pooling layers of self to pool and rep this here and so this is the first convolutional and pooling layer and then we do the same thing with the second convolutional layer and now we have to pass it to the first fully connected layer and for this we have to flatten it so we can do this by saying x equals x dot u and the first size we can simply say minus one so pi touch then can automatically define the correct size for us so this is the number of patches the number of samples we have in our patch here so for in this case and then here we must say sixteen times five times five and now we have our tens of flattens and now let's call the first fully connected layer by saying x equals self dot FC one and this will get X and then we apply an activation function again we simply use the riilu I also have a whole tutorial about activation functions so please check that out if you haven't already so now after this we apply the second one so x equals this the second fully connected layer with a real uu activation function and at the very end we simply have x equals self dot the last fully connected layer fc3 with x and no activation function at the end and also no softmax activation function here because this is already included in our loss that we set up here so then we can simply return X and this is the whole conlou net model now you should know how we can set up this and yeah so then we create our model here and then we continue with the training loop that I already showed you so now let's save this and let's run this so clear this and say Python C and n dot PI and hope that this will start the training so oh yeah one thing I forgot of course is to call the super init so never forget to call super and this has to get the confident and self and then dot underscore in it so let's clear this again and try this one more time and now this should start the training so I don't have GPU support on my macbook so this can take a few minutes so I think I will skip this and continue when the training is done so see you in a second alright so now we are back our training has finished and if we have a look we can see that the loss slowly decreased and then we have the final evaluation so the accuracy of the total network is 46.6% and the accuracy of each class is listed here so it's not very good and this is because we only specified for epochs here so you might want to try out and more epochs but yeah you now you should know how a convolutional neural net can be implemented and I hope you enjoyed this tutorial if you enjoyed this please leave a like and subscribe to the channel and see you next time bye"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"Implementing CNNs with TensorFlow\/PyTorch",
        "Video Title":"TensorFlow in 100 Seconds",
        "URL":"https:\/\/www.youtube.com\/watch?v=i8NETqtGHms",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/i8NETqtGHms\/hqdefault.jpg",
        "ID":"i8NETqtGHms",
        "Publish Time":"2022-08-03T15:23:57Z",
        "Channel":"Fireship",
        "Channel ID":"UCsBjURrPoezykLs9EqgamOA",
        "Transcript":"tensorflow an open source machine learning framework famous for powering deep neural networks with high-level code it was developed by the google brain team and first released in 2015. it's most commonly used with python but can run in other languages like javascript c plus plus and java at its core it's just a library for programming with linear algebra and statistics as you know the word tensor describes a multilinear relationship between sets of algebraic objects within a vector space aka a multi-dimensional array what makes it special is its collection of apis for data processing visualization model evaluation and deployment that make deep learning accessible to the average developer it's extremely portable and is able to run on tiny mobile cpus or microcontrollers with tensorflow lite can run in the browser with tensorflow.js while the core library can scale up to multiple gpus or run on tensor processing units ships engineered specifically to run tensorflow at a massive scale it's used in medicine for object detection and mri images by twitter to sort your timeline by tweet relevance by spotify to recommend music by paypal for fraud detection in addition to many other applications like self-driving cars natural language processing and so on to build your own neural network right now create a python file and install tensorflow next we'll need some data like fashion mnist which we can automatically import the goal is to train a model that can predict the clothing type of each image tensorflow has a subclassing api for expert users but also integrates with the beginner-friendly keras library which has a sequential api that can easily build neural networks layer by layer we start with a flattened layer that takes the 28 by 28 pixel image as an input and converts it into a one-dimensional array this input layer is then fed into a dense layer with 128 fully connected neurons or nodes you can think of each node like its own linear regression as each data point flows through it it'll try to guess the output and gradually update a mapping of weights to determine the importance of a given variable in this case it uses a rectified linear activation function that will output the input if a certain threshold is met otherwise it will just output zero and the behavior of this layer can be customized by tuning as hyperparameters finally we have our output layer which is also dense but is limited to 10 nodes which corresponds to the total number of clothing types in the data set now we can compile the model and tell it to optimize a certain loss function like sparse categorical cross entropy as we train the model for multiple epochs its accuracy should gradually improve the end result is a model that makes a prediction with the likelihood that an image is a certain type of clothing congratulations you just built a neural network this has been tensorflow in 100 seconds hit the like button if you want to see more short videos like this thanks for watching and i will see you in the next one"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"Implementing CNNs with TensorFlow\/PyTorch",
        "Video Title":"PyTorch vs. TensorFlow",
        "URL":"https:\/\/www.youtube.com\/watch?v=fSgNjOvXTCs",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/fSgNjOvXTCs\/hqdefault.jpg",
        "ID":"fSgNjOvXTCs",
        "Publish Time":"2024-09-27T12:13:36Z",
        "Channel":"Plivo",
        "Channel ID":"UCNL8MQasO7O-q_g_8X6CI2g",
        "Transcript":"pytorch or tensorflow both are leading deep learning Frameworks and they both have pros and cons so let's get into it pytorch developed by meta aai dominates research with 60% of published papers using it as of June of 2024 and people love its Dynamic computational graph intuitive model building and seamless integration with python tools like numpy the fact that it's pythonic also helps to make it more useful for a wider audience which means that it has an excellent ecosystem of models and libraries for pie torch users to tap into tensor flow on the other hand developed by Google is generally considered to be the industry standard for production environments it offers robust deployment capabilities through tensorflow serving and strong cloud service integration too tensorflow's comprehensive tooling includes tensor board for visualization tensorflow light for mobile deployment and tensorflow.js for web deployment it also supports Hardware acceleration through gpus and tpus ensuring high performance in large scale environments"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"Transfer Learning with Pre-trained CNN Models",
        "Video Title":"Transfer Learning | Deep Learning Tutorial 27 (Tensorflow, Keras &amp; Python)",
        "URL":"https:\/\/www.youtube.com\/watch?v=LsdxvjLWkIY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/LsdxvjLWkIY\/hqdefault.jpg",
        "ID":"LsdxvjLWkIY",
        "Publish Time":"2020-11-23T13:30:02Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"Transfer learning has become quite popular in the field of image classification and Natural Language Processing. Here we take a pre-trained model and then we try to retrain it for the new problem. So if you remember from our data augmentation tutorial, we had flowers dataset where we are trying to classify five type of flowers. So in this video we will use a Mobilenet pre-trained model from Google's Tensorflow hub and we will use that pre-trained model to classify our flowers dataset and you will see that previously it used to take many epochs to train the complete model and achieve high accuracy. In this case using a pre-trained model it takes only like two or five iteration or epochs to get a superb accuracy. So using transfer learning saves lot of computation power because many times these pre-trained models that you can get from places like Tensorflow hub they are trained on millions of images. If you try to train that model on your computer it might take days or even months. But all you're doing is you're taking that pre-trained model, you're getting all the weights and everything and then you kind of change only the last layer or last few layers for your new problem and then uh you can get a superb high accuracy uh with this kind of approach. So let's get started we'll go over some theory and then we'll uh do coding. This is Wikipedia's definition of Transfer Learning which is you focus on storing knowledge gained while solving one problem and apply it to a different but related problem. For example if you have a model that can recognize cars it can be used to recognize trucks as well because the basic features, for example the tires, the steering wheel and some of the components between cars and trucks will be still similar. So you can use this knowledge of this visual world to transfer that knowledge into solving a different problem. In today's coding problem what we are going to do is we will take a Google's trained Mobilenet V2 model which is trained on 1.4 million images and total thousand classes. So this is a deep learning model that is trained at Google it would have taken a long time and a lot of computational resources you can see 1.5 4 million images is pretty huge dataset and the output is 1000 classes and these classes are little diverse you know. You have a goldfish, shark, some animals then some Hammerhead military uniform so you have it's not just the animals it's animals and some other objects total thousand classes and when this model is trained it will have input layer, then some deep layers and hidden layers in between then in the end you have a softmax layer which is just you know classifying it into thousand categories. In deep learning what happens is we freeze all the layers except the last one. So you know all these layers that you see, we will freeze it and then we'll use this model to classify flowers which could be one of the five flower types which I have shown here and we are going to use same dataset that we use in our data eggman augmentation tutorial. So when you freeze this layer what happens is the model weights don't change. So now when I'm performing my training, so by the way you take the model and then you still have to perform the training that's very important. But when you're performing a training the the weights in these frozen layers are not changing. So it almost you know looks like a con equation. So you are having this one big non-linear equation so you are passing your flower and this is a training phase, and then during using this weight you will get a feature vector. You are almost doing a feature engineering and then you use soft mix to classify into five classes instead of thousand. So I hope you get an idea that you're almost generating the features feature vector using this frozen layers. So during the training none of the weights nothing changes okay, and omitting the last layer is a very common approach in some approaches they also freeze only let's say three layers or two layers and remaining layers uh go through the usual neural network training okay? So we're going to now uh do a python coding uh to use Mobilenet V2 model and then use it to classify the flowers. We will download a pre-trained model from a place called Tensorflow hub. So Google has come up with this Tensorflow hub where you can get an access of different pre-trained models. So right now for tax domain problems they have all these models, for example for embedding they have 176 models, for image classification they have 188 models. So these are like pre-trained models which are trained on a huge dataset which you can import and directly use it. For video also see they have some video and audio so they have some problem. So if I look at image classification here there is this model called Mobilenet V2 okay? So this is the model we are going to use so this model as I said is trained on 1.4 million uh images and 1000 different classes, and the image is like 2224 by 224. you know it is that dimension. So now here in my jupyter notebook I have imported all essential libraries, and the first thing I am going to do is create is basically import that Mobilenet V2 classification model. So this is how we import it. So I have imported Tensorflow hub now this doesn't come with your regular tensorflow installation you have to install it separately. So make sure you run pip install Tensorflow hub otherwise it will give you model not found error. Here I am creating a classifier directly using this particular Mobilenet. So if you look at see so you have to give this this particular string or you know they have a code snippet. So you just copy that and by the way I have uh used some of the code from Tensorflow uh official tutorial. So thanks Tensorflow credit goes to you. But I have made it little simpler, you know so I have I have omitted the things which are not needed. So it is I have done some customization, so now here the image shape you know the image shape as you saw was 225 4 by 224, so you need to give two to four, two to four and I'm adding the third dimension for the channel. So what happens is when you do this in numpy okay let me just import it it will just make it 224 by 224 by 3 okay? So whenever it comes up you see that so that is the input shape I am giving and once you do that see you have the the model ready only. So now if you want to classify among those thousand classes okay, so let me open the file that I have so here in my current project directory I have downloaded the those thousand classes and if I open that file these are the classes see total thousand classes and uh goldfish is one of the class. So I'm like okay let me try to classify goldfish. So I downloaded goldfish picture and I'm going to use this model to classify that. So I have imported a pillow model and image from that, and you know you can just say image. open the file name is gold goldfish this is how the image looks but we have to resize it to 224. So I will just say resize to image shape, and I will just store it here okay? So it's a smaller image now and let me try to classify this image. So now before you classify it you have to uh scale it you've seen in all of our previous tutorials that before giving it for classification or training, we always scale or normalize the image and how do you do that. Well the the color scale is 0 to 255 so you divide it by 255. So see here I'm dividing it by 255 and when you do that uh the value of goldfish is like like if you look at just this array, see now these values are in between zero and one range okay? I'm gonna do one more thing which is see when you do something like this, what you're doing is you are changing you are adding uh one more dimension which is one, and the reason I am doing it is because when you do prediction you know prediction accepts multiple image as an input. You cannot have like only one image as an input so that is the only reason I am doing it. So now I can do classifier predict like this so now uh you have a total thousand classes okay? So this is making a prediction for each classes, each class like zero classes this probability one class has this probability and so on. So here I am going to store this in less a result and let's look at result.shape it's thousand okay? Now I need to get the max. So when you do np .arg max from result it will give you the value the index which has a maximum value and if you notice previously say it's very upfront 0,1,2 see this has a this has a bigger value at least in this view 9. So that's what it is giving you. Now how do I know which class 9 is? Well uh if you look at the just a second if you look at our image labels two classes goldfish okay, so it's very clear but just to make it proper here what I will do is I will uh open this file so I will just say with open okay with open what well this particular file as f and f.read will read the files and when you do split lines it will split the lines, and you want to store this into an array called image labels okay? So image label is nothing but a label array and if you look at these labels you will find that now you are having those thousand classification labels, and if you supply your predicted label index you get a goldfish here. So this looks good so far. We used pre-trained model and we just did classification straight away this is like you know almost loading a pickled model and doing a classification. Now we want to do a classification for our flowers dataset and you can download flowers dataset by running this command. Now we have done data augmentation tutorial on this flower dataset before in the previous video and majority of the code is same. That's why I'm not going into too much details. But if you check this code here all you're doing is downloading this zip file of all the flowers from Google website this is how you you you download it and if you look at data directory the directory is dot means local directory that has data set folder that has flower photos. So if you look at our folder see dataset folder has flower photos and that has all five flowers. So daisy will have daisy flowers see daisy will have daisy, roses will have resist flower and so on. So let's uh use a pathlib direct path lib python module to convert this path this is a string path all I'm doing is converting it into windows path. Now why am I doing it? Well so that I can do functions like this so when I have Windows path and if I do star.jpg it will go recursively into all the directories and get me the file path of individual images, and those paths will be needed and if you look at image count we have these many images and now I am going to get all the roses images so from data directory I am saying go to roses folder, roses folder and star means get me all the files, and that file path you are getting in this roses' is our directory our roses are list okay? Let's try opening some files you know. So I'm using this image is a pillow library so you can use this code to open first row's image, second row's image and so on see similar thing you can do with Tulips. So if I let's say supply Tulips here and what is this Tulips? Tulips is the name of the folder you see Tulips here is a name of the folder and if you do that you get all this Tulips and if you open some Tulips images Tu lip s so you get all this beautiful looking images. Now I'm going to conver make a python dictionary so that the key of the dictionary is the flower name, and the value is the list of images. So in this dictionary now if I do roses this will give me a pile path of all rose images. Similarly Tulips gives me all Tulips images okay? We have we have seen all of this in previous videos so you should be aware and I'm creating a label directory as well because machine learning module doesn't understand text. So you need to say okay roses is 0, daisy is 1 and so on. Now if you um look at let's say any particular file path it looks something like this, and this you can now read into opencv. So cv2 is opencv model which I have imported at the top, and I am saying iamread which means image read and this thing is same as this. So I'm let's say reading one image and if you look at image path you know image paths are image shape sorry image shapes are different. So I need to resize it because I want to in before training your model you need to make sure all images are of same size. So here I will make image the same size see this is how we do it. So now I will run a for loop on my dictionary and create x and y this is something again we did in the previous video that's why I'm not going into detail. But if you this code is very simple you're going through your this particular dictionary, for each rose you're going through each images. So going through each image is a second for loop, then you read image one by one then you resize it and you append the resize image to x and you append the label you know to y. So if you look at x of zero it's a three dimensional array of between 0 and 255. But we saw in previous videos that before doing image classification training we have to divide it by 255 so that it can scale. See if you do that it will bring it bring the value to 0 to 1. And if you want to do it on the entire dataset this is a numpy is so convenient, you can um convert first into numpy array then we'll divide it into 255. So let's do train test split first. This is a standard code we have seen in enough video so it doesn't need any explanation, and then we can divide it divide these images by 255. So when I look at this thing you know it's it's in this range 0 to 255. Now I want to use that pre-trained model and classify some of these images. So let's say first one is daisy, the second one is a beautiful rose, the third image is let's say again it's another rose. So let's try to use our classifier to predict this model. So this classifier is what? Well we saw previously, it is our pre-trained model that we imported from Tensorflow hub you know ready-made model, and I can now predict x of 0. But you know this takes numpy array. So you have to give numpy array I will I will give x of 0, x of 1, and x of 2 okay, and it return this um array of predicted arrays. So I will store it in predicted and then I can do an argmax arg mix will give you the maximum argument and what it is saying is the first flower this flower is 7 9 95 this flower is 880 the third flower is 795. So what is 795? Well we had our image levels remember in that if you supply 795 it's saying this is a flower curtain. Maybe on Mobilenet when when Google trained it maybe if some shower curtain had this flower pattern that's why it is saying. Even 795 that this image is also saying it's a flower curtain and 880 what is 880? So what is this? Oh this it is predicting as umbrella. So you see you cannot here use your ready-made model because ready-made model only has daisy as a flower. It even doesn't have all these or four different flowers. So it's gonna make some random guess out of those thousand classes, thousand meaning all these classes. And by the way this file and this notebook everything is in available in video description below. So make sure you download this from my Github. Now I'm going to retrain this model and here I have a feature extractor model. So how is it different than the previous one? So previously if you remember look at it this whole path is same the only thing I have classification here, here I have feature vector. So this gives the same model as the previous one except the last layer. So if you look at our presentation you know this is the whole model but from that model you want to take only the layers which doesn't include the last layer, and all these layers excluding last layer is given by this feature vector. So you can now create a model like this: so again I'm using Tensorflow hub creating Keraslayer and passing this URL here input shape is standard shape, this is an important parameter. You are saying trainable false which means freeze. See freeze means do not train. So when now you perform a training all those layers will have their fixed weights, and then I can create my model like this. So I am putting that that ready-made model and then creating the last layer which is that the classification of five flowers. See, so only last layer is mine the previous layers are are already trained, and then I will run only five epochs by the way. So these parameters are standard Adam, Sparse category, cross entropy, etc., and I am now running only five epochs. Now if you remember from our data augmentation tutorial, to train the same model with CNN previously it took us you know 30 epochs. In 30 epochs we got 85% accuracy. Now check this- in second epoch you got 85% accuracy so you can see that deep learning training is very expensive. When you run so many trainings uh so many epochs your GPU, CPU your electricity power is burnt. You might get a big electricity bill, but with transfer learning you can save all that computation time. It is not just the bill sometimes when you're building a training a big model, it might take you days, weeks or months. But with pre-trained model you can retrain it for your problem so much easily and let's look at the the performance of our test dataset- that is also very good 85 percent. So this is the reason why transfer learning is so popular in computer vision and nature language processing. If you are solving any computer vision or NLP problem try to see if you can use transfer learning. If you cannot then only try to build the model from scratch. I hope you like this video the notebook and other links are available in the video description below so make sure you check it, and make sure you watch all these videos in this deep learning series. I have a separate tutorial series on machine learning python, I'm doing python projects as well nowadays so if you want to learn python or small projects. I have a complete playlist on variety of projects as well. So make sure you check it out, and thank you for watching! Goodbye!"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"Transfer Learning with Pre-trained CNN Models",
        "Video Title":"Transfer Learning with Keras and TensorFlow: How to Fine-Tune a Pretrained Neural Network",
        "URL":"https:\/\/www.youtube.com\/watch?v=oHl8U3NccAE",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/oHl8U3NccAE\/hqdefault.jpg",
        "ID":"oHl8U3NccAE",
        "Publish Time":"2021-02-09T13:39:16Z",
        "Channel":"Nicolai Nielsen",
        "Channel ID":"UCpABUkWm8xMt5XmGcFb3EFg",
        "Transcript":"hey guys welcome to a new video in this new networks and deep learning tutorial in this video here we're going to talk about transfer learning and fine-tuning of an evil network so we can use transfer learning for a lot of different kind of stuff that we're going to talk about in this video here as well and how you can fine-tune already like pre-trained or pre-built neural network but first of all i've linked the discord server down in the description so make sure to check that out and join the channel uh if you want to join the community where we're talking about a lot of different kind of stuff about neural networks and deep learning and we're also talking about computer vision and a lot of different kind of projects so if you have some projects and you have some questions go ask them in there or if you just want some inspiration for your own projects so also hit the subscribe button and bell notification under the video here and like this video here if you enjoy it so also i'm in the first light here we're going to first of all talk about what is transfer learning and how can we use translating in neural networks so transfer learning is when we have a pre-trained model where we wanted to use it for a new and similar problem so we have a pre-trained model and it can be trained on a really large data set so we have a pre-trained uh model where we already have the weights that are trained on on a really large data set and it could be like a number of like even like thousands of different kind of classes that we have pre-trained our our model on and we also already have some some pre-trail and trained models that we usually take as a standpoint and then we do transfer learning with those and also do some fine tuning on those as well so transfer learning is usually done when the data set is too small to train like a full-scale model from scratch so when we're not able to like create our own neural network from scratch and then train it on the data that we have because our data set is just too small uh that we're not able to like train a full-scale model then we can actually like use transfer link where we already have like a pre-built a pre-built or pre-trained model um that we can use for any new and similar problem with a smaller data set so let's say that we have a new network that is like that we want to like uh have trying to predict like for example trucks and trucks that were passed to the neural networks and then we find a needle network that is already pre-trained to like maybe like do predictions on normal like cars and then we can actually like do transfer learning to add this new class here with the truck um so we can both predict that if it is a truck or if it's a car and then we can actually use trampoline where we go in and use a pre-trained model to do the actual predictions on cars and then we can extract some of the features that that pre-trained model has learned about like some of the features and cars so like in the first couple layers here it could be like the symbol uh symbol like features and structures and and stuff like that like lines and circles and a lot of different kind of structures and objects and then in some of the later layers like it's maybe head headlights or like door handles and stuff like that that it learns uh to extract from the images and then at the end then at the end like we have some other different kind of features in shocks where we can add some layers for our uh for our own neural network and when we're doing transfer learning so when we're doing trend filling we actually like want to freeze the layers that we have already trained our model on or like the pre-trained model on so we freeze those layers so they won't be trained uh during training and then we'll add some additional layers or like another class here in the output layer here for example if we want to do some classification problem for for trucks as well and then we actually like um train our model again with the smaller data set that we have with the class that we want to do predictions and like be able to do predictions on uh with our pre-trained model as well and then at the end we can actually like do some fine-tuning which we're going to cover uh shortly here in this video so transfer line can be used for a lot of different kind of uh things and is really useful if you have a pre-trained model or if you just like have your own model that you're trained from scratch and you want to add another class uh in the classification problem that you're doing then you can actually just use transfer learning free some of the layers you can actually also delete some of the layers in the pre-turned model and then retrain it again on the new data that it hasn't seen before so you're able to predict new classes um without having to like full re retrain your new network from scratch again so you can use some of the weights and some of the features that the neural network has already learned to extract um from the images so we can also do some fine tuning of neural network where we have trained on our neural network then we can actually like fine tune it to make it better and to make insights to make it able to like generalize better so we can retrain the whole model and the model with new with a new data set so we're like between the whole new model um with a data set so this is like what we're doing in fine tuning so first of all we're doing transfer learning where we're freezing some of the layers and we can actually maybe add some layers or like remove some of the layers from the pre-trained model and then we can retrain the whole new model with the data set in fine tuning here where we're actually going to unfreeze all the layers so all of the parameters here will be trainable and with that when we're doing that we need to like actually like um have a really low learning rate because we can incrementally like adapt the pre-trained features to new data when we're unfreezing all like the layers and the weights in our neural network so if we're not having like a really small learning rate then we can actually like um then we can actually like destroy some of the features that we have already learned in our pre-trained model and it will like destroy is this the destroyer the idea of having this pre-trained model where we just like adapted to a new feature or like a couple new features to do some extraction of a new class for example so we can incrementally adapt the pre-trained features to the new data set by retraining the whole new model with the data set or like the smaller data set that we have and then have we have a low learning rate so it won't like affect the weights that much and we won't destroy the features that we have already learned in a new network so this fine tuning here can be a good example of like when we have a pre-trained model we have done transfer learning to do new classification problems or like to do to like to like do a new um classification problem or like a similar problem where we just wanted some of the features from the older appreciation model and then to optimize it a bit more and like make it able to like predict better on the new data set and like the smaller data set that we have compared to like the larger data set that it already predicted on then we can retrain the whole model um with the new data set and and and then our new network will be more stable and it will be able to generalize all the classes more uh compared to just like the original classes the whole date and like the whole model pre-trained model was trained on and then the smaller data set that we did with our transfer lining so we can actually do this fine tuning here and transfer learning in keras so in carriage we already have some built-in uh pre-trained models in like neural networks here in carriage where we have some different kind of uh different kind of neural networks that has different kind of applications and we're going to talk about uh some of them and we're also actually going to see in code how we can do this transfer learning and fine tuning in keras where we're going to freeze some layers we're going to like take that model uh do a classification problem on a new data set and then we're going to retrain it and do some fine tuning and we can see that we just have these functions here where we can actually just import import all of these built in pre-trained neural networks over here and we can then initialize it with the ways that we want so we want to initialize uh this vtgg's 19 neural network here that is pre-trained and we want to in like we want to initialize it with the weights where this model was trained on imminent image net data set and then we can also specify some other different kind of parameters here like how many classes we want to train on and then we can actually like use the neural network here freeze the layers we can remove some of the layers or like we can add additional layers for our application and then we can um pass in our new data set that we want to train it on with the frozen layers and then we do the transforming and then after that we've done the transfer learning we can actually like do fine tuning where we unfreeze all the layers and train the whole neural network with a low learning rate on the new data set where we want to like add a new class uh to our neural network so we're not going to google collab here and i'm going to show you like how we can import an already pre-trained model and then how we can do transfer learning in carriage here um with the carriers like api and the pre-turned model that we have and then we have a new data set that we're going to to train our already pre-trained model on with transfer learning and then we're going to do some fine tuning at the end as well so first of all here we're going to import these different kind of modules here that we want so we want some different kind of um modules from the layers and optimizes and carriers and the tensorflow backend module here and also numpy and then second here we're going to specify that we want to use the tpu for training our needle network so i'll just run these blocks of code here so we have all already like set everything up with gpu and the modules that we need and then we go down here and we need to like imp and like uh import the data set that we're going to use so we're going to use a cad versus dorks data set so we're just loading in a data set that contains a lot of different kind of dogs and cats and then here we're going to split it into a training set and a validation set and and and a test set so we can see here that number of training samples is at 9305 and we also have a number of validation samples and also a number of test samples that we're going to test our and trained neural network on when we have done the transforming and fine-tuning of the already pre-trained neural network so then first of all here we're going to import matplotlib here so we can actually like plot the different kind of like um images here that we have in our data set so just like so i can show you like what is going on in the data set and what type of data that we have do we have that we're training our naval network on so if we import this and run this uh for loop here we can see that we have these images here in our in our in our training set so we can see that we have these images here and then we have the labels up over here uh where a one is the dork and the zero here is the cat so we can see that uh all of our images here is labeled um correctly and this is our training set that we're going to train are our needle network on with transfer learning so if we come down here first of all we need to resize our images so all of the images here uh have this the same dimensions um both for like our our test set our validation set and our our training set of course so we're going to rescale it here to 150 by 150 so we have squared images and with this resolution here so we're going to do it both for the for the training data set the validation data set and the test data set and we're going to just going to to do it with this map and then this land the function here where we're going to use this uh tf image.resize here which will just resize all the images here um to this dimension that we specified up here in the size and then we're going to specify the batch size where we want to like we want to um store our our images like from our data set in batches so i've already mentioned batches in the previous tutorials or if you don't know what batches is and and how we can specify the bad size and what it is and what it's used for make sure to check that video out as well before uh you're continuing um with this video here so we're just going to use uh this patch us here so we're going to like uh make it into batches in our training set our validation set and our tessa and then we're also going to to call this a cache cache function here and the prefetch uh function here as well so we're going to like make it easier to like load it into memory and when we're going to actually like train the neural network so we're storing it in the cast and we're also doing some pre-fetching of our data that we have in our training set and so if you just run these two modules here as well so we're following along then we're gonna go down here and actually like import uh the layers here from keras so first of all here we're going to create this like sequential model here where we're going to do some data augmentation of the data that we already want so first of all here the first uh like pre-files method that we're going to use for data augmentation is that we're going to randomly flip our images in the horizontal direction and then we're also going to apply this random rotation uh in our data augmentation so we're going to random rotate our image with 0.1 so we'll get some data augmentation so we'll get more samples into our data set and we'll also get some different kind of symbols where our images are randomly flipped and also randomly rotated uh by some degree here that we specify here so i'll run this blog code here as well and then down here we have this we're going to import like the numpy here where we're actually like going to uh split in our our training set into images and labels and then we're actually like going to so like show it with the augmented images here with this folder here that we're going to run through our our training set so we're only going to actually like do a data augmentation on our training set we want to keep our validation set and our and our test set uh as it is so we can like validate and test our model later on on like data that it should seem uh when we're like like doing predictions with a new network so i'm going to run this blog code here it'll just go through this here and then we can see like the data augmentation from from for one image in our data set here so we can see that the image here is rotated like randomly around some axis with some degree and we can also see that this image here is also also flipped around the horizontal like axis here so we can see that it just flipped horizontally so this is like how we can do data augmentation and get more samples from our like actual data so we see that we have these the nine different kind of scenarios here uh with its corresponding labels and then we can actually go down here and and actually like import uh the pre-trained model so we're going to use this base model here where we're just calling this like as i showed you in the slides we can call this carriers.applications and then we have some different kind of pre-built modules with pre-trained like parameters and weights so we're going to use this exception pre-trained model here and the weights that we're going to initialize it with uh is is the weights from the image net so this neural network here has been trained on the internet data set and then these loads these weights here will be loaded in um in this model here and our input shape is 150 by 150 because we have this rescale image that we did on the whole data set and then we have at the last parameter here which is the three channels because we're we're operating with colored images um in this example here and include top here we just set that to false so we're not going to include the last layer in the neural network because we want to do this transplanting here where we're going to specify a new class that we want to do predictions on and to do to be able to like classify with our pre-trained neural network when we're doing the transfer learning and later on fine tuning so first of all we're going to freeze the base model here so when we're training the neural network here or like doing transfer learning uh we won't be training the already layers so we already have some features that we can extract from the layers like from the neural network which can be like uh basic shapes and also some other different kind of features like for example when we're going to train on cats and dogs maybe it has already been trained on some images where we can see some like eyes and ears and tails and stuff like that so we set this a refreshed base model here by calling this uh trainable here and set it equal to uh to false and then we can create a new model on top of that already pre-trained model uh where we're going to specify the actual like layers that we want to have to be able to like to create our own needle network for the application that we want to do so first of all here we need to specify the inputs that we want and again we just want 150 by 150 and then three dimensions for the colored image and then we're going to see it's just like i can apply like random data augmentation here so we're going to apply this data augmentation on the inputs here that we specified up here when we're creating our new model on top of the already built and trained and pre-trained model that we specified up here with the exception that is initialized with the with the weights from the imagenet dataset so down here we can see that where the pre-trained exception weights uh required that uh requires that then that input is normalized from 0 to 255 to a range of minus one to positive one which we've already been over um in one previous tutorials where we talked about standardization and normalization and then the normalization layer does the following so it takes like the output which is equal to the input divided by the mean and like subtracted by the mean and then we divided by the square root of the variance which is uh which is the standard deviation and we went over these different kind of formulas in the last video as well where we talked about some pattern normalization and stuff like that so we're going to have this normalized layer here we'll just call this normalization functions of the of the like the layers here that we have in our pre-crossing modules and then down here we can calculate the mean and also the variance for like this layer that we're going to normalize and then we can call this a normal layer here of the x value where we just have this normalized layer here and then we set the weights for our normalized layers with the mean and the variant here that we calculated up here with this with this formless up here so now we have our our layer uh normalized in the needle network here that we're creating on top of the already pre-trained model and then the base model here it contains the bad normalized layers so we want to keep them in in inference mode um when on when we unfreeze the base model for fine tuning so we make sure that the base model is running um in interference mode here so we wanted to run in inference mode so we're not like training our already like pre-trained weights and models so we set this base model here x and then we set the training here uh equal to false which means that we're now um in inference mode and then we just set the the layers here to like a global average pooling uh two two dimensions and we set it to this x x value here which is the normalized layer that we have and then we also apply some drop out here so we drop out like uh 20 of like the neurons are like um the weights and the trainable parameters in our neural network for this layer here that we're specifying on top of our pre-trained model and then we set the outputs here equal to a dense layer here where we just want to have a binary output so we either have a zero or one in case of uh we're predicting if it's cat or a dog and then we set the model here equal to this carriage model here which where we're going to specify the input and then we're also going to specify the outputs here so the input up here is the input that we're creating on top of the model and then the output is is like the dent layer here which is a binary output and so we're just specified in this model here and then when we run this blog code here we will just do a summary of the model that we have now created on top of already pre pre-trained neural network so first of all here we see that we have this first input layer here and we have 150 by 150 and by three and then we're going to create this sequential layer here and then we're going to normalize the layer and then we have the exception model here which is the pre-trained model on the imagenet dataset and we can see that we have a lot of different kind of like shapes here so it's 5x5 and 248 and we can see the number of parameters or like the number of parameters that we have in that neural network that is already like pre-trained over here to the right so we have like over over like um 20 million 20 million parameters in this neural network here and we can see down here um that we have the total parameters here and we only have 2 000 trainable parameters which comes from the layers that we added on top of the ovary pre-trained model because we have set um we have set them layers in the exception model here and to like non-trainable so we've rows the layers here in the exception model so they won't be trained when we're like doing this transfer learning on the new data set that we have and then we can also see like uh we don't have like that many uh parameters that are new network can learn from but we know from the exception model here that we already have some features that we can that we can extract and use for new classification problem that we did so this is the perfect uh like the purpose of um transfer learning when we have a new network that is already pre-trained and we want to like have um have it have it do predictions on a new application which is like kind of like similar um to that previous one so we can use some of the features from the albert pre-trend one so we can see that we have this global average pooling layer here that we added as well and then we also added this dropout layer here and then at the end we created this dense layer here which just has a binary output so we can see that if if the predict if the nail network is predicting accad or dork uh to the image that would pass in of the training so then we could go down here and compile the model where we specified optimizers in this case we're specifying it to the atom optimizer and we can also specify this loss function here where we're going to use the binary cross-entropy because we're having this binary output and the matrix here where we're going to we just want to see the binary accuracy here where we're going to train our model and in this case here we're just going to run it for three epochs and then we call this function here fit which which will actually like do the training of the neural network so in this fit function here we need to specify the training data set that we want to train our neural network on and then we need to specify the epochs and then we're going to specify uh the validation data set that we're going to validate our data like our new network on while it's training so we run this blog code here it will actually like do the training of the new network and then we'll go over like epoch or epoch and we already specified the batch size uh that would pass through a parameter for each epoch here in the neural network and then we can see here while we're training like for a first epoch here we can see like how much time is it estimated left here for this epoch and then we can also see like how the last changes uh while running through this epoch here and the different kind of patches in our neural network and we'll also get this binary accuracy here which we specified up here as we want at the binary accuracy as the metric and we also get the validation loss here and the binary uh accuracy for our validation set so we can validate if our model is underfitting or overfitting to the data that we pass it to and when we're already operating with a pre-trained model like these um like these metrics here will already be pretty good uh when we have like a model that's trained on on a lot of different kind of data on the image net and then we just want to like do predictions on cats and dogs which is has already trained some features on in that new network so the metric seal will be very accurate uh already from the beginning uh when we're training it and we can see that now we've been through like all the three epochs here in training process and a loss has gone here from point 23 here to point 11 so we can see that we have actually like made our naval network better by by training it over three epochs here and we can continue training it over more epoch on table converts towards like a steady loss and also like a steady binary accuracy here and we can also see like the uh the planar registry also uh increased on um on like the training data that we're doing and the same cases over here to the right where we have the validation uh data where we can see like a loss gets a bit a bit smaller and also like the binary accuracy is pretty steady here in this example here like around 0.97 and we can optimize this by adding my like for example more layers um to make it more accurate or we can do more training and like for more epochs but in this example here we can see that we get a pretty good result here and we get like an accuracy of 0.95 and we get a validation accuracy of 0.97 and these are very similar so we're not really over fitting the data when our validation binary actually is actually like better than our training binary accuracy so when we've done this here we can actually unfreeze the base model when we're doing the fine tuning of the neural network and we just note here that it keeps running in inference mode simply passed like training equals false when we're calling it and this means that the batch normalization layers will not update the past statistics and this will prevent the past normalization layers from undoing all the training we've done so far so this is very um important that we specify so now we set the base model here to trainable so we can actually like do the fine tuning so all the weights are now trainable in our in our nail network and we just specify a low learning rate so we will just like be better to generalize and and like actually like do predictions on the new data that we have done uh with our transfer learning so first of all here we're going to set it equal to trainable and the base model here trainable equal to two and then we do a summary of the model here so we can see like the number of trainable parameters that we have now and then we're going to compile the model again with a lower learning rate in this case here we have a really low uh low learning rate um and we still use the binary cross entropy loss function here and also the accuracy uh definer accuracy which is the metric that we're going to evaluate our training process on and then we're just going to train for two epochs in this example here like you can train it for more epochs if you want to and if it makes the results better and then we just use this fit function here again where we just have the same training data that we're going to fit uh to our model here but in this case here and in this and this blog code here we won't just train it on the new layers that we have added but we will train it on the whole neural network including the already pre-trained model and the exception model trained on the imagenet dataset so when i'm going to run this blog code here we can see that we get this um summary of the model here and the training process here down here is started so we can see that now we have like before we only had two thousand trainable parameters which was from the top layers that we built on top of the overhead pre-trained neural network and then we froze the layers inside of that pre-trained model but now we have unversioned the the the like the base model or like the pre-trained neural network and we can see that now we have like close to uh 21 million trainable parameters that we're now going to train our neil network on uh with the training set uh that we have like trained our new network on previously when we did the transfer learning and but in this case here we just have a really low learning rate uh to make a new network like to be able to to generalize more and be able to have more accurate predictions and in general on the new data that it hasn't trained on in like uh in the general or like in the pre-trained model so we can see that here it takes a bit longer for the model to train now because we have a lot more parameters that it needs it needs to pass through and make the calculations for and then we again we get the metrics here for the loss and the binary accuracy so the neural network is now done training and we can see that we're running from 42 epochs here and took like around three to four minutes so make sure you have the the gpu hardware accelerate on because if you're running this on the cpu it would just take like a lot longer time by only just running these two epochs here because we have 21 million training both rammers and we're operating with images but we can see that just for these two epochs here we can see that our loss here for our our training has decreased and also the accuracy here like the binary accuracy here has increased for these two epochs here and also the validation loss is the same and like the validation binary which uh decreased and the validation binary accuracy actually like also uh increased by a bit so if you're training this more for like a couple more epochs like you might get better results uh but this is just to luxury like how we can train the new level here and do the fine tuning so now we're training all like our data set we're now we're training the data set on the whole neural network with all of the different kind of parameters with a really low learning rate uh so we're so we're able to like do more stable predictions on the new data set that we're going to like transfer our already pre-trained model to so when we've done this here we can go down here and actually like do new predictions so we can actually just call this predictions and then set it equal to model.predict and then the test data set here that we specified and now we have actually like done the predictions of the test set here that we have and then we can just like see the different kind of like predictions that we have here and results uh so if you pass a new uh cat or dog image to our neural network here now we would actually like get a predictions of what it predicted like if if this is a cat or a dork so last block through here is just to evaluate the model that we have already trained now so we're going to we're going to pass the data set here to this evaluate function here and then we're just going to print the result so now what it's doing is that it's passing all the images in our test data set to the neural network here that we have trained and then it will like calculate the loss and the binary accuracy for all the test images that are now passing through the neonatal network and we see here that we get a loss of 0.06 and a binary accuracy of 0.97 so it's a really good uh accuracy on new data that it hasn't seen before and also a really low loss here so we're actually like been able to do this trend following here where we have a pre-trained model on the imagenet dataset and then we did this transfer learning uh so we have another application where we want to do some classification of if of cats and dogs as well and then we used um and then we used transfer learning to actually like transfer our pre-trained neural network to that other application that we want which is also similar and we can use some of the other different kind of features that we already have trained the naval network on in the pre-trained one with the image net data set so after we've done that we actually like did the transfer learning we we actually like fine-tuned our neural network where we unfroze all the layers in the pre-trained neural network and then we trained the whole neural network with the data set with our cats and dog and did the fine tuning with a really low learning rate and then at the end here we did predictions where we get uh where we get a really good binary accuracy of predicting cats and dogs and with a really low loss in our neural network so thank you guys for watching this video and rip the subscribe button and notification under the video here and also like this video here if you like the content and you want more than feature because it just really helps me and the youtube channel out in a massive way and i just really appreciate the support if you're interested in one of the other journals i'm doing i'm currently doing a computer vision tutorial in cbs plus with opencv and later on we're going to combine that with neural networks here and convolutional neural networks and we're going to like combine those two and see like how computer vision and deep learning work together so if you're interested in that tutorial i'll link to it up here or else and see in the next video guys bye for now [Music]"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"Transfer Learning with Pre-trained CNN Models",
        "Video Title":"What is Transfer Learning? [Explained in 3 minutes]",
        "URL":"https:\/\/www.youtube.com\/watch?v=vmjP6LjGaag",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/vmjP6LjGaag\/hqdefault.jpg",
        "ID":"vmjP6LjGaag",
        "Publish Time":"2022-10-19T12:09:31Z",
        "Channel":"Levity",
        "Channel ID":"UC3_1NF6DnzTYQKB7cfWDlxg",
        "Transcript":"foreign we're going to discuss transfer learning let's Dive Right In first what is the definition of transfer learning and what does this actually mean the formal definition is a transfer learning is the Improvement of learning in a new task through the transfer of Knowledge from a related task that has already been learned okay now what does that actually mean yourself as a child learning to ride a bicycle for the first time is difficult and took lots of time to learn you need to learn everything from scratch how to keep the balance how to steer the wheel how to break now imagine yourself today wanting to learn how to ride a motorcycle you don't need to start from zero it is much easier for you to learn how to keep your balance or use the brakes even though you're in a different setting you can transfer the skills learned from riding a bicycle that is the essence of transfer learning okay so in a machine learning context and not a learning to ride a bike context what does this look like transfer learning is a technique that enables algorithms to learn a new task by using pre-trained models take an algorithm that has learned how to recognize dogs this algorithm can be trying to recognize cats with relativities by transferring certain abstract Concepts before transfer learning traditional learning meant that machines learned in isolation once the machine was given a large enough data set it learned how to perform a specific task however one task with a new problem to solve it cannot resort to any previously gained knowledge instead a conventional algorithm needs a second data set to begin a new learning process however in transfer learning the learning of new tasks relies on previously learned tasks the algorithm can store and access this knowledge the module is General instead of specific why is this beneficial this technique of transfer learning unlocks two major benefits first transfer learning increases learning speed with less new things to learn the algorithm is faster to generate high quality output to use another analogy a nice hockey player is likely to learn more quickly to play field hockey than an average person because certain Concepts apply to both disciplines second transfer learning reduces the amount of data required in additional learning an algorithm can only learn when fed with enough training data sometimes millions of data points this data might not be available at all or too expensive to generate and prepare for the model so there you have it transfer learning is how machines learn by sharing knowledge now that you're here I wanted to tell you about levity levity is a tool that allows you to train AI models on images documents and Text data you can rebuild manual workflows and connect everything to your existing systems without writing a single line of code if you liked this video you'll probably love levity and we can't wait to hear from you foreign [Music]"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Convolutional Neural Networks (CNNs)",
        "Topic":"Transfer Learning with Pre-trained CNN Models",
        "Video Title":"What Is Transfer Learning? | Transfer Learning in Deep Learning | Deep Learning Tutorial|Simplilearn",
        "URL":"https:\/\/www.youtube.com\/watch?v=MQkVIYzpK-Y",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/MQkVIYzpK-Y\/hqdefault.jpg",
        "ID":"MQkVIYzpK-Y",
        "Publish Time":"2023-08-06T06:30:07Z",
        "Channel":"Simplilearn",
        "Channel ID":"UCsvqVGtbbyHaMoevxPAq9Fg",
        "Transcript":"foreign do you know training a classifier to distinguish beverages can help predict the cuisine of an image to know more about cancer learning and how it works stay tuned till the end of this video in this video we will cover topics like what transfer learning is how transfer Learning Works moving forward we will dive into why you should use transfer learning after that we will cover the steps to use transfer learning and at the end we will see popular model trained using transfer learning let me tell you guys that we have regular updates on multiple Technologies if you are a tech geek in a continuous hunt for the latest technological Trends then consider getting subscribed to our YouTube channel and press that Bell icon to never miss any update from sip leader by the end of this video I can ensure that all your questions and doubts related to transfer learning will have been cleared also accelerate your career in Ai and ml with our comprehensive postgraduate program in Ai and machine learning boost your career with this Ai and ml course delivered in collaboration with party University and IBM learn in demand skills such as machine learning deep learning NLP computer vision reinforcement learning generative AI prompt engineering chargedy and many more you will receive a prestigious certificate and ask me anything session by IBM with 5 Capstone in different domains using real data set you will gain practical experience master classes by Buddy University and IBM experts ensure top-notch education simply learn job assist helps you get noticed by Leading companies this program covers statistics python supervised and unsupervised learning NLP neural network computer vision Gans Keras tensorflow and many more skills so why wait enroll now and unlock exciting Ai and ml opportunities the course Link in is in the description box below so without any further Ado let's get started so what is transfer learning transfer learning is machine learning refers to G using a pretend model to improve prediction on a new task it involves using knowledge gained from a previous assignment to tackle a related problem for instance a model trained to recognize backpacks can also be used to identify other objects like sunglasses due to the substantial CPU power required this approach is widely utilized income Division and natural language processing tasks including sentiment analysis so moving forward let's see how transfer Learning Works so how does transfer learning work in computer vision neural network have distinct objectives for each layer detecting edges in the first layer and identifying forms in the middle layer and capturing tasks specific features in the later layer transfer learning utilize the early and Center layers for a pre-pretent model and only retrains the later layer it leverages the label data from its original task for instance if you have a model trained to identify backpacks in images and now want to use it to detect sunglasses we will retrain the later layers to understand the distinguished features of sunglasses from the other objects so moving forward let's see why should you use transfer learning transfer learning offers several advantages including reduced training time improved neural network performance in most cases and the ability to work with limited data training a neural model from scratch typically requires a substantial amount of data which may not always be readily available transfer learning becomes valuable in such scenario here is why you should consider using transfer learning first one is efficient use of data with pre-trained models you can perform well even with limited training data that is specially beneficial in tasks like NLP where obtaining large label data set can be challenging and time Computing the second one is faster training building a deep neural network from a scratch of a complex task can be time consuming taking days or even weeks by leveraging transfer learning the training time is significantly reduced as you start with a model that has already learned General features from a related problem now moving forward let's see steps to use transfer learning the first one is training a model to reuse it in machine learning training a model involves providing it with the data to learn patterns and make prediction once a model is trained on a specific task it can be reused and repurpose for related tasks saving time and computational resources the second one is using a pre-trained Model A pre-trained model is a model that has already been trained on a larger set for a specific task instead of training a model from scratch using a pre-trained model as a starting point allow us to benefit from the knowledge it has gained during its previous training the third one is extraction of features feature extraction is a process in which meaningful patterns and characteristics are identified and separated from a raw data in the context of machine learning it involves identifying relevant information from input data to feed into a model for a better prediction the fourth one is extraction of features in neural networks a neural networks feature extraction involves identifying important patterns or features in the data at different network layers the early layers typically capture simple features like edges while deeper layers capture more complex feature relevant to the task at hand this hierarchical representation enables neural network to learn and generalize from the data effectively so moving forward let's see some popular models trained using transfer learning so numerous machine learning models have been trained using transfer learning some popular ones include for the first one is vgg16 and vgg 90. these models were trained on the image net data set for image classifications test the second one is inception V3 these models were pre-trained on imagenet and are known for their effectiveness in which rapid in object detect repeat in object detection and object recognition the third one is bird bi-directional encoder representation from Transformer this language model is written on the extensive text collection and find extensive application in NLP tasks like sentimental analysis and name entity recognition the fourth one is GPT generative pre-trained Transformer series these models are printed language models for various NLP tasks these are just a few example of pre-trained models that have been used in transfer learning to accelerate training and improve performance across different tasks and with that we have come to end of this video on what is transfer learning I hope you found it useful and entertaining please ask any question about the topics covered in this video in the comments box below our experts will assist you in addressing your problem thank you for watching stay safe and keep learning with simply learning staying ahead in your career requires continuous learning and upskilling whether you're a student aiming to learn today's top skills or a working professional looking to advance your career we've got you covered explore our impressive catalog of certification programs in Cutting Edge domains including data science cloud computing cyber security AI machine learning or digital marketing designed in collaboration with leading universities and top corporations and delivered by industry experts choose any of our programs and set yourself on the path to Career Success click the link in the description to know more hi there if you like this video subscribe to the simply learned YouTube channel and click here to watch similar videos turn it up and get certified click here foreign"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Recurrent Neural Networks (RNNs) for Sequence Modeling",
        "Video Title":"Recurrent Neural Networks (RNNs), Clearly Explained!!!",
        "URL":"https:\/\/www.youtube.com\/watch?v=AsNTP8Kwu80",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/AsNTP8Kwu80\/hqdefault.jpg",
        "ID":"AsNTP8Kwu80",
        "Publish Time":"2022-07-11T04:00:06Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"Hello, I'm Josh Starmer and welcome to StatQuest. Today we're going to talk about recurrent neural networks, and they're going to be clearly explained! Lightning and Grid are totally cool. Check them out when you've got some time. NOTE: This StatQuest assumes that you are already familiar with the main ideas behind neural networks, backpropagation and the ReLU activation function. If not, check out the 'Quest. ALSO NOTE: Although basic, or vanilla recurrent neural networks are awesome, they are usually thought of as a stepping stone to understanding fancier things like Long Short-Term Memory Networks and Transformers, which we will talk about in future StatQuests. In other words, every Quest worth taking take steps, and this is the first step. So with that said, let's say Hi to StatSquatch. Hi! And StatSquatch says, Hello! The other day I bought stock in a company called Get Rich Quick, but the next day their stock price went down and I lost money. Bummer. So, I was thinking, maybe we could create a neural network to predict stock prices. Wouldn't that be cool? That sure would be cool 'Squatch, unfortunately the actual stock market is crazy complicated and we'd probably both get in a lot of trouble if we offered advice on how to make money with it, but if we go to that mystical place called StatLand things are much simpler and there are far fewer lawyers. So let's build a neural network that predicts stock prices in StatLand. However, first let's just talk about stock market data in general. When we look at stock prices, they tend to change over time. For example, the price of this stock went up for four days before going down. Also, the longer a company has been traded on the stock market, the more data we'll have for it. For example, we have more time points for the company represented by the blue line then we have for the company represented by the red line. What that means is, if we want to use a neural network to predict stock prices, then we need a neural network that works with different amounts of sequential data. In other words, if we want to predict the stock price for the Blue Line company on day 10, then we might want to use the data from all nine of the preceding days. In contrast, if we wanted to predict the stock price for the Red Line company on day 10, then we would only have data for the preceding five days. So we need the neural network to be flexible in terms of how much sequential data we use to make a prediction. This is a big difference compared to the other neural networks we've looked at in this series. For example, in Neural Networks Clearly Explained, we examined a neural network that made predictions using one input value, no more and no less. And if you saw the StatQuest on neural networks with multiple inputs and outputs, you saw this neural network that made predictions using two input values, no more and no less. And in the StatQuest on Deep Learning Image Classification, you saw a neural network that made a prediction using an image that was six pixels by six pixels, no bigger and no smaller. However, now we need a neural network that can make a prediction using the nine values we have for the blue company and make a prediction using the five values we have for the red company. The good news is that one way to deal with the problem of having different amounts of input values is to use a Recurrent Neural Network. Just like the other neural networks that we've seen before, recurrent neural networks have weights, biases, layers and activation functions. The big difference is that recurrent neural networks also have feedback loops. And, although this neural network may look like it only takes a single input value, the feedback loop makes it possible to use sequential input values, like stock market prices collected over time, to make predictions. To understand how, exactly, this recurrent neural network can make predictions with sequential input values, let's run some of StatLand's stock market data through it. In StatLand, if the price of a stock is low for two days in a row, then, more often than not, the price remains low on the next day. In other words, if yesterday and today's stock price is low, then tomorrow's price should also be low. In contrast, if yesterday's price was low and today's price is medium, then tomorrow's price should be even higher. And when the price decreases from high to medium, then tomorrow's price will be even lower. Lastly, if the price stays high for two days in a row, then the price will be high tomorrow. Now that we see the general trends in stock prices in StatLand, we can talk about how to run yesterday and today's data through a recurrent neural network to predict tomorrow's price. The first thing we'll do is scale the prices so that low equals 0, medium equals 0.5, and high equals 1. Now let's run the values for yesterday and today through this recurrent neural network and see if it can correctly predict tomorrow's value. Now, because the recurrent neural network has a feedback loop, we can enter yesterday and today's values into the input sequentially. We'll start by plugging yesterday's value into the input. Now we can do the math just like we would for any other neural network. Beep. Boop. Beep. Boop. Boop. At this point, the output from the activation function, the y axis coordinate that we will call Y sub 1, can go two places. First Y sub 1 can go towards the output. And if we go that way and do the math beep, boop, boop then the output is the predicted value for today. However, we're not interested in the predicted value for today because we already have the actual value for today. Instead, we want to use both yesterday and today's value to predict tomorrow's value. So, for now, we'll ignore this output, and instead, focus on what happens with this feedback loop. The key to understanding how the feedback loop works is this summation. The summation allows us to add Y sub 1 times W sub 2, which is based on yesterday's value, to the value from today times W sub 1. In other words, the feedback loop allows both yesterday and today's values to influence the prediction. Hey, this feedback loop has got me all turned around. Is there an easier way to see how this works? Yes! There's an easier way to see what's going on. Instead of having to remember which value is in the loop, and which value is in the input, we can unroll the feedback loop by making a copy of the neural network for each input value. Now, instead of pointing the feedback loop to the sum in the first copy, we can point it to the sum in the second copy. By unrolling the recurrent neural network, we end up with a new network that has two inputs and two outputs. The first input is for yesterday's value, and if we do the math straight through to the first output like we did earlier, we get the predicted value for today. However, as we saw earlier, we can ignore this output. This second input is for today's value and the connection between the first activation function and the second summation allows both yesterday and today's values to influence the final output, which gives us the predicted value for tomorrow. Now, when we put yesterday's value into the first input and we do the math just like before beep boop beep boop then we follow the connection from the first activation function to the summation in the second copy of the neural network. Now we put today's value into the second input and keep doing the math beep boop beep boop beep boop beep and that gives us the predicted value for tomorrow, zero, which is consistent with the original observation. In other words, the recurrent neural network correctly predicted tomorrow's value. Likewise, when we run yesterday and today's values for the other scenarios through the recurrent neural network we predict a correct values for tomorrow. This recurrent neural network performs great with two days worth of data, but what if we have three days of data? When we want to use three days of data to make a prediction about tomorrow's price, like this, then we just keep unrolling the recurrent neural network until we have an input for each day of data. Then we plug the values into the inputs, always from the oldest to the newest. In this case, that means we start by plugging in the value for the day before yesterday, then we plug in yesterday's value, and then we plug in today's value. And when we do the math, the last output gives us the prediction for tomorrow. NOTE: Regardless of how many times we unroll a recurrent neural network, the weights and biases are shared across every input. In other words, even though this unrolled network has three inputs the weight, W sub 1, is the same for all three inputs. And the bias, B sub 1, is also the same for all three inputs. Likewise, all of the other weights and biases are shared. So, no matter how many times we unroll a recurrent neural network, we never increase the number of weights and biases that we have to train. Okay, now that we've talked about what makes basic recurrent neural networks so cool, let's briefly talk about why they are not used very often. One big problem is that the more we unroll a recurrent neural network, the harder it is to train. This problem is called The Vanishing \/ Exploding Gradient Problem. Which is also known as the \"hey wait, where the gradient go?\" problem. In our example, The Vanishing \/ Exploding Gradient Problem has to do with the weight along the squiggle that we copy each time we unroll the network. NOTE: To make it easier to understand the Vanishing \/ Exploding Gradient Problem, we're going to ignore the other weights and biases in this network and just focus on W sub 2. Also, just to remind you when we optimize neural networks with backpropagation, we first find the derivatives, or gradients, for each parameter. We then plug those gradients into the gradient descent algorithm to find the parameter values that minimize a loss function, like the sum of the squared residuals. Bam. Now, even though the Vanishing \/ Exploding Gradient Problem starts with Vanishing, we're going to start by showing how a gradient can explode. In our example, the gradient will explode when we set W sub 2 to any value larger than one. So let's set W sub 2 equal to 2. Now, the first input value, input sub 1, will be multiplied by 2 on the first squiggle and then multiplied by 2 on the next squiggle and again on the next squiggle and again on the last squiggle. In other words, since we unrolled the recurrent neural network four times, we multiply the input value by W sub 2, which is 2, raised to the number of times we unrolled, which is 4. And that means the first input value is amplified 16 times before it gets to the final copy of the network. Now, if we had 50 sequential days of stock market data, which to be honest, really isn't that much data, then we would unroll the network 50 times, and 2 raised to the 50 power is a huge number. And this huge number is why they call this an Exploding Gradient Problem. If we tried to train this recurrent neural network with backpropagation, this huge number would find its way into some of the gradients, and that would make it hard to take small steps to find the optimal weights and biases. In other words, in order to find the parameter values that give us the lowest value for the loss function, we usually want to take relatively small steps. Bam. However, when the gradient contains a huge number, then we'll end up taking relatively large steps and instead of finding the optimal parameter we will just bounce around a lot. Bummer. One way to prevent the Exploding Gradient Problem would be to limit W Sub 2 to values less than 1. However, this results in the Vanishing Gradient Problem. \"Hey, wait, where the gradient go?\" To illustrate the Vanishing Gradient Problem, let's set W sub 2 to 0.5. Now, just like before, we multiply the first input by W sub 2 raised to the number of times we unroll the network. So if we have 50 sequential input values, that means multiplying input sub 1 by 0.5 raised to the 50th power and 0.5 raised to the 50th power is a number super close to zero. Because this number is super close to zero. This is called The Vanishing Gradient Problem. Now when optimizing a parameter, instead of taking steps that are too large, we end up taking steps that are too small. And as a result, we end up hitting the maximum number of steps we are allowed to take before we find the optimal value. Hey, Josh, these Vanishing \/ Exploding Gradients are a total bummer. Is there anything we can do about them? Yes, and we'll talk about a popular solution called Long Short-Term Memory Networks in the next StatQuest. Now, it's time for some Shameless Self Promotion. If you want to review statistics and machine learning offline, check out my book. The StatQuest Illustrated Guide to Machine Learning at statquest.org. It's over 300 pages of total awesomeness. Hooray! We've made it to the the end of another exciting StatQuest. If you liked this StatQuest and want to see more, please subscribe. And if you want to support StatQuest, consider contributing to my patreon campaign, becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie, or just donate. The links are in the description below. Alright, until next time Quest on!"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Recurrent Neural Networks (RNNs) for Sequence Modeling",
        "Video Title":"Recurrent Neural Networks (RNNs)",
        "URL":"https:\/\/www.youtube.com\/watch?v=1jnxVAvlwS4",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/1jnxVAvlwS4\/hqdefault.jpg",
        "ID":"1jnxVAvlwS4",
        "Publish Time":"2023-12-02T15:00:02Z",
        "Channel":"Computing For All",
        "Channel ID":"UC-NR0V8EWwisDqbET9XCGoA",
        "Transcript":"recurrent neural networks or rnns are a type of neural network specially designed for sequential data like words in text or time series in finance data unlike traditional networks rnns have a memory element rnn's process sequences step by step retaining information from previous steps this memory allows them to make predictions considering the context of earlier data rnns are used for tasks like language translation where understanding the sequence of words is crucial however rnns face challenges like the vanishing gradient problem making them less efficient for long sequences that's where Advanced versions like lstms and grus come in"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Recurrent Neural Networks (RNNs) for Sequence Modeling",
        "Video Title":"Recurrent Neural Networks (RNNs) Explained in Depth: Working with Sequential Data",
        "URL":"https:\/\/www.youtube.com\/watch?v=5jiqJymfRf0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/5jiqJymfRf0\/hqdefault.jpg",
        "ID":"5jiqJymfRf0",
        "Publish Time":"2023-01-05T06:26:39Z",
        "Channel":"Active Learning",
        "Channel ID":"UCc8q7XD_EgmLtOYyfkYNa9w",
        "Transcript":"hey everyone it's active learning here welcome back to another video in today's video we'll be discussing about recurrent neural networks and specifically we will be working with sequential data so let's jump right into our video so some of the types of data that we have worked with before are tabular data and image data and tabular data are basically data organized and rows and columns or in other words tables and image data are basically this image of this doc here made of tiny tiny pixel values for these colors right okay but what if our type of input is a sequence of images or in other words a video or what if it's a piece of audio or the price of a stock over a period of Time how can we deal with those kind of data well that's where we will be introducing recurrent neural networks so let's first go over some of the applications and things we can do with recurring neural networks there are countless numbers of tasks that actually deals with sequential data and they are extremely common recurrent neural networks basically allows us to deal with sequential data or data that are in sequences such as that example of an audio right some of the examples can be text to speech image captioning and music generation they're all common examples of tasks requiring RN or recurring neural networks so let's see how our ends actually work right um they're actually a type of artificial neural network that are able to process sequential data like we talked about right and this structure might look very grotesque and strange to you because we haven't worked with anything like this before but how this works is that instead of a single Vector of input like like a typical neural net we now have multiple um input vectors and it depends on the size of the sequential data right for example here we can have an input Vector here input Vector here and input Vector all the way to T and each of these are called a Time step and within each time step we have the output the hidden layer and the input and how recurring neural networks work is that they're doing this by recurrent connections which allow information to flow through the network multiple times allowing the network to maintain a memory of the previous input right for example if I'm at the first time step I could have access to the memory of the zeroth time step right this makes recurring neural networks very well suited for tasks that involve sequential data as they can take into account the context and dependencies between elements in the sequence right um you can think of recurring neural networks as a feed for neural network where each layer's parameters are shared across the time step right the parameters are shared okay and feed 4 basically means that the data is only in One Direction which is forward or to the right in our case right and we aren't actually looping backwards we will not be looping back and a Time step is essentially a specific point in the sequential processing of the data and it is not necessarily a standard unit of time such as seconds or minutes right it's highly dependent on the type of data you're working with okay now let's talk about working with sequences or sequential data now we need to have t feature vectors instead of a single feature Vector where each feature Vector lies in a timestamp right this is it time step remember and the feature of vectors are each time step maybe dependent on previous feature vectors right this is why we use recurrent neural networks right we need to capture the um the relationship between all the previous time steps for example right we may have the medical the medical prescription of a patient might change the on the 10th day right and that is entirely dependent on what happened in the previous nine days right so that is an example of where we might apply our ends right okay and the output for a recurring neural network could be a single value or a sequential value right or sequential output right and for example if you have a sequence of stock prices and you only want to return buy or sell that would be a single value now it could be a sequence of values such as when you want to caption what is going on in an image or video now let's talk about how we can work with sequential to sequential value tasks right all right so sequential to sequential task basically sequential data is the input and the output is also sequential sequential values in this case so there are two types and the first one is called a line which means the input at each time step aligns with a corresponding output right and an example of this will be speech recognition and the second one is called online where the input and output do not have a specific step for step correspondence and this could be translating a piece of text right right okay so I think that was pretty easy to understand so each one will align with it out with a part of the output right now let's go discuss about Auto regressive models so let's let's do an example here let's say our data is the stock price of the NASDAQ 100 composite over the past uh 30 years right so when it comes to statistical tools how how might we um yeah let's first look at the data how might we model this data right so we can come up with some intuition when it comes to statistics um we will focus on the stock price data from this Index right we'll look at the price and the price of the index I A Time step is done denoted by x sub T right let's just say that now how might we predict the time uh let yeah let's talk about price prediction if we only look at the historical price of the stock without considering news financial data and others we are then only interested in knowing the probability distribution of the prices right we want to only know the probability of a certain price so we could train a linear regression model for estimating the conditional expectation and these are called Auto regressive models since it predicts the current value as a function of its own past values right the term Auto refers to the self-dependence of the time series while regressive refers to the use of a regression-like model to estimate the dependency between the time steps right okay so we could try this but there is a problem because the value of T varies depending on how long a stock has been in the market so we're facing the problem of different number of features right so how might we fix this problem right how can we treat this problem well there are two potential Solutions and we will work with these type of solutions later on as well they're very common right so the first one since we only care about what happens in the near future we might limit T to a specific number right and that number it has to be less than T right because for example we only want to go back five years right if the stock has only been around for three years then we can't actually limit T to five years right so that's something to think about now the number of features in our feature vectors are the same now if we limit our T to be we limit T to a specific number that is less than t and the second solution is that we could also have a model that maintained the summary uh let's call that H at time step T in addition to predicting the new price this allows us to model the new price with age right with our summary of the previous steps and we can update H afterwards with a new T value right so these two are the solutions that we might apply and these could apply to a lot of different problems that might encounter this problem right okay so now let's talk about sequence models well what is this sequence model sequence model essentially allows us to estimate the joint probability of an entire sequence right um so for example evaluating the likelihood of specific sentences occurring right when you do Google search it will evaluate oh what's likely to be typed next right this is using recurring neural network this is using sequence models right this will use an auto regressive model in the form of a probabilistic classifier since we're doing classification to Output a full probability distribution over the vocabulary for what Warren or phrases will come next given the previous sentences right this is an application of Auto regressive models now let's talk about Markov models right what are mock Markov models well Markov models follow a very strict property called the Markov property it basically states that the future is independent of the past and we only we are only given the current state right we are training the future given the only current state we aren't training based on the past days now how does this apply to sequence models if we only care about the T previous time steps rather than the entire sequence then we don't need the previous States right we can just remove them and that won't actually affect our predictions at all in the case of predicting next words in a sentence right the dependency on previous text will only diminish as we move onwards right for example if I'm predicting the next word I I wouldn't rely on something from two sentences ago right that wouldn't make that much sense and in most modern return neural networks and Transformer based models there are rarely any tea that are that is greater than the thousands right that's typically the limit there and um for naming convention wise we usually call it the kth Markov model right for example if T is one we'll call it the first order Markov model right that's an example now let's go over an example of apply Markov models is specifically Markov chains um suppose we have a system that can be in one of two states ready or sunny and at each timestamp the system can either stay in the same state or transition to the next state we can represent the probability of transitioning between states using a Markov chain as shown below right so how this essentially works is that if our current state is rainy we can say that um the likelihood of being rainy again after this current state is 0.7 and the likelihood of Being Sunny following a rainy day is 0.3 and same thing for sunny here now what is the probability of the sequence of weathers this sequence of weather rainy sunny rainy what what are what what is the probability of this pattern repeating right well we can use we can calculate this probability by using Markov chains to estimate the probability of this sequence occurring by multiplying the probabilities of the transitions between states that will produce this sequence so the probability of this sequence occurring is in our case 0.063 right we're multiplying the probabilities together right grainy to Sunny 0.3 times Sunny to rainy which is 0.3 and then rainy 2 range which is 0.7 right so the um yeah so this will give us some idea of how likely that this particular sequence of weathers weather conditions would occur so this is our first series at first video in our Series so describing and analyzing recurring neural networks in depth and if you got this far in the video and if you found it helpful please like And subscribe these do take a long time to make so yeah I hope you guys found this video helpful thank you for watching and see you guys next time"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Recurrent Neural Networks (RNNs) for Sequence Modeling",
        "Video Title":"What is Recurrent Neural Network (RNN)? Deep Learning Tutorial 33 (Tensorflow, Keras &amp; Python)",
        "URL":"https:\/\/www.youtube.com\/watch?v=Y2wfIKQyd1I",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Y2wfIKQyd1I\/hqdefault.jpg",
        "ID":"Y2wfIKQyd1I",
        "Publish Time":"2021-01-12T13:30:10Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"So far in our deep learning tutorial series we looked at Artificial Neural Network and Convolutional Neural Network which is mainly used for image processing. In this video we will talk about Recurrent Neural Network which is used mainly for natural language processing tasks so if you think about deep learning overall, CNNs are mainly for images, RNNs are mainly for NLP. There are other use cases as well so we'll understand how Recurrent Neural Network works and we'll look at different applications of RNN in the field of NLP as well as some other domains. We will be looking at some real-life use cases where sequence models are useful. You must have used Google mail-Gmail. Here, when you type in a sentence it will auto complete it. So just see, when I type \"not interested at this time\" is something it auto completed. So google has this RRN or Recurrent Neural Network embedded into it where, when you type in a sentence \"not interested at\" it will auto complete with \"this time\". If you say \"we'll let you know if it changes\" it will also say \"in the future\" so this saves you time. It will write the sentence for you. Another use case is translation. You must have used Google Translate where you can translate sentence from one to another language easily. Third use case is Named Entity Recognization where in the X you know you give Neural Network a statement and in the Y Neural Network will tell you the person name the company and time. Rudolph Smith must be a millionaire with Tesla's prices skyrocketing. So these are various use cases where using sequence models or RNN-Recurrent Neural Network helps. The fourth use case is Sentiment Analysis where you have a paragraph and it will tell you the sentiment whether this product review is One star, Two star and so on. Now you would think - Why can't we use a simple Neural Network to solve this problem? See all these problems they are called Sequence Modeling problem because the sequence is important. When it comes to human language sequence is very important. For example when you say, \"how are you?\" versus \"you are how\" doesn't make sense, right? So the sequence is important here and you would think - Why don't we use simple neural network for that? Well, let's try it. So for language translation how about we build this kind of neural network we know where input is the English statement and the output could be Hindi statement Once I build this network, what if my sentence size changes? So i might be inputting different sentence size and with a fixed neural network architecture it's not going to work because you have to decide how many neurons are there in the input and output layer. So with language translation, number of neurons becomes a problem. Like what do you decide as a size of neurons? Now one would argue okay I would decide let's say a huge size let's say 100 neurons and remaining if I am saying, did you eat biryani? So it will occupy 4 neuron. Remaining 96 I will just say 0 or you know blank statement. That might work but still it's not ideal. The second issue is too much computation. You all know neural networks work on numbers, they don't work on string. So you have to convert your word into a vector. So one of the ways of converting that into a vector is -l et's say there are 25000 words in your vocabulary and you will do one hot encoding where you know \"how\" let's say is at 46th position \"are\" is let's say second position \"you\" is let's say at 17000th position. So at that position you put 1, remaining position you put 0 and that's called one hot encoding. You have to do similar thing for output as well. But you realize this will increase too much computation. Each of the word when you convert it to a vector you know how many neurons you need in the input layer itself. Its humongous. The third issue is this - Sometimes when you translate language you for let's say two different English statements you might have a Hindi statement. So in this case when I say \"On sunday I ate golgappa\" let's say I train this network based on this statement and then for 'On Sunday' let's say it will adjust the weights of all these edges which I have highlighted in yellow color. Same statement I can say differently. I can say \"I ate golgappa on Sunday\". So now on Sunday the meaning of on Sunday is same but here neural network has to learn different set of edges you see all these edges are in yellow color. So the parameters are not shared. We looked at in our Convolutional Neural Network tutorial as well that by using convolution operation we can share the parameters. Here, the use of ANN or Artificial Neural Network doesn't allow you to do that okay. Also the most important part in all this discussion is the sequence. See when you have structured data, for example you're trying to figure out if the transaction is fraud or not and let's say your features are transaction amount, whether the transaction was made out of country or whether the SSN that customer provided is correct or not. Now here if you change the order of this features, let's say 'ssn correct?' I supply you know my first neuron it's not going to affect anything you know because the sequence in which you supply the input doesn't matter. Whereas if you have English to Hindi translation and instead of saying \"I ate golgappa on sunday' and if I say 181 00:06:25,360 --> 00:06:31,600 \"I ate Sunday on golgappa\" the meaning becomes totally different. So now you cannot say that the Hindi translation is 'ravivar ko mene golgappe khaye\" because it becomes invalid so sequence is very very important that's why Artificial Neural Network doesn't work in this case. Just to summarize these are the three major problems with using ANN for sequence problems. Let's once again talk about Named Entity Recognition. Let's say 'Dhaval loves baby yoda' I love my baby grogu. I love Mandalorian series and we have got this nice baby grogu at our home which actually talks with us. In this statement Dhaval and baby yoda are person names, okay. So the whole purpose of Named Entity Recognization is to find out the entity you know like 'Dhaval' as an entity is a person 'baby yoda' as an entity as a person so that's the whole goal of NER. Now you can represent this as ones and zero. So if the word is person's name you would mark it as one and if it is not a person's name you would mark it as zero. so let's see how RNN works here. RNN is also called Recurrent Neural Network. so first of all you have to convert Dhaval into some vector. It doesn't matter how you convert 216 00:07:57,199 --> 00:08:01,680 it you can take a vocabulary and use one hot encoding and there are other ways of vectorizing a word. Then you have a layer of neurons. So these are all individual neurons. Let's say this is one layer. It's a hidden layer you supply that and you get one output okay. So each neuron all you know has a sigma function and activation function. So now while processing the statement 'Dhaval loves baby yoda' now I will process it word by word. So I supply 'Dhaval', get the output and then I go back again. Now I supply 'loves' converted into vector and the previous output which I got which was y Dhaval I now supply that as an input to this layer. So you see the input of the layer is not only the next word but the previous output because the language makes sense. Language needs to carry the context if I have just a word loss and if I don't have Dhaval in front of it it might mean a different thing. So there is a context that you need and this kind of architecture provides your context or a memory. In the third word again you supply 'baby' to the same network right. So our network has only one layer. it has only one layer, so there is input layer output layer and the hidden layer is just one and it has bunch of neurons. In that we are repeatedly processing word one by one okay and you keep on doing this. Now the benefit of this is when i'm processing 'baby' when i get why loves that 'why loves' carries the state, the previous state or previous memory of 'Dhaval loves' you know the whole statement. Now i'm presenting this in a different way make sure these are not four different hidden layers. This is a time travel okay so actual hidden layer is only one. I am just doing a time travel. So first when I supplied word 'Dhaval' i got this output and output was nothing but the activation function which I am denoting with a1 and you need some previous activation a0 as well. Let's say it's a vector of all zeros then you supply second word 'loves' and use the previous output which was yDhaval so yDhaval and a1 they are both same here and then you get another output a2 where that you supply along with the third word 'baby' to the same network. So these four neurons it's the same single layer. I am just showing the status of it at different times okay. So you have to be very clear on this that these are not four different layers. It's just one layer just because I am showing different time steps that's why I'm showing you almost a time travel here and once the network is trained of course it will output like 'Dhaval' is one 'loves' is zero 'baby' is one and so on okay. So you get your NER output individually here. One other way of representing the same network okay because just to avoid confusion and to make presentation little more clear. Many times in literature you will see presentation like this - Where each word which is an input comes from the bottom and there is activation. So again this and these two diagrams are exactly same okay I'm just putting this word at the bottom. Generic Representation of RNN is this. So this is the real representation. You have only one layer and you are you are kind of almost in a loop. You are supplying the output of previous word as an input to the second word. So now let's talk about training. So again the problem we are talking about is NER where these are my training samples okay x and y. 'x' is a statement 'y' is whether a given word is person name or not so we are processing first training sample 'Dhaval loves baby yoda' so this one I will first initialize my neural network weights with some random values, then I supply each word, then I calculate y hat which is predicted y, then I compare with the real y so real y here is 1 0 1 1 so I compare that with here so 1 0 1 1 I compare that with y hat and I find out the loss okay and then I sum the loss. So that will be my total loss. You all know about grade and descent right. So we compute the loss then we back propagate the loss and we adjust the weights. So now I take the second statement 'Iron man punched on hulk's face' he was very angry with hulk. Again i calculate all the losses then I find total loss and then I do grid and decent to reduce the loss. So i keep on doing this for all my training samples. Let's I have 100 training samples. Passing all hundred training samples through this network will be one epoch. We might do let's say 20 epochs and at the end of the 20 epoch my loss might become very minimum. At that point we can say my neural network is trained. Let's take a look at language translation. So in language translation what happens is you supply first word to your network then you get the output then again same network you supply second word and the output from previous step as an input and of course when you supply first where you have to pass in some activation values let's say all a vector of all zeros. Then you supply third word for fourth word and so on and when you're done with all the words that's when the network starts to translate it because you cannot translate one word by one, because after the statement I can push maybe one more word and that will just totally change my translation. That's why for language translation you have to supply all the words and only then the network can translate for you. So the network will translate it like this and the first part is called encoder the second part is called decoder. We will go more in depth into all this but I want to quickly demonstrate how the neural network looks in the case of language translation. Now this layer doesn't have to be just single layer. It can be a deep RNN as well where the actual network might have multiple hidden layers okay. So I hope that clarifies the architecture behind RNN and you understand why you can't use simple neural network here and you have to use specialized neural network called RNN which can memorize for you, which can remember previous state because language is all about sequence. If you change the sequence the meaning changes so if you like this video please give it a thumbs up and we'll be having more Recurrent Neural Network and NLP type tutorials in the future videos Thank you."
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Long Short-Term Memory (LSTMs) and Gated Recurrent Units (GRUs)",
        "Video Title":"Illustrated Guide to LSTM&#39;s and GRU&#39;s: A step by step explanation",
        "URL":"https:\/\/www.youtube.com\/watch?v=8HyCNIVRbSU",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/8HyCNIVRbSU\/hqdefault.jpg",
        "ID":"8HyCNIVRbSU",
        "Publish Time":"2018-09-19T23:02:15Z",
        "Channel":"The AI Hacker",
        "Channel ID":"UCYpBgT4riB-VpsBBBQkblqQ",
        "Transcript":"hi and welcome to an illustrated guide to LST ms in gr use I learned vector MMA machine learning engineering a I voice assistant space in this video we'll start with the intuition behind LST M s and G are used then I'll explain the internal mechanisms that allow the LST M s and G are use to perform so well I'll follow up with an example if you want to understand what's happening under the hood for these two networks then this videos for you LST M's and GR use are the more evolved versions of the vanilla recurrent neural networks during back propagation recurrent know now we're suffering from the vanishing gradient problem the gradient is the value used to update a neural networks weight the vanishing gradient problem is when a gradient strengths as it back propagates through time of a gradient value becomes extremely small it doesn't contribute to much learning so in recurrent no networks layers that get a small gradient update doesn't learn those are usually the earlier layers so because these layers don't learn RN ends can forget what is seen and longer sequences does having short term memory if you want to know more about the mechanics of recurrent neural networks in general you can watch my previous video in illustrated guide to recurrent neural networks LST MS in gr use were created as the solution to short-term memory they have internal mechanisms called gates that can regulate the flow of information these gates can learn which data in a sequence is important to keep or throw away by doing that it learns to use relevant information to make predictions almost all state-of-the-art results based on recurrent neural networks are achieved with these two networks you can find LS CMS and gr use and speech recognition models speech synthesis text generation you can even use them to generate captions for videos okay so by the end of this video you should have a solid understanding of why LST ms and gr use or get up processing long sequences i am going to approach this with an intuitive explanation and illustrations and avoid as much math as possible let's start with a thought experiment let's say you're reading reviews online to determine if you want to buy a product when you read review your brain subconsciously only remember important keywords if your goal is trying to judge if a certain review is good or bad you pick up words like amazing and will definitely buy again you don't care much for words like this gave all should etc if a friend asked you the next day what the review said you wouldn't remember it word by word you might remember the main points though like it was a perfectly balanced breakfast the other words would just fade away from memory unless of course you're one of those people with perfect memory that is essentially what an LST m and a GRU does I can learn to keep only relevant information to make predictions in this case the words you remember made you judge that with a good review to understand how STM's or GRU so chooses let's review two recurrent neural network in our networks like this first words gets transformed to machine readable vectors then dione and processed a sequence of vectors one by one while processing it passes the previous head state to next step of the sequence in a hidden state access to neural networks of memory it holds information on previous data that the network has seen before let's zoom into the cell of an RNN to see how the hidden state is calculated first the input in the previous hidden state is combined to form a vector that vector has information on the current input in the previous inputs the vector goes through the teen activation in the output is the new hidden state or the memory of the network the tan activation is used to help regulate the values flowing through the network the tan function squishes values to always be between negative 1 and 1 when vectors are flung the rain' neural network undergoes many transformations due to various math operations so imagine a value that continues to be multiplied by let's say 3 you can see how some values can explode and become astronomical causing other values to seem insignificant let's see what a tan does a tan function ensures that the values stay between negative 1 and 1 thus regulated and neural networks output so that's an R net it has very few operations internally but works pretty well RN n uses a lot less computational resources than its involved variants let's take a look at LS and LS TM has the same control flow as a recurrent neural network it processes data sequentially passing on information as it propagates forward the difference are the operations within the LST upsells' these operations are used to allow to lsdm to forget or keep information now looking at these operations can get a little overwhelming so we'll go over this one by one I want to thank Chris Ola sorry about butchered that name he has an excellent blog post on LS TMS the following information is inspired by his exceptionally written blog post I'll include the links in the description the core concepts of L SCM sort are cell states and its various gates the cell state acts as a transport Highway that transfers relative information all the way down to the sequence chain you can think of as a memory of the network because the cells they can carry information throughout the sequence processing and theory even information from earlier time steps could be carried all the way to the last time step thus reducing the effects of short-term memory as it goes on it's dirty information gets added or removed to the cell state via gates the gates are just different neural networks that decides which information is allowed on the cell state the gates learn what's information is relevant to keep or forget during training gates contain sigmoid activation X sigmoid activation is similar to the 10 instead of squishing values between negative 1 and 1 squishes values between 0 & 1 that is helpful to update or forget data because any number getting multiplied by 0 is 0 causing values to disappear or be forgotten any number multiplied by 1 is the same value therefore that value stays the same or is kept the network can learn what data should be forgotten or what data is important to keep let's dig a little deeper into what the various gates are doing so we have three different gates that regulate information flow in an LS TM cell I'll forget gate input gate and output gate first we have to forget gate this gate decides what information should be thrown or kept away information from a previous hidden State and information from the current input is passed through the sigmoid function values come out between 0 and 1 the closer to 0 means forget and a closer to 1 means to keep to update the cell state we have the input gate first we passed that previous head and stay in the current input to a sigmoid function that decides which values will be updated by transforming values to be between 0 and 1 0 means not important one means important you also pass the hidden state and current input into the tan function to squish values between negative 1 and 1 this helps regulate the network then you multiply the tan output with a sigmoid output the sigmoid apple will decide which information is important to keep from the tan output now we should have enough information to calculate the cell state first the cell state is multiplied by the forget vector this has a possibility of dropping values in a Cell State if it is x values near zero then we take the output from implicate and do a polarize Edition which updates the cell state to new values this gives us our new cell state last we have the output gate the applicate decides what the next hidden state should be remember that the hidden state contains information of previous inputs the hidden state is also used for prediction first we pass a previous hidden state in the current input until a sigmoid function then we passed newly modified cell state to the tamp function we multiply the Tanner output with the sigmoid output to decide what information the hidden state should carry the output is the hidden state the new cell state and a new hidden state is then carried over to the next time step for those of you who understand better through seeing code here is an example using python pseudocode to showcase the control flow first the previous hidden state in the current input gets concatenated we'll call it combined combined gets fed into the forget layer this layer removes non-relevant data then the candidate layer is created using combine the output holds possible values to add to the cell state combined also gets fed into the input layer this layer decides what data from a candidate layer should be added to the new cell state after computing to forget layer the candidate layer in the input layer the cell state is computed using those vectors in a previous cell state the output is then computed point wise multiplying the output and a new cell state gives you the new hidden state that's it the control flow of an LS TM is simply a for loop the hem state output it from the LS TM cell can be used for predictions using all those mechanisms annales TM is able to choose which information is relevant to remember or forget during processing so now we know how an ALICE TM works let's look at the GRU the GRU is a newer generation of recurrent neural networks and is pretty similar to an LSD M gr use got rid of the cell state and used a hidden state to transfer information instead it also has two gates a reset gate and an update gate the update gate acts similar to forget in input gate of MLS TM it decides what information to throw away and what new information to add the reset gate is a gate used to decide how much pass information to forget so that's the gr you do use has less tensor operations therefore they are little speedier to train in LS TMS researchers and engineers usually try both to determine which one works better for their use case to sum this up are an ends you're good for processing sequence data for predictions but suffer from short-term memory LS TMS and your use were created as a method to mitigate short-term memory using mechanism called gates gates are just no no words that regulate the flow of information being passed from one time step to the next LS TMS in G reuse are used in stated they are deep learning applications like speech recognition speech synthesis natural language understanding etc if you're interested in going deeper I've added links in a description on some amazing resources that can give you a different perspective and understanding LS TNS and gr use I had a lot of fun making this video so let me know in the comments that this was helpful or what you would like to see the next one if you like this video please subscribe for more AI content thanks for watching"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Long Short-Term Memory (LSTMs) and Gated Recurrent Units (GRUs)",
        "Video Title":"Simple Explanation of GRU (Gated Recurrent Units) | Deep Learning Tutorial 37 (Tensorflow &amp; Python)",
        "URL":"https:\/\/www.youtube.com\/watch?v=tOuXgORsXJ4",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/tOuXgORsXJ4\/hqdefault.jpg",
        "ID":"tOuXgORsXJ4",
        "Publish Time":"2021-02-21T15:00:12Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"We are going to learn about GRU or gated recurrent unit today. This version of RNN is newer than LSTM, this was invented in 2014 and is gaining a lot of popularity. So let's see how this works. You all know that the basic RNN suffer from a short-term memory problem. For example when you are trying to make an autocomplete for a statement where it says Dhaval eats Samosa almost every day it shouldn't be hard to guess that this favorite cuisine is let's say after is you want to make a guess. So in order to guess that it's an Indian cuisine you have to remember samosa but traditional RNN has shorter memory. So for example when you are at the statement is, hypothetical let's assume it remembers only last two words. So when it has cuisine is it cannot predict that this is an Indian cuisine it has to remember samosa. So if you have a network like this which can remember remember the important keywords okay, so let's say when I say Dhaval eats it doesn't remember anything because it's not important and when it comes to samosa it remembers that and it carries that long-term memory all the way then then when you may want to make a guess on what that cuisine is, you can use that memory and you can deduce that this is an Indian cuisine. Now in last video we learned about LSTM and LSTM can do this it can remember long term memory. So if you have not seen that last last video uh please go and watch LSTM video because it will give you understanding on certain concepts. So it has long term memory and short term memory. GRU is a modified or a lightweight version of LSTM where it combines long and short-term memory into its hidden state okay? So you see the basic difference: LSTM has two like cell state and hidden state, here it has only hidden state which can combine both long and short term memory. If you look into GRU box it will have two gates, LSTM had three gates input output and forget, GRU has only two gates update get and reset get and the function of update gate is to remember is to basically it knows how much of pass memory to retain. Whereas reset gate knows how much of pass memory to forget. Now they sound little similar but actually there is some subtle difference and we'll look into it using an example. And there is certain other mechanism which I'm going to show and then in the end you get like output as well as output hidden state. So let's go back to our original uh NLP problem of completing an english sentence. So when you have sentence like this of course the answer is Indian you might have seen gmail right when you type something it tries to auto complete so we are trying to solve the same problem and it knows that it's an Indian cuisine based on this word samosa. But when you have a longer statement like this where you start a second statement saying that his brother loves pasta and cheese, then the answer of this autocomplete would be Italian. So now watch this carefully what happens is when you are until this word okay when you are say cuisine is, then you want to remember about samosa because you have that much in context okay? And when you go further and when you come here till pasta now you know the context has changed. So when it comes to pasta you want to forget about samosa okay? Similarly when it comes to cheese you want to retain the memory of pasta. So now you already see kind of the difference that here you want to forget about samosa when pasta comes, but when you want to go to end and cheese and words like that, you want to retain the memory of pasta such that you can auto complete it saying that it's an Italian cuisine. So in GRU now now this cell that I'm showing you is GRU the first thing is when pasta comes you want to forget about samosa and the way it is handled is using a reset gate. A reset gate takes hidden state ht minus 1 and the current word xt and it will do this mathematical operation like weighted sum applied sigmoid activation on top of it, and the rt value that you get is your reset gate value. When it comes to cheese for example you want to retain the memory of pasta and same applies to the word end okay and that is done by update gate. So what update gate will do is exactly same as reset gate. But it will have different type of weights like if you look at this mathematical equation this w z like previously it was you see w r so the weights are different but mathematical equation is still same. Weighted sum of x d w z ht minus 1 uz and you apply sigmoid activation, so zt that you get here is the value of your update gate. So so far it looks little simple that you are taking update gate value and reset gate value. Now get a little bit complex you will use this value ztrt and then ht minus 1 xt to produce your new hidden state as well as your output. So this is how the final GRU looks like here this multiplication operation that you're seeing is hadamard product so it's a product between two matrices and if you look at the wikipedia article on on you know you can search for hadamard product you will see you have two matrices and it's just taking element by element and just you know multiplying them a1 b11c a1 b1 and then a 1 2 b 1 2 you see a 3 3 b 3 so element wise element product and if you look at the mathematical equation they look like this. So I know it's little complex but maybe you can consume this information you can look at this mathematical equation and you kind of get an idea on what this is. But overall the crux of this whole GRU unit is it combines short-term memory, long-term memory into one state and it is a little bit lightweight than LSTM. So let's see the differences between the two: so LSTM has three gates, GRU has only two. LSTM is more accurate on longer sequences it takes little more time or a little less efficient. There are cases where they both perform equally well but GRU is overall more efficient computation wise and it's getting more popular it was invented in 2014, and LSTM is little old was invented between 95 and 97 and GRU is gaining more popularity. So I hope this video gives you some understanding of GRU. This was all about theory, in the future I will try to cover coding as well. Uh if you like this video please give it a thumbs up and share it with your friends who want to learn about GRU. Thank you!"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Long Short-Term Memory (LSTMs) and Gated Recurrent Units (GRUs)",
        "Video Title":"Long Short-Term Memory (LSTM), Clearly Explained",
        "URL":"https:\/\/www.youtube.com\/watch?v=YCzL96nL7j0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/YCzL96nL7j0\/hqdefault.jpg",
        "ID":"YCzL96nL7j0",
        "Publish Time":"2022-11-07T05:00:15Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"Long short-term memories. I've got them both and so does this network. Hooray! StatQuest! Hello, i'm Josh Starmer and welcome to StatQuest. Today we're going to talk about a Long Short-Term Memory, LSTM, and it's going to be clearly explained! Lightning, yeah! Gonna deploy your models in just a few days not months. Yeah! This StatQuest is also brought to you by the letters A, B and C. A always. B be. C curious. Always be curious. Note: This StatQuest assumes you are already familiar with recurrent neural networks and the vanishing exploding gradient problem. If not, check out the Quest. Also note: Although Long Short-Term Memory totally awesome. It is also a stepping stone to learning about Transformers which we will talk about in future StatQuests. In other words, today we're taking the second step in our Quest. Now, in the StatQuest on basic, vanilla recurrent neural networks we saw how we can use a feedback loop to unroll a network that works well with different amounts of sequential data. However, we also saw that when we plug in the numbers, when the weight on the feedback loop is greater than one, and in this example the weight is 2, then when we do the math we end up multiplying the input by the weight, which in this case is 2, raised to the number of times we unrolled the network. And thus if we had 50 sequential data points, like 50 days of stock market data, which isn't really that much, then we would raise 2 by 50. And 2 to the 50th power is a huge number. And this huge number would cause the gradient, which we need for gradient descent, to explode. Kaboom! Alternatively, we saw that if the weight on the feedback loop was less than 1, and now we have it set to 0.5, then we'll end up multiplying the input value by 0.5 raised to the 50th power. And 0.5 raised to the 50th power is a number super close to zero. And this number super close to 0 would cause the gradient, which we need for gradient descent to vanish. Poof! In summary, basic vanilla recurrent neural networks are hard to train because the gradients can explode. Kaboom! Or vanish. Poof! The good news is that it doesn't take much to extend the basic vanilla recurrent neural network so that we can avoid this problem. So today we're to talk about Long Short-Term Memory, LSTM, which is a type of recurrent neural network that is designed to avoid the exploding \/ vanishing gradient problem. Hooray! The main idea behind how Long Short-Term Memory works is that instead of using the same feedback loop connection for events that happened long ago and events that just happened yesterday to make a prediction about tomorrow, Long Short-Term Memory uses two separate paths to make predictions about tomorrow. One path is for long-term memories, and one is for short-term memories. Bam! Now that we understand the main idea behind Long Short-Term Memory, that it uses different paths for long and short-term memories, let's talk about the details. The bad news is that compared to a basic vanilla recurrent neural network, which unrolls from a relatively simple unit, Long Short-Term Memory is based on a much more complicated unit. Holy smokes, this looks really complicated! Don't worry Squatch, we will go through this one step at a time so that you can easily understand each part. Bam! Note: Unlike the network's we've used before in this series, Long Short-Term Memory uses sigmoid activation functions and tan-h activation functions. So let's quickly talk about sigmoid and tan-h activation functions. In a nutshell, the sigmoid activation function takes any x-axis coordinate and turns it into a y-axis coordinate between 0 and 1. For example, when we plug in this x-axis coordinate, 10, into the equation for the sigmoid activation function, we get 0.99995 as the y-axis coordinate. And if we plug in this x-axis coordinate, -5, then we get 0.01 as the y-axis coordinate. In contrast, the tan-h, or hyperbolic tangent activation function takes any x-axis coordinate and turns it into a y-axis coordinate between -1 and 1. For example, if we plug this x-axis coordinate, 2, into the equation for the tan-h activation function, we get 0.96 as the y-axis coordinate. And if we plug in this x-axis coordinate, -5, we get -1 as the y-axis coordinate. So, now that we know that the sigmoid activation function turns any input into a number between 0 and 1 and the tan-h activation function turns any input into a number between -1 and 1, let's talk about how the Long Short-Term Memory unit works. First, this green line that runs all the way across the top of the unit is called the cell state and represents the long-term memory. Although the long-term memory can be modified by this multiplication and then later by this addition, you'll notice that there are no weights and biases that can modify it directly. This lack of weights allows the long-term memories to flow through a series of unrolled units without causing the gradient to explode or vanish. Now, this pink line, called the hidden state, represents the short-term memories. And as we can see, the short-term memories are directly connected to weights that can modify them. To understand how the long and short-term memories interact and result in predictions, let's run some numbers through this unit. First, for the sake of making the math interesting, let's just assume that the previous long-term memory is 2, and the previous short-term memory is 1, and let's set the input value to 1. Now that we have plugged in some numbers, let's do the math to see what happens in the first stage of a Long Short-Term Memory Unit. We'll start with the short-term memory, 1, times its weight, 2.7. Then we multiply the input, 1, by its weight, 1.63. And then we add those two terms together. And, lastly, we add this bias, 1.62, to get 5.95, an x-axis coordinate for the sigmoid activation function. Now we plug the x-axis coordinate into the equation for the sigmoid activation and function and we get the y-axis coordinate 0.997. Lastly, we multiply the long-term memory, 2, by the y-axis coordinate, 0.997, and the result is 1.99. So this first stage of the Long Short-Term Memory unit reduced the long-term memory by a little bit. In contrast, if the input to the LSTM was a relatively large negative number, like -10, then, after calculating the x-axis coordinate, the output from the sigmoid activation function will be 0. And that means the long-term memory would be completely forgotten because anything multiplied by 0 is 0. Thus, because the sigmoid activation function turns any input into a number between 0 and 1, the output determines what percentage of the long-term memory is remembered. To summarize, the first stage in a Long Short-Term Memory unit determines what percentage of the long-term memory is remembered. Bam! Oh no, it's the dreaded terminology alert. Even though this part of the Long Short-Term Memory unit determines what percentage of the long-term memory will be remembered, it is usually called the Forget Gate. Small bam. Now that we know with the first part of the LSTM unit does, it determines what percentage of the long-term memory will be remembered, let's go back to when the input was 1 and talk about what the second stage does. In a nutshell, the block on the right combines the short-term memory and the input to create a potential long-term memory. And the block on the left determines what percentage of that potential memory to add to the long-term memory. So let's plug the numbers in and do the math to see how a potential memory is created and how much of it is added to the the long-term memory. Starting with the block furthest to the right, we multiply the short term memory and the input by their respective weights. Then we add those values together and add a bias term to get 2.03, the input value for the tan-h activation function. Now we plug 2.03 into the equation for the tan-h activation function and we get the y-axis coordinate, 0.97. Remember the tan-h activation function turns any input into a number between -1 and 1. And in this case, when the input to the LSTM is 1, then after calculating the x-axis coordinate, the tan-h activation function gives us an output close to 1. In contrast, if the input to the LSTM was -10, then after calculating the x-axis coordinate, the output from the tan-h activation function would be -1. Going back to when the input to the LSTM was 1, we have a potential memory, 0.97, based on the short-term memory and the input. Now the LSTM has to decide how much of this potential memory to save. And this is done using the exact same method we used earlier when we determined what percentage of the long-term memory to remember. In other words, after multiplying the short-term memory and the input by weights and adding those products together and adding a bias, we get 4.27, the x-axis input value for the sigmoid activation function. Now we plug the x-axis coordinate into the equation for the sigmoid activation function and we get the y-axis coordinate 1.0. And that means the entire potential long-term memory is retained, because multiplying it by 1 doesn't change it. Note: If the original input value was -10, then the percentage of the potential memory to remember would be 0, so we would not add anything to the long-term memory. Now, going back to when the original input value was 1, we add 0.97 to the existing long-term memory and we get a new long-term memory, 2.96. Double bam! Oh no, it's the dreaded terminology alert! Even though this part of the Long Short-Term Memory unit determines how we should update the long-term memory, it's usually called the input gate. Tiny bam. Now that we have a new long-term memory, we're ready to talk about the final stage in the LSTM. This final stage updates the short-term memory. We start with the new long-term memory and use it as input to the tan-h activation function. After plugging 2.96 into the tan-h activation function, we get 0.99. 0.99 represents a potential short-term memory. Now, the LSTM has to decide how much of this potential short-term memory to pass on. And this is done using the exact same method we used two times earlier: When we determined, what percentage of the original long-term memory to remember and when we determined what percentage of the potential long-term memory to remember. In all three cases, we use a sigmoid activation function to determine what percent the LSTM remembers. In this case when we do the math, we get 0.99. And we create the new short-term memory by multiplying 0.99 by 0.99 to get 0.98. This new short-term memory, 0.98, is also the output from this entire LSTM unit. Oh no, it's the dreaded terminology alert again. Because the new short-term memory is the output from this entire LSTM unit, this stage is called the Output gate. And at long last, the common terminology seems reasonable to me. Triple bam! Now that we understand how all three stages in a single LSTM unit work, let's see them in action with real data. Here we have stock prices for two companies Company A and Company B. On the y-axis we have the stock value and on the x-axis we have the day the value was recorded. Note: If we overlap the data from the two companies, we see that the only differences occur on day 1 and on day 5. On day 1, Company A is at 0 and Company B is at 1. And on day five, Company A returns to 0 and Company B returns to 1. On all of the other days, days 2, 3 and 4, both companies have the exact same values. Given this sequential data, we want the LSTM to remember what happened on day 1 so it can correctly predict what will happen on day 5. In other words, we're going to sequentially run the data from days 1 through 4 through an unrolled LSTM and see if it can correctly predict the values for day 5 for both companies. So let's go back to the LSTM and initialize the long and short-term memories to 0. Now, because this single LSTM unit is taking up the whole screen, let's shrink it down to this smaller simplified diagram. Now, if we want to sequentially run Company A's values from days 1 through 4 through this LSTM, then we'll start by plugging the value for day 1, which is 0, into the Input. Now, just like before we do the math. Boop be doop boop boop boop boop boop. And after doing the math, we see that the new or updated long-term memory is -0.20 and the new updated short-term memory is -0.13. So we plug in -0.2 for the updated long-term memory and -0.1, rounded, for the updated short-term memory. Now we unroll the LSTM using the updated memories and plug the from day 2, 0.5, into the input. Then the LSTM does its math using the exact same weights and biases as before, and we end up with these updated long and short-term memories. Note: If you can't remember the StatQuest on recurrent neural networks very well, the reason the LSTM reuses the exact same weights and biases is so that it can handle data sequences of different lengths. Small bam. Anyway, we unroll the LSTM again and plug in the value for day 3. Then the LSTM does the math again using the exact same weights and biases and gives us these updated memories. Then we unroll the LSTM one last time and plug in the value for day 4. And the LSTM does the math again using the exact same weights and biases and gives us the final memories. And the final short-term memory, 0.0, is the output from the unrolled LSTM. And that means the Output from the LSTM correctly predicts Company A's value for day 5. Bam! Now that we have shown that the LSTM can correctly predict the value on day 5 for Company A, let's show how the same LSTM, with the same weights and biases can correctly predict the value on day 5 for Company B. Note: Remember, on days 1 through 4, the only difference between the companies occurs on day 1, and that means the LSTM has to remember what happened on day 1 in order to correctly predict the different output values on day 5. So let's start by initializing the long and short-term memories to 0. Now, let's plug in the value for day one from Company B, 1. And the LSTM does the math, just like before using the exact same weights and biases. Beep boop. After doing the math, we see that the updated long-term memory is 0.5 and the updated short-term memory is 0.28. So we plug in 0.5 for the updated long-term memory and 0.3, rounded, for the updated short-term memory. Now we unroll the LSTM and do the math with the remaining input values. And the final short-term memory, 1.0, is the output from the unrolled LSTM. And that means the output from the LSTM correctly predicts Company B's value for day 5. Double bam! In summary, using separate paths for long-term memories and short-term memories, Long Short-Term Memory networks avoid the exploding\/vanishing gradient problem, and that means we can unroll them more times to accommodate longer sequences of input data than a vanilla recurrent neural network. At first, i was scared of how complicated the LSTM was, but now I understand. Triple Bam! Now it's time for some Shameless Self-Promotion. If you want to review statistics and machine learning offline, check out the StatQuest PDF study guides and my book the StatQuest Illustrated Guide to Machine Learning at statquest.org. There's something for everyone! Hooray! We've made it to the end of another exciting StatQuest. If you liked this StatQuest and want to see more, please subscribe. And if you want to support StatQuest, consider contributing to my Patreon campaign becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie or just donate, the links are in the description below. Alright, until next time Quest on!"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Long Short-Term Memory (LSTMs) and Gated Recurrent Units (GRUs)",
        "Video Title":"Simple Explanation of LSTM | Deep Learning Tutorial 36 (Tensorflow, Keras &amp; Python)",
        "URL":"https:\/\/www.youtube.com\/watch?v=LfnrRPFhkuY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/LfnrRPFhkuY\/hqdefault.jpg",
        "ID":"LfnrRPFhkuY",
        "Publish Time":"2021-02-06T15:30:08Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"Seen this movie called Memento or there is a bollywood movie called Ghajni? our basic RNNs are like the heroes of that movie they suffer from a short-term memory problem LSTM is a special version of RNN which solves the short-term memory problem and in this video I will explain LSTM in a very simple manner using real-life examples let's say you have NLP task for centers completion here in both the sentences based on what word appeared in the beginning your autocomplete sentence might be different for example for the first one I would say I need to take a loan whereas for the second one I would say I had to take a loan and this decision between need and head was made based on what appeared at the very beginning now looking at our traditional RNN and architecture if you have seen my previous videos you would know this is how RNN architecture looks like so if you're not seeing those videos I highly recommend you watch them because they are kind of a prerequisite here when you feed the sentence word by word okay so first you will feed today it will learn some weight there is an activation which is fed back now you uh work on the second word which is due then the third word which is two so this is how basic RNN works if you unroll this thing in time then this is how the architecture will look like. Many people get confused they think that this is a neural network with so many layers. Actually there is only one layer look at this time axis t1 t2 t3 so it's the same layer represented in different time and when you unroll it this is how it looks. now to predict this word need it needs to know about the word today which appeared at the very beginning of the sentence and because of vanishing gradient problem the traditional RNNs have short-term memory so they don't remember what appeared in the beginning of the sentence they have a very short-term memory of just few words which are like nearby, hence to autocomplete this kind of sentence RNN won't be able to do a good job. similarly this is the second sentence you know where head was derived based on the earlier word which is last year now let's look at the network layer a little more in detail so I'm going to just expand this particular network layer which looks like this. so there are set of neurons in that layer and this hidden state is nothing but a short-term memory. okay so i'm just going to remove this neurons just to kind of make it simple and this square box is called a memory cell because this hidden state is actually containing the short-term memory. now if you want to remember long-term memory we need to introduce another state called long-term memory. so that state is called C so there are two states now hidden state which is short-term memory and the there is a self-state which is a long-term memory. and we will look into this in detail how exactly this will work but going back to our short-term memory cell in traditional RNN if it looks something like this so I have drawn the vertical neurons here but you can draw on draw them this in a horizontal fashion as well so it's just a layer of neurons and your x(t) and h(t) are vectors so when you have a word for example you will first convert into a vector vector is nothing but a list of numbers and your hidden state will be also a vector and using both these vectors you will do you know like sigma operation like weighted multiplication and then you apply activation function which is which is tan h in the case of RNN and then you get a new hidden state so here is a simplistic example right so here you have 10 h you have weighted sum going on and this is how the short term memory cell looks in traditional RNN in LSTM we are going to introduce a new uh cell state for a long term memory so let's say there is this cell state okay now let's see how exactly this works by looking at one more example I love eating samosa by the way so i have one more sentence for you to auto complete can you tell me what would you put here at dot dor dot well obviously Indian. Samosa is an Indian cuisine so you will say his favorite cuisine is Indian now take a pause and think about this when is a human when you make this case when you are processing this sentence which words uh told you that this will be an Indian cuisine well it is this word samosa if i didn't have samosa here if i had just had eat or every day it so based on these words you can't guess it is an Indian cuisine, right? there is some key words if you are doing a movie review for example you're looking for key words like okay excellent or horrible you know terrible movie or amazing the hero performed very well so you're just looking for specific words and the remaining words you can actually ignore now let's see how our traditional RNN would behave for this sentence so a traditional RNN which is like Amir Khan of ghajni having short-term memory when you feed all these words it can remember only let's say last two words in reality RNNs can remember more words but i'm just giving a simple example you know just to explain this concept so let's say they have short-term memory just remembers only two words so when you are read this sentence almost for example it remembers almost and the samosa like the two words last two words when you are at here cuisine is it will remember is and cuisine so at this point it doesn't have a knowledge of samosa so then for a traditional RNN it is hard to make a guess that the cuisine is Indian what if we build this long-term memory along with short-term memory in such a way that we store the meaningful words or the key words into this long-term memory so when I feed the world or eats it will not store it this is a blank string it will not store it in a long-term memory but when you find things like samosa it will store it in a long-term memory see samosa when you get almost almost is also not important so i just store samosa here and when i go all the way here now when i have to make a prediction on cuisine I have that memory that this we are talking about samosa and hence it has to be Indian. Let's look at little more complicated example here while I love indian cuisine my brother bhavin lost which cuisine, do you have any guess? If you read this sentence carefully you will figure it is an italian cuisine and you made that decision based on the two key words which were pastas and cheese so again you are reading the sentence and you are keeping some keywords in memory and throwing everything out like and that means these are not important for you the the important keywords are pastas and cheese based on that you make a decision that it is an Italian cuisine so going back to our RNN with long-term memory when you encounter let's say samosa, you will store samosa in your long-term memory but you will keep on storing samosa until you encounter pasta so now the moment you encounter pasta you need to forget the previous long-term memory which is samosa so here I threw that thing out and I have new memory which is pasta and then you keep on preserving this until you hit cheese so when you hit cheese you need to add that so now you can't ignore pasta you need to add cheese on top of it and then in the end when you are about to be asked you know what is your answer for auto complete you will say Italian because you have the memory of pasta and cheese now you would ask me how do you make this decision how do you let's say when cheese comes you don't discard pasta but when pasta pasta comes you discard samosa well all of this happens during the training process so when you're training your RNN you are not giving only this particular statement this is this is a statement for prediction when the training is happening you are giving thousands and thousands of such statements which will build that understanding in RNN uh on what to discard and what to store so here we learned a very important concept okay so when you're talking about LSTM or a long short term memory cell so each of these cells are LSTM cells the first most important thing is the forget get. So the role of forget get is when you come at this word pasta it knows that it has to discard samosa okay so this is how forget get looks like so I have just expanded that cell xt is a one word you know you process sentence one word by one word and t is the timestamp so that's why x(t) so when you feed pasta into this forget get so the forget get is simple you have previous hidden state you take the current input which is your current word and you apply sigmoid function now you know that sigmoid function restricts your number between 0 and 1 so it will if it has to discard the previous memory it will output a vector which will have all zeros or all the values which are close to zero and when you multiply that with previous memory which is previous cell state you know you have a vector of let's say all zeros here and you have another vector which is a memory of previous cell state. the multiplication will of course be zero because you have discarded the previous memory. you know when this new word appeared so this is what a forget get looks like so here you forgot about samosa now there is another thing which is an input gate so when pasta came not only you forgot about samosa you need to add a memory of pasta so the way it will work is you will use sigmoid and tan h both on these two vectors okay so when you're doing by the way these vectors will have weights here so there will be some weight here some weight here so in this function what you are doing is h t minus 1 into that weight plus x t into that weight plus bias and then you are applying 10 h on top of it and it's the same equation here the only difference is instead of tan h you are using sigmoid function and you multiply both of this output and then add that as a memory for this word the third one is output gate so in the output gate again you are doing weighted sum of hidden state nxt and applying sigmoid function whatever is the output uh you take that and then you take long term memory apply 10 h and you multiply that and you will get your hidden state okay so this will be your hidden state there are cases like let's say the sentence auto complete case that we're looking at at there is no yt actually the state is carried using the short-term memory but you know if there is a task like a named entity recognization there you need y t which is an output and that output is same as h t i mean you might apply sigmoid uh not sigmoid but let's say softmax type of function here uh but other than that it is kind of similar to h d so now if you think about the long-term memory so long-term memory look at this line okay look at this highway for example it has two things forget get an input brigade so forget get is like it will help you forget things like samosa when pasta comes in and the input gate will add new things into memory like meaningful things you want to add in a memory if it is a movie review you want to add like horrible or amazing beautiful those kind of words you know is the there are so many words you don't care about so that's what this lSTM will do and i'm going to refer you to a nice article i personally found this article to be very useful I'm going to link in the video description below I did not mention any mathematical equation because you can use this article for any math c here they explain how ct is calculated in terms of math formulas all math formulas again I suggest you read this article properly because I myself have learned a lot from this and I hope you found this video useful if you did please give it a thumbs up share it with your friends I think this this can provide you a simple explanation of LSTM and in the coming videos we'll be doing coding on an LSTM we will also be going over GRU in the future videos all right so till then thank you very much and bye"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Implementing RNNs, LSTMs, and GRUs with TensorFlow\/PyTorch",
        "Video Title":"PyTorch Tutorial - RNN &amp; LSTM &amp; GRU - Recurrent Neural Nets",
        "URL":"https:\/\/www.youtube.com\/watch?v=0_PgWWmauHk",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/0_PgWWmauHk\/hqdefault.jpg",
        "ID":"0_PgWWmauHk",
        "Publish Time":"2020-09-03T15:00:01Z",
        "Channel":"Patrick Loeber",
        "Channel ID":"UCbXgNpp0jedKWcQiULLbDTA",
        "Transcript":"hi and welcome to a new pie torch tutorial today i show you how we can implement a recurrent neural net using the built-in rnn module in the last tutorial we implemented the rnn from scratch and i highly recommend to watch this one first to understand the internal architecture of rnns and now today we focus on the implementation with pytorch's own module so we don't have to implement everything by ourselves i will show you how to use the rnn module and then at the end i also show you how easily we can switch our rnn model and use special kinds of rnn's like lstm and gru so let's start and we are going to use my tutorial about a simple neural net as starting point so this is tutorial number 13 from my pytorch beginner course you can find the link to the video and also to the code on github in the description below so as i said this is tutorial number 13 so i already grabbed this code and copied it to my editor and in this example we are doing digit classification on the mnist data sets and i have to say that image classification is not the typical example for rnns but what i want to demonstrate here is how we must treat our input as a sequence and then set up the correct shapes and it also shows that our ends can indeed be used to get a high accuracy on this classification task so last time we learned that the special thing about rns is that we work on sequences of vectors here so we treat our input as a sequence and there can be different types of rns so in this example we use this many to one architecture so here we have a sequence as a input and then only one output at the end and this is our class label in this case so let's jump to the code and the first thing we must change is the hyper parameters so the mnist data set consists of images of size 28 by 28 pixels and last time we squeezed that into one dimension so our input size was 28 times 28 or 784 and this time as i said we treat the image as a sequence so what we do here now is we treat one image dimension as one sequence and the other image dimension as the input or feature size so you can see this as that we are looking at one row at a time so let's comment this out and let's create a new one so let's say our input size equals and now as i said we are looking at one row at the time so this is 28 and then we also create the sequence length and this is also 28 and then we change the hidden size to be 128 so you can try out different sizes here and we add another parameter and this is the number of layers and here i set this to two so by default it is one and this means that we are stacking in this case two are n's together and the second rnn takes the output from the first rnn as an input so this can further improve our model and now we want to implement the rnn class so let's change the name to rnn and also in this super method and then our model down here also now is the rnn and now let's delete all of this to start with a new fresh implementation so now our rnn has still has the input size and the hidden size and the number of classes as parameters and it also gets the new parameter number of layers so let's put it in here so let's say the number of layers here and then of course we must also pass it to our model when we create it so this is our hyper hyper parameter and then what we want to do here first is we simply want to store the number of layers and the hidden size so let's say self num layers equals num layers and also self dot hidden size equals hidden size and then we create the rnn model and use the built-in pi torch rnn module so you can find this here on the official documentation so this is the rnn class that pi torch provides for us so we're going to use this so we create an rnn and say self r and n equals and now this is in the nn module so nn dot rnn and the rnn needs the input size it needs the hidden size and it needs the number of layers in this order and then we also use a parameter that is called batch first and set this to true so this just means that we must have the batch as a first dimension so our input needs to have the shape batch size batch size and then the sequence length and then the input or feature size so this is the shape that we need to have for our input and again you can find this in the documentation so if you set batch first to true then here you need this shape so now what we want to do is before we pass the images to our model so last time we reshaped it to be this size so originally our batch or our images have the size the batch size then a 1 and then 28 and then 28 again so this time we only want to have our batch size and then 28 by 28 so here we reshape it to be this size and then the 28 the first one is our sequence length and the second one is our input size so these are both 28 and the same in our no so this is in our training loop and then later in our evaluation loop we do the same so here we also have to reshape it to this size so now we have our input in the correct shape and now we need one more layer so as i said we are using this many to one architecture so in the end we have a classification task so this means that we are using a linear layer and then later the softmax and the cross entropy loss so let's create one more linear layer so let's say self dot fc for fully connected equals nn dot linear and now here we want to be careful so for the input size we use the hidden size and the output size is the number of classes and i will explain this later again but basically as we can see in this image or also in this image we only use the last time step of our sequence to do the classification so we only need the last hidden size as the input size for the linear layer so this is basically the whole init function and now of course we also need to implement the forward pass so our rnn if we have a look at the documentation then it needs two inputs and the set the one is the the first one is the input and the second one is the initial hidden state so we need this in the correct shape and so let's create an a tensor with just zeros so we say h 0 equals and then let's say torch dot zeros and then here the first one is the number of layers the second one is the batch size so we get this by saying x dot size zero the next dimension is the hidden size so we say self dot hidden size and then we also want to push it to the device if you're using one so now this is our initial hidden state and now we can call our rnn model so we say out and then a underscore because we don't need this and then we say self dot rnn and this gets x and h0 so again let's have a look at the documentation so it delivers two outputs and the first tensor contains the output features or the hidden states from all the time steps and the other one is just the hidden state for the step n so we don't need this in this case so now we have the output and the output is of size this is batch batch size and then we have the sequence length and then we have the hidden size so this is the new shape of our output and now what we want to do is we want to decode the hidden state only of the last time step so what we have here again let's write this in numbers so this is n and then 28 and our hidden size is 128 and now we only want the last time step so we want to have our out to be in n and then 128 so we get this by saying out equals out and then we use this slicing here and take all the samples in our batch and then only the last time step so we can say -1 and then again a colon for all the features in the hidden size so now we have our out in this size and now that's why we need the hidden size as the input size for our linear layer so now we can call that so now we can say out equal self dot fully connected with our out and then we return the out so now this is the whole implementation that we need for our rnn so everything else stays the same in our training and evaluation loop and again what we have to be careful here is to treat our input as a sequence and then when we use the built in rnn that we use the correct shape and then we need the initial hidden state also in the correct shape and then we have to reshape it before we pass it to our fully connected layer so let's try it out so let's say python main dot pi all right so now training is done and as you can see we get the accuracy of 93 percent so our rnn works and you can see that it can be applied on this classification task and now at the end i also want to show you two more rnn modules so two special kinds the first one is the gru or gated recurrent unit and the second one is the lstm or long short term memory so both both are also very popular rnns and i will not explain the theory about them right now i will just show you how easily we can use them as well with this implementation so let's use the gru first so we can simply say n n dot g r u and let's also call this self dot g r u and down here self dot u and everything else stays exactly the same so it takes the same input parameters it also needs this hidden state and then the output is in the same shape so now let's run this with the cheer you and test this alright so now as you can see the gru works too so the accuracy was even higher here and now as last thing let's also try the lstm so as you might know for the lstm we need an initial cell state so let's use the lstm so let's first call this self.ls and then here we use nn.lstm the input parameters are still the same and then here um we need an initial tensor for the cell state so let's call this c0 and this has the same shape and then here we call the self.lstm and this needs the hidden state and the cell state as a in as a tuple here so now this is all we need to apply the lstm so let's clear this and run it one more time all right so this one worked too and you can see the accuracy is 97 percent so yeah so now you know how you can implement a rnn in pie tarts using the built-in rnn module and you also know how you can use the gru and the lstm and yeah i hope you enjoyed this tutorial if you liked it then please consider subscribing to the channel and hit the like button and see you next time bye"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Implementing RNNs, LSTMs, and GRUs with TensorFlow\/PyTorch",
        "Video Title":"Simple Explanation of LSTM | Deep Learning Tutorial 36 (Tensorflow, Keras &amp; Python)",
        "URL":"https:\/\/www.youtube.com\/watch?v=LfnrRPFhkuY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/LfnrRPFhkuY\/hqdefault.jpg",
        "ID":"LfnrRPFhkuY",
        "Publish Time":"2021-02-06T15:30:08Z",
        "Channel":"codebasics",
        "Channel ID":"UCh9nVJoWXmFb7sLApWGcLPQ",
        "Transcript":"Seen this movie called Memento or there is a bollywood movie called Ghajni? our basic RNNs are like the heroes of that movie they suffer from a short-term memory problem LSTM is a special version of RNN which solves the short-term memory problem and in this video I will explain LSTM in a very simple manner using real-life examples let's say you have NLP task for centers completion here in both the sentences based on what word appeared in the beginning your autocomplete sentence might be different for example for the first one I would say I need to take a loan whereas for the second one I would say I had to take a loan and this decision between need and head was made based on what appeared at the very beginning now looking at our traditional RNN and architecture if you have seen my previous videos you would know this is how RNN architecture looks like so if you're not seeing those videos I highly recommend you watch them because they are kind of a prerequisite here when you feed the sentence word by word okay so first you will feed today it will learn some weight there is an activation which is fed back now you uh work on the second word which is due then the third word which is two so this is how basic RNN works if you unroll this thing in time then this is how the architecture will look like. Many people get confused they think that this is a neural network with so many layers. Actually there is only one layer look at this time axis t1 t2 t3 so it's the same layer represented in different time and when you unroll it this is how it looks. now to predict this word need it needs to know about the word today which appeared at the very beginning of the sentence and because of vanishing gradient problem the traditional RNNs have short-term memory so they don't remember what appeared in the beginning of the sentence they have a very short-term memory of just few words which are like nearby, hence to autocomplete this kind of sentence RNN won't be able to do a good job. similarly this is the second sentence you know where head was derived based on the earlier word which is last year now let's look at the network layer a little more in detail so I'm going to just expand this particular network layer which looks like this. so there are set of neurons in that layer and this hidden state is nothing but a short-term memory. okay so i'm just going to remove this neurons just to kind of make it simple and this square box is called a memory cell because this hidden state is actually containing the short-term memory. now if you want to remember long-term memory we need to introduce another state called long-term memory. so that state is called C so there are two states now hidden state which is short-term memory and the there is a self-state which is a long-term memory. and we will look into this in detail how exactly this will work but going back to our short-term memory cell in traditional RNN if it looks something like this so I have drawn the vertical neurons here but you can draw on draw them this in a horizontal fashion as well so it's just a layer of neurons and your x(t) and h(t) are vectors so when you have a word for example you will first convert into a vector vector is nothing but a list of numbers and your hidden state will be also a vector and using both these vectors you will do you know like sigma operation like weighted multiplication and then you apply activation function which is which is tan h in the case of RNN and then you get a new hidden state so here is a simplistic example right so here you have 10 h you have weighted sum going on and this is how the short term memory cell looks in traditional RNN in LSTM we are going to introduce a new uh cell state for a long term memory so let's say there is this cell state okay now let's see how exactly this works by looking at one more example I love eating samosa by the way so i have one more sentence for you to auto complete can you tell me what would you put here at dot dor dot well obviously Indian. Samosa is an Indian cuisine so you will say his favorite cuisine is Indian now take a pause and think about this when is a human when you make this case when you are processing this sentence which words uh told you that this will be an Indian cuisine well it is this word samosa if i didn't have samosa here if i had just had eat or every day it so based on these words you can't guess it is an Indian cuisine, right? there is some key words if you are doing a movie review for example you're looking for key words like okay excellent or horrible you know terrible movie or amazing the hero performed very well so you're just looking for specific words and the remaining words you can actually ignore now let's see how our traditional RNN would behave for this sentence so a traditional RNN which is like Amir Khan of ghajni having short-term memory when you feed all these words it can remember only let's say last two words in reality RNNs can remember more words but i'm just giving a simple example you know just to explain this concept so let's say they have short-term memory just remembers only two words so when you are read this sentence almost for example it remembers almost and the samosa like the two words last two words when you are at here cuisine is it will remember is and cuisine so at this point it doesn't have a knowledge of samosa so then for a traditional RNN it is hard to make a guess that the cuisine is Indian what if we build this long-term memory along with short-term memory in such a way that we store the meaningful words or the key words into this long-term memory so when I feed the world or eats it will not store it this is a blank string it will not store it in a long-term memory but when you find things like samosa it will store it in a long-term memory see samosa when you get almost almost is also not important so i just store samosa here and when i go all the way here now when i have to make a prediction on cuisine I have that memory that this we are talking about samosa and hence it has to be Indian. Let's look at little more complicated example here while I love indian cuisine my brother bhavin lost which cuisine, do you have any guess? If you read this sentence carefully you will figure it is an italian cuisine and you made that decision based on the two key words which were pastas and cheese so again you are reading the sentence and you are keeping some keywords in memory and throwing everything out like and that means these are not important for you the the important keywords are pastas and cheese based on that you make a decision that it is an Italian cuisine so going back to our RNN with long-term memory when you encounter let's say samosa, you will store samosa in your long-term memory but you will keep on storing samosa until you encounter pasta so now the moment you encounter pasta you need to forget the previous long-term memory which is samosa so here I threw that thing out and I have new memory which is pasta and then you keep on preserving this until you hit cheese so when you hit cheese you need to add that so now you can't ignore pasta you need to add cheese on top of it and then in the end when you are about to be asked you know what is your answer for auto complete you will say Italian because you have the memory of pasta and cheese now you would ask me how do you make this decision how do you let's say when cheese comes you don't discard pasta but when pasta pasta comes you discard samosa well all of this happens during the training process so when you're training your RNN you are not giving only this particular statement this is this is a statement for prediction when the training is happening you are giving thousands and thousands of such statements which will build that understanding in RNN uh on what to discard and what to store so here we learned a very important concept okay so when you're talking about LSTM or a long short term memory cell so each of these cells are LSTM cells the first most important thing is the forget get. So the role of forget get is when you come at this word pasta it knows that it has to discard samosa okay so this is how forget get looks like so I have just expanded that cell xt is a one word you know you process sentence one word by one word and t is the timestamp so that's why x(t) so when you feed pasta into this forget get so the forget get is simple you have previous hidden state you take the current input which is your current word and you apply sigmoid function now you know that sigmoid function restricts your number between 0 and 1 so it will if it has to discard the previous memory it will output a vector which will have all zeros or all the values which are close to zero and when you multiply that with previous memory which is previous cell state you know you have a vector of let's say all zeros here and you have another vector which is a memory of previous cell state. the multiplication will of course be zero because you have discarded the previous memory. you know when this new word appeared so this is what a forget get looks like so here you forgot about samosa now there is another thing which is an input gate so when pasta came not only you forgot about samosa you need to add a memory of pasta so the way it will work is you will use sigmoid and tan h both on these two vectors okay so when you're doing by the way these vectors will have weights here so there will be some weight here some weight here so in this function what you are doing is h t minus 1 into that weight plus x t into that weight plus bias and then you are applying 10 h on top of it and it's the same equation here the only difference is instead of tan h you are using sigmoid function and you multiply both of this output and then add that as a memory for this word the third one is output gate so in the output gate again you are doing weighted sum of hidden state nxt and applying sigmoid function whatever is the output uh you take that and then you take long term memory apply 10 h and you multiply that and you will get your hidden state okay so this will be your hidden state there are cases like let's say the sentence auto complete case that we're looking at at there is no yt actually the state is carried using the short-term memory but you know if there is a task like a named entity recognization there you need y t which is an output and that output is same as h t i mean you might apply sigmoid uh not sigmoid but let's say softmax type of function here uh but other than that it is kind of similar to h d so now if you think about the long-term memory so long-term memory look at this line okay look at this highway for example it has two things forget get an input brigade so forget get is like it will help you forget things like samosa when pasta comes in and the input gate will add new things into memory like meaningful things you want to add in a memory if it is a movie review you want to add like horrible or amazing beautiful those kind of words you know is the there are so many words you don't care about so that's what this lSTM will do and i'm going to refer you to a nice article i personally found this article to be very useful I'm going to link in the video description below I did not mention any mathematical equation because you can use this article for any math c here they explain how ct is calculated in terms of math formulas all math formulas again I suggest you read this article properly because I myself have learned a lot from this and I hope you found this video useful if you did please give it a thumbs up share it with your friends I think this this can provide you a simple explanation of LSTM and in the coming videos we'll be doing coding on an LSTM we will also be going over GRU in the future videos all right so till then thank you very much and bye"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Implementing RNNs, LSTMs, and GRUs with TensorFlow\/PyTorch",
        "Video Title":"TensorFlow Tutorial 6 - RNNs, GRUs, LSTMs and Bidirectionality",
        "URL":"https:\/\/www.youtube.com\/watch?v=Ogvd787uJO8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Ogvd787uJO8\/hqdefault.jpg",
        "ID":"Ogvd787uJO8",
        "Publish Time":"2020-08-13T06:49:29Z",
        "Channel":"Aladdin Persson",
        "Channel ID":"UCkzW5JSFwvKRjXABI-UTAkQ",
        "Transcript":"[Music] what is going on guys hope you're doing awesome roll that intro and then let's do some rnn's and [Music] all right so i got the usual imports that we normally have this is for ignoring the tensorflow messages that can be quite annoying although we'll still get error messages and then tensorflow keras layers to construct our layers and then the mnist data set and then this is just so that the uh if you have any trouble running on the gpu these two lines will most likely help you all right so let's let's start with what we actually want to do we're gonna start with loading our data set so we're going to do x train y train and then x test y test is equal to mnist dot load data then we're going to do x train equals x train as type float32 currently it's float64 just to save on some computation we're going to convert it and then we're going to normalize by dividing by 255 so it's in between zero and one and uh let's do the same for the uh the test set so flow 32 divided by 255 so what we're doing here is that we have we have an image of 28 by 28 pixels and how this is going to work when we're going to send it in to an rnn or a gru or an lstm we're going to do all three of those but we're essentially going to view for each time step you're going to sort of unroll one row of the image at a time so for a particular time step let's say the first time step it's going to take the first row of the image and send that in and then for the second time step it's going to take the second row and send that in and just to be clear you wouldn't use sequence models to handle images it's not the best model for it you would use a comnet that we covered two videos ago but it works to use rnns and as we'll see we'll get reasonable performance although this is more to illustrate how you would actually uh implement an rnn uh energy or u and lstm in tensorflow and the data set is not the optimal one but we're just picking in a simple one to illustrate this example all right so with that said let's actually do our model we're going to do keras.sequential we're going to do model.add and then specify the input in this case we're going to specify none and then 28. so while we're specifying none here is because we have a we don't have to have a specific number of time step right so we have 28 pixels in each time step and then in this case we actually have 28 time step but we don't have to specify that dimension and then we're going to do model dot add and then layers dot simple rnn so that's just for a basic rnn and then let's say 512 nodes and then uh in as an additional argument we can do return sequences equals true so that it's it's returning the output from each time step and in that way we can stack multiple rnn layers on top of each other so the output from this rnn is going to be 512 nodes and then return sequences it's going to output 512 for each time step in this case we're going to have 28 time steps and then we can also do activation we can set it to relu and then we can add another one we can do model.add layers simple rnn and let's do 512 again and we're going to set activation equals relu and then for the output layer we're going to do model.add layers.dense and we're going to have 10 output nodes so you would notice here that we're not doing return sequences on this second simple rnn so here for the output we don't have return sequences equals true meaning that it's going to pass every time step and then at the last the last output of this simple rnn here we're going to take a layer dents on top of that one and we're going to have 10 output nodes let's do print model.summary first so as we can see here on the model.summary for this first rnn we're going to have none none and then 512 so we're going to have 512 output nodes and then we're going to have for each time step here and the reason that we have none and none is that we have the first one for the batches or one of these are for the batches and one of them are for the hidden states i think this this one is for the batches this one is for the hidden states and then at the second one we're not having return sequences equals true so we only have none for the batches and then we have 512 nodes from sort of the last hidden state when it's passed all of the the inputs and then at the end we just have a layer on top of that one so all that's left now is for us to do model.compile and we're going to specify our loss function keras.losses sparse categorical cross entropy then we're going to set from logits equals true because we do not have a soft max on our dense layer at the end and then optimizer we're going to do [Music] keras.optimizers.adam and let's set the learning rate to 0.001 and then metrics we're just going to keep track of the accuracy all that's left now is first to the model that fit on the training set and then specifying the batch size let's say 64. and then let's run for 10 epochs and verbose equals 2 2 just for printing every epoch and then at the end we want to do an evaluation on our test set so we're going to send an x test and then the labels y test we're also going to specify the batch size 64 and then again verbose equals equals two all right so let's run that and see if this works all right so after 10 epochs we got 98 on the on the training set and almost 98 on the test set as well i just want to say here that we used an activation relu uh the default when training our recurrent networks is that you use 10h so i don't know if that would work better in this case but anyways just wanted to mention that so if you wouldn't specify an activation function it would default be 10 h when uh when building these recurrent nets so uh also one thing here is that it took a little bit longer than i thought to run this so let's just use 256 for our next models so what we want to do now is pretty much the same thing but we want to build a a gru instead and uh all we got to do to do that is we're just going to change this this simple rnn to a gru and uh that's pretty much it that's all you have to do so if we now rerun that we can see uh what it gets and i guess this is not really a a fair comparison uh but jru should perform better than simple rnns although now we're using half of the units and then we're using 10h instead of relu but the point is not really to compare the two just uh see that it works and see and show you how to use simple rnn gru and then lstm which is quite simple as well we're just going to change this to an lstm but anyways then i also want to show you how to do a bi-directional layer all right so after 10 epochs with 99.5 percent on the training and we get close to 99 on the test set which is quite good actually it's a two layered gru's with 256 units i mean to get that is uh pretty decent actually so uh let's now change this to an lstm and see if it's any improvement so lstms and gru's are equivalent in terms of performance i think lstms are a little bit better than gru's but let's see if that's the case on this data set all right so it seems that we get pretty much the identical performance lstms were a little bit better perhaps on the test set but pretty much the same so what we want to do now is we want to add instead of using just a one directional lstm we want to use a bi-directional and it's pretty easy to add that as well we're just going to do layers dot bi-directional and let's do it like this and then like this so we're just going to add layers up bi-directional and then we're going to send in that lstm layer uh and uh let's do that for the second one or rather let's do all right so let's let's uh first do mod print model dot summary and just see how it looks like all right so i'm not going to let this train so what we get here is uh as you can see since we added this layer bi-directional we're going to get 512 nodes instead of this 256. so what it's doing here is we're specifying the number of nodes for each hidden for each of its computation for each hidden state in the lstm to be 256 nodes but since we add this bi-directional we're going to have one going forward and one going backwards so this is going to get doubled in the amount of nodes as we see here so what we can do then is uh for the second one we can do also a see layers dot bi-directional and then we can add that right there and that's also gonna have 512 nodes uh so let's run that and see if the bi-directional is any better than the just one direct the just having one direction on the lstm after 10 epochs we see that the performance is about the same as just using one direction so the bi-directional didn't really help and uh and i'm not really sure why that is maybe it just needs more training or it just doesn't help that much for this particular data set but in general uh using bi-directional as more of a default is a good option but anyways that was the basics of how to do a simple rnn a gre lstm and then also how to add bi-directionality and in this scenario we use the mnist data set so we made it very easy for ourselves when using the mnist data set and when training on more complex data you need to think about more things such as padding the data and masking the data for each batch and we're going to cover that in future videos when we're going to load more complex data and loading more custom data sets so with that said thank you for watching and i hope to see you in the next video [Music] you"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Implementing RNNs, LSTMs, and GRUs with TensorFlow\/PyTorch",
        "Video Title":"TensorFlow Tutorial 10 - Recurrent Neural Nets (RNN &amp; LSTM &amp; GRU)",
        "URL":"https:\/\/www.youtube.com\/watch?v=IrPhMM_RUmg",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/IrPhMM_RUmg\/hqdefault.jpg",
        "ID":"IrPhMM_RUmg",
        "Publish Time":"2020-10-23T15:00:05Z",
        "Channel":"Patrick Loeber",
        "Channel ID":"UCbXgNpp0jedKWcQiULLbDTA",
        "Transcript":"hey guys welcome to another tensorflow tutorial today we'll be learning about recurrent neural nets or short rns rns are a class of neural networks that allow previous outputs to be used as inputs while having hidden states so this means that we are working with a sequence here and this is super powerful and with this we can use rnns for many different applications like text generation text translation sentiment classification and many more so i already have a in-depth tutorial about rns where i explain these slides here in more detail so if you want to learn a little bit more about the theory then check out at least the first five minutes of this video because now i want to focus on the implementation with tensorflow so let's jump directly to the code [Music] so here i already have some code and this is the exact same code as in tutorial number three where we used the mnist data set and then we defined a simple neural net and then we defined a loss and a optimizer and compiled our model and then we trained it and evaluated it so we did digit classification with this data set and now the only thing we want to change here is to change the model and now use an rnn model so this is not the typical application for an rnn a lot of times it's used when we deal with text classification or text generation but this example should demonstrate that rnns can indeed be used for an image classification task and you will see how easily we can create our rnn model with the keras api so as i said when we deal with r and ends then we deal with a sequence here and in our case we have images but we don't have to change our data set we simply have to treat our images as a sequence now so right now our images have to shape 28 by 28 so 28 times 28 pixels and now we treat it as a sequence so this means that we say one time step is one row in our image and then we also have 28 columns so this means that our input size is 28 and our sequence length is also 28 so again this means we have 28 time steps in our sequence and in each time step we have 28 features and now when we treat it like this then we can simply use an r and n now so now let's go ahead and define our model so first let's define a empty sequential model and the first thing i want to do is to add a input so i say model dot at and then keras and then input and then here specify the shape and this is 28 by 28 so again the first um the first number here is the sequence length and the second number here is the input size and now we can add the rnn model so there are different ones available and we we start with the simple r n layer so later i also show you two other famous ones so for now let's say model dot at and then we can get this in keras dot layers so we already imported this here and then we can say layers dot and now we want a simple r and n model and now the only thing we have to specify is the number of units so the number of output units and this is also the size of the hidden cell so there are of course a lot of more parameters so i recommend that you check out the documentation for yourselves so one thing that you should note is that by default the activation function in an rnn is the ton h function so let's in our case let's try out the relu function and now this is all that we need for the r and n model so now we have that and now as we want to do classification so we have 10 different classes in the mns data set so then we also like in the other tutorial we add a dense layer at the end so we say layers dot dense and then we want 10 outputs and this is all we need so we don't use an activation function here at the end but then we must be careful and we must specify from logits equals true in our loss function and now this is all that we need for now so this is the whole um sequential model that we need for a simple rnn so first let's um import sys and say sys.exit so that it runs only until here so let's run the code until here and print the summary so let's say python and then the name of this file so this one oh and here i missed the equal sign of course so shape equals 28 by 28 so again let's try it and then here we see our simple r and n has this output shape and i explained this in a second and then we have the dense layer with 10 outputs so let's have another look at the r and ends so the output shape is n so the number of samples that we have and then 128 like we specified here and what this includes is so this is a single vector for each sample and the output that we get is the output of the only of the last cell the last time step but this includes all the information about the previous time step so this is all that we need so we only need this last cell here so this is why our output is in this shape but you can also get an output of the shape the number of batches times the number of sequences the number of time steps or the sequence length and then the number of units and we get this when we specify and there's an additional parameter and this is called return return sequences equals true so if we use this by default it's false and if we use this then our output is in this shape and this is for example useful when we want to stack multiple r and n's together so for example we can use the first one which will return all the time steps and then we use a second one where we say this is false and then um here we get this output shape so again let's have a look if that's correct so let's clear this and run this again and yeah what i told you was correct so the first r and n has this output shape because we said return sequences equals true and the other one has only this output shape and then again we have our dense layer so this might improve the performance of your model so again you can play around with stacking of multiple r ends so let's for now let's just use one and then let's remove this and then train it and see if it performs well for this classification task so again let's clear this and run this all right so we see that our accuracy at the end is 97 so our rnn indeed works well here for this image classification task and now you know how you can set up your rnn with this simple r and n layer and you should know how you can treat your input as a sequence and now i also want to show you two other famous rnn's so this is only the simple rn layer but there's also for example the lstm or the gru so both are two popular rnns as well they both typically perform a little bit better than the simple rnn and i think you don't have to change anything else so the parameters are mostly the same and also the structure of the outputs is the same so yeah play around with this this as well and try out the gru or the lstm and yeah i think that's it for now and in the next tutorial we learn how we apply this for a text classification task so i hope to see you in the next video then and if you enjoyed this tutorial please hit the like button and consider subscribing to the channel and see you next time bye"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Attention Mechanisms in Sequence Models",
        "Video Title":"Attention for Neural Networks, Clearly Explained!!!",
        "URL":"https:\/\/www.youtube.com\/watch?v=PSs6nxngL6k",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/PSs6nxngL6k\/hqdefault.jpg",
        "ID":"PSs6nxngL6k",
        "Publish Time":"2023-06-05T04:00:19Z",
        "Channel":"StatQuest with Josh Starmer",
        "Channel ID":"UCtYLUTtgS3k1Fg4y5tAhLbw",
        "Transcript":"it'll help if you pay attention when we add it to an encoded decoder model hooray stat Quest hello I'm Josh starmer and welcome to statquest today we're going to talk about attention and it's going to be clearly explained light down it makes it easy to start with nothing and then scale it up in the cloud this stat Quest is also brought to you by the letters a b and c a always b b c curious always be curious note this stat Quest assumes that you are already familiar with basic seek to seek and encoder decoder neural networks if not check out the quest I also want to give a special triple bam shout out to Lena voita and her GitHub tutorials NLP course for you because it helped me a lot with this stat Quest Hey look it's stat Squatch in the normal saurus hey Josh last time we used a basic encoder decoder model to translate let's go into Spanish and it worked great bam but now I want to translate my favorite book The statquest Illustrated guide to machine learning into Spanish and our basic encoder decoder model isn't doing a good job I'm sorry you're having trouble with translating Squatch the problem is that in the encoder in a basic encoder decoder unrolling the lstms compresses the entire input sentence into a single context vector this works fine for short phrases like let's go but if we had a bigger input vocabulary with thousands of words then we could input longer and more complicated sentences like this don't eat the delicious looking and smelling Pizza but for longer phrases even with lstms words that are input early on can be forgotten and in this case if we forget the first word don't then don't eat the delicious looking and smelling Pizza turns into eat the delicious looking and smelling pizza and these two sentences have completely opposite meanings so sometimes it's super important to remember the first word so what are we going to do well you might remember that basic recurrent neural networks had problems with long-term memories because they ran both the long and short-term memories through a single path and that the main idea of long short-term memory units is that they solve this problem by providing separate paths for long and short-term memories well even with separate paths if we have a lot of data both paths have to carry a lot of information and that means that a word at the start of a long phrase like don't can still get lost so the main idea of attention is to add a bunch of New Paths from the encoder to the decoder one per input value so that each step of the decoder can directly access input values bam note the basic encoder decoder plus attention models that we're going to talk about today are totally awesome but they are also a stepping stone to learning about Transformers which we'll talk about in future stack quests in other words today we're taking another step in our quest to understand Transformers which form the basis of big fancy large language models like Chad GPT now if you remember from the stack Quest on encoder decoder models an encoder decoder model can be as simple as an embedding layer attached to a single long short-term memory unit but if we want a slightly more fancy encoder we can add additional lstm cells now we'll initialize the long and short-term memories the cell and hidden states in the lstms in the encoder with zeros and if our input sentence which we want to translate into Spanish is let's go then we can plug in a 1 for let's into the embedding layer unroll and plug a 1 for go into the embedding layer and that can create a context vector that we use to initialize a separate set of lstm cells in the decoder note all of the input is jammed into the context vector however the idea of attention is for each step in the decoder to have direct access to the inputs so let's talk about how attention connects the inputs to each step of the decoder unfortunately just like adding the extra path for long-term memories to lstms isn't super simple adding the extra paths for attention to an encoder decoder model isn't super simple either ugh don't worry Squatch we'll go through it one step at a time bam first before we start talking about how this big mess of stuff works let's start by plugging in a 1 for the EOS end of sentence token into the embedding layer remember we do this because we just finished encoding the sentence let's go and because some of the original encoder decoder plus attention manuscripts do it that way however some people also start out with SOS or start of sentence now let's talk about how to add attention to our model note although there are conventions there are no rules for how attention should be added to an encoder decoder model and each manuscript has a slightly different way of doing it so what follows is just one example of how attention can be added to an encoder decoder model that said the main idea of attention adding an additional path for each input value so that each step of the decoder can directly access those values is consistent for all encoder decoder models with attention bam so in this example the first thing that attention does is determine how similar the outputs from the encoder lstms are at each step to the outputs from the decoder lstms in other words we want a similarity score between the lstm outputs the short-term memories or hidden States from the first step in the encoder and the lstm outputs from the first step in the decoder and we also want to calculate a similarity score between the lstm outputs from the second step in the encoder and the lstm outputs from the first step in the decoder there are a lot of ways to calculate the similarity of words or more precisely sequences of numbers that represent words and different attention algorithms use different ways to compare these sequences however one simple way to determine the similarity of two sequences of numbers that represent words is with the cosine similarity there's a lot to be said about the cosine similarity so if you don't already know about it check out the quest however for the purposes of this stat Quest all we need to know is that the cosine similarity is calculated with this equation and that the numerator is what calculates the similarity between two sequences of numbers and that the denominator scales that value to be between negative one and one Josh this equation looks complicated don't worry Squatch we'll plug some numbers into it one step at a time to illustrate how the cosine similarity would work in this context let's calculate the cosine similarity between the output values from the first pair of lstm cells in the encoder for the word let's and the output values from the first pair of lstm cells in the decoder for the EOS token the output values from the two lstm cells in the encoder for the word let's are negative 0.76 and 0.75 and the output values from the two lstm cells in the decoder for the EOS token are 0.91 and 0.38 and now we just plug the numbers into the equation for the cosine similarity and we get Negative 0.39 thus the cosine similarity between the output values from the two lstm cells in the encoder for the word let's and the output values from the two lstm cells in the decoder for the EOS token is negative 0.39 that being said a more common way to calculate similarity for attention is to just calculate the numerator for the cosine similarity this is because the denominator simply scales the magnitude of the similarity score to be between negative 1 and 1. this scaling could be useful if we wanted to compare a similarity score for two lstm cells like we did before to a similarity score calculated with three or more lstm cells because the scaling would ensure that no matter how many lstm cells we use to calculate the similarities the similarities would be between negative 1 and 1 and easily comparable in other words the denominator removes the magnitude of the similarity however in this case we'll always use the same number of lstm cells and in practice using just the numerator works well so we can save ourselves the trouble of doing extra math by just calculating the numerator which is also called The Dot product Josh this equation looks way easier bam so in this case when we do the math for the dot product we get Negative 0.41 anyway calculating the dot product is more common than the cosine similarity for attention because it's super easy to calculate and roughly speaking large positive numbers mean things are more similar than small positive numbers and large negative numbers mean things are more completely backwards than small negative numbers the other nice thing about the dot product is that it's easy to add to our diagram we simply multiply each pair of output values together and then add them all together and we get Negative 0.41 likewise we can compute a similarity score with the dot product between the second input word go and the EOS token and we get 0.01 and now we've got similarity scores for both input words let's and go relative to the EOS token in the decoder bam hey Josh it's great that we have scores but how do we use them well we can see that the similarity score between go and the EOS token 0.01 is higher than the score between let's and the EOS token negative 0.41 and since the score for go is higher we want the encoding for go to have more influence on the first word that comes out of the decoder and we do that by first running the scores through a soft Max function remember the softmax function gives us numbers between 0 and 1 that add up to one so we can think of the output of the softmax function as a way to determine what percentage of each encoded input word we should use when decoding in this case we'll use 40 of the first encoded word let's and sixty percent of the second encoded word go when the decoder determines what should be the first translated word so we scale the values for the first encoded word let's by 0.4 and we scale the values for the second encoded word go by 0.6 and lastly we add the scaled values together these sums which combine the separate encodings for both input words let's and go relative to their similarity to EOS are the attention values for Eos bam now all we need to do to determine the first output word is plug the attention values into a fully connected layer and plug the encodings for Eos into the same fully connected layer and do the math and run the output values through a softmax function to select the first output word vamos bam now because the output was not the EOS token we need to unroll the embedding layer and the lstms in the decoder and plug the translated word vamos into the decoder's unrolled embedding layer and then we just do the math except this time we use the encoded values for vamos and the second output from the decoder is eos so we're done decoding triple bam in summary when we add attention to a basic encoder decoder model the encoder pretty much stays the same but now each step of decoding has access to the individual encodings for each input word and we use similarity scores and the soft Max function to determine what percentage of each encoded input word should be used to help predict the next output word note now that we have a tension added to the model you might wonder if we still need the lstms well it turns out we don't need them and we'll talk more about that when we learn about Transformers bam now it's time for some Shameless self-promotion if you want to review statistics and machine learning offline check out the statquest PDF study guides in my book The statquest Illustrated guide to machine learning at stackquest.org there's something for everyone hooray we've made it to the end of another exciting stat Quest if you like this stat Quest and want to see more please subscribe and if you want to support stack Quest consider contributing to my patreon campaign becoming a channel member buying one or two of my original songs or a t-shirt or a hoodie or just donate the links are in the description below alright until next time Quest on"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Attention Mechanisms in Sequence Models",
        "Video Title":"Attention mechanism: Overview",
        "URL":"https:\/\/www.youtube.com\/watch?v=fjJOgb-E41w",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/fjJOgb-E41w\/hqdefault.jpg",
        "ID":"fjJOgb-E41w",
        "Publish Time":"2023-06-05T18:21:12Z",
        "Channel":"Google Cloud Tech",
        "Channel ID":"UCJS9pqu9BzkAMNTmzNMNhvg",
        "Transcript":"hi I'm sanjana Reddy a machine learning engineer at Google's Advanced Solutions lab there's a lot of excitement currently around generative Ai and new advancements including new vertex AI features such as gen AI Studio model Garden gen AI API our objective in this short session is to give you a solid footing on some of the underlying Concepts that make all the Gen AI magic possible today I'm going to talk about the attention mechanism that is behind all the Transformer models and which is core to the llm models let's say you want to translate an English sentence the cat ate the mouse to French you could use an encoder decoder this is a popular model that is used to translate sentences the encoder decoder takes one word at a time and translates it at each time step however sometimes the words in the source language do not align with the words in the target language here's an example take the sentence black cat ate the mouse in this example the first English word is black however in the translation the first French word is she which means cat in English so how can you train a model to focus more on the word cat instead of the word black at the first time step to improve the translation you can add what is called the attention mechanism to the encoder decoder attention mechanism is a technique that allows the neural network to focus on specific parts of an input sequence this is done by assigning weights to different parts of the input sequence with the most important parts receiving the highest weights this is what a traditional RNN based encoder decoder looks like the model takes one word at a time as input updates the hidden State and passes it on to the next time step in the end only the final hidden state is passed on to the decoder the decoder works with the final hidden state for processing and translates it to the target language an attention model differs from the traditional sequence to sequence model in two ways first the encoder passes a lot more data to the decoder so instead of just passing the final hidden state number three to the decoder the encoder passes all the hidden States from each time step this gives the decoder more context Beyond just the final hidden state the decoder uses all the hidden State information to translate the sentence the second change that the attention mechanism brings is adding an extra step to the attention decoder before producing its output let's take a look at what these steps are to focus only on the most relevant parts of the input the decoder does The Following first it looks at the set of encoder states that it has received each encoder hidden state is associated with a certain word in the input sentence second it gives each hidden State a score third it multiplies each hidden state by its soft Max score as shown here thus amplifying hidden states with the highest scores and downsizing hidden states with low scores if we connect all of these pieces together we're going to see how the attention network works before moving on let's define some of the notations on this slide Alpha here represents the attention weight at each time step H represents the hidden state of the encoder RNN at each time step H subscript D represents the hidden state of the decoder RNN at each time step with the attention mechanism the inversion of the black cat translation is clearly visible in the attention diagram an 8 translates as two words image and French we can see the attention Network staying focused on the word eight for two time steps during the attention step we use the encoder hidden States and the H4 Vector to calculate a context Vector A4 for this time step this is the weighted sum we then concatenate H4 and A4 into one vector this concatenated Vector is passed through a feed-forward neural network one trained jointly with the model to predict the next word the output of the feed forward neural network indicates the output word of this time step this process continues till the end of sentence token is generated by the decoder this is how you can use an attention mechanism to improve the performance of a traditional encoder decoder architecture thank you so much for listening"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Attention Mechanisms in Sequence Models",
        "Video Title":"Attention Mechanism In a nutshell",
        "URL":"https:\/\/www.youtube.com\/watch?v=oMeIDqRguLY",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/oMeIDqRguLY\/hqdefault.jpg",
        "ID":"oMeIDqRguLY",
        "Publish Time":"2021-05-30T13:30:12Z",
        "Channel":"Halfling Wizard",
        "Channel ID":"UC34Gj0-vHuBiTNEYlP7wczg",
        "Transcript":"in deep learning Attention mechanism Is inspired by human visual processing system. When you read a page in a book The majority of what is in your field of vision is actually disregarded. And you pay more attention to the word that you are currently reading. This allows your brain to focus on what matters most, while ignoring everything else. In order to imitate the same effect in our deep learning models, we assign an attention weight to each of our inputs. These weights represent the relative importance of each input element to the other input elements. This way, we guide our model to pay greater attention to particular inputs that are more critical to performing the task at hand. Attention mechanism in deep learning was first used by Bahdanau for machine translation. The traditional method of machine translation was to use sequence to sequence models. In these models, We pass the input sentence to a RNN that functions as an encoder. RNNs, as you may know, have a hidden state in addition to their output, which I represented with the letters h for the encoder and s for the decoder, which is also an RNN. These hidden states can contain information from all the previous words in our sentence. Using this capability of hidden states, a context vector is constructed from the last Hidden state in encoder RNN, which actually includes the content of the source sentence. This is then passed to Decoder, so that the Decoder can translate the words into the target language. The challenge with this approach was that if the sentence was long, all of the information could not be compressed in the last hidden state. and hence, Our translation would be incorrect and inaccurate if the input sentence was long and detailed. The main idea of \u200b\u200battention, which Bahdanau also used in his paper, is that we give context vector access to the entire input sequence, instead of just the last hidden state. In this way, even if the length of the sentence increases, the context vector can still contain the content of the sentence. Now, We just need to assign an attention weight on each of those inputs so that the decoder can focus on the relevant positions in the input sequence. But how can this be achieved? Well, as you may have noticed, in this Slide, we have translated only two words. Let's see how the third word is translated using our new attention based model... we should take the current decoder hidden state and every encoder hidden state, And feed them into a score function. What does this function do? The idea behind score function is to measure the similarity between two vectors. using the score function allows our model to selectively concentrate on helpful parts of the input sequence and thereby learn the alignments between them. There are many ways to calculate score. Here I\u2019ve outlined four options for doing so: dot, general, concat and a location-based function in which the alignment scores are computed from solely the target decoder state next step is to calculate the alignment vector. As the formula in this slide shows, we should simply use softmax function to convert our score values into probabilities. Now we have The attention weights we were searching for. Given the alignment vector as weights, the context vector is computed as the weighted average over all the source hidden states. Now we can pass the context vector into the decoder, So that our decoder can access the entire input sequence and also focus on the relevant positions in the input sequence So, to put it simply, attention model works like an accounting notebook For every query, which in our example, was the last hidden state of decoder The attention gives us a table which shows us how much attention we owe to each of the keys, which in our case, were the encoder hidden states as you can see here, Attention is employed in deep learning in a variety of way. If you're interested in learning more about them, I recommend watching my video on the review paper \"an attentive survey of attention models,\" which dives deeper into the topic."
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Attention Mechanisms in Sequence Models",
        "Video Title":"What are Transformers (Machine Learning Model)?",
        "URL":"https:\/\/www.youtube.com\/watch?v=ZXiruGOCn9s",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ZXiruGOCn9s\/hqdefault.jpg",
        "ID":"ZXiruGOCn9s",
        "Publish Time":"2022-03-11T10:00:37Z",
        "Channel":"IBM Technology",
        "Channel ID":"UCKWaEZ-_VweaEx1j62do_vQ",
        "Transcript":"no it's it it's not those transformers but but they can do some pretty cool things let me show you so why did the banana cross the road because it was sick of being mashed yeah i'm not sure that i quite get that one and that's because it was created by a computer i literally asked it to tell me a joke and this is what it came up with specifically i used a gpt-3 or a generative pre-trained transformer model the three here means that this is the third generation gpt-3 is an auto-regressive language model that produces text that looks like it was written by a human gpt3 can write poetry craft emails and evidently come up with its own jokes off you go now while our banana joke isn't exactly funny it does fit the typical pattern of a joke with a setup and a punch line and sort of kind of makes sense i mean who wouldn't cross the road to avoid getting mashed but look gpt3 is just one example of a transformer something that transforms from one sequence into another and language translation is just a great example perhaps we want to take a sentence of why did the banana cross the road and we want to take that english phrase and translate it into french well transformers consist of two parts there is an encoder and there is a decoder the encoder works on the input sequence and the decoder operates on the target output sequence now on the face of it translation seems like little more than just like a basic lookup task so convert the y here of our english sentence to the french equivalent of porcua but of course language translation doesn't really work that way things like word order in terms of phrase often mix things up and the way transformers work is through sequence to sequence learning where the transformer takes a sequence of tokens in this case words in a sentence and predicts the next word in the output sequence it does this through iterating through encoder layers so the encoder generates encodings that define which part of the input sequence are relevant to each other and then passes these encodings to the next encoder layer the decoder takes all of these encodings and uses their derived context to generate the output sequence now transformers are a form of semi supervised learning by semi sequence semi-supervised we mean that they are pre-trained in an unsupervised manner with a large unlabeled data set and then they're fine-tuned through supervised training to get them to perform better now in previous videos i've talked about other machine learning algorithms that handle sequential input like natural language for example there are recurrent neural networks or rnns what makes transformers a little bit different is they do not necessarily process data in order transformers use something called an attention mechanism and this provides context around items in the input sequence so rather than starting our translation with the word why because it's at the start of the sentence the transformer attempts to identify the context that bring meaning in each word in the sequence and it's this attention mechanism that gives transformers a huge leg up over algorithms like rnn that must run in sequence transformers run multiple sequences in parallel and this vastly speeds up training times so beyond translations what can transformers be applied to well document summaries they're another great example you can like feed in a whole article as the input sequence and then generate an output sequence that's going to really just be a couple of sentences that summarize the main points transformers can create whole new documents of their own for example like write a whole blog post and beyond just language transformers have done things like learn to play chess and perform image processing that even rivals the capabilities of convolutional neural networks look transformers are a powerful deep learning model and thanks to how the attention mechanism can be paralyzed are getting better all the time and who knows pretty soon maybe they'll even be able to pull off banana jokes that are actually funny if you have any questions please drop us a line below and if you want to see more videos like this in the future please like and subscribe thanks for watching"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Transformers and Self-Attention Architecture",
        "Video Title":"What are Transformers (Machine Learning Model)?",
        "URL":"https:\/\/www.youtube.com\/watch?v=ZXiruGOCn9s",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ZXiruGOCn9s\/hqdefault.jpg",
        "ID":"ZXiruGOCn9s",
        "Publish Time":"2022-03-11T10:00:37Z",
        "Channel":"IBM Technology",
        "Channel ID":"UCKWaEZ-_VweaEx1j62do_vQ",
        "Transcript":"no it's it it's not those transformers but but they can do some pretty cool things let me show you so why did the banana cross the road because it was sick of being mashed yeah i'm not sure that i quite get that one and that's because it was created by a computer i literally asked it to tell me a joke and this is what it came up with specifically i used a gpt-3 or a generative pre-trained transformer model the three here means that this is the third generation gpt-3 is an auto-regressive language model that produces text that looks like it was written by a human gpt3 can write poetry craft emails and evidently come up with its own jokes off you go now while our banana joke isn't exactly funny it does fit the typical pattern of a joke with a setup and a punch line and sort of kind of makes sense i mean who wouldn't cross the road to avoid getting mashed but look gpt3 is just one example of a transformer something that transforms from one sequence into another and language translation is just a great example perhaps we want to take a sentence of why did the banana cross the road and we want to take that english phrase and translate it into french well transformers consist of two parts there is an encoder and there is a decoder the encoder works on the input sequence and the decoder operates on the target output sequence now on the face of it translation seems like little more than just like a basic lookup task so convert the y here of our english sentence to the french equivalent of porcua but of course language translation doesn't really work that way things like word order in terms of phrase often mix things up and the way transformers work is through sequence to sequence learning where the transformer takes a sequence of tokens in this case words in a sentence and predicts the next word in the output sequence it does this through iterating through encoder layers so the encoder generates encodings that define which part of the input sequence are relevant to each other and then passes these encodings to the next encoder layer the decoder takes all of these encodings and uses their derived context to generate the output sequence now transformers are a form of semi supervised learning by semi sequence semi-supervised we mean that they are pre-trained in an unsupervised manner with a large unlabeled data set and then they're fine-tuned through supervised training to get them to perform better now in previous videos i've talked about other machine learning algorithms that handle sequential input like natural language for example there are recurrent neural networks or rnns what makes transformers a little bit different is they do not necessarily process data in order transformers use something called an attention mechanism and this provides context around items in the input sequence so rather than starting our translation with the word why because it's at the start of the sentence the transformer attempts to identify the context that bring meaning in each word in the sequence and it's this attention mechanism that gives transformers a huge leg up over algorithms like rnn that must run in sequence transformers run multiple sequences in parallel and this vastly speeds up training times so beyond translations what can transformers be applied to well document summaries they're another great example you can like feed in a whole article as the input sequence and then generate an output sequence that's going to really just be a couple of sentences that summarize the main points transformers can create whole new documents of their own for example like write a whole blog post and beyond just language transformers have done things like learn to play chess and perform image processing that even rivals the capabilities of convolutional neural networks look transformers are a powerful deep learning model and thanks to how the attention mechanism can be paralyzed are getting better all the time and who knows pretty soon maybe they'll even be able to pull off banana jokes that are actually funny if you have any questions please drop us a line below and if you want to see more videos like this in the future please like and subscribe thanks for watching"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Transformers and Self-Attention Architecture",
        "Video Title":"Attention in transformers, step-by-step | DL6",
        "URL":"https:\/\/www.youtube.com\/watch?v=eMlx5fFNoYc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/eMlx5fFNoYc\/hqdefault.jpg",
        "ID":"eMlx5fFNoYc",
        "Publish Time":"2024-04-07T12:53:54Z",
        "Channel":"3Blue1Brown",
        "Channel ID":"UCYO_jab_esuFRV4b17AJtAw",
        "Transcript":"In the last chapter, you and I started to step through the internal workings of a transformer. This is one of the key pieces of technology inside large language models, and a lot of other tools in the modern wave of AI. It first hit the scene in a now-famous 2017 paper called Attention is All You Need, and in this chapter you and I will dig into what this attention mechanism is, visualizing how it processes data. As a quick recap, here's the important context I want you to have in mind. The goal of the model that you and I are studying is to take in a piece of text and predict what word comes next. The input text is broken up into little pieces that we call tokens, and these are very often words or pieces of words, but just to make the examples in this video easier for you and me to think about, let's simplify by pretending that tokens are always just words. The first step in a transformer is to associate each token with a high-dimensional vector, what we call its embedding. The most important idea I want you to have in mind is how directions in this high-dimensional space of all possible embeddings can correspond with semantic meaning. In the last chapter we saw an example for how direction can correspond to gender, in the sense that adding a certain step in this space can take you from the embedding of a masculine noun to the embedding of the corresponding feminine noun. That's just one example you could imagine how many other directions in this high-dimensional space could correspond to numerous other aspects of a word's meaning. The aim of a transformer is to progressively adjust these embeddings so that they don't merely encode an individual word, but instead they bake in some much, much richer contextual meaning. I should say up front that a lot of people find the attention mechanism, this key piece in a transformer, very confusing, so don't worry if it takes some time for things to sink in. I think that before we dive into the computational details and all the matrix multiplications, it's worth thinking about a couple examples for the kind of behavior that we want attention to enable. Consider the phrases American shrew mole, one mole of carbon dioxide, and take a biopsy of the mole. You and I know that the word mole has different meanings in each one of these, based on the context. But after the first step of a transformer, the one that breaks up the text and associates each token with a vector, the vector that's associated with mole would be the same in all of these cases, because this initial token embedding is effectively a lookup table with no reference to the context. It's only in the next step of the transformer that the surrounding embeddings have the chance to pass information into this one. The picture you might have in mind is that there are multiple distinct directions in this embedding space encoding the multiple distinct meanings of the word mole, and that a well-trained attention block calculates what you need to add to the generic embedding to move it to one of these specific directions, as a function of the context. To take another example, consider the embedding of the word tower. This is presumably some very generic, non-specific direction in the space, associated with lots of other large, tall nouns. If this word was immediately preceded by Eiffel, you could imagine wanting the mechanism to update this vector so that it points in a direction that more specifically encodes the Eiffel tower, maybe correlated with vectors associated with Paris and France and things made of steel. If it was also preceded by the word miniature, then the vector should be updated even further, so that it no longer correlates with large, tall things. More generally than just refining the meaning of a word, the attention block allows the model to move information encoded in one embedding to that of another, potentially ones that are quite far away, and potentially with information that's much richer than just a single word. What we saw in the last chapter was how after all of the vectors flow through the network, including many different attention blocks, the computation you perform to produce a prediction of the next token is entirely a function of the last vector in the sequence. Imagine, for example, that the text you input is most of an entire mystery novel, all the way up to a point near the end, which reads, therefore the murderer was. If the model is going to accurately predict the next word, that final vector in the sequence, which began its life simply embedding the word was, will have to have been updated by all of the attention blocks to represent much, much more than any individual word, somehow encoding all of the information from the full context window that's relevant to predicting the next word. To step through the computations, though, let's take a much simpler example. Imagine that the input includes the phrase, a fluffy blue creature roamed the verdant forest. And for the moment, suppose that the only type of update that we care about is having the adjectives adjust the meanings of their corresponding nouns. What I'm about to describe is what we would call a single head of attention, and later we will see how the attention block consists of many different heads run in parallel. Again, the initial embedding for each word is some high dimensional vector that only encodes the meaning of that particular word with no context. Actually, that's not quite true. They also encode the position of the word. There's a lot more to say about the specific way that positions are encoded, but right now, all you need to know is that the entries of this vector are enough to tell you both what the word is and where it exists in the context. Let's go ahead and denote these embeddings with the letter e. The goal is to have a series of computations produce a new refined set of embeddings where, for example, those corresponding to the nouns have ingested the meaning from their corresponding adjectives. And playing the deep learning game, we want most of the computations involved to look like matrix-vector products, where the matrices are full of tuneable weights, things that the model will learn based on data. To be clear, I'm making up this example of adjectives updating nouns just to illustrate the type of behavior that you could imagine an attention head doing. As with so much deep learning, the true behavior is much harder to parse because it's based on tweaking and tuning a huge number of parameters to minimize some cost function. It's just that as we step through all of different matrices filled with parameters that are involved in this process, I think it's really helpful to have an imagined example of something that it could be doing to help keep it all more concrete. For the first step of this process, you might imagine each noun, like creature, asking the question, hey, are there any adjectives sitting in front of me? And for the words fluffy and blue, to each be able to answer, yeah, I'm an adjective and I'm in that position. That question is somehow encoded as yet another vector, another list of numbers, which we call the query for this word. This query vector though has a much smaller dimension than the embedding vector, say 128. Computing this query looks like taking a certain matrix, which I'll label wq, and multiplying it by the embedding. Compressing things a bit, let's write that query vector as q, and then anytime you see me put a matrix next to an arrow like this one, it's meant to represent that multiplying this matrix by the vector at the arrow's start gives you the vector at the arrow's end. In this case, you multiply this matrix by all of the embeddings in the context, producing one query vector for each token. The entries of this matrix are parameters of the model, which means the true behavior is learned from data, and in practice, what this matrix does in a particular attention head is challenging to parse. But for our sake, imagining an example that we might hope that it would learn, we'll suppose that this query matrix maps the embeddings of nouns to certain directions in this smaller query space that somehow encodes the notion of looking for adjectives in preceding positions. As to what it does to other embeddings, who knows? Maybe it simultaneously tries to accomplish some other goal with those. Right now, we're laser focused on the nouns. At the same time, associated with this is a second matrix called the key matrix, which you also multiply by every one of the embeddings. This produces a second sequence of vectors that we call the keys. Conceptually, you want to think of the keys as potentially answering the queries. This key matrix is also full of tuneable parameters, and just like the query matrix, it maps the embedding vectors to that same smaller dimensional space. You think of the keys as matching the queries whenever they closely align with each other. In our example, you would imagine that the key matrix maps the adjectives like fluffy and blue to vectors that are closely aligned with the query produced by the word creature. To measure how well each key matches each query, you compute a dot product between each possible key-query pair. I like to visualize a grid full of a bunch of dots, where the bigger dots correspond to the larger dot products, the places where the keys and queries align. For our adjective noun example, that would look a little more like this, where if the keys produced by fluffy and blue really do align closely with the query produced by creature, then the dot products in these two spots would be some large positive numbers. In the lingo, machine learning people would say that this means the embeddings of fluffy and blue attend to the embedding of creature. By contrast to the dot product between the key for some other word like the and the query for creature would be some small or negative value that reflects that are unrelated to each other. So we have this grid of values that can be any real number from negative infinity to infinity, giving us a score for how relevant each word is to updating the meaning of every other word. The way we're about to use these scores is to take a certain weighted sum along each column, weighted by the relevance. So instead of having values range from negative infinity to infinity, what we want is for the numbers in these columns to be between 0 and 1, and for each column to add up to 1, as if they were a probability distribution. If you're coming in from the last chapter, you know what we need to do then. We compute a softmax along each one of these columns to normalize the values. In our picture, after you apply softmax to all of the columns, we'll fill in the grid with these normalized values. At this point you're safe to think about each column as giving weights according to how relevant the word on the left is to the corresponding value at the top. We call this grid an attention pattern. Now if you look at the original transformer paper, there's a really compact way that they write this all down. Here the variables q and k represent the full arrays of query and key vectors respectively, those little vectors you get by multiplying the embeddings by the query and the key matrices. This expression up in the numerator is a really compact way to represent the grid of all possible dot products between pairs of keys and queries. A small technical detail that I didn't mention is that for numerical stability, it happens to be helpful to divide all of these values by the square root of the dimension in that key query space. Then this softmax that's wrapped around the full expression is meant to be understood to apply column by column. As to that v term, we'll talk about it in just a second. Before that, there's one other technical detail that so far I've skipped. During the training process, when you run this model on a given text example, and all of the weights are slightly adjusted and tuned to either reward or punish it based on how high a probability it assigns to the true next word in the passage, it turns out to make the whole training process a lot more efficient if you simultaneously have it predict every possible next token following each initial subsequence of tokens in this passage. For example, with the phrase that we've been focusing on, it might also be predicting what words follow creature and what words follow the. This is really nice, because it means what would otherwise be a single training example effectively acts as many. For the purposes of our attention pattern, it means that you never want to allow later words to influence earlier words, since otherwise they could kind of give away the answer for what comes next. What this means is that we want all of these spots here, the ones representing later tokens influencing earlier ones, to somehow be forced to be zero. The simplest thing you might think to do is to set them equal to zero, but if you did that the columns wouldn't add up to one anymore, they wouldn't be normalized. So instead, a common way to do this is that before applying softmax, you set all of those entries to be negative infinity. If you do that, then after applying softmax, all of those get turned into zero, but the columns stay normalized. This process is called masking. There are versions of attention where you don't apply it, but in our GPT example, even though this is more relevant during the training phase than it would be, say, running it as a chatbot or something like that, you do always apply this masking to prevent later tokens from influencing earlier ones. Another fact that's worth reflecting on about this attention pattern is how its size is equal to the square of the context size. So this is why context size can be a really huge bottleneck for large language models, and scaling it up is non-trivial. As you imagine, motivated by a desire for bigger and bigger context windows, recent years have seen some variations to the attention mechanism aimed at making context more scalable, but right here, you and I are staying focused on the basics. Okay, great, computing this pattern lets the model deduce which words are relevant to which other words. Now you need to actually update the embeddings, allowing words to pass information to whichever other words they're relevant to. For example, you want the embedding of Fluffy to somehow cause a change to Creature that moves it to a different part of this 12,000-dimensional embedding space that more specifically encodes a Fluffy creature. What I'm going to do here is first show you the most straightforward way that you could do this, though there's a slight way that this gets modified in the context of multi-headed attention. This most straightforward way would be to use a third matrix, what we call the value matrix, which you multiply by the embedding of that first word, for example Fluffy. The result of this is what you would call a value vector, and this is something that you add to the embedding of the second word, in this case something you add to the embedding of Creature. So this value vector lives in the same very high-dimensional space as the embeddings. When you multiply this value matrix by the embedding of a word, you might think of it as saying, if this word is relevant to adjusting the meaning of something else, what exactly should be added to the embedding of that something else in order to reflect this? Looking back in our diagram, let's set aside all of the keys and the queries, since after you compute the attention pattern you're done with those, then you're going to take this value matrix and multiply it by every one of those embeddings to produce a sequence of value vectors. You might think of these value vectors as being kind of associated with the corresponding keys. For each column in this diagram, you multiply each of the value vectors by the corresponding weight in that column. For example here, under the embedding of Creature, you would be adding large proportions of the value vectors for Fluffy and Blue, while all of the other value vectors get zeroed out, or at least nearly zeroed out. And then finally, the way to actually update the embedding associated with this column, previously encoding some context-free meaning of Creature, you add together all of these rescaled values in the column, producing a change that you want to add, that I'll label delta-e, and then you add that to the original embedding. Hopefully what results is a more refined vector encoding the more contextually rich meaning, like that of a fluffy blue creature. And of course you don't just do this to one embedding, you apply the same weighted sum across all of the columns in this picture, producing a sequence of changes, adding all of those changes to the corresponding embeddings, produces a full sequence of more refined embeddings popping out of the attention block. Zooming out, this whole process is what you would describe as a single head of attention. As I've described things so far, this process is parameterized by three distinct matrices, all filled with tunable parameters, the key, the query, and the value. I want to take a moment to continue what we started in the last chapter, with the scorekeeping where we count up the total number of model parameters using the numbers from GPT-3. These key and query matrices each have 12,288 columns, matching the embedding dimension, and 128 rows, matching the dimension of that smaller key query space. This gives us an additional 1.5 million or so parameters for each one. If you look at that value matrix by contrast, the way I've described things so far would suggest that it's a square matrix that has 12,288 columns and 12,288 rows, since both its inputs and outputs live in this very large embedding space. If true, that would mean about 150 million added parameters. And to be clear, you could do that. You could devote orders of magnitude more parameters to the value map than to the key and query. But in practice, it is much more efficient if instead you make it so that the number of parameters devoted to this value map is the same as the number devoted to the key and the query. This is especially relevant in the setting of running multiple attention heads in parallel. The way this looks is that the value map is factored as a product of two smaller matrices. Conceptually, I would still encourage you to think about the overall linear map, one with inputs and outputs, both in this larger embedding space, for example taking the embedding of blue to this blueness direction that you would add to nouns. It's just that it's a smaller number of rows, typically the same size as the key query space. What this means is you can think of it as mapping the large embedding vectors down to a much smaller space. This is not the conventional naming, but I'm going to call this the value down matrix. The second matrix maps from this smaller space back up to the embedding space, producing the vectors that you use to make the actual updates. I'm going to call this one the value up matrix, which again is not conventional. The way that you would see this written in most papers looks a little different. I'll talk about it in a minute. In my opinion, it tends to make things a little more conceptually confusing. To throw in linear algebra jargon here, what we're basically doing is constraining the overall value map to be a low rank transformation. Turning back to the parameter count, all four of these matrices have the same size, and adding them all up we get about 6.3 million parameters for one attention head. As a quick side note, to be a little more accurate, everything described so far is what people would call a self-attention head, to distinguish it from a variation that comes up in other models that's called cross-attention. This isn't relevant to our GPT example, but if you're curious, cross-attention involves models that process two distinct types of data, like text in one language and text in another language that's part of an ongoing generation of a translation, or maybe audio input of speech and an ongoing transcription. A cross-attention head looks almost identical. The only difference is that the key and query maps act on different data sets. In a model doing translation, for example, the keys might come from one language, while the queries come from another, and the attention pattern could describe which words from one language correspond to which words in another. And in this setting there would typically be no masking, since there's not really any notion of later tokens affecting earlier ones. Staying focused on self-attention though, if you understood everything so far, and if you were to stop here, you would come away with the essence of what attention really is. All that's really left to us is to lay out the sense in which you do this many many different times. In our central example we focused on adjectives updating nouns, but of course there are lots of different ways that context can influence the meaning of a word. If the words they crashed the preceded the word car, it has implications for the shape and structure of that car. And a lot of associations might be less grammatical. If the word wizard is anywhere in the same passage as Harry, it suggests that this might be referring to Harry Potter, whereas if instead the words Queen, Sussex, and William were in that passage, then perhaps the embedding of Harry should instead be updated to refer to the prince. For every different type of contextual updating that you might imagine, the parameters of these key and query matrices would be different to capture the different attention patterns, and the parameters of our value map would be different based on what should be added to the embeddings. And again, in practice the true behavior of these maps is much more difficult to interpret, where the weights are set to do whatever the model needs them to do to best accomplish its goal of predicting the next token. As I said before, everything we described is a single head of attention, and a full attention block inside a transformer consists of what's called multi-headed attention, where you run a lot of these operations in parallel, each with its own distinct key query and value maps. GPT-3 for example uses 96 attention heads inside each block. Considering that each one is already a bit confusing, it's certainly a lot to hold in your head. Just to spell it all out very explicitly, this means you have 96 distinct key and query matrices producing 96 distinct attention patterns. Then each head has its own distinct value matrices used to produce 96 sequences of value vectors. These are all added together using the corresponding attention patterns as weights. What this means is that for each position in the context, each token, every one of these heads produces a proposed change to be added to the embedding in that position. So what you do is you sum together all of those proposed changes, one for each head, and you add the result to the original embedding of that position. This entire sum here would be one slice of what's outputted from this multi-headed attention block, a single one of those refined embeddings that pops out the other end of it. Again, this is a lot to think about, so don't worry at all if it takes some time to sink in. The overall idea is that by running many distinct heads in parallel, you're giving the model the capacity to learn many distinct ways that context changes meaning. Pulling up our running tally for parameter count with 96 heads, each including its own variation of these four matrices, each block of multi-headed attention ends up with around 600 million parameters. There's one added slightly annoying thing that I should really mention for any of you who go on to read more about transformers. You remember how I said that the value map is factored out into these two distinct matrices, which I labeled as the value down and the value up matrices. The way that I framed things would suggest that you see this pair of matrices inside each attention head, and you could absolutely implement it this way. That would be a valid design. But the way that you see this written in papers and the way that it's implemented in practice looks a little different. All of these value up matrices for each head appear stapled together in one giant matrix that we call the output matrix, associated with the entire multi-headed attention block. And when you see people refer to the value matrix for a given attention head, they're typically only referring to this first step, the one that I was labeling as the value down projection into the smaller space. For the curious among you, I've left an on-screen note about it. It's one of those details that runs the risk of distracting from the main conceptual points, but I do want to call it out just so that you know if you read about this in other sources. Setting aside all the technical nuances, in the preview from the last chapter we saw how data flowing through a transformer doesn't just flow through a single attention block. For one thing, it also goes through these other operations called multi-layer perceptrons. We'll talk more about those in the next chapter. And then it repeatedly goes through many many copies of both of these operations. What this means is that after a given word imbibes some of its context, there are many more chances for this more nuanced embedding to be influenced by its more nuanced surroundings. The further down the network you go, with each embedding taking in more and more meaning from all the other embeddings, which themselves are getting more and more nuanced, the hope is that there's the capacity to encode higher level and more abstract ideas about a given input beyond just descriptors and grammatical structure. Things like sentiment and tone and whether it's a poem and what underlying scientific truths are relevant to the piece and things like that. Turning back one more time to our scorekeeping, GPT-3 includes 96 distinct layers, so the total number of key query and value parameters is multiplied by another 96, which brings the total sum to just under 58 billion distinct parameters devoted to all of the attention heads. That is a lot to be sure, but it's only about a third of the 175 billion that are in the network in total. So even though attention gets all of the attention, the majority of parameters come from the blocks sitting in between these steps. In the next chapter, you and I will talk more about those other blocks and also a lot more about the training process. A big part of the story for the success of the attention mechanism is not so much any specific kind of behaviour that it enables, but the fact that it's extremely parallelizable, meaning that you can run a huge number of computations in a short time using GPUs. Given that one of the big lessons about deep learning in the last decade or two has been that scale alone seems to give huge qualitative improvements in model performance, there's a huge advantage to parallelizable architectures that let you do this. If you want to learn more about this stuff, I've left lots of links in the description. In particular, anything produced by Andrej Karpathy or Chris Ola tend to be pure gold. In this video, I wanted to just jump into attention in its current form, but if you're curious about more of the history for how we got here and how you might reinvent this idea for yourself, my friend Vivek just put up a couple videos giving a lot more of that motivation. Also, Britt Cruz from the channel The Art of the Problem has a really nice video about the history of large language models. Thank you."
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Transformers and Self-Attention Architecture",
        "Video Title":"Self-attention in deep learning (transformers) - Part 1",
        "URL":"https:\/\/www.youtube.com\/watch?v=8fIJk1lJ4aE",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/8fIJk1lJ4aE\/hqdefault.jpg",
        "ID":"8fIJk1lJ4aE",
        "Publish Time":"2021-02-22T12:00:08Z",
        "Channel":"AI Bites",
        "Channel ID":"UCCW0ICn8IMRsgnJhCLAuClA",
        "Transcript":"self-attention is very commonly used in deep learning these days for example it is one of the main building blocks of the transformer paper which is fast becoming the go to deep learning architecture for several problems both in computer vision and language processing for example all these famous papers like bird gpt xlm performer use some variations of the transformers which in turn is built using self-attention layers as building blocks so let's try to understand self-attention in this video let's take this simple example from language processing where the inputs are these four words hi how are you it could be the problem of translation where your input will be a translated sentence of the input or it could be an input to a chatbot where the response will be i'm good how can i help we first pass the inputs through an embedding layer it could be anything like a vertebrate or even another neural network which converts the input into numeric outputs as machine learning systems can only deal with numbers the outputs of the embedding layer are vectors x1 to 4 corresponding to the four words the main point to note here is that each vector is independent of each other that means that x1 doesn't know anything about x2 and x2 doesn't know anything about x3 and so on now let's first form a matrix w star in this matrix the value at each location is the dot product of the vectors for example the value at location w12 is the dot product of vectors x1 and x2 the main property of this matrix w star is that the values are not normalized and the values can be more than one in machine learning we like numbers to be between zero and one so to address this we normalize the matrix along the horizontal direction so that the sum of the values is always 1 in this direction let's call the new matrix as w after we compute this w matrix we stack the inputs x1 to 4 to form a matrix x and then transpose it to form x transpose the product of these two matrices leads to our output matrix y the speciality of y is that it's now context aware by context what i mean is that each word knows which word is nearby or rather each word is influenced by the presence of its neighboring word for example the presence of high next to how is known to how and the same applies for other words too if you're more of a visual person then this could probably be a geometric illustration of self-attention on the left is without attention but on the right we can see the result of applying attention the representations have moved slightly closer together because each word is influenced by the neighboring word and this is a naive case where the feature dimensions is just two but practically the future dimension of each word is very high say 64 or 128 dimension and it's impractical to visualize the features this way what we just saw is a very simplified version of self-attention why because we just took the dot product of two feature vectors and computed the weight matrix w however what we do want is to be able to learn the weights so this matrix w should be learned from data or in other words self-attention has to be parametric in my next video let's look at how we can extend this to make it learnable thank you very much for watching you"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Transformers and Self-Attention Architecture",
        "Video Title":"Illustrated Guide to Transformers Neural Network: A step by step explanation",
        "URL":"https:\/\/www.youtube.com\/watch?v=4Bdc55j80l8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/4Bdc55j80l8\/hqdefault.jpg",
        "ID":"4Bdc55j80l8",
        "Publish Time":"2020-04-28T01:26:26Z",
        "Channel":"The AI Hacker",
        "Channel ID":"UCYpBgT4riB-VpsBBBQkblqQ",
        "Transcript":"transformers are taking the natural language processing world by storm these incredible models are breaking multiple NLP records and pushing the state-of-the-art they are used in many applications like machine language translation conversational chat BOTS and even a power better search engines transformers are the rage and deep learning nowadays but how do they work why are they outperformed a previous king of sequence problems like recurrent neural networks gr use and LS tiens you've probably heard of different famous transformer models like Burt CBT and GB t2 in this video we'll focus on the one paper that started it all attention is all you need to understand transformers we first must understand the attention mechanism to get an intuitive understanding of the attention mechanism let's start with a fun text generation model that's capable of writing its own sci-fi novel we'll need to prime in a model with an arbitrary input and a model will generate the rest okay let's make the story interesting as aliens entered our planet and began to colonize earth a certain group of extraterrestrials begin to manipulate our society through their influence of a certain number of the elite of the country to keep an iron grip over the populace by the way I then just make this up this was actually generated by open AI is GPT to transformer model shout out to hugging face for an awesome interface to play with I'll provide a link in description okay so the model is a little dark but what's interesting is how it works as a model generate tax word by word it has the ability to reference or tend to words that is relevant to the generated word how the model knows which were to attend to is all learned while training with backpropagation our intends are also capable of looking at previous inputs too but the power of the attention mechanism is that it doesn't suffer from short-term memory rnns have a shorter window to reference from so when a story gets longer rnns can't access word generated earlier in the sequence this is still true for gr use and L STM's although they do have a bigger capacity to achieve longer term memory therefore having a longer window to reference from the attention mechanism in theory and given enough compute resources have an infinite window to reference from therefore being capable of using the entire context of the story while generating the text this power was demonstrated in the paper attention is all you need when the author's introduce a new novel neural network called the Transformers which is an attention based encoder decoder type architecture on a high level the encoder Maps an input sequence into an abstract continuous representation that holds all the learned information of that input to decoder then takes our continuous representation and step by step generates a single output while also being fed to previous output let's walk through an example the attention is all you need paper applied to transformer model on a neuro machine translation problem our demonstration of the transformer model would be a conversational chat bot the example with taking an input tax hi how are you and generate the response I am fine let's break down the mechanics of the network step by step the first step is feeding our input into a word embedded layer a word embedding layer can be thought of as a lookup table to grab a learn factor of representation of each word neural networks learned through numbers so each word maps to a vector with continuous values to represent that word next step is to inject positional information into the embeddings because a transformer encoder has no recurrence like recurrent known networks we must add information about the positions into the input embeddings this is done using positional encoding the authors came up with a clever trick using sine and cosine functions we won't go into the mathematical details of the positional codings in this video but here are the basics for every odd time step create a vector using the cosine function for every even time step create a vector using the sine function then add those vectors to their corresponding embedding vector this successfully gives the network information on two positions of each vector the sine and cosine functions were chosen in tandem because they have linear properties the model can easily learn to attend to now we have the encoder layer the encoder layers job is to map all input sequence into an abstract continuous representation that holds the learned information for that entire sequence it contains two sub modules multi-headed attention followed by a fully connected network there are also residual connections around each of the two sub modules followed by a layer normalization to break this down let's look at the multi headed attention module multi-headed attention Indian code applies a specific attention mechanism called self attention self attention allows a model to associate each individual word in the input to other words in the input so in our example it's possible that our model can learn to associate the word you with how M are it's also possible that the model learns that word structured in this pattern are typically a question so respond appropriately to achieve self attention we feed the input into three distinct fully connected layers to create the query key and value vectors what are these vectors exactly I found a good explanation on stock-exchange stating the query key and value concept comes from the retrieval system for example when you type a query to search for some video on YouTube the search engine will map your query against a set of keys for example video title description etc associated with candidate videos in the database then present you with the best match video let's see how this relates to self attention the queries and keys undergoes a dot product matrix multiplication to produce a score matrix the score matrix determines how much focus should a word be put on other words so each word will have a score to correspond to other words in the time step the higher score the more the focus this is how queries are mapped to keys then the scores get scaled down by getting divided by the square root of the dimension of the queries and the keys this is to allow for more stable gradients as multiplying values can have exploding effects next you take the softmax the scaled score to get the attention weights which gives you probability values between 0 & 1 by doing the softmax the higher scores get heightened and the lower scores are depressed this allows the model to be more confident on which words to attend to then you take the attention weights and multiply it by your value vector to get an output vector the higher softmax scores will keep the value of the words the model learn is more important the lower scores will drown out their irrelevant words you feed the output vector into a linear layer to process to make this a multi-headed attention computation you need to split the query key in value into adding vectors before applying self attention to split vectors that goes through the same self attention process individually each self attention process is called a head each head produces an output vector that gets concatenated into a single vector before go through in a final linear layer in theory each head would learn something different therefore giving the encounter model more representation power okay so that's multi-headed attention to sum it up multi-headed attention is a module in a transformer network that you to the attention waits for the input and produces an output vector with encoded information on how each word should attend to all other words in a sequence next step the multi-headed attention output vector is added to the original input this is called a residual connection the output of the residual connection goes through a layer normalization the normalized residual output gets fed into a point-wise feed-forward network for further processing the point-wise feed-forward network are a couple of linear layers with a relict evasion in between the output of that is again added to the input of the point-wise feed-forward network and further normalized the residual connections helps the network train by allowing gradients to flow through the networks directly the layer normalizations are used to stabilize the network which results in sustained producing the training time necessary and a point-wise feed-forward layer are used to further process the attention output potentially giving it a richer representation and that wraps up the encoded layer all these operations is for the purpose of encoding the input to a continuous representation with attention information this will help the decoder focus on the appropriate words in the input during the decoding process you can stack the encoder and times to further encode the information where each layer has the opportunity to learn different attention representations therefore potentially boosting the predictive power of the transformer network now we move on to the decoder the decoders job is to generate text sequences the decoder has similar sub layers as the encoder it has two multi-headed attention layers a point-wise feed-forward layer with residual connections and layer normalization after each sub layer these sub layers behave similarly to layers in the encoder but each multi-headed attention layer has a different job it's capped off with a linear layer that acts like a classifier and a soft Max to get the word probabilities the decoder is auto regressive it takes in the list of previous outputs as inputs as well as the encoder outputs that contains the attention information from the input the decoder stops decoding when it generates an end token as an output let's walk through the decoding steps the input goes through an embedding layer in a position on coding layer to get positional embeddings the positional embeddings gets fed into the first multi-headed attention layer which computes the attention score for the decoders input this multi-headed attention layer operates slightly different since the decoders autoregressive and generates the sequence word-by-word you need to prevent it from condition into future tokens for example when computing attention scores on the word am you should not have access to the word fine because our word is a future word that was generated after the word am should only have access to itself and the words before this is true for all other words where they can only attend to previous words we need a method to prevent computing attention scores for future words this method is called masking to prevent the decoder from looking at future tokens you apply a look-ahead mask the mask is added before calculating the softmax and after scaling the scores let's take a look at how this works the mask is a matrix that's the same size as the attention scores filled with values of materials and negative infinity x' when you add the mask to the scale attention scores you get a matrix of scores with the top right triangle filled with negative infinity x' the reason for this is once you take the softmax of the mask scores the negative infinity is get zeroed out leaving a zero attention score for future tokens as you can see the attention scores for M have values for itself and all other words before it but zero for the word fine this essentially tells the model to put no focus on those words this masking is the only difference on how the attention scores are calculated in the first multi-headed attention layer this layers still have multiple heads that the masks are being applied to before getting concatenated and fed through a linear layer for further processing the output of the first multi-headed attention is a mask output vector with information on how the model should attend on the decoders inputs now on to the second multi-headed attention layer for this layer the encoders output are the queries in the keys in the first multi-headed attention layer outputs are the values this process matches the encoders input to the decoders input allowing the decoder to decide which encoder input is relevant to put focus on the output of the second multi-headed attention goes through a point wise feed-forward layer for further processing the output of the final point wise feed-forward layer goes through a final linear layer that access a classifier the classifier is as biggest number of classes you have for example if you have 10,000 classes for 10,000 words the output of that classifier will be of size 10,000 the output of the classifier again gets fed into a soft max layer the soft max layer produced probability scores between 0 and 1 for each class we take the index of the highest probability score and that equals our predicted word the decoder didn't taste the output and adds it to the list of decoder inputs and continue decoding again until end token is predicted for our case the highest probability prediction is the final class which is assigned to the end token this is how the decoder generates the output the decoder can be stacked n layers high each layer taking in inputs from the encoder and the layers before it by stacking layers the model can learn to extract and focus on different combinations of attention from its attention heads potentially boosting its predictive power and that's it that's the mechanics of the transformers transformers leverage the power of the attention mechanism to make better predictions recur known networks trying to achieve similar things but because they suffer from short term memory transformers are usually better especially if you want to encode or generate longer sequences because of the transformer architecture the natural language processing industry can now achieve unprecedented results if you found this helpful hit that like and subscribe button also let me know in comments what you'd like to see next and until next time thanks for watching"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Implementing Transformers with TensorFlow\/PyTorch",
        "Video Title":"PyTorch vs. TensorFlow",
        "URL":"https:\/\/www.youtube.com\/watch?v=fSgNjOvXTCs",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/fSgNjOvXTCs\/hqdefault.jpg",
        "ID":"fSgNjOvXTCs",
        "Publish Time":"2024-09-27T12:13:36Z",
        "Channel":"Plivo",
        "Channel ID":"UCNL8MQasO7O-q_g_8X6CI2g",
        "Transcript":"pytorch or tensorflow both are leading deep learning Frameworks and they both have pros and cons so let's get into it pytorch developed by meta aai dominates research with 60% of published papers using it as of June of 2024 and people love its Dynamic computational graph intuitive model building and seamless integration with python tools like numpy the fact that it's pythonic also helps to make it more useful for a wider audience which means that it has an excellent ecosystem of models and libraries for pie torch users to tap into tensor flow on the other hand developed by Google is generally considered to be the industry standard for production environments it offers robust deployment capabilities through tensorflow serving and strong cloud service integration too tensorflow's comprehensive tooling includes tensor board for visualization tensorflow light for mobile deployment and tensorflow.js for web deployment it also supports Hardware acceleration through gpus and tpus ensuring high performance in large scale environments"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Implementing Transformers with TensorFlow\/PyTorch",
        "Video Title":"Pytorch Transformers from Scratch (Attention is all you need)",
        "URL":"https:\/\/www.youtube.com\/watch?v=U0s0f995w14",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/U0s0f995w14\/hqdefault.jpg",
        "ID":"U0s0f995w14",
        "Publish Time":"2020-06-22T20:56:59Z",
        "Channel":"Aladdin Persson",
        "Channel ID":"UCkzW5JSFwvKRjXABI-UTAkQ",
        "Transcript":"in this video we're gonna build the transformer Network from scratch from the original paper attention is all you need which is one of the most impactful papers in deep learning and natural language processing let's roll that intro and then let's get started what is going on guys hope you're doing awesome so we're gonna start with taking a look at the paper and getting an understanding of what we're gonna do and I'm gonna try to explain two relevant parts and I do think when we step through the code it's gonna help with the understanding as well but if you still have questions about how transformers work and then I recommend this blog post by Peter bloom and it's one of the resources I used to gain a better understanding of transformers so with that said let's start with taking a look at the paper so the paper is attention is all you need and as I said it's one of the most impactful paper in deep learning and especially for natural language processing so we're just gonna go down to the overview of the transformer here and if this is the first time that you see the transformer network this is gonna feel very scary at least it did for me and it's kinda looks like some stuff from the matrix that you can never understand but trust me we're gonna try to go through it step-by-step so the first thing is that you know we have to the left here we have a encoder alright and then we have a decoder to the right so hopefully you have a bit understanding of sequence of sequence and sequences sequence with attention and if not I've made previous videos on those but you know that this right here is the encoder and so starting from the bottom we have some input let's say it's some source text and we you know so for machine translation we're gonna create our create some embeddings and then that's going to be sent in to this block right here alright and we're gonna call this block to transform a block okay so let's just restrict our attention to just this block right here what we can see first is that the the input is going to be sent into some multi-headed attention okay and this multi-headed attention this this is you know smaller block inside this transformer block is perhaps well it is the most difficult part and it's the essential core of the transformer alright so if we understand this everything else is going to be easy so what we can see first is that we have some input and what this is gonna do is gonna send into three different inputs to the multi-headed attention we're gonna call these the values the keys and the queries alright for the first one as we just have one input here it's gonna be the same input for all three of those so we're just gonna you know repeat it and send the same input for all three of these value key enquiry all right we're gonna go to the multi attention in more detail but and this is sort of just the overview in that that's the most important part and then what's gonna happen is it's just going to go through a normalization then it's gonna go through a feed-forward Network and it's gonna go to another normalization all right these three parts are very easy to understand and I think it's going to be clear as well when we go through the code all right so that's sort of the the the understanding part of what we're going to do here then there's these arrows here which if you familiar would resonate these are just skip connections so that we have our input here which is sent through a multi-headed attention and then the output from that is going to be sent to this norm this this normalization right here but we're also going to take this skip connection from the previous so the input before the multi-headed attention and that's also going to be sent into the normalization so just like resonates we're going to add them together and similarly this error here is also a skip connection so before we run it through the feed-forward we're gonna take that the input to the feed forward we're gonna send it through with this cab skip connection to the normalization and then we're also gonna add it with the output from the feed-forward Network all right so that's a transform a block and we're going to go through the attention part in more detail and also the feed-forward part but the again this is easy this multi-headed attention is the difficult part then for the decoder right here we can see first of all that the output from the the the encoder is sent as input so if you remember these are the values keys and queries so the output from the encoder is gonna be sent into some multi-headed attention here for the values and the and the keys and then the query is here for the multi detention is gonna be sent from from the previous part of the decoder but let's look again so we have a let's call it a decoder block right here and what it uses is first of all right here it's the transformer block this is the exact same part as we saw in the encoder right the only thing that's changed here is that we have now different inputs to the attention here where two of them are from the previous encoder and one of them is from the previous part of the decoder so what we can say is that the decoder block right here is this transformer block and it has two additional ones so the normalization and another attention so you know this is pretty much the same here we have the embedding for the for the output and then we send it into this attention right here we're gonna again send the same thing through everything every like the value key in query and then we're gonna have this skipped connection again to normalization and and then this skip connection right here is gonna come from this previous part here and the values and keys come from the output from the encoder and then the query here comes from this part and then we're gonna send it through a transformer block and then unless then finally we're gonna send it through some linear and some softmax all right so before we step you know go through the attention part in a little bit more detail first of all if we have this NX right here which is this block which we call the decoder block is going to be repeated a couple of times all right and similarly for the encoder so you know instead of having just one of these blocks and then send that out put it through the decoder we're gonna do that this block right here is gonna be the output from this block is gonna be sent to another identical of these blocks and then for a couple of times in a couple of couple of layers and then the output from that final block will be sent to the to the decoder alright that's just what the NX stands for here so we repeat this for a couple of times couple of layers and I guess the only thing we didn't talk about here is that button so this positional encoding the transformer network is permutation aliy invariant so if you have a sentence and you change the order of the words that means that you know the transformer will be invariant to those changes it's going to be the exact same output even if you change the order of the words which is not good right because if you change the order of a few words that can entirely change the meaning of the sentence so we have these positional encodings and we're gonna this is sort of like an embedding that we we add in back to the input embedding so that it becomes aware of the positions of the words and we're gonna use a we're going to implement a variant that's a little bit easier than they did in the paper we're gonna use positional embeddings and we're going to go through that more when we when we talk and we can go through the implementation but I think yeah so there's one more thing in that this attention right here is a masked multi head attention so let's just step back a little to understand this masking so the thing about transformers that made them so great is the fact that all operations are able to be done in parallel which is in contrast to sequence models like LST Msgr use in our Nan's but this really you know really big strength has one problem which is if we look at translation where we have a target translated text this translated text is all sent into decoder at the same time so you know let's just say that the first element is a star token and then the next element is the first translated word and then the first output that we've worn from the decoder just corresponds to the second element which we send in in the target sentence so if we allow the encoder to have all this information this is going to be super easy simple it will just learn to use the provided target translation and it will just learn a simple mapping and really not learn anything about translating text so what we do is that we mask the target input to the decoder so that the first output of the decoder only had access to the first element and then the second output only had access to the first and second you know input to the decoder now that we have an overview let's try to take a deeper look at the attention mechanism so the absolute first thing we're gonna do is we're gonna take the embedding input and let's say it's 256 dimensions and we're going to split it into several parts so let's say we split into eight parts now eight of these parts all have 32 dimensions 32 dimensions each and they're all going to be sent in three through linear layers and that's also why we have this linear shadow'd parts here is because we are sending in the input which has been split and then we send the output from those into this scaled dot product attention all right and they have a figure here but I think the formula is a little bit clearer so essentially the attention the scale dot-product attention is that we take the queries and we multiply them with the keys then we do a scaling which is that we divide by the square root of the embedding size essentially this is just for numerical stability then we do a soft max of that and lastly we multiply it with the values okay that's the scale dot product attention so we do then is that we concatenate all of those all of those different parts that we split so that we obtain the same as the as the input so the embedding size and then that is then sent through a linear layer and that's the output from the multi-headed attention now that we have a basic understanding of the transformer Network we're ready to start coding so there are a bunch of things I didn't mention in the implementation details and I'm gonna sort of talk about them as we go through it and but I think that the necessary understanding is there for us to start coding it so what we're going to do first is we're gonna do as usual import torch we're gonna do import torch n N as n N and the thing we're gonna start with which is perhaps the most complicated thing is a self attention so we're going to class self attention we're going to inherit from an in module and we're going to do define in it and we're gonna send in the embed size and we're also gonna send Dean heads so as I said we have an embedding and we're going to split this embedding into different parts I believe I said eight different parts and in that case how many parts we split it is what we're gonna call heads so if we have for example in bed size 256 and we have heads eight then we're gonna split it into eight by 32 parts so let's do we're gonna just call super first self attention and we're gonna do self and we're gonna do docked dot in it so we're gonna initialize a prank class and then we're gonna do self dot embed size equals embed size we're gonna do self dot heads equals heads and self dot head dimension equals embed size the integer division by heads all right so let's say that we want to split it we have 256 embedding size we want to split it into seven parts then that would not be possible since we can't make an integer division of that so what we can do is we can throw out an assert we can say assert self dot we can do it like this self dot head dimension times heads equals embed size and if it doesn't we can say embed size needs to be divisible by heads what we're gonna do then is we're gonna define the linear layers that we're gonna send our values keys and queries through so we can do self dot values is and then that linear and it's just going to take the head dimension and it's going to map it to the head dimension and we're gonna set by old bias equals false then we're going to yourself that keys is equal to n n dot linear I'm gonna do self that head I mentioned and I guess same thing could I just copy that could have copied it again but anyways queries and then linear self thought dimension to self dot head I mentioned again bias equals false and then we're gonna do after we concatenate we're gonna do fully connected out it's gonna be an inland ear of heads times self dot head dimension and D and then the embed size I guess yeah so we don't have to write it like that I guess you know since the heads times the self a damaging needs to be equal to the embed size but yeah maybe this makes it a little bit clearer that we're gonna concatenate them and then we're gonna do the forward and we're gonna send in the values keys query and we're also gonna send in a mask so the first thing we're gonna do is we're gonna get the number of training examples so n we're gonna set to query dot shape of 0 and that's gonna be how many examples we send in at the same time then we're gonna do the value line the key length and the query length and remember the value and I guess those length or girth are going to depending on where we use the attention mechanism is going to be be corresponding to the source sentence length and the target sentence length but since we don't know exactly where this mechanism is used either in the encoder or which part in the decoder those are going to vary so we just use the abstract of saying we just use it abstractly and say value link key length and query length but really they will always correspond to the source sentence length and the target sentence length and that's just one thing to keep in mind and then we're just going to values that shape of one keys that shape shape of one and then query shape of one and that's just where they're going to be in the in the dimensions so what we want to do now is want to split embedding into self dot heads pieces so we're gonna do values is values dot reshape reshape and then we have n comma value Ling comma self dot heads call myself that head dimension so these two right here is where we're splitting it since this time this was before a single dimension of just in bed size now it's going to be self that heads and then self that head dimension and we're gonna do the same thing for the keys so key start reshape we're gonna do n key lengths self thought heads and then self dot head dimension moving along to the queries we're gonna do pretty much the same thing query dot reshape and key length self dot heads and then self thought head dimension and then what we're gonna do is we want to multiply the queries with the keys and so we're gonna call the output from that energy and what I'm gonna use is maybe something that you're not familiar with but it's towards that in some and I'll explain what it does but essentially it's we're gonna use it for matrix multiplication where we have several other dimensions so let's just bring out the shapes first we have queries so queries shape is n comma query link come ahead come ahead dimension and then we have keys shape and I guess pretty much the same and comma key lang heads heads dimension and so what we want so we energy is gonna be n comma heads comma query length comma key length all right so you know you can kind of view this as you know let's say that the query length is the is it is the target source sentence and the key length is is there is the source sentence then this will kind of say that okay for each word in our target how much should we pay attention to each word in our input in the store sentence and what we want here anyways for the matrix multiplication is that we want to multiply well actually let's do this first so right see right here we're gonna do n query for the query length H for the heads and then D for the heads dimension and then we're gonna do comma N K again for the key key length n for the batch size key K for the Kaling and then age for the heads and then D for the head dimension and then we're gonna create a let's see if I can do this we're gonna do in an arrow like like this no not like that like this and then we're gonna specify the output shape so we still want n to be the output shape and then we want head to be the first one and then we want query length and then we want key length all right this is um has quickly become sort of my favorite notation instead of using major to multiply or you know in this case what you would have to do is you would have to use batch matrix multiply since we have a batch here and you would even have to do it a little bit more complicated since we have this additional dimension here for their heads you would first have to flatten the the training examples with the heads and then use torch that batch matrix multiply so you would have to use torch dot B mmm but you know if you don't want to remember sort of those syntax for how to do that you can just use n some and it's going to be super easy and what we have to do here as well is that we need to send in the queries and then the keys and that's it we're gonna add a mask now so we're gonna do if mask is not none so if we send in a mask then we're gonna do energy equals energy dot masked underscore fill and then we're gonna do mask equals equals zero so essentially if the element of the mask is zero then that means that we want to shut that shut that off so that it doesn't impact any other I guess yeah so essentially you know as we saw previously the the mask for the for the target is gonna be a triangular matrix but we're gonna define that later on but anyways the element that you know when we're gonna close it is zero and what it means to close it is that we're gonna replace those elements with a float where it's we're gonna set it to essentially minus infinity but just for numerical so it doesn't bring any numerical under overflowed anything like that we want it just to set it to a very very small value and so let's see what's going to happen is you know run this through south max now so when we have those essentially minus infinity then they're going to be you know set to zero so we're going to do a tension now and we're gonna do torch that softmax and we're gonna do energy and then we're also going to divide it with the embed size or the square root of the embed size just for that numerical stability so we're gonna do raise to 1\/2 and then we're gonna do dimension equals 3 and this means that we're normalizing across the key length which for example would be depending on again where we use the attention mechanism it's going to let's say this is the source sentence and this is the target sentence length then that would say you know how much we want to essentially we're making the attention scores normalized to one across the source sentence so that we if the first for example is 0.8 that means we're paying 80% attention to the first word in the source sentence what we can do then is out and we're gonna now want to multiply the attention with the values so again we're gonna use them my favorite notation find some and we're gonna do that by C let's bring out the shapes first so we have attention shape we have n comma heads comma query link comma key length and then we have the values of shape and we're gonna have n comma value length heads and then heads dimension and what we want the after the matrix multiply is we want it to be n comma query length comma heads comma head dimension all right that's so what we want here is the key key length and the value lengths are always going to be the same if you just check how we are sending it in you know in the encoder they are all going to be the same obviously the the it's just going to be the same input we're gonna see that later on as well but the value and the key lengths are always going to be the same value so we're gonna multiply across that dimension so again we can we can do see you can do this right here and we're gonna call it n for the batch size for the attention and then heads and then query lengths and then we're just gonna call it L for the for the dimension that we want to multiply across then we're gonna do again n comma I guess n L so that's gonna be for the value length and since the key length and value of match now so both are L then we're going to do H and then we're going to do D and we're going to map this let's see we're gonna do like this and we're gonna do it to N and then the quarreling heads and then the dimension and magically you know I mean somebody's gonna know what to do with that so all we're going to do then is just do the attention and sending the values and also so we're now gonna want to do the concatenation part so we can do that we can do that instantly after this tour shot eins on so we can do dark reshape and we can do n comma query link and then we can just self that heads x self dot head dimension so we're just concatenated us so we can do and we can write it here after eins um then we can just do then flatten last two dimensions something like that and what we want to do lastly is just send it through FC out so we're just going to do that and we're going to return out and this FC out won't change the dimension since the FC FC out just Maps the embed size to embed size alright so now that we have the attention this is gonna be a lot easier for us so we're just gonna create the you know now we're gonna let's see we're gonna create the transformer block we're just going to class transformer block and we're going to end in module [Music] I'm going to define in it and we're going to do the embed size I'm gonna send in the embed size the heads drop out and we're also going to send in something called forward expansion and I'm going to talk about that when we use it so we're gonna super and we're gonna do transform a block and we're gonna do self and then see thought in it and we're going to do self that attention is equal to self attention of embed size and heads so essentially you know we're using that attention that we implemented above and then we're gonna do the so then the normal normalization so we're gonna do self that norm one is going to be because we're gonna use to normalize it and we're first gonna send it through a normalization a attention block then going to send it to a normalization we're gonna send to a feed-forward and then we're going to send it to another normalization so we can do a layer norm of embed size and layer norm again one of those details so if you're familiar with with batch norm what that is Lane room and bathroom are very similar except that batch norm takes the average across the batch and then normalizes whereas layer norm just takes an average for every single example so I guess layin worm has more computation than Bachelor but essentially it uses the same formula it just does it per example rather than for the batch so then we're going to do another one self dot norm - it's going to be a nun layer norm of again the embed size and then the feed forward part of the transformer block we're going to do self dot feed-forward it's gonna be a nun dot sequential so we're going to use a different a nun linear here and what we can do is we're going to do any linear for from the embed size as input and then we're going to map it to forward expansion times the embed size so essentially you know we're mapping it to some more nodes which is dependent on the forward expansion so in the paper they use forward expansion to four so it'll they have them embed size and then it just times that by 4 which is the intermediate number of nodes they do an NN relu I really like this and then they use another nonlinear and they're gonna do the forward expansion times the embed size and you're just gonna map it back to the embed size so you know the this feed-forward block is not really changing anything it's just doing some extra computation and then mapping it back and then lastly we want to do self dot drop out well self dot drop out is equal to n n dot drop out of drop out and for our forward see if I can make this a little bit more centered all right so now we can scroll past the bottom alright so we're gonna do the forward and we're gonna send in you know the value the key the query and we're also going to send in a mask then we're going to do attention equals self dot attention and we're just going to send them the value key the query and the mask and then you know we're gonna do self dot norm one and one thing to remember is that we're gonna send in a skip connection and that's going to be the attention that we just computed and we're gonna add it with the query so that's the skip connection then we're going to do self dot drop out see like this and we're gonna do x equals that then we're going to do forward equals self dot feed-forward of of X and then we're gonna do out equals self dot let's you suffer drop out of self dot norm two and again we're gonna add the skipped connection so we're going to do forward but we're also going to add the the X from the after the norm norm one then we're just going to return out and that's the transformer block so as I said the attention part right here is really the most difficult part the transformer block is pretty easy and now we're just gonna try to stick this together and form the encoder and the decoder so we're going to do class encoder we're going to do any module and we're gonna do define in it and we're gonna let's see we're gonna do a self we're gonna send in the source vocab size because remember now what we're gonna do is we're gonna do the embedding and all of those things as well and we're also gonna use the transformer block for a couple of layers so we're gonna send in the embed size we're gonna send in the number of layers we're gonna send in heads we're gonna send in the device we're gonna send in forward expansion all right all of these are hyper parameters of our model we're sending the drop out and we're gonna send in something called max length and so first thing we're gonna call a super of encoder self dot in it sighs equals embed size and I'll cover let's see so I guess the only thing that we haven't talked about this is max length and this is related to the positional embedding so essentially we're gonna use a positional embedding and I'm gonna talk more about that when we code that one but essentially the positional embedding is depending on position right and so we need to send in how long is the is the max sentence length so let's say that we have a couple of sentences there are you know let's say that they are extremely long let's say all of our data set is fifty in length and then we have a couple of examples there are a thousand then what would you would have to do is delete those that are a thousand and just keep the ones so that we have sort of a normal size of our of our data and so max line could be various depending on data but perhaps it's like a hundred or something like that depending on the data set so we're just going to do stuff to embed size we're going to set that device equals device self dot word I'm betting it's going to be n n dot M embedding we're gonna send you the source vocab size and we're gonna map it to embed size and then we're going to do the positional embedding and this is just going to be n n dot embedding and it's it's gonna be max length to embed size all right so this is where we use the max length then we're gonna do self dot layers it's gonna be N and that module list we're just going to use that to map several different different modules together and those modules are going to be the transformer block so we're going to do transformer block and we're gonna send in the embed size we're gonna send in heads we're gonna send them drop out equals drop out we're gonna send in forward expansion forward expansion equals forward expansion and that's it for the number of layers let's scroll down again and then we're just gonna do self dot drop out equals and then drop out of drop out all right so where do the four parts we're gonna be forward self we're gonna send in just one input to the to the forward and we can also going to send in a mask then we're just gonna do n comma sequence length is X out shape so that's that's all we have from the beginning right we have an example sent in and we have some some sequence length and just going to do first of all we're just going to do positions that we're gonna do torture arranged 0 to sequence length and then we're gonna do dot expand and comma sequence length so that we have sequence guess arranged so 0 1 2 3 etc up to sequence length for every example and then we're just gonna do dot to self dot device and then we're gonna send through X through an embedding and we're just gonna and we're also gonna add a positional embedding so self that word embedding of X plus self top position embedding of positions and this together is then going to be sent in through drop and drop out so we're just gonna do something like that and we're gonna do out equal static and so you know if you really think about it the only thing that makes it aware of the positions is this thing right here we're all we're gonna send in is the positions which is just 0 1 2 3 etc and magically this learns how to how to you know how words are structured in in you know the permutation of the words I find that a little bit magical that that actually word works but anyways so we're gonna do for a layer in self-taught layers we're gonna do out equals layer of this is gonna be a little bit word but since we are in the encoder the let's see its value key and the what is the value key inquiry are all gonna be the same so we're just gonna send in out out out I was sending the mask all right so that's looks a bit odd but that's just the special case in the in this scenario in the encoder all of the inputs are going to be the same so and then we're gonna do return out all right so now we have the encoder done and all we want to do now is we want to create the decoder and to make this I guess hopefully clear we're gonna do the decoder block first so we're going to do any module then do in it we are sending the embed size heads forward expansion dropout and device the super of decoder block self dot in it like this and so or that the first thing we're going to do is we're going to send it through an attention layer then we're going to do the normalization and then we're just going to use the transformer block so we can we can do self dot norm is or let's do a tension first so step that attention is self attention of embed size comma heads and then the norm is just going to be an mayor norm of embed size and then transformer block will just be transform a block of let's see embed sized heads dropout and then forward expansion and also we're just gonna do our drop out so and then that drop out I'll drop out now ready to do the forward part so we're going to do forward and we're gonna do X we're gonna send in value and and key and then this marks gonna send in a source mask and we're gonna send in a target mask all right so the target mask is the one we've talked about and this one is uh I guess this one is is essential you need to have this one the source mask is kind of option where you know if we if we send in a couple of examples then we need to Pat it to make sure that all of our equal lengths and then we can also send in a source mask so that we don't compute we don't do Ness it unnecessary computations for the ones that are padded so that one is optional but we're gonna do it and then also remember in the decoder block we take X which is we take an input from up from our target and then we also take the value in the key and we're gonna get those from the encoder that we've already run before this so what we're gonna do is we're going to do attention is just going to be self dot attention of of X and xxx because all of those are the same and then we're gonna send in the target mask because remember that's the mask multi-headed attention the first one in the decoder block and then the query is just going to be self to drop out of self-taught norm of attention plus X which is with the I guess with the skip connection and then out is just going to be self dot transformer block of the value the key and the query and then also the source mask which is optional but we're gonna do it and then return out so and now we can do class of the decoder so so we're going to do class decoder and in module we're gonna do define in it we're gonna do target vocab size and bed size on on layers heads a forward expansion dropout device and also max length um yeah so we're gonna do it like that this looks a little bit weird all right so now we can start by writing in super of decoder and self and we're gonna do underscore in it like this and then self-doubt device equals device setup that word embedding and is going to be and then dot embedding of target vocab size and then the embed size and then we're going to do self dot position embedding it's going to be a nun dot embedding of again max length and then the embed size and then self that lay a set of layers can be an endowed module list we're going to do a decoder block of embed size heads forward expansion drop out and in the device and we're gonna do that for you know in range of numb layers so that's going to be the layers and we're going to do set up FC out it's going to be a nonlinear of sighs and then target vocab size and let's see kind of lost my track here so we've done the layers and then yeah so we're just going to do a last linear layer so now we're done with the sort of after the decoder block that's just going to be another linear layer and we're just going to do itself that dropout is and then drop out of drop out then again we're going to do forward self and we're gonna do X here for input to the decoder and then we're gonna do the encoder out we're gonna send an encoder out and then we're gonna do source mask and then target mask and let's see her so we're gonna do n comma sequence length equals x shape we're gonna create the positions which are just going to beat or shut arrange zero sequence sequence Ling and then we're just going to expand n comma sequence length so that we have it over all of the training examples we're gonna get it to CUDA if it's a bit available and then we're just gonna do X is self to drop out of self that word let's see self that word embedding of X and then plus self doubt position embedding of positions and then you know we're just gonna do for layer in set up layers we're gonna do X is layer of X and then so that's going to be the let's see that it's going to be the value to the decode so the input to the decoder block and then we're just gonna send in the encoder out encoder out for the let's see it for the value and the key alright so I guess we can do this so here we have X we have value in the key and those are gonna be you know the same thing which is output from the encoder so let's see here we're gonna do also we're gonna send in let's see the source mask and then the target mask and then out is just going be self-taught fully connected out of X so we're gonna get we're gonna get a prediction of which word is is next which is going to be dependent on our vocabulary size for target all right I think this is gonna be a long video but you know now we've done the encoder with on the decoder and we're ready to put this together and do class transformer of end off module and we're gonna send do some init here and we're gonna do our self we're gonna do source of vocab sighs we're gonna do target vocab sighs we're gonna do source pad index we're going to target pad index because those are necessary to compute the mask that we're going to use and then the embed size we're going to set to 512 we can use 2:56 as we did and then number of layers we can set to 6 the forward expansion to default for the heads because the default eight drop out we can set default zero device we can set default CUDA and then max length is gonna be let's say a hundred then we're just going to call super of transformer self dot in it and then we need to just first define the encoder so we're gonna do self that encoder is gonna be in the encoder and we're just going to send in the source vocabularies source vocabulary size the embed size the number of layers heads odd device forward expansion dropout and then max length all right that's just how we defined the input ordering of the encoder then we can do the same for the decoder it's just going to be decoder and we're gonna send in a target book target vocabulary size the embed size number of layers heads the forward expansion the drop out the device and then the max length like that and then we can also just that we can also you know do the source had index is source had index self that target pad index is target pad index and self dot device is just device and so what we got to do first is we got to make a function to make them source masks and then the target mask so we can do make source masks and we're just gonna send in the source and we're gonna do a source mask it's gonna be source and then we're gonna do on C not equal to self-taught source pad index and then we're just gonna do one squeeze one and then unscrews to so the source mass is going to be n the shop shapes add up so we're going to N 1 1 and then the source length that's all the shapes are going to be and then we're just gonna return source mask dot to self dot device so again if it is a source pad index then it's gonna be set set to zero and if it's not it's gonna be set to one and that's how we define the source mask you can also use I guess the opposite but that's just how we define it here and then we're going to define make target mask and really are we gonna do here is we're gonna get the target ling by targeted shape and then we're gonna do target mask it's gonna be we're gonna do a a triangular matrix so we're gonna do torch dot triangular I'll lower and we and we're going to torch that once so the element values on this lower triangular matrix is going to be just values of 1 and then it's gonna be targeting and target length and we're just gonna do another thing which is expanded so that we have one for each training example and then we can just do return target mask da to self dot device we also going to define forward and it's gonna be source and target so now we send in the source and target and basically you know we're gonna do source mask is self that make source mask of source and then target mask is gonna be self that make target mask of target so I guess that makes sense and then we're just gonna do encode source it's gonna be self that encoder of source and source mask and the decoder or let's say out it's just gonna be self the decoder of target and then encode source and then the source mask and the target mask and then we're gonna do return out and yeah so now we created the entire transformer so what we can do is I have an example right here and I'm gonna copy that in so we don't have to write that but just say a a I guess a small example that just a toy example to see that it runs so we have device CUDA and I have some example right here and so the one is for the star token zero is for padding and two would be some end of sentence so this is just random and this is no data but just as an example and then we have two examples right here and then we have some target right here not necessarily the same shape as the input and then we just define the source path index we define what is the vocabulary size so here I use you know I doesn't I don't go above 10 so 9 is the maximum right here so the source vocabulary size is 10 then we just do model and we do transformer of sort of a capillary and then pad index and then we just do model and we send in X and we also send in the target except the target will be shifted by by one so that it it doesn't have the end of sentence token because we wanted to learn to predict the end of sentence but I guess that's more how you would use the transformer and this is sort of more focus on how to actually implement it from scratch but this would be used later on you know to send to cross entropy and etc yeah but not to take too much focus on that so let's try and run this there's probably gonna be a few errors we've written a lot of code and yeah so invalid syntax we can do line 119 yes we're gonna have to do what we're gonna do like this and we run it again no more okay yeah I remember so I need to do see so and the import torture error was just because I didn't I had to activate my corner environment but the first but then we get another error when I rerun it so that error is because in this line right here in line 25 for the query reshape we use the key length which we should use the the query linked right here and let's see if this runs now all right so it took a bit a little bit of time but I found the air now so here this one wasn't actually causing there but this is an error so we're gonna do the transformer block for a couple of layers we're gonna do a range of num layers like that and then what caused the error is that in the decoder somehow I returned I forgot to return so turn out also there was one very important thing that I missed in the self attention which is that after we do the reshape we need to send them through the linear layers so we're gonna do values equals self-taught values of values and then keys equals self dot keys of keys and queries equal self dot queries of queries and if we now run this we we get the the the correct shape and I've tried I've also tried to train this on some translation tasks and it seems to work similarly as a Python implementation but of course there might be some error in it that no I might have missed but so you know there there you have it the transformer from scratch this was definitely a more challenging video and definitely for me at least so you know hopefully this was helpful for a couple of you and yeah thank you so much for watching the video [Music]"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Implementing Transformers with TensorFlow\/PyTorch",
        "Video Title":"Coding a Transformer from scratch on PyTorch, with full explanation, training and inference.",
        "URL":"https:\/\/www.youtube.com\/watch?v=ISNdQcPhsts",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ISNdQcPhsts\/hqdefault.jpg",
        "ID":"ISNdQcPhsts",
        "Publish Time":"2023-05-25T23:01:05Z",
        "Channel":"Umar Jamil",
        "Channel ID":"UCtAcpQcYerN8xxZJYTfWBMw",
        "Transcript":"hello guys welcome to another episode about the Transformer in this episode we will be building the Transformer from scratch using pytorque so coding it from zero we will building the model and we will also build the code for training it for inferencing and for visualizing the attention scores stick with me because it's going to be a long video but I assure you that by the end of the video you will have a deep knowledge of the Transformer model not only from a conceptual point of view but also from a practical point of view we will be building a translation model which means that we our model will be able to translate from one language to another I choose a data set that is called Opus books and it's a synthesis taken from famous books I chose the English to Italian because I am Italian so I can understand and I can tell that if the translation is good or not but I will show you which point you can change the language so you can test the same model with the language of your choice let's get started let's open the IDE of our choice in my case I really love Visual Studio code and let's create our first file which is the model of the Transformer okay let's go have a look at the Transformer model first so we know which one part we are going to build first and then we will build each part one by one the first part that we will be building is the input embeddings as you can see the input embeddings take the input and convert into an embedding what is the input embedding as you remember from my previous video the input embeddings allows to convert the original sentence into a vector of 512 dimensions for example in this sentence your cat is a lovely cat first we convert the sentence into a list of input IDs that is numbers that correspond to the position of each word inside the vocabulary and then each of this number corresponds to an embedding which is a vector of size 512 so let's build this layer first let's the first thing we need to do is to import torch and then we need to create our class this is the Constructor we will need to tell him what is the dimension of the model so the dimension of the vector in the paper this is called D model and we also need to tell him what is the vocabulary size so how many words there are in the vocabulary we save these two values and now we can create the actual embedding okay actually Pi torch already provides with a layer that does exactly what we want to do that is taken given a number it will provide you with the same Vector every time and this is exactly what a embedding does it's just a mapping between numbers and a vector of size 512 512 here in this uppercase is the D model so this is done by the embedding layer and N dot embedding and vocab size and D model let me check why am I auto complete is not working okay so now let's implement the forward method what we do in the embedding is that we just use the embedding layer provided by pi torch to do this mapping so return self-taught learning of act now actually there is a little detail that is written on the paper that is let's have a look at the paper actually let's go here and if we check the embedding and soft Max we will see that in this sentence in the embedding layer we multiply the weights of the embedding by square root of D model so what what the outers do they take the the embedding given by this embedding layer which I remind you is just a dictionary kind of a layer that just Maps numbers to the same Vector every time and this Vector is learned by the model so we just multiply this by Matt dot sqrt of the model you also need to import matte okay now the embedding the input embeddings are ready let's go to the next module the next module we are going to build is the positional encoding let's have also a look at what are the positional encoding very fast so we saw before that our original sentence gets mapped to a list of vectors by the embedding the the embeddings layer and this is our embeddings now we want to do we want to convey to the model the information about the position of each word inside the sentence and this is done by adding another Vector of the same size as the embedding so of size 512 that includes some special values given by a formula that I will show later that tells the model that this particular word occupies this position in the sentence so we will create these vectors called the position embedding and we will add them to the embedding okay let's go to it okay let's define the class position positional encoding and we Define The Constructor okay what we need to give to the Constructor is for sure the D model because this is the size of the vector that the positional encoding should be and the sequence length this is the maximum length of the sentence and because we need to create one vector for each position and we also need to give the dropout dropout is to make the model less overfit okay let's actually build a positional encoding okay first of all the positional encoding is a we will build a matrix of shape sequence length to D model y sequence like 2D model because we need vectors of D model size so 512 but we need sequence length number of them because the maximum length of the sentence is sequence length so let's do it okay before we calculate we create the Matrix and we know how to create the Matrix let's have a look at the formula used to create the positional encoding so let's go have a look at the formula used to create the positional encoding this is my previous uh the slide from my previous video and let's have a look at how to build the vectors so as you remember we have a sentence let's say in this case we have three words we use these two formulas taken from the paper we create a vector of size 512 and one for each possible position so up to sequence length and in the even positions we apply the first Formula in the odd positions of the vector we apply the second formula in this case I will actually simplify the calculation because I saw online it has been simplified also so we will do a slightly slightly modified calculation using log space this is for numerical stability so when you apply the exponential and then the log of something inside the exponential the the result is the same number but it's more numerically stable so first we create a vector called the position that will represent the position of the word inside the sentence and this word this Vector can go from 0 to sequence length minus 1. so actually we are creating a tensor of shape sequence land to one this is wrong okay okay now we create the denominator of the formula and these are the two terms we see inside the formula let's go back to the slide so the first tensor that we built that's called position it's this pause here and the second answer that we build is the denominator here but we calculated it in log space for numerical stability the value actually will be slightly different but the result will be the same the model will learn this positional encoding don't worry if you don't fully understand this part it's just very special let's say functions that convey this impositional information to the model and if you watch my previous video you will also understand why now we apply this to a denominator and denominator to the sign and the cosine as you remember the sign is only used for the even positions and the cosine only for the odd position so we will apply it twice let's do it so apply foreign Position will have the sign but only so every word will have the sign but only the even Dimensions so starting from zero up to the end and going forward by two means every from zero then the number two then the number four etc etc position multiplied by div term then we do the same for the cosine in this case we start from one and go forward by two it means one three five Etc and then we need to add the batch Dimension to this tensor so that we can apply it to the whole sentences so to all the batch of sentence because now the shape is sequence length to the model but we will have a batch of sentences so what we do is we add a new dimension to this PE and this is done using unsqueeze and in the first position so it will become a tensor of shape one to sequence length to the model and finally we can register this tensor in the buffer of this module so what is the buffer of the module let's first do it register buffer so basically when you have a tensor that you want to keep inside the module not as a parameter learned parameter but but you want it to be saved when you save the file of the model you should register it as a buffer this way the tensor will be saved in the file along with the state of the model then we do the forward method so as you remember from before we need to add this positional encoding to every word inside the sentence so let's do it so we just do X is equal to X plus the positional encoding for this particular sentence and we also tell the model that we don't want to learn this positional encoding because they are fixed they will always be the same they are not learned along the training process so we just do it requires grad false this will make the this particular sensor not learned and then we apply the dropout and that's it this is the positional encoding let's have a look at the next module okay we first we will build the encoder part of the Transformer which is this left side here and we still have the multi-head attention to build the ADD and norm and the feed forward and actually there is another layer which connects this skip connection to all these sub layers so let's start with the easiest one let's start with laser normalization which is this add and Norm as you remember from my previous video let's have a look at the layer normalization a little briefing so later normalization basically means that if you have a batch of n items in this case only three each item will have some features let's say that these are actually sentences and each sentence is made up of many words with its numbers so this is our three items and layer normalization means that we for each item in this batch we calculate a mean and the variance independently from the other items of the batch and then we calculate the new values for each of them using their own mean and their own variants in the layer normalization usually we also introduce some parameters called the gamma and the beta some some people call it Alpha and beta some people call it Alpha and bias okay it doesn't matter one is multiplicative so it's multiplied by each of these X and one is additive so it's added to each one of these X Y because we want the model to have the possibility to amplify these values when he needs this value to be Amplified so the the model will learn to multiply this gamma by these values in such a way to amplify the values that it wants to be Amplified okay let's go to build the code for this layer let's define the layer normalization class and Constructor as usual in this case we don't need any parameter except for one that I will show you now which is Epsilon and usually EPS stands for Epsilon which is a very small number that you need to give to the model and I will also show you why you we need this number in this case we use a 10 to the power of -6 let's save it okay this Epsilon is needed because if we look at the slide we have this Epsilon here in the denominator of this formula here so x with cap is equal to x j minus mu divided by the square root of Sigma square plus Epsilon why we need this Epsilon because imagine this denominator if Sigma happens to be 0 or very close to zero this x Nu will become very big which is undesirable as we know that the CPU or the GPU can only represent numbers up to a certain position and the scale so we don't want very big numbers or very small numbers so usually for numerical stability we use this Epsilon also to avoid division by zero let's go forward so now let's introduce the two parameters that we will use for the layer normalization one is called Alpha which will be multiplied and one is bias which will be added usually the the additive is called the bias it's always added and the alpha is the one that is multiplied in this case we will use n n dot parameter this makes the parameter learnable and we Define also the bias this I want to remind you is multiplied and this is added let's define the forward okay as you remember we need to calculate the mean and the standard deviation or the variance for both of these we will calculate the standard deviation of the last Dimension so everything after the batch and we keep the dimension so this parameter keep Dimension means that usually the mean cancels the the dimension to which it is applied but we want to keep it and then we just apply the formula that we saw on the slide so Alpha multiplied by what x minus its mean divided by the standard deviation plus self PPS everything added to bias and this is our layer normalization okay let's go have a look at the next layer we are going to build the next layer we are going to build is the feed forward you can see here and the feed forward is basically a fully connected layer um that's the model uses both in the encoder and in the decoder let's first have a look at the paper to see what are the details of this feed forward layer in the paper the feed forward layer is basically two matrices one W one one W2 that are multiplied by this x one after another with a relu in between and with the bias we can do this in pytorch using a linear layer in which we Define the first one to be the Matrix with the W1 and B1 and the second one to be the W2 and the B2 and in between we apply in the paper we can also see the dimensions of these matrices so the first one is basically D model to dff and the second one is from dff to D model so dff is 2048 and D model is 512. let's go build it class feed forward block we also built in this case the Constructor and in the Constructor we need to Define these two values that we saw on the paper so D model dff and also in this case dropout we Define the first Matrix so W1 and B1 to be the linear one and it's from D model to dff and then we apply the dropout actually we Define the dropout and then we Define the second Matrix W2 and B2 so let me write the comments here it's W1 and B1 of dff to D model and this is W2 and B2 why we have B2 because actually as you can see here bias is by default it's true so it's already defining a bias Matrix for us okay let's define the forward method in this case what we are going to do is we have an input sentence which is batch it's a tensor with Dimension batch sequence length and D model first we will convert it using linear one into another tensor of patch to sequence land to dff because if we apply this linear it will convert the D model into dff and then we apply the linear to which will convert it back to the model you apply the Dropout in between and this is our feed forward block let's go have a look at the next block our next block is the most important and most interesting one and it's the multi-head attention uh we saw briefly uh in the not briefly actually in detail in the last video how the multi-head attention works so I will open now the slide again to show uh to rehearse how it actually works and then we will do it practically by coding as you remember in the encoder we have the multi-head attention that takes the input of the encoder and uses it three times one times it's called query one time it's called key and one time it's called values you can also think it like a duplication of the input three times or you can just say that it's the same input applied three times and the multi-headed tension basically works like this we have our input sequence which is sequence length by D model we transform into into three matrices q k and V which are exactly the same as the input in this case because we are talking about the encoder we will see that in the decoder it's a slightly different and then we multiply this by a matrices called wqw K and WV and this results in a new Matrix of Dimension sequence by D model we then split these matrices into H matrices molar matrices y h because it's the number of head we want for this multi-head attention and we split these matrices along the embedding Dimension not along the sequence Dimension which means that each head we will will have access to the full sentence but a different part of the embedding of each word we apply the attention to each of these smaller matrices using this formula which will give us smaller matrices as a result then we combine them back so we can cut them back just like the paper says so concatenation of head 1 up to head Edge and finally we multiply it by w o to get the multi-head attention output which again is a Matrix Matrix that has the same Dimension as the input Matrix as you can see it's the output of the multihead attention is also sequenced by D model in this slide actually I didn't show the batch Dimension because we are talking about one sentence but when we code the Transformer we don't work only with one sentence but with multiple sentences so we need to think that we have another dimension here which is the batch okay let's go to code this multi-head attention I will do it a little more slower so we can see in detail everything how it's done but I really wanted to you to have an overview again of how it works and why we are doing what we are doing so let's go code it class also in this case we Define the what's the Constructor and what we need to give to this multi-head attention as parameter for sure the D model of the model which is in our case 512 the number of heads which we call H just like in the paper so actually indicates the number of heads we want and then the Dropout value we save these values as you can see we need to divide this embedding Vector into H heads which means that this D model should be divisible by H otherwise we cannot divide equally the the same domain the same Vector representing the embedding into equal matrices for each head so we make sure that the model is divisible by H basically and this will make the check if we watch again my slide we can see that the the value D model divided by H is called Decay as we can see here if we divide the D model by D by H heads we get a new value which is called Decay and to be aligned with what the paper with the nomenclature used in the paper we will also call it DK so DK is D model divided by h okay let's also Define the matrices by which we will multiply the query the key and the values and also the output Matrix w o so this again is a linear so from D model to D model y from D model to the model because as you can see from my slides this is D model by D model so that the output will be sequenced by the model so this is WQ this is w k and this is WB finally we also have the output Matrix which is called wo here this wo is H by DV by D model so H by DV DV is what DV is actually equal to Decay because it's the D model divided by H but why it's called DV here and DK here because this head is actually the result or this head comes from this multiplication and the last multiplication is by V and in the paper they call this value DB but on a practical level it's equal to Decay so our wo is also a matrix that is D model by D model because H by DV is equal to the model foreign we create the dropout let's implement the forward method and let's see how the multi-head attention Works in detail during the coding process we Define the query the key and the values and there is this mask so what is this mask the mask is basically if we want some words to not interact with other words we mask them and we saw in my previous video but now let's go back to those slides to see what is the mask doing as you remember when we calculated the attention are using this formula So Soft Max of Q multiplied by KT divided by square root of DK and then by V we get this head Matrix but before we multiply by V so only this multiplication here with the Q by K we get this Matrix which is each word with each other word it's a sequence by sequence Matrix and if we don't want some words to interact with other words we basically replace their value so their attention score with something that is very small before we apply the soft Max and when we apply the softmax this values B will become zero because as you remember the soft Max on the numerator has e to the power of X so if x goes to minus infinity so very small number e to the power of minus infinity will become very small so very close to zero so basically we hide the uh the attention for those two words so this is the the the the the job of the Mask just following my slide we do the multiplication one by one so as we remember we calculate first the query are multiplied by the WQ so self.w Q multiplied with the query gives us a new Matrix which is called the Q Prime in my slides I just call it query here we do the same with the keys and the same with the values let me also write the dimensions so we are going from batch sequence length to D model with this multiplication we are going to another Matrix which is batch sequence length and D model and you can see that from the slides so when we do sequence by D model multiplied by D model by D model we get a new Matrix which has the same Dimension as the initial Matrix so sequence by D model and it's the same for all three of them now what we want to do is to we want to divide this query key and value into smaller matrices so that we can get give each small Matrix to a different head so let's do it we will divide into using the view method of Pi torch which means that we keep the batch Dimension because we don't want to split the sentence we want to split the embedding into H parts we also want to keep the second dimension which is the sequence because we don't want to split it and the third dimension so the D model we want to split it into two smaller Dimension which is H by d k so self dot h self Dot d k as you remember DK is basically D model by divided by H so this multiplied by this give you back gives you a D model and then we transpose one two why do we transpose because we prefer to have the um the edge Dimension instead of being the third dimension we want it to be the second dimension and this way each viewh head will see the all the sentence so we will see this Dimension so the sequence line by decay let me also write the comment here so we are going from batch sequence length key model to batch sequence length h d k and then by using the transposition we are going to patch Edge sequence length and decay this is really important because we will we want if we want each batch have um we want each head to watch this stuff so the sequence length by DK which means that each head we will see the full sentence so each word in the sentence but only a smaller part of the embedding we do the same thing for the query the key and the value [Music] okay now that we have the smaller matrices so let me go back to the slide so I can show you where we are so we did this multiplication we obtained query key and values we split into smaller matrices now we need to calculate the attention using this formula here before we can calculate the attention let's create a function to calculate the extension so if we create a new function that can be used also later so self attention let's define it as a static method so static method means basically that you can call this function without having an instance of this class you can just say multihead attention block dot attention instead of having an instance of this class we also give him the Dropout layer okay what we do is we get the Decay what is the Decay is the last dimension of the query key and the value and we will using this function here let me first call it so that you can understand how we will use it and then we we Define it so we want from this function we want two things the output and we want the attention scores so the output of the soft Max attention scores and we will call it like this so we give it the query the key the value the mask and the Dropout layer now let's go back here so we have the Decay now what we do is first we apply the first part of the formula that is the query multiplied by the transpose of the key divided by the square root of decay so these are our attention scores query matrix multiplication so this add sign means matrix multiplication in pi torch if we transpose the last two Dimensions minus two minus one means transpose the last two Dimensions so this will become the last Dimension is the sequence by sequence length by Decay it will become Decay by sequence length and then we divide this by math Dot DK we before as we saw before before applying the soft Max we need to apply the mask so we want to hide some interaction between words we apply the mask and then we apply the softmax so the softmax will take care of the values that we replaced how do we apply the mask we just all the values that we want to mask will replace them with very very small values so that the soft Max will replace them with zero so if a mask is defined apply it this means basically replace all the values for which this statement is true with this value okay the mask we will Define in such a way that um where this value with this expression is true it we want it to be replaced by this later we will see also how we we will build the mask for now just take it for granted that these are all all the values that we don't want to have in the attention so we don't want for example some word to watch future words for example when we will build the decoder or we don't want the padding values to to interact with other values because they are just filler words to reach the sequence length we will replace them with minus 1 to the power of minus 10 to the power of 9. and which is a very big number in the negative range and which basically represents minus infinity and then when we apply now the soft Max it will be replaced by zero we applied to this Dimension okay let me write some comments so in this case we have a batch by H so each head will and then sequence length and sequence length all right if we also have a Dropout so if Dropout is not known we also apply the dropout and finally as we saw in the original slide we multiply this the output of the soft Max by the V Matrix matrix multiplication so we return attention scores multiplied by value and also the attention score itself so why are we returning a tuple because we want this of course we need it because for the model because we need to give it to the next layer but this will be used for visualization so the output of the um the self self attention so the multi had attention in this case is actually going to be here and we will use it for visualizing so for visualizing what is the score given by the model for that particular interaction let me also write some comments here so here we are doing like this batch and let's go back here so now we have our multi-head attention so the output of the multi-head attention what we do is finally we okay let's go back to the slide first Where We Are we calculated this smaller matrices here so we applied the softmax Q by KT divided by the square root of DV and then we multiplied it also by V we can see it here which gives us this small Matrix here head one head two head three and thread four now we need to combine them together concat just like the formula says from the paper and finally multiply it by w o so let's do it uh we transpose because before we transform the Matrix into sequence length by we had the sequence length as the third dimension we wanted back in the first place to combine them because the resulting tensor we want the sequence length to be in the second position so let me write it first what we want to do batch we started from this one so you can select first we do a transposition and then what we want is this so this transposition takes us here and then we we do a view but we cannot do it we need to use contiguous this means basically that Pi torch to to transform the shape of a tensor needs to to put the memory to be contiguous so he can just do it in place foreign and the self dot h multiplied by self.dk which as you remember this is the um the model because we defined DK to be here the model by H divided by h okay and finally we multiply this x by w o which is our output Matrix of x this will give us we go from batch to and this is and this is our multi-head attention block uh we have I think all the ingredients now to combine them all together we just miss one small layer let's go have a look at it first there is one last layer we need to build which is the connection we can see here for example here we have some output of this layer so add a norm that is taken here with this connection and this one part is sent here then the output of this is sent to the addend norm and then combined together by this layer so we need to create this the layer that manages this skip connection so we take the input we give it to to we skip it by one layer we take the output of the previous layer so in this case the multi-head attention we give it to this layer but also combining with this part so let's build this layer we I will call it a residual connection because it's basically a skip connection okay let's build this residual connection as usual we Define the Constructor and in this case we just need a dropout uh as you remember the the skip connection is between the ADD and the norm and the previous layer so we also need the norm which is our layer normalization which we defined before and then we Define the forward method so and the sub layer which is the previous layer what we do is we take the X and we combine it with the output of the next layer which is in this case is called sub layer and we apply the dropout so this is the definition of ADD and the norm actually there is a slight difference that we first apply the normalization and then we apply the sub layer in the case of the paper they apply first the sub layer and then the normalization I saw many implementation and most of them actually did it like this so we will also stick with this particular as you remember we have these blocks are combined together by this bigger block here and we have n of them so this big block we will call it encoder block and each of this encoder block is repeated n times where the output of the previous is sent to the next one and the output of the last one is sent to the decoder so we need to create this block which will contain one multi-het attention to add and the norm and one feed forward so let's do it we will call this block the encoder block because the decoder has three blocks inside the encoder has only two [Music] and as I so before we have the self-attention block inside which is the multi-head attention we call it self-attention because in the case of the encoder it is applied to the same input with three different roles the role of query of the key and the value which is our fifth forward and then we have a dropout which is a floating point and then we Define and then we Define the two residual connections we use the model list which is a way to organize a list of modules in this case we need two of them okay let's define the forward method I Define The Source mask what is the source mask is the mask that we want to apply to the input of the encoder and why do we need a mask for the input of the encoder because they we want to hide the interaction of the padding word with other words we don't want the padding word to interact with other words so we apply the mask and let's do the first residual connection let's go back to check the video actually to check the slide so we can understand what we are doing now so so the first script connection is this x here is going to here but before it's added and with add a norm we first need to apply the multi-headed tension so we take this x we send it to the multihead attention and at the same time we also send it here and then we combine the two so the first clip connection is between X and then the other X is coming from the self attention so this is a function so I will Define the sub layer user using a Lambda so this basically means first apply the self attention self-attention in which we give the query key and the value is over X so our input so this is why it's called self-attention because the role of the query key and the value is X itself so the input itself so it's the sentence that is watching itself so each word of one sentence is interacting with other words of the same sentence we will see that in the decoder it's different because we have the cross attention so the keys coming from the decoder are watching the sorry the query coming from the decoder are watching the key and the values coming from the encoder we give it the source mask so what is this basically we are calling this function the forward function of the multi-head attention block so we give query key value and the mask this will be combined with this by using the residual connection then again we do the second one the second one is the feed forward I will need Lambda here actually and then we return X so this means combine the the feed forward and then the X itself so the output of the previous layer which is this one and then apply the residual connection and this defines our encoder block now we can Define the encoder object so because the encoder is made up of many encoder blocks we can have up to n of them according to the paper so let's define the encoder [Music] how many layers we will have we will have n so we'll have many layers and they are applied in one after another so this is a model list and at the end we will apply a layer normalization [Music] so we apply one layer after another the output of the previous layer becomes the input for the next layer I here I forgot something and and finally we apply the normalization and this concludes our journey around the encoder let's go have a brief overview of what we have done we have taken the inputs send it to the we didn't okay we didn't combine all the blocks together for now we just built this big block here com control called encoder which contains two smaller blocks that are this clip connection the script connection first one is between the multi-header tension and this x that is sent here the second one is between this feed forward and this x that is sent here we have n of these blocks one after another the output of the last will be sent to the decoder before but before we apply the normalization now we will we build the the decoder part now in the decoder the output embeddings are the same as the input embeddings I mean the the class that we need to Define is the same so we will just initialize it twice and the same goes for the positional encodings we can use the same values that we use for the encoder also for the decoder what we need to Define is this big block here which is made of Muscat multi-head attention add a norm so one skip connection here another mostly had attention with another skip connection and the feed forward with the skip connection here the way we defined the multi-head attention class actually already takes into consideration The Mask so we don't need to reinvent the wheel also for the decoder we can just Define the decoder block which is this big block here made of three sub layers and then we build the decoder using this n n number of these decoder blocks so let's do it let's define first the decoder block in the decoder we have the self-attention which is let's go back this is a self-attention because we have this input that is used three times in the musket multi-hypertension so this is called self-attention because the same input plays the role of the query the key and the values which means that the same sentence is each word in the sentence is matched with each other word in the same sentence but in this part here we will have an attention calculated using the query coming from the decoder while the key and the values will come from the encoder so this is not a self-attention this is called cross attention because we are crossing two kind of different objects together and matching them somehow to calculate the relationship between them okay let's define this is the cross attention block which is basically the multi-hypertension but we will give it the different parameters is our feet forward and then we have a dropout okay we Define also the residual Connection in this case we have three of them wonderful okay let's build the forward method which is very similar to the encoder with a slight difference that I will highlight we need x what is X is the input of the decoder but we also need the output of the encoder we need the source mask which is the mask applied to the encoder and the target mask which is the mask applied to the decoder why they are called Source mask and Target mask because in this particular case we are dealing with a translation task so we have a source language in this case it's English and we have a target language which in our case is Italian so you can call it encoder mask or decoder mask but basically we have to mask one is the one coming from the encoder one is the one coming from the decoder so in our case we will call it source so the source mask is the one coming from the encoder so the source language and the target mask is the one coming from the decoder so the target language and just like before we calculate the self attention first which is the first part of the decoder block in which the query the key and the values are the same input but with the mask of the decoder because this is the self attention block of the decoder and then we need to combine the we need to calculate the cross attention which is our second residual connection we give him okay in this case we are giving the query coming from the decoder so the X the key and the values coming from the encoder and the mask of the encoder and finally the feed forward block just like before and that's it we have all the ingredients actually to build the decoder now which is just n times this block one after another just like we did for the encoder [Music] also in this case we will provide very many layers so layers it's just a model list and we will also have a normalization at the end just like before we apply the the input to the to one layer and then we use the use the output of the previous layer and give it as an input of the next layer so uh delay each layer is the decoder block so we need to give it X we need to give it the encoder output then the source mask and the target mask so each of them is this we are calling the forward method here so nothing different and finally we apply the normalization and this is our decoder there is one last ingredient we need to to have what is a full Transformer so let's have a look at it the last ingredient we need is this layer here the linear layer so as you remember from my slides the output of the multi-head attention is something that is sequenced by D model so here we expect to have the outputs to be sequenced by D model if we don't consider the batch dimension however we want to map these words into the work back into the vocabulary so that's why we need this linear layer which will convert the embedding into a position of the vocabulary I will call this a layer called the projection layer because it's projecting the embedding into the vocabulary let's go build it [Music] what we need for this layer is the D model so the B model which is an integer and the vocabulary size but this is basically a linear layer that is converting from the model to vocabulary size so so start projection layer is let's define the forward method okay what do we want to do let me write this little comment we want to batch sequence length remodel converted into patch sequence land vocabulary size and in this case we will also already apply the soft Max and actually we will apply the log soft Max for numerical stability um like that like I showed before to the last dimension and that's it this is our projection layer now we have all the ingredients we need for the Transformer so let's define our Transformer block in a Transformer we have an encoder which is our encoder we have a decoder which is our decoder we have a source embedding why we need the source embedding and the target embedding because we are dealing with multiple languages so we have one input embedding for the source language and one input embedding for the target language and we have the target embedding then we have the source position and the target position which will be the same actually and then we have the projection layer we just save this now we Define three methods one two encode one to the code and one to project we will apply them in succession uh why why we don't just build one forward method because as we will see during inferencing we can reuse the output of the encoder we don't need to calculate it every time and also we prefer to keep the uh that this outputs separate also for visualizing the attention so for the encoder we have the source of the because we have the source a language and the source mask so the so what we do is we apply first the embedding then we apply the positional encoding and finally we apply the encoder then we Define the decode method which takes the encoder output which is a tensor Source mask which is a tensor the target and the target mask and what we do is Target we first apply the target embedding to the Target sentence then we apply the positional encoding to the Target sensor sentence and finally with the code this is basically the method the forward method of this decoder so we have the same order of parameters yes finally we Define the project method in which we just apply the projection so we take from the embedding to the vocabulary size okay this is also the this is the last block we had to build but we didn't make a method to combine all these blocks together so we built many blocks we need one that given the hyper parameters of the Transformer builds for us one single Transformer in initializing all the encoder decoder the embeddings ETC so let's build this function let's call it the build Transformer that given all the hyper parameters will build the transformer for us and also initialize the parameters with some initial values what we need to define a transformer for sure in this case we are talking about translation okay this model that we're building we will be using for translation but you can use it for any task so the naming I'm using are basically the ones used in the translation task later you can change the naming but the structure is the same so you can use it for any other task for which the the Transformer is applicable so the first thing we need is the vocabulary size of the source and the target because we need to build the embedding the embedding because the embedding need to convert the from the token of the vocabulary into a vector of size 512 so it needs to know how much public is the vocabulary so how many vectors it needs to create then the target which is also an integer then we need to tell him what is the source sequence length and the target sequence length this is very important they could also be the same in our case it will be the same but they can also be different for example in case you are using the Transformer that is dealing with the two very different languages for example for translation in which the tokens needed for the source languages as language are much higher or much lower than the other one so you don't need to keep the same length you can use different lengths the next hyper parameter is the D model which we initialize with 512 because we want to keep the same values as the paper then we Define the hyper parameter n which is the number of layers so the number of encoder blocks and the number of decoder blocks that we will be using is according to the paper is six then we Define the hyper parameter H which is the number of heads we want and according to the paper it is eight the Dropout is 0.1 and finally we have the hidden layer dff of the feed forward layer which is 2048 as we saw before on the paper and this first we do is we create the embedding layers so Source embedding then the target embedding then we create the positional encoding layers we don't need to create two positional encoding layers because actually they do the same job they and we they also don't add any parameter but because they have the Dropout and also because I want to make it verbal so you can understand each part without making any optimization I think actually it's it's fine because this is for educational purpose so I don't want to optimize the code I want to make it as much comprehensible as possible so I do every part I need I don't take shortcuts and then we created the encoder blocks we have n of them so let's define create an empty array so we have n of them so each encoder block has a self-attention so I encoded potential which is a multi-head attention block the multi-head attention requires the D model the h and the Dropout value then we have a feed forward block as you can see also the name that I'm using are quite long mostly because I want to make it as comprehensible as possible for everyone so each encoder block is made of a self-attention and a feed forward and finally we tell him how much is the dropout finally we added this encoder block and then we can create the decoder blocks we also have the cross attention for the decoder block we also have the feed forward just like the encoder then we're defining decoder block itself which is decoder block cross attention and finally the feed forward and the dropout and finally we save it in its array we now can create the encoder and the decoder we give him all his blocks which are n and then also the decoder and we create the projection layer which will convert the model into vocabulary size which vocabulary of course the Target because we want to take from The Source language to the target language so we want to project our output into the target vocabulary and then we build the Transformer foreign er a decoder Source embedding Target embedding then in Source position encoding Target positional encoding and finally the projection layer and that's it now we can just initialize the parameters using the Xavier uniform this is a way to initialize the parameters to make the training faster so they don't don't just start with random values and there are many algorithms to do it I saw many implementations using Xavier so I think it's a quite good start for the model to learn from and finally we return our beloved Transformer and this is it this is how you build the model and now that we have built the model we will go further to use it so we will create the we will first have a look at the data set then we will build the training Loop after the training Loop we will also build the inferencing part and the code for visualizing the attention so hold on and take some coffee take some tea because it's gonna be a little long but it's gonna be worth it now that we have built the the code for the model our next step is to build the training code but before we do that we first I let's recheck the code because we may have some typos I actually already made this check and there are few mistakes in the code I compare the old with the new one it is various miners problems so we wrote feed forward instead of feed forward here and so the same problem is also present every in every reference to feed forward and also here when we are building the decoder block and the other problem is that here when we build the decoder block we just wrote NN dot module instead it should be NN dot module list and then the feed forward should be also fixed here and here in the build Transformer method now I can delete the old one so we don't need it anymore let me check the model it's the correct one with feed forward yes okay our next step is to build the training code but before we build the training code we have to look at the data what kind of data are we going to work with so as I said before we are dealing with the translation task and I have chosen this data set called Opus books which we can find on hugging face and we will also use the library from hugging face to download this data set for us and this is the only Library we will be using beside it to pytorch because we of course we cannot reinvent the head data set by ourselves so we will use this data set and we will also use the hugging fix tokenizer library to transform this text into a vocabulary because our the our goal is to build the Transformer so not already match the wheel about everything so we will be only focusing on building and training the Transformer and in my particular case I will be using the subset English to Italian but we will build the code in such a way that you can choose the language and the code will act accordingly if we look at the data we can see that each data item is a pair of sentences in English and in Italian for example there was no possibility of taking a walk that day which in Italian means so we will train our Transformer to translate from The Source language which is English into the target language which is Italian so let's do it we will do it step by step so first we will make the code to download this data set and to create the tokenizer so what is the tokenizer let's go back to the slides to just have a brief overview of what we are going to do with this data the tokenizer is what comes before the input embeddings so we have an English sentence so for example your cat is a lovely cat but this sentence will come from our data set the goal of the tokenizer is to create this token so split this sentence into single Words which has many strategies as you can see here we have a sentence which is your cat is a lovely cat and the goal of the tokenizer is to split this sentence into single Words which can be done in many ways there is the bpe to organizer there is the word level tokenizer there is the sub Word level word part organizer there are many tokenazers the one we will be using is the simplest one called the word level tokenizer so the word level tokenizer basically will split this sentence let's say by space so each space defines the boundary of a word and so into the single words and each word will be mapped to one number so this is the job of the tokenizer to build the vocabulary and of these numbers and to map each word into a number the one we built the tokenizer we can also create special tokens which we will use for the transformer for example the tokens called padding they call the token called the start of sentence end of sentence which are necessary for training the Transformer but we will do it step by step so let's build first the code for the um building the tokenizer and to download the data set okay let's create a new file let's call it train dot Pi okay let's import our usual Library so torch we will also import towards Dot and then and we also because we we are using a library from hugging phase we also need to import the these two libraries we will using the we will be using the data sets Library which you can install using pip so data sets actually we will be using load data set and we will also using we'll be using the tokenizers library also from hugging phase which you can install with Pip we also need the which tokenizer we need so we are using we will use the word level tokenizer and there is also the trainers so the the the tokenizer the the class that will train the tokenizer so that will create the vocabulary given the list of sentences and we will split the word according to the white space um I will build one method by one by one so I will build first the methods to create the tokenizer and I will describe each parameter for now you will not have the bigger picture but later when we combine all these methods together you will have the bigger picture so let's first make the method that builds the tokenizer so we will call it get or build tokenizer and this method takes the configuration which is the configuration of our model we will Define it later the data set and the language for which we are going to build the tokenizer we Define the tokenizer path so the file where we will be saved this tokenizer and we do it path of config okay let me Define some things first of all this path is coming from the path lead so from pathlib this is the library that allows you to create absolute path giving relative paths and we pretend that we have a configuration called the tokenizer file which is the path to the tokenizer file and this path is formattable using the language so for example we can have something like this for example something like this and this will be um given the language it will create a tokenizer English or synchronize or Italian for example doesn't exist we create it I took all this code actually from the hugging phase there is it's nothing complicated I just taken their quick tour of their tokenizers library and it's really easy to use it so and saves you a lot of time because tokenizer to build a tokenizer is really Reinventing the wheel and we will also introduce the unknown word uh unknown so what does it mean if our tokenizer sees a word that it doesn't recognize in its vocabulary it will replace it with this word unknown it will map it to the number corresponding to this word unknown the print organizer means basically that we split by white space and then we train we build the trainer to train our tokenizer okay this is the trainer what does it mean it means it will be a word level trainer so it will split words using the white space and using the single words and it will also have four special tokens one is unknown which means that if you cannot um find that particular word in the vocabulary just to replace it with unknown it will also have the padding which we will use to train the to train the Transformer the start of sentence and the end of sentence special tokens mean frequency means that a word for a word to appear in our vocabulary it has to have a frequency of at least two now we can train the tokenizer we use this method which means we built first a method that gives all the sentences from our data set and we will build it later okay so let's build also this method called get all sentence so that we can iterate through the data set to get all the sentences corresponding to the part the particular language for which we are creating the tokenizer as you remember each item in the data set it's a pair of sentences one in English one in Italian we just want to extract one particular language this is the item representing the pair and from this pair we extract only the one language that we want and this is the code to build the tokenizer now let's write the code to load the data set and then to build the tokenizer we will call this method to get data set and which also takes the configuration of the model which we will Define later so let's load the data set we will call it the S row okay hugging face allow us to download its data sets very easily we just need to tell him what is the name of the data set and then tell him what is the subset we want we want the subset that is English to Italian but we want to also make it configurable for you guys to change the language very fast so let's build this subset um dynamically we will have two parameters in the configuration one is called language source and one is called language Target later we can also Define what split we want of this data set in our case there is only the training split in the original data set from hugging phase but we will split by ourselves into the validation in the training data so let's build the tokenizer this is the raw data set and we also have the target okay now because we only have the training splits from hugging phase we can split it by by ourselves into a training and the validation we keep 90 of the data for training and 10 for validation the method random split allows it's a method from pytorch that allows to split a data set using the size that we give as input so in this case it means split this data set into this two smaller data set one of this size and one of this size but let's import the the the method from torch let's also import the one that we will need later and random split now we need to create the data set the data set that our model will use to access the 10 Source directly because now we just created the tokenizer and we just loaded the data but we need to create the tensors that our model will use so let's create the data set let's call it bilingual data set and for that we create a new file [Applause] also here we import torch and that's it we will call the data set we will call it bilingual data set okay as usual we Define the Constructor and in this contractor we need to give him the data set downloaded from hugging face the tokenizer of the source language the tokenizer of the target language The Source language the name of the source language the name of the target language and the sequence length that we will use okay we save all these values we can also save the the tokens the particular tokens that we will use to create the pencils for the model so we need the start of sentence end of sentence and the padding token so how do we convert the token start of sentence into a number into the input ID there is a special method of the tokenizer to do that so let's do it so this is the start of sentence token we want to build it into a tensor this stencil will contain only one number which is given by we can use this tokenizer from The Source or the target it doesn't matter because they both contain these particular tokens this is the method to convert into the token into a number so start of sentence and the type of this token of this tensor is we want it long because the vocabulary can be more than 32-bit long this vocabulary size so we usually use the long 64-bit and we do the same for the end of sentence and the padding token we also need to define the length method of this data set which tells the length of the the data set itself so basically just the length of the data set from hugging phase and then we need to define the get item method okay first of all we will extract the original pair from the hugging phase data set then we extract The Source text and the target text and finally we convert each text into a token into tokens and then into input IDs what does it mean we will first the tokenizer will first split the sentence into single words and then we'll map each word into its corresponding number in the vocabulary and it will do it in one path only this is done by the encode method dot IDs this gives us the input IDs so the numbers corresponding to each word in the original sentence and it will be given as an array we did the same for the decoder now as you remember we also need to pad the sentence to reach the sequence length this is really important because we we want our model to always work I mean the model always works with a fixed length sequence length but we don't have enough words in every sentence so we use the padding token so this pad here as the padding token to fill the sentence until it reaches the sequence length so we calculate how many padding toolkeys we need to add for the encoder side and for the decoder side which is basically how many we need to reach the sequence length minus 2 y minus 2 here so we already have this amount of tokens we need to reach this one but we will add also the starts of sentence token and the end of sentence token to this to the encoder side so we also have -2 here and here only -1 if you remember my previous video when we do the training we add only the start of sentence token to the decoder side and then in the label we only add the end of sentence token so in this case we only need to add one token special token to the sentence we also make sure that this sequence length that we have chosen is enough to represent all the sentences in our data set and if we choose a too small one we wanted to raise an exception so if so basically this number of padding tokens should never become negative okay now let's build the um the two tensors for the encoder input and for the decoder input but also for the label so one sentence will be sent to the input of the encoder one cent sentence will be sent to the input of the decoder and one sentence is the one that we expect as the output of the decoder and that output we will call label usually it's called Target or label I call it level we can cut the tensor of the start okay we can cut three tensors first is this start of sentence token then this the tokens of the source text okay then the end of sentence token and then enough padding tokens to reach the sequence length we already calculated how many embedding tokens we need to add to this sentence so let's just do it and this is the encoder input so let me write some comment here this is add SOS and ARS to the source text then we build the decoder input which is also a concatenation of tokens in this case we don't have the start of sentence we just have the we don't have the end of sentence we just have the start of strength sentence and finally we added the embed enough padding tokens to reach the sequence length we already calculated how many we need just use this value now and then we build the label in the label we only added the end of sentence token let me copy it's faster yeah because we need the same number of padding tokens as for the decoder input and let's double just for debugging let's double check that we actually reached this sequence length okay now that we have made this check uh let me also write some comments here here we are only adding EOS not here SOS with a decoder input and here is add EOS to the lab level but we expect is output from the decoder now we can return all these tensors so that our training can use them we return a dictionary comprised of encoder input what is the encoder input it's basically offside's sequence length then we have the decoder input which is also just a sequence length number of tokens I forgot a comma here and then we have the encoder mask so what is the encoder mask as you remember our we are increasing the size of the encoder input sentence by adding padding tokens but we don't want these padding tokens to participate in the self-attention so what we need is to build a mask that says that we don't want these tokens to be seen by the self-attention mechanism and so we built the mask for the encoder how do we build this mask we just say that all the tokens that are not padding are okay all the tokens that are padding are not okay we also on squeeze to add this sequence Dimension and also to add the batch Dimension later and we convert into integers so this is one one sequence length because this will be used in the self-attention mechanism however for the decoder we need a special mask that is a causal mask which means that each word can only look at the previous words and each word can only look at not known padding words so we don't want again that we don't want the padding tokens to participate in the self-attention we only want real words to participate in this and we also don't want each word to watch at words that come after it but only that words come come before it so I will use a method here called causal mask that will build it later we will build it also so now I just call it to show you how it's used and then we will proceed to build it also in this case we don't want the padding tokens and we add the necessary dimensions and also we do a Boolean end with capsule mask which is a method that we will build right now and this causal mask need to build a matrix of size sequence length to sequence length what is sequence length is basically the size of our decoder input and this let me write a comment for you so this is one two sequence line combined with so the end with one sequence length sequence and and this can be broadcasted Co Define this method causal mask so what is causal mask causal mask basically means that we want let's go back to the slides actually as you remember from the slides we want each word in the decoder to only watch words that come before it so what we want is to make all these values Above This diagonal that represents the multiplicity this Matrix represents the multiplication of the queries by the keys in the self-attention mechanism what we want is to hide all these values so your cannot watch the word cat is a lovely cat it can only watch itself but this word here for example this word lovely can watch everything that comes before it so from your up to lovely itself but not the word cat that comes after it so what we do is we want all these values here to be masked out so which also means that we want all the values Above This diagonal to be masked out and there is a very practical method in pytorch to do it so let's do it let's go build let's go build this method so the mask is basically torch dot t-r-i-u which means give me the every value that is above the diagonal that I am telling you so we want a matrix which Matrix Matrix made of all ones and this method will will return every value above the diagonal and everything else will become zero so we want diagonal one type we want it to be integer and what we do is return mask is equal to zero so this will return all the values above the diagonal and everything below the diagonal will become zero but we want actually the opposite so we say okay everything that is zero should will become true with this expression and everything that is not 0 will become false so we apply it here to build this mask so this mask will be um one by sequence length by sequence length which is exactly what we want okay let's add also the label the label is also oh I forgot the comma sequence length and then we have the source text just for visualization we can send it Source text and then the target text and this is our data set now let's go back to our Training Method to continue writing the training Loop so now that we have the data set we can create it we can create two data set one for training one for validation and then we send it to a data loader and finally to our training Loop uh we forgot to import the data set so let's import it here let's import the causal mask which we will need later what is our source language it's in the configuration what is our target language and what is our sequence length is also in the configuration we do the same for the validation but the only difference is that we use this one now and the rest is same we also just for choosing the max sequence length we also want to watch what is the maximum length of each sentence in the source and the target for each of the two splits that we created here so that if we choose as very small sequence length it we will know so basically we do I load each sentence from each language from the source and the target language I convert into IDs using the tokenizer and I check the length if the length is let's say 180 we can choose 200 as sequence length because it will cover all the possible sentences that we have in this data set if it's let's say 500 we we can use 510 or something like this because we also need to add the start of sentence and the end of sensor sentence tokens to this sentences foreign then let's create also the target IDs and this is the language of Target and then we just say the source maximum length is the maximum of the and the length of the current sentence the target is the Target and the target ID is then we print these two values we also do it for the Target and that's it now we can proceed to create the data loaders we Define the batch size according to our configuration which we still didn't Define but you can already guess what are its values we wanted to shuffled okay for the validation I will use a batch size of one because I want to process each sentence one by one and this method Returns the data order of the training the data loader of the validation the tokenizer of the source language and the tokenizer of the target language now we can start building the model so let's define a new method called get model which will according to our configuration our vocabulary size build the model the Transformer model so the model is we didn't import the model so let's import it build Transformer what is the first The Source vocabulary size and the target vocabulary size and then we have the sequence length and we have the sequence length of the source language and the sequence length of the target language we will use the same for both and then we have the D model which is the size of the embedding we can keep all the rest the default as in the paper if the model is too big for your GPU to be trained on you can try to reduce the number of heads or the number of layers of course it will impact the performance of the model but I think given the data set which is not so big and not so complicated it should not be a big problem because we are not building a huge data set anyway okay now that we have the model we can start building the training Loop but before we build the training Loop let me just Define this configuration because it keeps coming and I think it's better to define the the structure now so let's create a new file called config dot pi in which we Define two methods one is called get config and one is to map to get the the the path where we will save the weights of the model okay let's define the batch size I'll choose eight you can choose something bigger if your computer allows it the number of epochs for which we will be training I would say 20 is enough the learning rate I am using 10 to the power of -4 you can use other values um I saw I thought this learning rate is reasonable I it's possible to change the learning rate during training uh actually it's quite common to give a very high learning rate and then reduce it gradually with every Epoch we will not be using it because it will just complicate the code a little more and this is not actually the goal of this video the goal of this video is to teach how the Transformer works uh I have already checked this sequence uh length that we need for this particular data set from English to Italian which is 350 is more than enough and the D model that we will be using is the default of 512. the language source is English so we are going from English the language Target is Italian we are going to translate into Italian we will save the model into the folder called weights and the file name of which model will be T model so Transformer model I also built the code to Prelude the model in case we want to restart the training after maybe it's crash and this is the tokenizer file so it will be saved like this or tokenizer n and tokenizer it according to the language and this is the experiment name for tensorboard on which we will save the the losses while training I think there is a comma here okay now let's define another method that allows to find the PATH where we need to save the weights why I'm creating such a complicated structure is because um we I will provide also notebooks to run this training on Google collab so we just need to change these parameters to make it work on Google collab and save the weights directly on your Google Drive I have already created actually this this code and it will be provided on GitHub and I will also provide the link in the video thank you okay the file is built according to model place name then the epoch dot PT let's import also here the path Library okay now let's go back to our training Loop okay we can build the training Loop now finally so train model given the configuration okay first we need to Define which device on which we will put all the tensors so Define the device if I have good on my computer so um okay then we also print we make sure that the weight folder is created and then we load our data set you can just take these values here and say it's equal to get DS of config which is also the model to get the vocabulary size there is Method called get for Hub size and I think we don't have any other parameter and finally we transfer the model to our device we also start sensorboard tensorboard allows to visualize the loss the the graphics the charts let's also import tensorboard let's go back let's also create the optimizer I will be using the Adam optimizer okay since we also have the configuration that allow us to resume the training in case the model crashes or something crashes let's Implement that one and that will allow us to restore the state of the model and the state of the optimizer let's Implement import this method we defined in the data set we load the file here we have a title okay the loss function we will be using is the cross entropy loss we need to tell him what is the ignore index so we don't we want him to ignore the padding token basically we don't want the loss to the padding token to contribute to the loss and we also will be using label smoothing label smoothing basically allows us our model to be less confident about its decision so um how to say imagine our model is telling us to choose the word number three and with a very high probability so what we will do with labels booting is take a little percentage of that probability and distribute to the other tokens so that our model becomes less sure of its choices so kind of less overfeed and this actually improves the accuracy of the model so we will use the levels putting of 0.1 which means from every highest probability probability token take 0.1 percent of score and give it to the others okay let's build finally the training Loop return the model to train I build a batch iterator for the um for the data loader using tkodm which will show a very nice progress bar and we need to import tqdm okay finally we get the tensors the encoder input what is the size of this tensor it's batch to sequence length the decoder input is batch of decoder input and we also move it to our device to sequence length we get the two masks also this is the size and then the decoder mask okay why these two masks are different because in the one case we are only telling him to hide only the padding tokens in the other case we are also telling him to hide all this subsequent words for each word to hide all the subsequent words to mask them out okay now we run the let's make some run the tensors to the Transformer so first we calculate the output of the encoder and we encode using what the encoder input and the mask of the encoder then we calculate the decoder output using the encoder output The Source the mask of the encoder then the decoder input and the decoder mask okay as we know this the result of this so the output of the model dot encode will be a batch sequence length D model also the output of the decoder will be batch sequence length T model but we want to map it back to the vocabulary so we need the projection so let's cut the projection output and this will produce a b so batch sequence length and Target vocabulary size okay now that we have the output of the model we want to compare it with our label so first let's extract the label from the batch and we also put it on another device so what is the label it's B so batch 2 sequence length in which each position tell so the label is already for each B and sequence length so so for each Dimension tells us what is the position in the vocabulary of that particular word and we want these two to be comparable so we first need to compute the loss into this I show you now projection output view -1 okay what does this do this basically transforms the I show you here this size into this size P multiplied by sequence length and then Target vocabulary size vocabularies okay because we want to compare it with this this is how the cross entropy wants the tensors to be and also the label okay now we can we have calculated the loss we can update our progress bar this one with the loss we have calculated and this is this will show the loss on our progress bar we can also log it on tensorboard essentially flush it okay now we can back propagate the loss so low start backward and finally we update the weights of the model so that is the job of the optimizer and finally we can zero out the the grid and remove the global step by one the global step is being used mostly for tensorboard to keep track of the loss we can save the model every yearbook OKAY model file name which we get from our special methods this one you tell him the configuration we have and the name of the file which is the epoch but with zeros in front and we save our model it is very good idea when we want to be able to resume the training to also save not only the the state of the model but also the state of the optimizer because the optimizer also keep tracks of some statistics one for each weight to to understand how to move each weight so independently and usually actually I I saw that the the optimizer the dictionary is quite big so even if it's big if you want your training to be resumable you need to save it otherwise the optimizer will always start from zero and will have to figure out from zero even if you start from a previous Epoch how to move each weight so every time we save some snapshot I always include it will state of the model this is all the weights of the model we also want to stay save the optimizer let's do also the global step and we want to save all this into the file name so model file name and that's it now let's build the code to run this so if name I really find the warnings frustrating so I want to filter them out because I have some some a lot of libraries especially Cuda I already know what's the content and so I don't want to visualize them every time but for sure for you guys I suggest watching them at least once to understand if there is any big problem otherwise they're just complaining from Cuda okay let's try to run this code and see if everything is working fine we should what we expect is that the code should download the data set the first time then it should create the tokenizer and save it into its file and it should also um start training the model for 30 epochs of course it will never finish but let's do it let me check again the configuration tokenizer okay let's run it foreign izer and we have some problem here sequence length okay finally the model is training I show you recap you guys what I had mistaken first of all the sequence length was written incorrectly there was a capital l here and also in the data set I forgot to save it here and here I had it also written capitalized so L was capital and now the training is going on and as you can see the training is quite fast or at least on my computer uh actually not so fast but because I choose a batch size of 8 I could try to increase it and it's happening on Cuda the loss is decreasing and the weights will be saved here so if we reach the end of the epoch it will create the first weight here so let's wait until the end of the ebook and see if the weight is actually created before actually finishing the training of the model let's do another thing we also would like to visualize the output of the model while we are training and this is called validation so we want to check how our model is evolving while it is getting trained so what we want to build is a validation Loop which will allow us to evaluate the model which also means that we want to inference from this model and change some sample sentences and see if how they get translated so let's start building the validation Loop the first thing we do is we build a new method called run validation and this method will accept some parameters that we will use for now I just write all of them and later I explain how they will be used okay the first thing we do uh to run the validation is we put our mode our model into evaluation mode so we do model dot eval and this means that this tells Pi torch that we are going to evaluate our model and then what we will do we will inference uh two sentences and see how they what is the output of the model so with the torch.not grad we are disabling the gradient calculation for this uh for every tensor that we will run inside this with block and this is exactly what we want we just want to inference from the model we don't want to train it during this Loop so let's get a batch from the validation data set because we're working friends only two so we keep a count of how many we have already processed and we get the input from this current patch I want to remind you that for the validation DS we only have a batch size of one this is the encoder input and we can also get the encoder mask let's just verify that the the size of the batch is actually one and now let's go to the interesting part so as you remember when we um calculate the where we want to inference the model we need to calculate the encoder output only once and reuse it for every token that we will the model will output from the decoder so let's create another function that will run the greedy decoding on our model and we'll use and we will see that it will run the encoder only once so let's call this function really decode okay let's create some tokens that we will need so the SOS token which is the start of sentence we can get it from either a tokenizer doesn't matter if it's the Target or the source they both have it ARS okay and then we what we do is we pre-compute the encoder output and reuse it for every token we get from the decoder so we just give the source and the source mask which is the encoder input and the encoder mask we can also call it encoder input and encoder mask then we get the then we okay how do we do the inferencing the first thing we do is we give to the decoder the start of sentence token so that the decoder will output the first token of the sentence of the translated sentence then at every iteration just like we saw in my slides at every iteration we add the previous token to the to the decoder input and so that the decoder can output the next token then we take the next token we put it again in front of the input to the decoder and we get the successive token so let's build the decoder input for the first iteration which is only the start of sentence token we fill this one with the start of sentence token and it has the same type as the encoder input okay now we will keep in asking the decoder to Output the next token until we reach either the end of sentence token or the max land we have defined here so we can do a while true and then our first stopping condition is if we the the decoder output which is becomes the input of The Next Step becomes large larger than Max plan or reaches Max land here why do we have two Dimensions one is for the batch and one is for the tokens of the of the decoder input now we also need to create a mask for this we can use our function causal mask to say that we don't want the input to watch future words and we don't need the other mask because here we don't have any padding token as you can see now we calculate the output we reuse the output of the encoder for every iteration of the loop we reuse the source mask so the input the mask of the encoder then we give the decoder input and along with its mask the recorder mask and then we get the next token so we get the probabilities of the next token using the projection layer but we only want the projection of the last token so the next token after the last we have given to the encoder now we can use the max so we get the token with the maximum probability this is the greedy search and then we get this word and we append it back to this one because it will become the input of the next iteration and we concat so we take the decoder input and we append the next token so we create another tensor for that yep should be correct okay if the next token so if the next word or token is equal equal to the end of sentence token then we also Stop the Loop and this is our greedy search now we can just return the output so the output is basically the decoder input because every time we are appending the next token to it and we remove the batch Dimension so we squeeze it and that's our greedy decoding now we can use it here in this function so in the validation function so we can finally get the model output is equal to 3D decode in which we we give him all the parameters and then we want to compare this model output with what we expected so with the label so let's append all of these so what we give to the input we gave to the model what the model output the output of the model so the predicted and what we expected as output we saved all of this in this lists and then at the end of the loop we will print them on the console to get the text of the output of the model we need to use the tokenizer again to convert the tokens back into text and we use of course the Target tokenizer because this is the target a language okay and now we save then all of this into their respective lists and we can also print it on the console while we are using why we are using this function called print message and why not just use the print of the Python because we are using here in the main Loop in the training Loop we are using uh here TKO DM which is our really nice looking progress bar but it is not suggested to print directly on the console when this progress bar is running so to print on the console there is one method called The Print provided by tqdm and we will give this method to this function so that the output does not interfere with the progress part printing can we print some bars and then we print all the messages and if we have already processed number of examples then we just break so why we have created these lists actually we can also send all of this to [Music] um to a tensorboard so we can so for example if we have tensorboard enabled we can send all of this to the tensorboard and to do that actually we need another library that allow us to calculate some metrics I think we can skip this part but if you are really interested I I in my in the code I published on GitHub you will find that I use this Library called the torch metrics that allows to calculate the Char error rate and the blue the blue metric which is really useful for translation tasks and the word error rate so if you really interested you can find the code on the GitHub but for our demonstration I think it's not necessary and actually this we can also remove it given that we are not doing this part okay so now that we have our run validation method we can just call it okay what I usually do is I run the validation at every few steps but because we want to see it as soon as possible uh the what we will do is we will first run it at our iteration and we also put this model.train inside of this Loop so that every time after we run the validation the model is back into it into its training mode so now we can just run validation and we give it all the parameter that it needs to to run the validation so give it more leader model okay for printing message are we printing any message we are so let's create a Lambda and we just do and this is the message to write with the tqdm then we need to give the global step and the writer which we will not use but okay now I think we can run the training again and see if the validation works all right looks like it is working so the model is okay it's running the validation at every step which is not desirable at all but at least we know that the greedy search is working and it's not at least looks like it is working and the model is not predicting anything useful actually it's just predicting a batch of commas because it's not trained at all but if we train the model after a while we should see that after a few epochs the model should become better and better and better so let's stop this training and let's put this one back to where it belongs so at the end of every ebook here and this one we can keep it here no problem yeah okay I will now skip fast forward to a model that has been pre-trained I pre-trained it for a few hours so that we can influence it and we can visualize the attention I have copied the pre-trained weights that I pre-calculated and I also created this notebook reusing the functions that we have defined before in the train file the code is very simple actually I just copy and pasted the code from the train file I just load the model and run the validation the same method that we just wrote and then I ran the validation on the pre-trained let's run it again for example and as you can see the model is inferencing 10 examples sentences and the result is not bad I mean we can see that 11 smile Levinson race 11 series it's matching and most of them matching actually we could also say that it's nearly over fit um for this particular data but this is the power of the Transformer I didn't train it for many days I just trained it for a few hours I if I remember correctly and the results are really really good and now let's write let's make the notebook that we will use to visualize the attention of this pre-trained model given the file that we built before so train.pi you can also train your own model choosing the language of your choice which I highly recommend that you change the language and try to see how the model is performing and try to diagnose why the model is performing bad if it's in performing bad or if it's performing well try to understand how can you improve it further so let's try to visualize the attention so let's create a new notebook let's call it let's say attention visualization okay so the first thing we do we import all the libraries we will need I will also be using this Library called altire uh it's a visualization library for charts it's nothing related to deep learning actually it's just a visualization function and the particular the visualization function actually I found it online it's not written by me just like most of the visualization functions you can find easily on the internet if you want to build a chart or if you want to build a histogram Etc so I am using this Library mostly because I copied the code from the internet to visualize it but all the rest is my own code so let's import it okay let's import all of these and of course you will have to install this particular Library when you run the code on your computer let's also Define the file the device you can just copy the code from here and then we load the model which we can copy from here like this okay let's paste it here and this one becomes vocabulary source and vocabulary Target foreign to load the batch oops I will convert the batch into tokens Now using the tokenizer and of course for the decoder we use the target vocabulary so the target tokenizer so let's just infer using our gridity code algorithm so we provide the model we return all this information okay now I will build the necessary functions to visualize the visualize the attention I will copy some functions from another file because actually what we are going to build is nothing interesting from a learning point of view for with regards to the Deep learning it's mostly functions to visualize the data so I will copy it because it's quite long to write and the Salient part I will explain of course and this is the function okay what does this function do basically we have the attention that we will get from the encoder how to get the attention from the encoder for example the attention we have in three positions first is in the encoder the second one is in the ink decoder at the beginning of the decoder so the self attention of the decoder and then we have the cross attention between the encoder and the decoder so we can visualize three type of attention how to get the information about the attention well we load the other model we have the encoder we choose which layer we want to get the attention from and then from each layer we can get the self-attention block and then its attention scores how do where does this variable come from if you remember when we defined the attention calculation here here when we calculate the attention we not only return the output to the next layer we also give this attention scores which is the output of the soft Max and we also and we save it here in this variable self dot attention scores now we can just retrieve it and visualize it so this function Will based on which attention we want to get from which layer and from which head we'll select the um the Matrix the correct Matrix this function builds a data frame to visualize the information so the tokens and the score extracted from this Matrix here so it will this Matrix we extract the row and the column and then we also built the chart the chart is built with Altair and what we will build actually is we will get the attention for all the we I built this method to get the attention for all the heads and all the layers that we pass to this function as input so let me run this cell now okay let's create a new cell and then let's just run it okay first we want to visualize the sentence that we are dealing with so the batch order input tokens so we load a batch and then we visualize what is the source and the target and to the Target finally we calculate also the length what is the length Okay it's uh basically all the characters that come before the padding character so the first occurrence of the padding character because this is the batch taken from the data set which is already the tensor build for training so they already include the padding in our case we just want to retrieve the number of actual uh characters in our sentence so this one we can the number of actual words in our sentence so we can check the number of words that come before padding so let's run this one and there is some problem here I forgot to this function was wrong so now it should work okay this sentence is too small let's get a longer one okay let me check the quality you cannot remain as you are especially you know okay looks not bad okay let's print the attention for the layers let's say 0 1 and 2 because we have six of them if you remember the parameter is n is equal to 6 so we will just visualize three layers and we will visualize all the heads we have eight of them for each layer so the head number zero one two three four five six seven and seven okay let's first visualize the encoder self attention and we do get all attention Maps which one we want so the encoder one and we want these layers and these heads and what are their row tokens the encoder input tokens and what are the what what do we want in a column because we are going to build a grid so as you know the the attention is a grid that correlates rows with columns in our case we are talking about the self attention of the encoder so it's the same sentence that is attending itself so we need to provide the input sentence of the encoder on both the rows and the columns and what is the maximum number of length that we want to visualize okay let's say we want to visualize no more than 20 so the minimum of 20 and sentence length okay this is a visualization we can see and as we expected actually when we visualize the attention we expect the values along the diagonals to be high because it's the dot product of each token with itself and we can see also that there are other interesting relationships for example we said that the start of sentence token and the end of sentence token at least for the head 0 and layer 0 they are not related to other words like I would expect actually and but other heads they do learn some very small mapping we can if we hover over each of the grid cells we can see the actual value of the self-attention so the score of the self-attention for example we can see the straw the attention is very strong here so the word especially and especially are related so it's the same word with itself but also especially and now and we can visualize this kind of attention for all the layers so because each head will will watch different aspect of each word because we are Distributing the word embedding among the heads equally so each head will see a different part of the embedding of the word we also hope that they learn different kind of mapping between the words and this is actually the case and between one layer and the next we also have different w q w k and WV metrics so they should also learn different relationships now we can also want we may also want to visualize the attention of the decoder so let's do it let me just copy the code and just change the parameters okay here we want the decoder one we want the same layers Etc but the tokens that we will be on the um rows and the columns are the decoder tokens so decoder input tokens and decoder input tokens let's visualize and also we should see Italian language now because we are using the decoder self attention and it is so here we see a different kind of attention on the decoder side and also here we have multiple heads that should learn different mapping and also different layers should learn different mappings between words the one I find most interesting is the cross attention so let's have a look at that okay let me just copy the code and run it again Okay so if you remember the method it's encoder decoder same layer so here on the rows we will show the encoder input and on the columns we will show the decoder input tokens because it's a cross attention between the encoder and the decoder okay this is how the um more or less how the interaction between the encoder and the decoder works and how it happens so this is where we find the the cross attention calculated using the keys and the values coming from the encoder while the query is coming from the decoder so this is actually where the translation task happens and so this is how the model learns to relate these two sentences to each other to actually calculate the translation so I invite you guys to run The Code by yourself so the first suggestion I give you is to write the code along with me with the video you can pause the video you can write run write the code for by yourself okay let me give you some practical examples for example when I'm writing the model code I suggest you watch me write the code for one particular layer and then stop the video write it by yourself take some time don't watch the solution right away try to figure out what is going wrong and if you really cannot after one two minutes you cannot really figure out what is the problem you can have a glimpse at the video but try to do it by yourself some things of course you cannot come up by yourself so for example for the positional encoding and all this calculation it's all basically just a application of formulas but the point is you should at least be able to come with a structure by yourself so oh how all the layers are interacting with each other this is my first recommendation and why about the training Loop the the training part actually is quite standard so it's very similar to other um training Loops that you may have seen the interesting part is how we calculate the loss and how we use the Transformer model and the last thing that is really important is how we inference the model which is in this greedy decode so thank you everyone for watching the video and for staying with me for so long I can assure you that it was worth it and I hope in the next videos to make more examples of Transformers and other models that I am familiar with and I also want to explore with you guys so let me know if you there is something that you don't understand or you want me to explain better I will also for sure follow the comment section and please write me thank you and have a nice day"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Implementing Transformers with TensorFlow\/PyTorch",
        "Video Title":"PyTorch in 100 Seconds",
        "URL":"https:\/\/www.youtube.com\/watch?v=ORMx45xqWkA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ORMx45xqWkA\/hqdefault.jpg",
        "ID":"ORMx45xqWkA",
        "Publish Time":"2023-03-20T15:11:37Z",
        "Channel":"Fireship",
        "Channel ID":"UCsBjURrPoezykLs9EqgamOA",
        "Transcript":"pie torch an open source deep learning framework used to build some of the world's most famous artificial intelligence products it was created at The Meta AI research lab in 2016 but is actually derived from the Lua based torch library that dates back to 2002. fundamentally it's a library for programming with tensors which are basically just multi-dimensional arrays that represent data and parameters in deep neural networks sounds complicated but its focused on usability will have you training machine learning models with just a few lines of python in addition it facilitates high performance parallel Computing on a GPU thanks to nvidia's Cuda platform developers love prototyping with it because it supports a dynamic computational graph allowing models to be optimized at runtime it does this by constructing a directed acyclic graph consisting of functions that keeps track of all the executed operations on the tensors allowing you to change the shape size and operations after every iteration if needed pytorch has been used to train models for computer vision AI like Tesla autopilot image generators like stable diffusion and speech recognition models like open AI whisper just to name a few to get started install Pi torque and optionally Cuda if you want to accelerate Computing on your GPU now import it into a python file or notebook like I mentioned a tensor is similar to a multi-dimensional array create a 2d array or Matrix with python then use torch to convert it into a tensor now we can run all kinds of computations on it like we might convert all these integers into random floating points we can also perform linear algebra by taking multiple tensors and multiplying them together what you came here to do though is build a deep neural network like an image classifier to handle that we can define a new class that inherits from the neural network module class inside the Constructor we can build it out layer by layer the flattened layer will take a multi-dimensional input like an image and convert it to one dimension from there sequential is used to create a container of layers that the data will flow through each layer has multiple nodes where each node is like its own mini statistical model as each data point flows through it it'll try to guess the output and gradually update a mapping of weights to determine in the importance of a given variable linear is a fully connected layer that takes the flat and 28 by 28 image and transforms it to an output of 512. this layer is followed by a non-linear activation function when activated it means that feature might be important and outputs the node otherwise it just outputs zero and finally we finish with a fully connected layer that outputs the 10 labels the model is trying to predict with these pieces in place that next step is to define a forward method that describes the flow of data and now instantiate the model to a GPU and pass it some input data this will automatically call its forward method for training and prediction congratulations you just built a neural network this has been pytorch in 100 seconds thanks for watching and I will see you in the next one"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Pre-trained Transformer Models: BERT, GPT, T5",
        "Video Title":"Transformers, explained: Understand the model behind GPT, BERT, and T5",
        "URL":"https:\/\/www.youtube.com\/watch?v=SZorAJ4I-sA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/SZorAJ4I-sA\/hqdefault.jpg",
        "ID":"SZorAJ4I-sA",
        "Publish Time":"2021-08-18T23:00:16Z",
        "Channel":"Google Cloud Tech",
        "Channel ID":"UCJS9pqu9BzkAMNTmzNMNhvg",
        "Transcript":"[MUSIC PLAYING] DALE MARKOWITZ: The neat thing about working in machine learning is that every few years, somebody invents something crazy that makes you totally reconsider what's possible, like models that can play Go or generate hyper-realistic faces. And today, the mind-blowing discovery that's rocking everyone's world is a type of neural network called a transformer. Transformers are models that can translate text, write poems and op-eds, and even generate computer code. They could be used in biology to solve the protein folding problem. Transformers are like this magical machine learning hammer that seems to make every problem into a nail. If you've heard of the trendy new ML models BERT, or GPT-3, or T5, all of these models are based on transformers. So if you want to stay hip in machine learning and especially in natural language processing, you have to know about the transformer. So in this video, I'm going to tell you about what transformers are, how they work, and why they've been so impactful. Let's get to it. So what is a transformer? It's a type of neural network architecture. To recap, neural networks are a very effective type of model for analyzing complicated data types, like images, videos, audio, and text. But there are different types of neural networks optimized for different types of data. Like if you're analyzing images, you would typically use a convolutional neural network, which is designed to vaguely mimic the way that the human brain processes vision. And since around 2012, neural networks have been really good at solving vision tasks, like identifying objects in photos. But for a long time, we didn't have anything comparably good for analyzing language, whether for translation, or text summarization, or text generation. And this is a problem, because language is the primary way that humans communicate. You see, until transformers came around, the way we used deep learning to understand text was with a type of model called a Recurrent Neural Network, or an RNN, that looked something like this. Let's say you wanted to translate a sentence from English to French. An RNN would take as input an English sentence and process the words one at a time, and then sequentially spit out their French counterparts. The keyword here is sequential. In language, the order of words matters, and you can't just shuffle them around. For example, the sentence Jane went looking for trouble means something very different than the sentence Trouble went looking for Jane. So any model that's going to deal with language has to capture word order, and recurrent neural networks do this by looking at one word at a time sequentially. But RNNs had a lot of problems. First, they never really did well at handling large sequences of text, like long paragraphs or essays. By the time they were analyzing the end of a paragraph, they'd forget what happened in the beginning. And even worse, RNNs were pretty hard to train. Because they process words sequentially, they couldn't paralellize well, which means that you couldn't just speed them up by throwing lots of GPUs at them. And when you have a model that's slow to train, you can't train it on all that much data. This is where the transformer changed everything. They're a model developed in 2017 by researchers at Google and the University of Toronto, and they were initially designed to do translation. But unlike recurrent neural networks, you could really efficiently paralellize transformers. And that meant that with the right hardware, you could train some really big models. How big? Really big. Remember GPT-3, that model that writes poetry and code, and has conversations? That was trained on almost 45 terabytes of text data, including almost the entire public web. [WHISTLES] So if you remember anything about transformers, let it be this. Combine a model that scales really well with a huge data set and the results will probably blow your mind. So how do these things actually work? From the diagram in the paper, it should be pretty clear. Or maybe not. Actually, it's simpler than you might think. There are three main innovations that make this model work so well. Positional encodings and attention, and specifically, a type of attention called self-attention. Let's start by talking about the first one, positional encodings. Let's say we're trying to translate text from English to French. Positional encodings is the idea that instead of looking at words sequentially, you take each word in your sentence, and before you feed it into the neural network, you slap a number on it-- 1, 2, 3, depending on what number the word is in the sentence. In other words, you store information about word order in the data itself, rather than in the structure of the network. Then as you train the network on lots of text data, it learns how to interpret those positional encodings. In this way, the neural network learns the importance of word order from the data. This is a high level way to understand positional encodings, but it's an innovation that really helped make transformers easier to train than RNNs. The next innovation in this paper is a concept called attention, which you'll see used everywhere in machine learning these days. In fact, the title of the original transformer paper is \"Attention Is All You Need.\" So the agreement on the European economic area was signed in August 1992. Did you know that? That's the example sentence given in the original paper. And remember, the original transformer was designed for translation. Now imagine trying to translate that sentence to French. One bad way to translate text is to try to translate each word one for one. But in French, some words are flipped, like in the French translation, European comes before economic. Plus, French is a language that has gendered agreement between words. So the word [FRENCH] needs to be in the feminine form to match with [FRENCH]. The attention mechanism is a neural network structure that allows a text model to look at every single word in the original sentence when making a decision about how to translate a word in the output sentence. In fact, here's a nice visualization from that paper that shows what words in the input sentence the model is attending to when it makes predictions about a word for the output sentence. So when the model outputs the word [FRENCH],, it's looking at the input words European and economic. You can think of this diagram as a sort of heat map for attention. And how does the model know which words it should be attending to? It's something that's learned over time from data. By seeing thousands of examples of French and English sentence pairs, the model learns about gender, and word order, and plurality, and all of that grammatical stuff. So we talked about two key transformer innovations, positional encoding and attention. But actually, attention had been invented before this paper. The real innovation in transformers was something called self-attention, a twist on traditional attention. The type of attention we just talked about had to do with aligning words in English and French, which is really important for translation. But what if you're just trying to understand the underlying meaning in language so that you can build a network that can do any number of language tasks? What's incredible about neural networks, like transformers, is that as they analyze tons of text data, they begin to build up this internal representation or understanding of language automatically. They might learn, for example, that the words programmer, and software engineer, and software developer are all synonymous. And they might also naturally learn the rules of grammar, and gender, and tense, and so on. The better this internal representation of language the neural network learns, the better it will be at any language task. And it turns out that attention can be a very effective way to get a neural network to understand language if it's turned on the input text itself. Let me give you an example. Take these two sentences-- Server, can I have the check? Versus, Looks like I just crashed the server. The word server here means two very different things. And I know that, because I'm looking at the context of the surrounding words. Self-attention allows a neural network to understand a word in the context of the words around it. So when a model processes the word server in the first sentence, it might be attending to the word check, which helps it disambiguate from a human server versus a mail one. In the second sentence, the model might be attending to the word crashed to determine that the server is a machine. Self-attention can also help neural networks disambiguate words, recognize parts of speech, and even identify word tense. This, in a nutshell, is the value of self-attention. So to summarize, transformers boil down to positional encodings, attention, and self-attention. Of course, this is a 10,000-foot look at transformers. But how are they actually useful? One of the most popular transformer-based models is called BERT, which was invented just around the time that I joined Google in 2018. BERT was trained on a massive text corpus and has become this sort of general pocketknife for NLP that can be adapted to a bunch of different tasks, like text summarization, question answering, classification, and finding similar sentences. It's used in Google Search to help understand search queries, and it powers a lot of Google Cloud's NLP tools, like Google Cloud AutoML Natural Language. BERT also proved that you could build very good models on unlabeled data, like text scraped from Wikipedia or Reddit. This is called semi-supervised learning, and it's a big trend in machine learning right now. So if I've sold you about how cool transformers are, you might want to start using them in your app. No problem. TensorFlow Hub is a great place to grab pretrained transformer models, like BERT. You can download them for free in multiple language and drop them straight into your app. You can also check out the popular transformers Python library, built by the company Hugging Face. That's one of the community's favorite ways to train and use transformer models. For more transformer tips, check out my blog post linked below, and thanks for watching. [MUSIC PLAYING]"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Pre-trained Transformer Models: BERT, GPT, T5",
        "Video Title":"Confused which Transformer Architecture to use? BERT, GPT-3, T5, Chat GPT? Encoder Decoder Explained",
        "URL":"https:\/\/www.youtube.com\/watch?v=wuj8Hao1TT4",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/wuj8Hao1TT4\/hqdefault.jpg",
        "ID":"wuj8Hao1TT4",
        "Publish Time":"2023-01-09T06:43:23Z",
        "Channel":"Datafuse Analytics",
        "Channel ID":"UCN9NJju-JIxTXtNeBT4D35w",
        "Transcript":"hello guys welcome to data shoes analytics where the concepts are simplified and explained intuitively in this video we will be studying the entire tree of Transformers and I will be introducing you to important Transformer architectures which are available as you all know there are three important architectures for Transformer models namely encoder decoder and encoder decoder architecture as you all know the early Transformer models initial success poured an explosion in models development in this explosion the researchers started creating models using a variety of data sets of varying size and types adopting new pre-training objectives and modifying the model architectures to further boost performance in different tasks although the family of models is still growing at a rapid Pace they still can be divided into the above discussed categories namely encoders decoders and encoder decoder architectures up till now there are more than 50 different architectures available in hugging phase Transformers in this video I will cover some of the few important milestones let's start with encoder branch do you know the first encoder model only based on Transformer was bird when bird paper was published it outperformed various existing state-of-the-art or Sota models on different evaluation metrics like blue various nlu or natural language understanding challenges like text classification named entity recognition Etc can be solved using encoder only models now let's look at different famous and important encoder only models the first model that we will be discussing is bird in this bird stands for bi-directional encoder representations from Transformers birth is pre-trained by keeping following two training objectives which are as follows the first running objective is predicting Mass tokens in the text which is also called x marks language modeling or MLM the second pre-training objective is determining if one text passage is likely to follow another text Passage this is also called as NSP or next sentence prediction objective the next encoder only model is distal word as discussed bird outperform many state-of-the-art architectures but the industry needed some lightweight version of Bolt to deploy in production environment this gave rise to digital bird which is a distilled version of bird this distal bird architecture is trained using knowledge distillation technique distal bird is a whooping sixty percent fast than bird and its memory footprint is 40 percent less too distal but achieves this by maintaining 97 percent of the birds performance in terms of accuracy the next encoder only architecture is Roberta Roberta stands for robustly optimized bird pre-training approach Roberta aims to improve Birds performance by slightly modifying the pre-training scheme or pre-training objective robota is trained on longer sequences or longer sentences on more training data than bullet and it drops the NSP or next sentence prediction training objective of birth these two changes made Roberta improve its performance when compared with world the next encoder only architecture is called as xlm xlm is an improved version of bird which is capable to perform cross-lingual language tasks like text classification and machine translation xlm learns to map words from different languages by using byte pair encoding or bpe and a dual language training mechanism xlm introduces and additional training objective which is called as translation language modeling or tlm in short tlm or translucent language modeling can be viewed as an extension of MLM or Max language modeling which we discussed during World xlm model achieved state-of-the-art results on multilingual nlu or natural language understanding benchmarks as well as on translation tasks the next encoder only architecture is xlm Roberta it is also called as xlmr this is an extension of xlm that incorporates massive training data to train xlmr common crawl Corpus is used this common crawl Corpus does not contain parallel text hence the tlm or translation language modeling training objective which was used in xlm was removed an interesting point is that xlmr beats xlm and even Birth by huge margin in different tasks especially those including low resource languages the next encoder only architecture is Albert Albert is an efficient Transformer architecture the following are the three modification that makes Albert efficient the first modification is that the token embedding is decoupled from the hidden Dimension this makes embedding Dimension to be small especially when vocabulary is huge or when vocabulary is large which help it to save model parameters the second change is that all the layers share parameters this helps to decrease the final total effective parameters the final modification which makes Albert efficient is that the NSP or next sentence prediction objective is replaced with sentence ordering prediction in sentence ordering prediction objective the model predicts whether the order of two consecutive sequences was swapped or not this three modification or these three changes made Albert to train for larger model with fewer parameters efficiently the next encoder only architecture is called as Elektra one of the major limitations of marks language modeling or MLM training objective is that only the marks tokens are updated at each step while the other input tokens remain as it is Electra solves this issue by using a two model approach model 1 is like a regular MLM and tries to predict marks tokens whereas model 2 act as a discriminator and the aim of this model too is to predict which of the token in the first model's output were originally marked the final encoder architecture that we'll be discussing is diberta debater is the first model to beat human Baseline on the super glue Benchmark for people who do not know what super glue Benchmark is a super glue Benchmark is a more difficult version of glue consisting of several subtasks used to measure nlu performance or natural language understanding task performance makes two major architectural changes which are as follows first architectural changes is that each token is represented as two vectors one vector is for the content and the another Vector is for relative position this makes the self attention layers to better model the dependency of nearby token pairs diverter uses relative position representations this is achieved by modifying the internal mechanism Itself by introducing a few additional terms or parameters now that we have gone through important encoder only architectures let's look into decoder only architectures now the development of Transformer decoder models has mostly been driven by open AI these models are mostly utilized for Tech generation tasks because of how well they predict the next word in a sequence because they are so accurate at predicting the next word in the sequence these models are mostly used for text generation tags let's examine the development of this interesting text generation models the first decoder model that we will be talking about is GPT GPT stands for generative pre-trained Transformer GPT is pre-trained by predicting the next word based on the previous ones GPT is trained on the book Corpus and achieved significant great results on Downstream tags such as classification then the next decoder model is Ctrl in which Ctrl stands for conditional Transformer language we all know that GPT is used to auto complete an input given the input prompt but the major limitation is that we as user has very less control over the style of generated sequence or text this Ctrl model addresses this issue by introducing something called as control tokens at the beginning of the sequence this allows the user to control the text generation allowing for diverse text generation the next decoder only model we will be looking into is gpt2 you all might have heard about gpt2 gpt2 is inspired by its predecessor which is named as GPT GPT is upskilled and the training data is increased which gave birth to gpt2 highlight of gpt2 is that it can produce long coherent text gpt2 was released in a stagewise fashion due to few concerns about its misuse smaller models were published first and then the final full model was published the next decoder model that we will be looking into is gpt3 GPT and gpt2 were a huge success in the tech generation domain and Analysis was conducted on a few parameters like compute data set size model size and performance of language model the results of this analysis was upscaling GPT 200 times to yield this gpd3 gpt3 has whopping 175 billion parameters this model has excellent Tech generation capacities but the highlight of gpt3 is few short learning capability it means that jpt3 was able to solve novel tasks with very few input examples open AI has not yet open source this model but gpt3 can be accessed via an interface which is provided by open AI the final model of decoder only that we will be looking into is GPT Neo or GPT j6b these are like GPT models which are trained by a lifter AI these models are smaller versions when we compare it with gpt3 these models have about 1.3 2.7 and 6 billion parameters the last branch in Transformer tree is the encoder decoder Branch let's take a look in some of the famous models the first encoder decoder architecture that we will be seeing is T5 the T5 stands for text to text transfer Transformer as the full form suggests the T5 model combined nlu and nlg tasks by converting them to text to text task for text classification the T5 models encoder takes input text and decoder generates a label as a prediction T5 model is pre-trained using colossal cleaned version of common crawls web crawl Corpus which is also called as C4 Corpus the training objective of T5 is MLM T5 has several variants like T5 small T5 base T5 large t53b and t511b T5 small has about 60 million parameters T5 base to 20 million parameters T5 are 770 million parameters t53b 3 billion parameters and T5 will even be with a whooping one 11 billion parameters then the next encoder decoder only architecture we will be looking into is Bart bar stands for bi-directional auto regressive Transformers Bart combines the pre-training objectives of birth and GP tree with the encoded decoder architecture the input sentence undergoes one of the following simple masking sentence permutation token deletion and document rotation this transformation aims to distort the inputs and then the decoder tries to construct the original sentence this pre-training objective makes Bart more flexible and good to use for energy and nlu tasks then we have M2M 100 as another encoder decoder architecture this m2m00 is the first model to translate between 100 languages While most of the language models deal with only one language per translation this m2m100 leverages the information and patterns of multiple languages to translate over 100 different languages this model uses a prefix token which is similar to that of CLS token this prefix token indicates the source and target language then we have big bird the maximum context size and transform models is limited because it utilizes memory in terms of quadratic requirements Big Bird model solves this memory issue or memory requirement challenge by using a sparse form of attention mechanism which enables it to scale in linear fashion this allows for scaling from 512 tokens in most bird models to 4096 tokens in Big Bird this model is heavily used in tasks like text summarization due to its ability to model long term dependencies so guys that's all concerning with all the major and important Transformer architectures or what I call as a Transformer tree it should be noted that all the models which are discussed in this video are available on hugging phase Hub and can be fine-tuned as per the problem statement which we want to solve if you like this short video of Transformer 3 please give a like to this video share this video with your friends and subscribe to this channel thank you"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Pre-trained Transformer Models: BERT, GPT, T5",
        "Video Title":"Transformer models and BERT model: Overview",
        "URL":"https:\/\/www.youtube.com\/watch?v=t45S_MwAcOw",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/t45S_MwAcOw\/hqdefault.jpg",
        "ID":"t45S_MwAcOw",
        "Publish Time":"2023-06-05T18:23:04Z",
        "Channel":"Google Cloud Tech",
        "Channel ID":"UCJS9pqu9BzkAMNTmzNMNhvg",
        "Transcript":"hi I'm sanjana Reddy a machine learning engineer at Google's Advanced Solutions lab there's been a lot of excitement around generative Ai and all the new advancements including new vertex AI features that are coming up such as gen AI Studio model Garden genei API our objective in this short session is to give you a solid footing on some of the underlying Concepts that make all the Gen AI magic possible today I'm going to talk about Transformer models and the Bert model language modeling has evolved over the years the recent breakthroughs in the past 10 years include the usage of neural networks to represent text such as word to whack and engrams in 2013. in 2014 the development of sequence to sequence models such as rnns and lstms helped improve the performance of ml models on NLP tasks such as translation and text classification in 2015 the excitement came with attention mechanisms and the models built based on it such as Transformers and the bird model in this presentation we'll focus on Transformers Transformers is based on a 2017 paper named attention is all you need although all the models before Transformers were able to represent verbs as vectors these vectors did not contain the context and the usage of words changes based on the context for example Bank in Riverbank versus Bank in bank robber might have the same Vector representation before attention mechanisms came about a Transformer is an encoder decoder model that uses the attention mechanism it can take advantage of parallelization and also process a large amount of data at the same time because of its model architecture attention mechanism helps improve the performance of machine translation applications Transformer models were built using attention mechanisms at the core a Transformer model consists of encoder and decoder the encoder encodes the input sequence and passes it to the decoder and the decoder decodes a representation for irrelevant task the encoding component is a stack of encoders of the same number the research paper that introduced Transformers Stacks 6 encoders on top of each other six is not a magical number it's just a hyper parameter the encoders are all identical in structure but with different weights each encoder can be broken down into two sub-layers the first layer is called the self-attention the input of the encoder first flows through a self-attention layer which helps the encoder look at relevant parts of the words as it encodes a center word in the input sentence and the second layer is called a feed forward layer the output of the self-attention layer is fed to the feed forward neural network the exact same feed forward neural network is independently applied to each position the decoder has both the self-attention and the feed forward layer but between them is the encoder decoder attention layer that helps the decoder focus on relevant parts of the input sentence after embedding the words in the input sequence each of the embedding Vector flows through the two layers of the encoder the word at each position passes through a self-attention process then it passes through a feed-forward neural network the exact same network with each Vector flowing through it separately dependencies exist between these paths in the self attention layer however the feed forward layer does not have these dependencies and therefore various paths can be executed in parallel while they flow through the feed forward layer in the self-attention layer the input embedding is broken up into query key and value vectors these vectors are computed using weights that the Transformer learns during the training process all of these computations happen in parallel in the model in the form of Matrix computations once we have the query key and value vectors the next step is to multiply each value vector by the softmax score in preparation to sum them up the intuition here is to keep intact the values of the words you want to focus on and leave out irrelevant words by multiplying them by tiny numbers like 0.001 for example next we have to sum up the weighted value vectors which produces the output of the self-attention layer at this position for the first word you can send along the resulting Vector to the feedforward neural network to sum up this process of getting the final embeddings these are the steps that we take we start with the natural language sentence embed each word in the sentence after that we perform multi-headed attention eight times in this case and multiply this embedded word with the respective weighted matrices we then calculate the attention using the resulting qkv matrices finally we concatenate the matrices to produce the output Matrix which is the same Dimension as the final Matrix that this layer initially got there's multiple variations of Transformers out there now some use both the encoder and the decoder component from the original architecture some use only the encoder and some use only the decoder a popular encoder only architecture is Bert Bert is one of the trained Transformer models Bert stands for bi-directional encoder representations from Transformers and was developed by Google in 2018. since then multiple variations of bird have been built today Bert Powers Google search you can see how different the results provided by Bert are for the same search query before and after it was trained in two variations one model contains bird base which had 12 stack of Transformers with approximately 110 million parameters and the other bird large with 24 layers of Transformers with about 340 million parameters the bird model is powerful because it can handle long input context it was trained on the entire Wikipedia Corpus and Books Corpus the bird model was trained for 1 million steps Bert is trained on different tasks which means it has multi-task objective this makes Bert very powerful because of the kind of tasks it was trained on it works at both a sentence level and at a token level these are the two different versions of Bert that were originally released one is bird base which had 12 layers whereas bird large had 24 layers and compared to the original Transformer which had six layers the way that bird works is that it was trained on two different tasks task one is called a masked language model where the sentences are masked and the model is trained to predict the masked words if you were to train bird from scratch you would have to mask a certain percentage of the words in your Corpus the recommended percentage for masking is 15 percent the masking percentage achieves a balance between too little and too much masking do little masking makes the training process extremely expensive and too much masking removes the contacts that the model requires the second task is to predict the next sentence for example the model is given two sets of sentences Bert aims to learn the relationships between sentences and predict the next sentence given the first one for example sentence a could be a man went to the store and sentence B is he bought a gallon of milk bird is responsible for classifying if sentence B is the next sentence after sentence a this is a binary classification task this helps Bert perform at a sentence level in order to train Bert you need to feed three different kinds of embeddings to the model for the input sentence you get three different embeddings token segment and position embeddings the token embeddings is a representation of each token as an embedding in the input sentence the words are transformed into Vector representations of certain dimensions bird can solve NLP tasks that involve text classification as well an example is to classify whether two sentences say my dog is cute and he likes playing are semantically similar the pairs of input texts are simply concatenated and fed into the model how does bird distinguish the inputs in a given pair the answer is to use segment embeddings there is a special token represented by SCP that separates the two different splits of the sentence another problem is to learn the order of the words in the sentence as you know bird consists of a stack of Transformers Bert is designed to process input sequences up to a length of 512. the order of the input sequence is incorporated into the position embeddings this allows Bert to learn a vector representation for each position vert can be used for different Downstream tasks although Bert was trained on mass language modeling and single sentence classification it can be used for popular NLP tasks like single sentence classification sentence pair classification question answering and single sentence tagging tasks thank you for listening"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Recurrent Neural Networks (RNNs) and Transformers",
        "Topic":"Pre-trained Transformer Models: BERT, GPT, T5",
        "Video Title":"What are Transformers (Machine Learning Model)?",
        "URL":"https:\/\/www.youtube.com\/watch?v=ZXiruGOCn9s",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/ZXiruGOCn9s\/hqdefault.jpg",
        "ID":"ZXiruGOCn9s",
        "Publish Time":"2022-03-11T10:00:37Z",
        "Channel":"IBM Technology",
        "Channel ID":"UCKWaEZ-_VweaEx1j62do_vQ",
        "Transcript":"no it's it it's not those transformers but but they can do some pretty cool things let me show you so why did the banana cross the road because it was sick of being mashed yeah i'm not sure that i quite get that one and that's because it was created by a computer i literally asked it to tell me a joke and this is what it came up with specifically i used a gpt-3 or a generative pre-trained transformer model the three here means that this is the third generation gpt-3 is an auto-regressive language model that produces text that looks like it was written by a human gpt3 can write poetry craft emails and evidently come up with its own jokes off you go now while our banana joke isn't exactly funny it does fit the typical pattern of a joke with a setup and a punch line and sort of kind of makes sense i mean who wouldn't cross the road to avoid getting mashed but look gpt3 is just one example of a transformer something that transforms from one sequence into another and language translation is just a great example perhaps we want to take a sentence of why did the banana cross the road and we want to take that english phrase and translate it into french well transformers consist of two parts there is an encoder and there is a decoder the encoder works on the input sequence and the decoder operates on the target output sequence now on the face of it translation seems like little more than just like a basic lookup task so convert the y here of our english sentence to the french equivalent of porcua but of course language translation doesn't really work that way things like word order in terms of phrase often mix things up and the way transformers work is through sequence to sequence learning where the transformer takes a sequence of tokens in this case words in a sentence and predicts the next word in the output sequence it does this through iterating through encoder layers so the encoder generates encodings that define which part of the input sequence are relevant to each other and then passes these encodings to the next encoder layer the decoder takes all of these encodings and uses their derived context to generate the output sequence now transformers are a form of semi supervised learning by semi sequence semi-supervised we mean that they are pre-trained in an unsupervised manner with a large unlabeled data set and then they're fine-tuned through supervised training to get them to perform better now in previous videos i've talked about other machine learning algorithms that handle sequential input like natural language for example there are recurrent neural networks or rnns what makes transformers a little bit different is they do not necessarily process data in order transformers use something called an attention mechanism and this provides context around items in the input sequence so rather than starting our translation with the word why because it's at the start of the sentence the transformer attempts to identify the context that bring meaning in each word in the sequence and it's this attention mechanism that gives transformers a huge leg up over algorithms like rnn that must run in sequence transformers run multiple sequences in parallel and this vastly speeds up training times so beyond translations what can transformers be applied to well document summaries they're another great example you can like feed in a whole article as the input sequence and then generate an output sequence that's going to really just be a couple of sentences that summarize the main points transformers can create whole new documents of their own for example like write a whole blog post and beyond just language transformers have done things like learn to play chess and perform image processing that even rivals the capabilities of convolutional neural networks look transformers are a powerful deep learning model and thanks to how the attention mechanism can be paralyzed are getting better all the time and who knows pretty soon maybe they'll even be able to pull off banana jokes that are actually funny if you have any questions please drop us a line below and if you want to see more videos like this in the future please like and subscribe thanks for watching"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Generative Adversarial Networks (GANs)",
        "Video Title":"What are GANs (Generative Adversarial Networks)?",
        "URL":"https:\/\/www.youtube.com\/watch?v=TpMIssRdhco",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/TpMIssRdhco\/hqdefault.jpg",
        "ID":"TpMIssRdhco",
        "Publish Time":"2021-11-11T15:32:08Z",
        "Channel":"IBM Technology",
        "Channel ID":"UCKWaEZ-_VweaEx1j62do_vQ",
        "Transcript":"One of my favorite machine learning algorithms is Generative Adversarial Networks, or GAN. It pits two AI models off against each other, hence the \"adversarial\" part. Now, most machine learning models are used to generate a prediction. So we start with some input training data. And we feed that into our model. A model then makes a prediction in the form of output. And we can compare the predicted output with the expected output from the training data set. And then based upon that expected output and the actual predicted output, we can figure out how we should update our model to create better outputs. That is an example of supervised learning. A GAN is an example of unsupervised learning, it effectively supervises itself, and it consists of two submodels. So we have a generator submodel. And we have a discriminator submodel. Now, the generator's job is to create fake input or fake samples. And the discriminator's job is to take a given sample and figure out if it is a fake sample or if it's a real sample from the domain. And therein lies the adversarial nature of this. We have a generator creating fake samples and sending them to a discriminator. The discriminator is taking a look at a given sample and figuring out, \"Is this a fake sample from the generator? Or is this a real sample from the domain set?\" Now, this sort of scenario is often applied in image generation. There are images all over the internet of generators that have been used to create fake 3D models, fake faces, fake cats and so forth. So this really works by the generator iterating through a number of different cycles of creating samples, updating its model and so forth until it can create a sample that is so convincing that it can fool a discriminator and also fool us humans as well. So let's let's take an example of how this works with, let's say, a flower. So we are going to train a generator to create really convincing fake flowers, and the way that we start by doing this is we need to, first of all, train our discriminator model to recognize what a picture of a flower looks like. So our domain is lots of pictures of flowers, and we will be feeding this into the discriminator model and telling it to look at all of the attributes that make up those flower images. Take a look at the colors, the shading, the shapes and so forth. And when our discriminator gets good at recognizing real flowers, then we'll feed in some shapes that are not flowers at all. And make sure that it can discriminate those as being not-flowers. Now, this whole time our generator here was frozen, it wasn't doing anything. But we're our discriminator gets good enough at recognizing things from our domain, then we apply our generator to start creating fake versions of those things. So a generator is going to take a random input vector and it is going to use that to create its own fake flower. Now, this fake flower image is sent to the discriminator, and now the discriminator has a decision to make: is that image of a flower the real thing from the domain, or is it a fake from the generator? Now, the answer is revealed to both the generator and the discriminator. The flower was fake and based upon that, the generator and discriminator will change their behavior. This is a zero sum game, there's always a winner and a loser. The winner gets to remain blissfully unchanged. Their model doesn't change at all, whereas the loser has to update their model. So if the discriminator successfully spotted that this flower was a fake image, then lead discriminator remains unchanged. But the generator will need to change its model to generate better fakes. Whereas if the reverse is true and the generator is creating something that fools the discriminator, the discriminator model will need to be updated itself in order to better be able to tell where we have a fake sample coming in, so it's fooled less easily. And that's basically how these things work, and we go through many, many iterations of this until the generator gets so good that the discriminator can no longer pick out its fakes. And there we have built a very successful generator to do whatever it is we wanted it to do. Now, often in terms of images, the generator and the discriminator implemented as CNNs. These are Convolutional Neural Networks. CNN's are a great way of recognizing patterns in image data and entering into sort of the area of object identification. We have a whole separate video on CNNs, but they're a great way to really implement the generator and discriminator in this scenario. But the whole process of a GAN, isn't just to create really good fake flowers or fake cat images for the internet. You can apply it to all sorts of use cases. So take, for example, video frame prediction. If we fit in a particular frame of video from a camera, we can use a GAN to predict what the next frame in this sequence will look like. This is a great way to be able to predict what's going to happen in the immediate future and might be used, for example, in a surveillance system. If we can figure out what is likely to happen next, we can take some action based upon that. There's also other things you can do, like image enhancement. So if we have a kind of a low resolution image, we can use a GAN to create a much higher resolution version of the image by figuring out what each individual pixel is and then creating a higher resolution version of that. And we can even go as far as using this for things that are not related to images at all, like encryption. But we can create a secure encryption algorithm that can be decrypted and encrypted by the sender and receiver, but cannot be easily intercepted, again by going through these GAN iterations to create a really good generator. So that's GAN. It's the battle of the bots where you can take your young, impressionable and unchanged generator and turn it into a master of forgery. If you have any questions, please drop us a line below. And if you want to see more videos like this in the future, please like and subscribe."
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Generative Adversarial Networks (GANs)",
        "Video Title":"Generative Adversarial Networks (GANs) - Computerphile",
        "URL":"https:\/\/www.youtube.com\/watch?v=Sw9r8CL98N0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Sw9r8CL98N0\/hqdefault.jpg",
        "ID":"Sw9r8CL98N0",
        "Publish Time":"2017-10-25T16:31:17Z",
        "Channel":"Computerphile",
        "Channel ID":"UC9-y-6csu5WGm29I7JiwpnA",
        "Transcript":"So today, I thought we talk about generative adversarial networks because they're really cool, and they've They can do a lot of really cool things people have used them for all kinds of things Things like you know you draw a sketch of a shoe And it will render you an actual picture of a shoe or a handbag They're fairly low-resolution right now, but it's very impressive the way that they can produce real quite good-looking images You could make a neural network That's a classifier right you give it lots and lots of pictures of cats and lots and lots of pictures of dogs and you say you know you present it with a picture of a cat and It says it outputs a number. Let's say between zero and one and Zero represents cats and one represents dogs and so you give it a cat and it puts out one and you say no That's not right should be zero and you keep training it until eventually it can tell the difference right? so somewhere inside that Network It's... it must have formed some model of what cats are and what dogs are, at least as far as images of images of them are concerned But That model really... you can only really use it to classify things You can't say \"ok draw me a new cat picture\", \"draw me a cat picture I haven't seen before\" It doesn't know how to do that so quite often you want a model that can generate new Samples you have so you give it a bunch of samples from a particular distribution, and you want it to Give you more samples which are also from that same distribution, so it has to learn the underlying Structure of what you've given it. And that's kind of tricky, actually. There's a lot of... Well there's a lot of challenges involved in that. Well, let's be honest I don't think as a human you can find that tricky You know if... if I know what a cat looks like but, uh, being not the greatest artist in the world I'm not sure that I could draw you a decent cat. So, you know, that this is not confined to just Computing is it? This... Yeah, that's true. That's really true. but if you take Let's do like a really simple example of a generative model say you you give your network one thing It looks like this. And then you give it another one you're like these are your training samples looks like this You give it another one that looks like this, and then... What are those dots in the systems? Instances of something on two dimensions? Yeah, I mean right now, it's literally just data. We just... it doesn't matter what it is Just some... yeah, these are these are data points And so these are the things you're giving it, and then it will learn You can train it. It will learn a model, and the model it might learn is something like this, right? It's figured out that these dots all lie along a path, and if its model was always to draw a line Then it could learn by adjusting the parameters of that line It would move the line around until it found a line that was a good fit, and generally gave you a good prediction. But then if you were to ask this model: \"Okay, now make me a new one\" unless you did something clever, what you get is probably this, because that is on average The closest to any of these, because any of these dots you don't know if they're going to be above or below or, you know, to the left or the right. There's no pattern there. It's kind of random. So the best place you can go that will minimize your error, is to go just right on the line every time. But anybody looking at this will say: \"well, that's fake\" That's not a plausible example of something from this distribution, even though for a lot of the like, error functions, that people use when training networks this would perform best, so it's this interesting situation where There's not just one right answer. you know, generally speaking the way that neuron networks work is: you're training them towards a specific you have a label or you have a you have an output a target output and You get penalty the further away you are from that output, whereas in in a in an application like this There's effect... there's basically an infinite number of perfectly valid Outputs here But, so, to generate this what you actually need is to take this model and then apply some randomness, you say: \"they're all Within, you know, They occur randomly and they're normally distributed around this line with this standard deviation\" or whatever. But a lot of models would have a hard time actually picking one of all of the possibilities And they would have this tendency to kind of smooth things out and go for the average, whereas we actually just want \"Just pick me one doesn't matter\". So that's part of the problem of generating. Adversarial training is is help is a way of training Not just networks, actually, a way of training machine learning systems. Which involves focusing on the system's weaknesses. So, if you are learning... let's say you're teaching your Network to recognize handwritten digits. The normal way you would do that you have your big training sample of labeled samples You've got an array of pixels that looks like a three and then it's labeled with three and so on. And the normal way that you would train a network with this is you would just Present all of them pretty much at random. You'd present as many ones as two as threes and just keep throwing examples at it \"What's this?\", you know, \"Yes, you got that right\", \"no. You've got that wrong, It should really be this\". And keep doing that and the system will eventually learn but If you were actually teaching a person to recognize the numbers, if you were teaching a child you wouldn't do that, like, if you'd been teaching them for a while, presenting them and You know, getting the response and correcting them and so on, and you noticed that they can do... you know... with 2 3 4 5 6 8 & 9 they're getting like 70 80 percent You know, accuracy recognition rate. But 1 & 7 it's like 50\/50, because any time they get a 1 or a 7 they just guess because they can't Tell the difference between them. If you noticed that you wouldn't keep training those other numbers, right? You would stop and say: \"Well, You know what? we're just gonna focus on 1 & 7 because this is an issue for you\". \"I'm gonna keep showing you Ones and 7s and correcting you until The error rate on ones and 7s comes down to the error rate that you're getting on your other numbers\". You're focusing the training on the area where the student is failing and there's kinda of a balance there when you're teaching humans because if you keep relentlessly focusing on their weaknesses and making them do stuff they can't do all the time They will just become super discouraged and give up. But neural networks don't have feelings yet, so that's really not an issue. You can just continually hammer on the weak points Find whatever they're having trouble with and focus on that. And so, that behavior, and I think some people have had teachers where it feels like this, It feels like an adversary, right? it feels like they want you to fail. So in fact you can make them an actual adversary. If you have some process which is genuinely Doing its best to make the network give as high an error as possible that will produce this effect where if it spots any weakness it will focus on that and Thereby force the learner To learn to not have that weakness anymore. Like one form of adversarial training people sometimes Do is if you have a game playing program you make it play itself a lot of times Because all the time. They are trying to look for weaknesses in their opponent and exploit those weaknesses and when they do that They're forced to then improve or fix those weaknesses in themselves because their opponent is exploiting those weaknesses, so Every time the Every time the system finds a strategy that is extremely good against this opponent The the opponent, who's also them, has to learn a way of dealing with that strategy. And so on and so on. So, as the system gets better it forces itself to get better Because it's continuously having to learn how to play a better and better opponent It's quite elegant, you know. This is where we get to generative adversarial. Networks. Let's say You've got a network you want to... Let's say you want cat pictures You know, you want to be able to give it a bunch of pictures of cats and have it Spit out a new picture of a cat that you've never seen before that looks exactly like a cat the way that the generative adversarial network works is it's this architecture where you actually have two networks one of the networks is the discriminator How's my spelling? Yeah, like that The discriminator Network is a classifier right it's a straightforward classifier You give it an image And it outputs a number between 0 & 1 and your training that in standard supervised learning way Then you have a generator and the generator Is... Usually a convolutional neural network, although actually both of these can be other processes But people tend to use in your networks for this. And the generator, you give it some random noise, and that's the random, that's where it gets its source of randomness, so That it can give multiple answers to the same question effectively. You give it some random noise and it generates an image From that noise and the idea is it's supposed to look like a cat So the way that we do this with a generative adversarial Network is it's this architecture whereby you have two networks Playing a game Effectively it's a competitive game. It's adversarial between them and in fact It's a very similar to the games we talked about in the Alpha go video. it's a min\/max game Because these two networks are fighting over one number one of them wants the number to be high one of them wants the number to be low. And what that number actually is is the error rate of the discriminator? so The discriminator Wants a low error rate the generator wants a high error rate the discriminators job is to look at an image which could have come from the original data set or It could have come from the generator and its job is to say yes. This is a real image or no. This is a fake any outputs a number between 0 & 1 like 1 for its real and 0 for its fake for example and the generator Gets fed as its input. Just some random noise and it then generates an image from that and it's Reward you know it's training is Pretty much the inverse of what the discriminator says for that image so if it produces an image Which the discriminator can immediately tell this fake? It gets a negative reward you know it's a That's it's trained not to do that if it manages to produce an image that the discriminator Can't tell is fake Then that's really good so you train them in a inner cycle effectively you you give the discriminator a real image get its output, then you generate a fake image and get the discriminator that and Then you give it a real so the discriminator gets alternating real image fake image real image fake image usually I mean there are things you can do where you Train them at different rates and whatever but by default they're generally to get any help with this at all, or is it purely Yes, so if you this is this is like part of what makes this especially clever actually the generator does get help because if You set up the networks right you can use the gradient of the discriminator to train the generator so when I Know you done back propagation before about how neural networks are trained its gradient descent right and in fact we talked about this in like 2014 sure if you were a You're a blind person climbing a mountain or you're it's really foggy, and you're climbing a mountain you can only see directly What's underneath your own feet? You can still climb that mountain if you just follow the gradient you just look directly under me which way is the You know which way is the ground sloping? This is what we did the hill climb algorithm exactly Yeah, sometimes people call it hill climbing sometimes people call it gradient descent It's the same metaphor Upside down effectively if we're climbing up or we're climbing down you're training them by gradient descent, which means that You're not just you're not just able to say Yes, that's good. No. That's bad You're actually able to say and you should adjust yours you should adjust your weights in this direction so that you'll move down the gradient right So generally you're trying to move down the gradient of error for the network If you're like if you're training if your training the thing to just recognize cats and dogs you're just moving it You're moving it down the gradient towards the correct label whereas in this case The generator is being moved sort of up the gradient for the discriminators error So it can find out not just you did well you did badly But here's how to tweak your weights so that you will so that the discriminator would have been more wrong So so that you can confuse the discriminator more so you can think of this whole thing? An analogy people sometimes use is like a a forger and An expert investigator person right at the beginning, you know let's assume There's one forger in there's one investigator and all of the art buyers of the world are idiots at the beginning the the Level of the the quality of the forgeries is going to be quite low right the guy Just go get some paint, and he he then he just writes you know Picasso on it And he can sell it for a lot of money and the investigator comes along and says yeah I do I don't know that's right or maybe it is. I'm not sure I haven't really figured it out And then as time goes on the investigator who's the discriminator will? Start to spot certain things that are different between the things that the forger produces and real paintings And then they'll start to be able to reliably spot. Oh, this is a fake You know this uses the wrong type of paint or whatever So it's fake and once that happens the forger is forced to get better right you can't sell his fakes anymore He has to find that kind of paint So he goes and you know Digs up Egyptian mummies or whatever to get the legit paint and now he can forge again and now of the discriminator the investigator is fooled and They have to find a new thing That distinguishes the real from the fakes and so on and so on in a cycle they force each other to improve And it's the same thing here So at the beginning the generator is making just random noise basically because it's it's it's getting random noisy And it's doing something to it who knows what and it spits out an image and the discriminator goes that looks nothing like a cat you know and then eventually because the discriminator is also not very smart at the beginning right and And they just they both get better and better The generator gets better at producing cat looking things and the discriminator gets better and better at identifying them until eventually in principle if you run this for long enough theoretically you end up with a situation where the Generator is creating images that look exactly Indistinguishable from Images from the real data set and the discriminator if it's given a real image, or a fake image always outputs 0.5 5050 I Don't know could be either these things are literally indistinguishable, then you pretty much can throw away the discriminator And you've got a generator, which you give random noise to and it outputs brand-new Indistinguishable images of cats there's another cool thing about this Which is every every time we ask the generator to generate new image We're giving it some random data, right we give it just this vector of random numbers Which you can think of as being a randomly selected point in a space because you know if you give it If you give it ten random numbers you know between zero and one or whatever that is effectively a point in a 10 dimensional space and the thing that's cool is that as the generator learns It's forced to You if the generator is effectively making a mapping from that space into cat pictures This is called the lateness base by the way generally Any two nearby points in that latent space will when you put them through the generator produce similar cabbages you know similar pictures in general Which means sort of as you move Around if you sort of take that point and smoothly move it around the latent space you get a smooth l\u00e9a varying picture of a cat and so the directions you can move in the space Actually end up corresponding to Something that we as humans might consider meaningful about cats so there's one you know there's one direction, and it's not necessarily one dimension of the space or whatever but And it's not necessarily linear or a straight line or anything But there will be a direction in that space which corresponds to How big the cat is in the frame for example or another dimension will be the color of the cat or? whatever so That's really cool, because it means that by Intuitively you think The fact that the generator can reliably produce a very large number of images of cats means it must have some like understanding understanding of What cats are right or at least what images of cats are And it's nice to see that it has actually Structured its latent space in this way that it's by looking at a huge number of pictures of cats it has actually extracted some of the structure of cat pictures in general In a way, which is meaningful when you look at it? So and that means you can do some really cool things, so one example was they trained Annette one of these systems on a really large Database of just face photographs and so it could generate arbitrarily large number of well as largest the input space a number of different faces and So they found that actually by doing basic arithmetic like just adding and subtracting vectors on the Latent space would actually produce meaningful changes in the image if you took a bunch of latent vectors, which when you give them to the generator produce pictures of men and a bunch of them that produce pictures of women and average those you get a point in your latent space which corresponds to a picture of a man or a picture of a woman which is not one of your input points, but it's sort of representative and Then you could do the same thing and say oh, I only want Give me the average point of all of the things that correspond to pictures of men wearing sunglasses right and Then if you take your sunglass vector, you're you're men wearing sunglasses vector Subtract the man vector and add the woman vector you get a point in your space And if you run that through the generator you get a woman wearing sunglasses right So doing doing basic vector arithmetic in your input space actually is? Meaningful in terms of images in a way that humans would recognize, which means that? There's there's a sense in which the generator really does Have an understanding of wearing sunglasses or not or being a man or being a woman Which is kind of an impressive result All the way along But it's not a truly random thing because if I know the key and I can start want to generate the same Yeah I'm so I mean that's about Unfortunate is the problem with cryptography is that we couldn't ever use truly random because we wouldn't be able to decrypt it again We have our message bits, which are you know naught 1 1 naught something different? And we XOR these together one bit at a time, and that's how we encrypt"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Generative Adversarial Networks (GANs)",
        "Video Title":"A Friendly Introduction to Generative Adversarial Networks (GANs)",
        "URL":"https:\/\/www.youtube.com\/watch?v=8L11aMN5KY8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/8L11aMN5KY8\/hqdefault.jpg",
        "ID":"8L11aMN5KY8",
        "Publish Time":"2020-05-05T18:00:55Z",
        "Channel":"Serrano.Academy",
        "Channel ID":"UCgBncpylJ1kiVaPyP-PZauQ",
        "Transcript":"[Music] hello i'm luis serrano and this video is about generative adversarial networks or Ganz for short develop IDN good fellow in our researches in montreal ganz are a great advance in machine learning and they have numerous applications perhaps not the fanciest applications of Ganz's face generation if you go to this page this person does not exist calm you'll see it in action these images you see here are of people who don't exist they have been fully generated by a neural network this is fascinating considering how detailed our expressions are in this video we learn how to generate faces with again in a very simple way and great news for those of you who like to sing along to the videos as you type code in this video we'll be coding a pair of one layer neural networks which will generate some very simple images and it's all in github under this repo which you can find in the links but if you don't want to write code this video is for you as well since we develop the intuition and the equations by hand as well first let me give you a general idea of what ganzar ganz consists of a pair of neural networks that fight with each other one is called a generator and the other one the discriminator and they behave a lot like a counter feeder and a cup the counter feeder is constantly trying to make fake paintings and the cop is constantly trying to catch the counter feeder the counter feeder is a generator and the cop is a discriminator so as he gets caught by the cop the counter feeder keeps improving and improving his paintings until one day he learns to finally paint a perfect one that completely fools the cop a neural network language what happens is that we have a pair of neural networks the generator and the discriminator and we train them with a set of real images and a set of fake images generated by the generator the discriminator is tend to identify which images are real or which ones come from the generator and the generator is strained to fool the discriminator into classifying its images as real images in this video we'll build a very very simple pair of guns so simple that we'll be able to code them straight in Python without any deep learning packages this is our setting we live in a world called slanted land where everybody looks slightly elongated and walks at a 45 degree angle this world has technology but not as developed as ours they have computer screens which display images but the best resolution they've been able to work out is 2 times 2 so they have black and white 2 pixels by 2 pixel screens also they have developed neural networks but only very simple ones as a matter of fact they only know neural networks of one layer but I will show you that in this simple world we can still create a pair of gas that will generate faces of the people who live in slanted land here are four faces of people who live in slanted land notice that everybody is elongated and tilted 45 degrees to the left since the screens are only 2 pixels by 2 pixels this is how the pictures of the people look in the screen they SMO sucked like a diagonal or a backward slash and this is how noisy images look notice that these are not faces since they don't look like a backlash and these are mostly generated randomly so the goal for networks is to be able to distinguish faces like these from noisy images or non faces like these ones let's attach some numbers here we'll have a scale where a white pixel has values 0 and a black pixel has a value of 1 you may have seen this in the opposite way but we'll do it like this for clarity this way we can attach a value to every pixel in the 2 x 2 screens ok we're ready to build our networks first I'll build them by hand and then I'll show you how to get the computer to train them let's start by building the discriminator the first question is how are we gonna tell faces apart from non faces easy notice that in the faces the top left and the bottom right corners have large values because or pixels are dark whereas the other two corners have small values because their pixels are light on the other hand in noisy images anything can happen therefore the way to tell faces apart is by adding the two values corresponding to the top left and bottom right corners and subtracting the values corresponding to the other two corners and faces this result will be very high whereas in noisy images it will be low for example for this face the value is 2 and the value for the noise image is minus 0.5 we can add a cutoff or a threshold of say 1 and say that any image that's course 1 or higher is a face and any image 2 is course less than 1 is not a face or it's a noisy image a neural network lingo this is how things look the values of our 4 pixels get multiplied by plus or minus 1 depending on what diagonal they are on and then we subtract a total value of 1 or the bias we add these 4 numbers and if the score is 1 or more than the image is classified as a face and if it's less than 1 that is classified as another face in this case the images face because it gets the score of 1 we can also put the probability that something is a face by using the sigmoid function we apply the sigmoid which is this function that sends high numbers to numbers close to 1 and low numbers the numbers close to 0 and we get sigmoid of 1 which is 0.73 the discriminator network then assigns to this image a probability of 73% that it is a face since it is a high probability greater than 50% we conclude that the discriminator thinks that the image is a face which is correct notice that in this neural network the thick edges are positive and that thin ones are negative and this is a convention we'll be using throughout this video now if we enter the second image which is not a face into a discriminator then we do the same calculation we get negative 0.5 for the score sigmoid of negative 0.5 is 0.37 this is lower than 50% so we conclude that this discriminator thinks that this image is not a face the discriminator is correct again now in the same way let's build a generator this is a neural network that will output faces to build a generator will again take into account that along faces these two corners are high value these two corners are low value whereas in noisy images or non faces anything can happen so this is how the generator works first we start by picking an input set which is a random number between 0 & 1 in this case let's say will be 0.7 in general the input will be a vector that comes from some fixed distribution now let's build a neural network what we really want is to have some large values and some small ones the larger ones are represented by thick edges and the small ones by thin edges because remember that we want large values for the top left and bottom right corner and small values for the top right and bottom left corner so since the top output has to be large we want these weights coming in to be large so let's make them plus 1 now what do we get for the output here well first we're gonna get a score of plus 1 times 0.7 plus 1 which is 1 point 7 now let's look at the second note it corresponds to the top-right corner so it has to be a small value so let's put in negative numbers here what do we get for the score we get minus 1 times 0.7 minus 1 which is minus 1.7 in the same way we want a small value here so we put weights of minus 1 and we get minus 1.7 and for the last one we're on a higher value so we put plus ones and we get plus one times zero point seven plus one which is one point seven now those are just the scores we need to apply sigmoid to find the probabilities so we apply the sigmoid and we get 0.85 0.15 0.15 10 0.85 those are the values that will go in our pixels and notice that our image looks like a diagonal which is how we define our faces noticed by the way we built it this neural leopard will always generate large values for the top left and bottom right corners and small values for the top right and bottom left corners no matter what values that we input because remember that is between 0 & 1 therefore this neural network will always generate a face that means it's a good generator of course we built this neural networks by eyeballing the weights but that's not how it's normally done in general we have to train the neural networks to get the best possible weights for this let me tell you a little bit about error functions and error or cost function is a way to tell the network how it's doing in order for it to improve if the error is large then the network is not doing well and it needs to reduce the error to improve the error function that we'll use to train these games is the log loss which is the natural logarithm this is the logarithm base e Y the logarithm well a logarithm appears a lot of error functions from many deeper reasons which will not cover but we can think of it for now so very convenient function let's first say that we have a label of 1 and our neural network predicted is as 0.1 this is a bad prediction and we should produce a large error because 0.1 is very far from what on the other hand it was label is 1 and the prediction is 0.9 then that's a good prediction because the prediction is very close to the label so this should produce a small error how then can we find a formula for this error well notice that the negative natural logarithm of 0.1 is 2.3 which is large while the negative logarithm of 0.9 is 0.1 which is low as a matter of fact the closer number is to 1 the smaller is negative logarithm gets therefore when the label is 1 the function negative logarithm of the prediction is a good error function now let's go in the other extreme when the label is 0 in this case if we had a prediction of 0.1 it would be good because it's close to the label therefore the error should be small on the other hand a prediction of 0.9 is terrible so the error should be large the function that we need here is similar to the previous one with it with slight difference it is the negative logarithm of 1 minus the prediction notice that in the first case the error is the negative logarithm of 0.9 which is 0.1 and in the second case is the negative logarithm is your appointment 1 which is 2 points to be and this match is what we want it which is that the first error is small the second is large to summarize if the label is 1 which means we want the prediction to be 1 we define the law class to be negative logarithm of the prediction notice that from the graph of the negative logarithm of X on the right this is big when the prediction is close to 0 and small when the prediction is close to 1 and then when the label is 0 which means I want the prediction to be close to 0 then we use as an error function the negative logarithm of 1 minus the prediction from the graph of negative logarithm of 1 minus X in the right we see that when the prediction is close to zero there's a low error and when the prediction is close to 1 there is a high error so these two are the error functions that we're going to use for training the generator and the discriminator based on if we want the prediction to be 0 or 1 now that we have our functions we get to the meat of our training process which is back propagation I will explain it very briefly the way we train your networks by first taking a data point and performing a forward pass calculating the prediction and then calculating the error based on the log loss that we previously defined then we proceed to take the derivative of the error based on all the weights using the chain rule this will tell us how much to obtain each weight in order to best decrease the error the way we do the error using a derivative is a process called gradient descent long story short what you do is you plot the arrows back to the then calculate gradient which is the direction of greatest growth and then take a tiny step in the direction of the negative of this gradient in order to find new parameters that decrease this error as much as possible now we're ready to train the generator and the discriminator and what we have to do is put the right error functions on the right places here's our parent neural network and notice that the weights are not yet defined so we start by defining them as random in numbers now we select set some random number between 0 and 1 which is going to serve as the input to the generator we do a forward pass of the generator to obtain some image which is probably in our face since the weights are random this is gonna be our generated image now we pass that generated image through the discriminator so the discriminator can tell if it's fake or not the discriminator outputs a probability so let's say that it's for example 0.68 now pay close attention because this is where the rubber meets the rope this is where we define the correct error functions first let's think what does the discriminator want to do here in other words if the discriminator was great what should it output well since the image is not a face but it's a generated image from the generator then the discriminator should be saying that it's fake that means that the discriminator should be outputting a 0 if we remember the error function the way we measure an error when we want the neural network to output a 0 is the negative logarithm of 1 minus the predation this is an error that will help us train the discriminator now that we've figured out what the discriminate at once let's go to the generator what does the generator want well the generators wildest dreams are to generate an image so good so real that the discriminator classifies it as real therefore the generator wants this whole neural network the connection of the two to output a 1 that means that the error function from the generator is negative logarithm of the prediction so that is the error function that will help us train the weights of the generator in other words if G of Z is the output of the generator and D F G of Z is the output of the discriminator then the error function for the generator is negative logarithm of D of G of Z and the error function from the discriminator is negative logarithm of 1 minus D of G of Z the derivatives of these two are what will help us update the weights of both neural networks in order to improve that particular prediction notice that these two error functions fight against each other but that is okay because the read error function only changes the generator weights and the blue error function only changes the discriminator weights therefore they don't collide they simply improve both neural networks as one to produce different outputs which is fascinating so what we have to do now is repeat this process many times we pick a random value for Z we apply the generator to produce a fake image apply the discriminator to that image and use back propagation to update the weights on both the generator and the discriminator then we take a real image plug it into a discriminator on updated weights again using back propagation so what happens after many of these iterations or epochs well we left the training in the notebook and we got these values for the weights to make it easier I've drawn thick edges for the positive weights and thin edges for the ways that are negative or zero feel free to pause the video and convince yourself that these two neural networks actually work really well both the generator for generating realistic with them faces and the discriminator for being able to tell apart the faces from the non-faces and the reason for this is that if we remember from the beginning of the video the top left and bottom right corners off a face should be big and the other two should be small so let's just convince ourselves let's look at the generator notice that since the input is between 0 & 1 and the tube top edges are positive then the sigmoid value of this output is large which is the value this pixel over here therefore that pixel has a large value similarly these 2 values are also positive and they give us a large value for this pixel over here now these two over here are negative so they give us a small value for this pixel and these 2 over here negative so they give us a small value for this one and therefore our image looks a lot like a diagonal and given the resolution of the screens and slanted land that are all two pictures by two pixels and this picture looks and candidly like this resident of slanted land over here thus we have built a network that generates faces now as promised here's the code for you to sing along to the video it's in this repo called Yuans and they're my github first we have the faces that we hard-code and the random noise the images that we generate that are not necessarily faces then we also develop the derivatives carefully for the discriminator based on faces then for the discriminator based on noisy images both with respect to the weights and respect to the biases these are all coded in this discriminated class we also work out the derivatives corresponding to the error functions for the generator again with respect to the weights and the bias and these are all coded in the generated class we also have error function plots we can plot the error function for the generator and the error function for the discriminator notice that the generator error function goes down and stabilizes but since the generator and so following the discriminator then the discriminative function doesn't do so well and actually goes up at the end and finally we ask our generator to generate some random images and here they are notice that they all look like faces and slanted land which is what we wanted from the beginning therefore we have successfully created a pair of gas that generate faces in slanted lead now time for some acknowledgments this video would not be the same if not for the help of my friend so a big thanks to Diego Sahil and Alejandra who helped me in various ways either encourage me to learn Gans more seriously or helping me with endless questions or gave me great feedback on my code Diego in particular has a great series of blog posts where he cold scans from scratch and by torch and tensor flow I use them as an inspiration for this video so I highly recommend them and that's all for today I hope you've enjoyed it I'd like to remind you that I have a machine-learning book called rocking machine learning in which I explain the concepts of machine learning and down-to-earth way with real examples for everybody to understand in the description you can find the link to the book and a very special 40% discount for the viewers of this channel and as usual if you enjoyed this video please subscribe to my channel for more content or hit like or share amongst your friends and feel free write comment I really enjoy reading your comment especially those with suggestions for future topics and if you'd like to tweet at me my twitter handle is luis likes math all the informations videos writings etc can be found at this link serrano academy so check it out thank you very much for your attention and see you in the next video you"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Generative Adversarial Networks (GANs)",
        "Video Title":"The Math Behind Generative Adversarial Networks Clearly Explained!",
        "URL":"https:\/\/www.youtube.com\/watch?v=Gib_kiXgnvA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/Gib_kiXgnvA\/hqdefault.jpg",
        "ID":"Gib_kiXgnvA",
        "Publish Time":"2020-06-20T14:00:08Z",
        "Channel":"Normalized Nerd",
        "Channel ID":"UC7Fs-Fdpe0I8GYg3lboEuXw",
        "Transcript":"hello people from the future welcome to normalise nerd in this video I'm gonna explain the gang yes the famous generative adversarial networks I know that this is one of those topics if you don't approach it properly then this might feel really intimidating but trust me by the end of this video you will feel very comfortable with gangs now I put a lot of effort in making these videos so if you like my content please subscribe and hit the bell icon let's get started okay the first thing that you need to know is gang is not a single model it's a combination of two models the first one is a generative model called G and the second one is a discriminative model called D now what the hell are discriminative and generative models well in machine learning we have two main methods for building predictive models the most famous one is the discriminative method well in this case the model learns the conditional probability of the target variable given the input variable most common examples are logistic regression linear regression etc on the other hand in a generative model the model learns the joint probability distribution of the input variable and the output variable if the model wants to predict something then it uses Bayes theorem and it computes the conditional probability of the target variable given the input variable the most common example is the naive Bayes model the biggest advantage of generative models over discriminative models is that we can use generative model to make new instances of data because in this case we are learning the distribution function of the data itself which is simply not possible using a discriminator in our Gantz we are using this generative model to produce new data points that is we are producing fake data points using our generator and we are using this discriminator to tell if a given data point is an original one or it has been produced by our generator now these two models work in an adversarial setup that means they compete with each other and eventually both of them gets better and better in their job let me show you the structure of this thing okay so here's the high-level view of our gang this G and D are nothing but multi-layered neural networks and this theta G and theta D are just the weights okay we are using neural networks here because they can approximate any function we know that from the universal approximation theorem now look at here suppose this is our distribution function of the original data now in reality we can't really draw that or even mathematically compute that because we input images we mean put voices we input videos and they are higher dimensional complex data so this is only for mathematical analysis okay and look at here this is a noise distribution and you can see that this is just the normal distribution I am taking and I am gonna sample randomly some data from this distribution and we'll feed that to our generator will to get something from the generator we must input something right and we are inputting here noise that means this Z contains no information and after passing this Z to our model generator it will produce something called G of Z now look at that I have described the distribution of the G of Z with the same X that I have written for the earlier data well I am doing this because the domain of our original data is same as the range of G of Z this is important because we are trying to replicate our original data so just remember the short forms when I say P data this represents the probability distribution of our original data when I say PZ it represents the distribution of the noise and when I say PG it represents the distribution function for the output of our generator and we are going to pass reconstructed data and original data to our discriminator and this will provide us a single number and the single will tell the probability of the input belonging to the original data so you can see this discriminator is just a simple binary classifier and for the training purpose when you are putting the original data to the discriminator we will say Y is equal to 1 and when we are going to pass reconstructed data we will say the level is 0 and the D will try to maximize the chances of predicting correct classes but G will try to fool D so we can say that G and D are playing a two-player minimax game what the hell is that well a minimax game is just a two-player game like tic-tac-toe where we can interpret the objective as one player is trying to maximize its probability of winning what the other player is trying to minimize the probability of winning of the first player okay now we are saying about maximizing and minimizing but what should they maximize or minimize we need a mathematical expression right and that's called as value function let me show you the value function for this minimax game here well this is the value function for gaen and here mean and Max simply represents that G wants to minimize this expression and D wants to maximize this expression now I know that at first this might feel gibberish but if you look here closely you will find that this expression is surprisingly similar to the binary cross Centauri function and if you are feeling like that then you are absolutely correct let me show you why so this is our ordinary binary cross interval function for a moment just ignore the negative sign and the summation so this is just the binary kazantip function for one input right why is the ground truth that is the label and Y hat is just the prediction of the model when y is equal to 1 that is when we are passing the original data the wipe read is equal to D of X so if you just replace these things in the formula you will get lost to Ln of D of X now when we are giving the data as our input the wipe red will be d of G of Z because obviously first we have passed the noise to our generator and it has produced something and then we are giving the produced fake data to the model B and if you replace these things in the function you will get Ln of 1 minus D of G of Z now let's combine them so I have just added them together and we get this does it look similar to the value function yes but here we are missing the capital E's at the front well they are just expectations understand that this expression is valid for only one data point but we have to do this for the entire training data set right and to represent that mathematically we need to use expectation well expectation is just the average value of some experiment if you perform this experiment a large number of times suppose you are playing a game where you need to roll a die and your score is the number on the upper face so if you play this game for a really long time then the expected score is 3.5 the formula is very simple you just need to add all the possible outcomes multiplied with their probability so it's kind of a weighted mean so let's apply the expectation on this equation and look at here that we are adding all the scores with their probability same thing goes for here but this is only true for a discrete distribution if we assume that our P data and PZ are actually continuous distribution then the integral sign will replace the summation and we have to place the DX and DZ accordingly and this whole thing is written in the short form as e ok so now you know the value function for Gann does it look intimidating now I don't think so now I'm gonna tell you how we optimize this function in practice well this is our big training loop and just like every other neural network we have to optimize the loss function using some stochastic process I am using here the stochastic gradient descent okay so first we enter our big training loop and we fix the learning of G and then we are entering the inner loop for B well this loop will continue for K steps okay and in this loop first we take m data points from the original distribution and M data points from the fake data okay and then we update the parameters of our discriminator by gradient ascent why because remember that our discriminator is trying to maximize the value function so after we have performed K updates of D we get out of this loop and we fix the learning of D now we are going to train our generator for this case we take only M fake data samples and update the parameters of our generator by gradient descent why because remembers generator is trying to minimize the value function now you might ask why I haven't taken this portion in the update step of generator well look closely does this expression contains any term corresponding to the generator no so the partial derivative of this term with respect to theta GU will be zero that's why we are taking only this portion one important thing you should note that for every key updates of the discriminator we are updating the generator once okay if you have understood this video so far then you know what is the value function for Gann and how we optimize this in practice but if you are like me and want to know what is the guarantee that our generator will surely replicate the original distribution then take a deep breath and continue watching okay just to be clear we want to prove that PG will converge to P data if our generator is able to find the global minimum for the value function in other words we want to show that PG is equal to P data at the global minimum of the value function okay this is a two-step process first of all we are fixing the G we wanna see for which value of the discriminator the value function is maximum look here that I have replaced G of Z with X well we can do this because the domain of both of them are same now if you differentiate this then you will see that the maximum value of this expression will occur if the d of x attains this expression P data over P data plus P G well obviously one can differentiate that and attain this expression but let us look into it ibly so we can represent our value function like this formula a ln x plus b ln 1 minus x and we want to find the value of x for which this expression is maximum so if I take B is equal to 0.6 and a is equal to 0.45 then you will see that the graph looks something like this and the Maxima occurs at point 4 to 9 which is nothing but a Upon A plus B now let's fix the BX as this and replace that into our value function so after fixing D and substituting that in the value function we get this and after a little modification we are getting this long expression and here mimsey just represents that G will try to minimize this thing now understand what we want to do here well we want to prove that probability distribution of generator will be exactly same as the probability distribution of the data so it makes sense to talk about some of the methods to measure the difference between two distributions and one of the most famous methods are G is divergence that is Jenson Shannon divergence now if you look at the formula for J's divergence then it looks surprisingly close to this long expression isn't it just for a refresher this e here just represents the expectation in the first portion to find the expectation of this value we are using the probabilities from the first distribution but in the second portion we are using the probabilities from the second distribution okay now let's see if we can somehow get to the J is divergence from this thing okay so after the little modification we are getting this so what have we done here we have just multiplied two in these two logarithms and for this we need to subtract two times the Ln two here all right and if we look closely here then this whole portion is actually equals to two times the J is divergence of P data and PG and obviously we have the negative two Ln two here so G wants to minimize this what is the minimum value of this expression well the J is divergence between any two distribution cannot be negative the minimum it can get is zero and it will attend zero only when p1 is equal to p2 that is if P data is equal to P G then only this term will be zero and the whole expression will attain its minimum that is minus 2 Ln 2 so voila now you we have proved that add the global minimum of our value function the P G will be exactly same as P data and our generator is actually trying to attain that state now let me show you how G achieves that state that is different phases of training so at the beginning neither the discriminator nor the generator knows what they are doing so the P G is not replicating the P data and the classifier discriminator is not classifying as well after updating the theta D that is when the discriminator has learned something so the classifier will be better so now the discriminator can actually distinguish between the real data and the fake data now after the generator has learned something look at that the distribution P G is now closer to the P data and the discriminator is trying to predict the true level of the data points but it is not performing as well now at the end when the generator has attained the minimum of the value function then it has successfully replicated the distribution function of the data point so now PG is indistinguishable from P data so now it is impossible for the discriminator to tell which data point is an original one and which data point is a generated one so the discriminator will output 0.5 for every input and that is what we want to achieve well this is a very simplistic view of the gaen in reality training the Gann is really hard the goal of this video was to make you understand Gantz I hope you are now very comfortable with the concept of ganz and if you have understood everything that I have talked about in this video then do congratulate yourself because now you know the math behind one of the finest inventions in AI I hope you have liked this video please share this video and subscribe to my channel stay safe and thanks for watching [Music]"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Variational Autoencoders (VAEs)",
        "Video Title":"Variational Autoencoders",
        "URL":"https:\/\/www.youtube.com\/watch?v=9zKuYvjFFS8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/9zKuYvjFFS8\/hqdefault.jpg",
        "ID":"9zKuYvjFFS8",
        "Publish Time":"2018-02-25T16:21:11Z",
        "Channel":"Arxiv Insights",
        "Channel ID":"UCNIkB2IeJ-6AmZv7bQ1oBYg",
        "Transcript":"hello everybody and welcome back to our kevinsites so in a lot of real-world problems we have a whole bunch of data that we're looking at it could be images or text or audio whatever it is but the underlying factor the underlying process of the data could be much simpler in a much lower dimensional space than the actual data that we're looking at so a lot of techniques in machine learning they try to compress the dimensionality of your data into a smaller space one very popular technique that's used a lot in recent papers is called variational autoencoders it is going to be a pretty technical episode so I hope you're ready to dive into the mechanics of variational autoencoders my name is Xander and welcome to archive insights [Laughter] [Music] so before we dive into the mechanics of variational autoencoders I first want to introduce normal autoencoders so I'm going to assume that you're already familiar with the neural network architectures that we have things like back propagation and all of that so what an autoencoder does is it takes some kind of input data it could be an image or a vector anything at all with a very high dimensionality it's gonna run it through this neural network and it's gonna try and compress the data into a smaller representation it does this with two principal components the first component is what we call the encoder the encoder is simply a bunch of layers they could be fully connected layers or convolutional that are going to take the input and they're going to compress it down to a smaller representation which has less dimensions than the input and this is what we call the bottleneck and then from the bottleneck is going to try and reconstruct the input by using again fully connected or convolutional layers and then the last function of training an auto encoder is simply looking at the reconstructed version at the end of your decoder network and then you're going to simply compute the reconstruction loss with respect to your input and by simply comparing pixel to pixel differences in the output we can create a loss function and we can start training our network to compress images and so obviously you have simple encoders that use fully connected layers but you can just as well swap them out with convolution if you're working with images or something like audio for example and if you look at what's going on here if you train a deep convolutional net work to do encoding and decoding of a whole bunch of images you're actually creating a whole new kind of compression algorithm and Google is actually thinking of using these types of networks for reducing the amount of bandwidth that you use on your phone so if you download an image then the full resolution image is first downscaled then it's sent through you over the wireless internet connection and then in your phone there is actually a decoder that reconstructs the full resolution image from the compressed representation and if you apply this to something like m-miss for example then it's very interesting to see what these hidden representations are actually learning so here you can see a bunch of images where on the left side you can see the input digits that are being fed through the network and then on the right side all of those are reconstructed images but you can see what happens if we change the size of the hidden representation so if we use only a 2d hidden representation that means that our bottleneck you know in the middle of the network is only two variables then we get reconstructions that look pretty okay but they are very fuzzy and the fuzziness is because you force the entire information of your image to go through two single variables and then when you reconstruct obviously you lose some of that detail and that is why the images look so fuzzy if you use more dimensions in your latent representation that you can get reconstructions that are much clearer and much sharper but you need more information in that bottleneck and it's interesting to note that the exact same technique is applied to image segmentation as well so you take an input image you run it through your convolutional encoder it goes through a bottleneck representation and then it gets remapped to a full output image but in this case instead of reconstructing the original image you're actually trying to target a segmented version of your image and it's exactly this type of network that is used in self-driving cars to segment the different parts of the public road into specific objects that a car needs to detect okay so that's the basic idea behind auto-encoders but there are a few very clever tricks that you can apply to an auto encoder to have it do some really fancy stuff so imagine that you start with a normal m-miss digit it's a clean image nothing's wrong with it but then you add a whole lot of to it and you're going to run that noisy image through your encoder network you get through the bottleneck representation and then you try to reconstruct the image but instead of reconstructing the noisy image what you're going to do is try and reconstruct the original clean image and if you train this network and a whole bunch of these noisy and nice digits you're going to try and force the encoder step to actually get rid of the noise and this is what we call a denoising auto-encoder and so you can see here that by using this approach you can actually train a denoising auto-encoder that is very good at removing noise from input images and denoising images isn't the only thing that you can do with this type of approach so in this case for example you take an input and instead of adding noise to it you simply crop a rectangular area out of the image and you throw it away you replace it with white or black pixels you feed that input image through the network and you try to reconstruct the original full image and this technique is what we call neural impeding it's where you take a small part of the image you throw it away and then you ask the network to reconstruct whatever was there in the input image and with this approach you can do simple things like removing watermarks from images but you could also remove a parked car for example if you are filming on a movie set in a natural setting okay so now that we have the basic concept behind a normal auto encoder let's introduce variational autoencoders so the idea behind variational auto-encoders is that instead of mapping any input to a fixed vector you want to map your input onto a distribution and so the only thing that's different in a variational auto encoder is that your normal bottleneck vector C is replaced by two separate vectors one representing the mean of your distribution and the other one representing the standard deviation of that distribution and so whenever you need a vector to feed through your decoder network the only thing you have to do is take a sample from the distribution and then feed it to the decoder and so to train a variational auto encoder the loss function in this case actually consists of two terms the first term represents the reconstruction loss so this is really the same as the auto encoder step except that here there is an expectation operator because we are sampling from a distribution and then the second part of the loss functions is what we call the KL divergence I'm not going to go into all of the details because there is a lot of math involved there but basically what you want to make sure is that the distribution that you're learning is not too far removed from a normally distributed discussion so you're going to try and force your latent distribution to be relatively close to a mean of zero and a standard deviation of one and so finally before we can start training our variational autoencoder we have one final trick that we have to use because if you look at the computation graph of our network right now we have a problem in the middle of that mat work after the bottleneck we have a sampling operation there is a node there that takes a sample from a distribution and then feeds that sample through the decoder but the problem is that you cannot run back propagation you cannot push gradients through a sampling node so this is an issue and so in order to run your gradients through the entire network and train everything end-to-end we're going to use what we call the reaper ammeter ization trick and so the trick goes as follows if you look at the latent vector that you're sampling you can actually look at that vector as the sum of a fixed mu which is just the parameter that you're learning plus some kind of a Sigma which is also a parameter that you're learning and then multiplied with an epsilon and this epsilon is where we're gonna put the Siq a stick part so this epsilon is always going to be standard caution it's always gonna have zero mean and standard deviation of one we're gonna sample from that epsilon and then multiply it with Sigma ad mu and we have our latent vector and so the clever thing here is that now our mu and our Sigma those are the only things that we actually want to train so there we have to be able to compute gradients and run back propagation but that epsilon well that doesn't really matter because we don't want to change that epsilon ever again that epsilon is a fixed elastic node okay it's still sarcastic but we don't have to run back propagation through it so it doesn't matter that it's sampling operation and so this is the repair matter is a tient trick where instead of having a full stochastic node that is blocking all of your gradients because you can't do back propagation through it you're going to split it up into a part where you can do back prop and then another part which is still stochastic but which you don't want to train because it's fixed pretty clever right so let's take a quick look at some code in tensor flow so here you can see the encoder network which is training two sets of parameters the means and the standard deviations of our distribution and then in the actual auto-encoder we're going to do a sampling operation from the distribution to actually get our latent vector and then the you can see where they compute the KL divergence and then you're actually going to compute your loss and back proc through it alright and so before we go and look at some visual results of what you can do with variational autoencoders I want to note one final thing there is a new class of variational autoencoders which has a lot of promising results that's called disentangled variational autoencoders and the basic idea behind this disentanglement is that you want to make sure that the different neurons in your latent distribution but they are uncorrelated that they all try and learn something different about the input data and so to implement this the only thing you have to change is add one hyper parameter through your loss function that weighs how much this KL divergence is present in the loss function and so in the disentangled version the auto encoder will only use a specific latent variable if it really has a benefit and if it doesn't benefit the compression then it will simply stick to the normal so in order to show the results of a disentangled representation let's look at a very simple data set the data set consists of images that are generated from for latent factors so you have the x position the y position the size of the objects and the rotation of the object and by just picking a sample from that distribution you have four values you can just generate an image that is generated from exactly that hidden representation and then the idea is if you train a disentangled variational auto encoder what you would like to see is that the auto encoder is able to reconstruct and come up with that exact mapping of those four latent variables to encode the information in its inputs and it turns out that if you use the normal loss function of a variational auto encoder it simply comes up with a whole bunch of latent spaces but it's not really finding exactly those latent variables that we use to generate the images but if you disentangle your representations it gets much closer so here on the left side you can see that by increasing that beta factor in your auto encoder you're actually forcing your auto encoder to map the information onto only a few of those latent variables so instead of using all ten of them the autoencoder only uses five of the latent variables to encode the information and you can see that the first one represents the Y position the second is the x position then you have the scale which is the third one and then in fact there are two latent representations that the auto encoder used for representing the rotation of an object but interestingly all the other latent representations even though they are there they are still fixed at the harshin distribution and this is because they weren't really necessary to encode the information of our input and here you can see really interesting results where they applied variational autoencoders at google deepmind's on their deepmind lab environment so you can see a 3d world where an agent can sort of run around and what they did is they they compress the input images that the agent is seeing in two latent space and then they reconstruct it but what you can also do is you can start changing the latent variables and then see what happens to the reconstruction and so it turns out that if you use a disentangled variational auto encoder then changing the latent variables actually corresponds to some very interpretive things so here you can see that changing the first latent variable actually changes the color of the floor but nothing else and then there are other latent variables that correspond to turning to the left or turning to the right and there are even some that changed the rotation and the identity of specific objects that the agent is looking at and in contrast if you don't use this disentanglement then whenever you start changing a latent representation everything starts blurring up in the image and it's not really clear what this latent vector was trying to encode and so I think this image sums everything up on the left side you have a disentangled variational auto encoder and you can see that if you change the first I mention in your latent space then the face is rotated but nothing else changes if you do the same thing in a normal variational or encoder the face also rotates but you can see that a lot of other stuff is changing as well and then as a comparison on the right side you can see the results for a generative adversarial Network and so the holy grail of disentangle variational auto-encoders is to have some kind of a network that can extract very useful causal features from a very high dimensional space and then use those for some tasks that it's trying to learn and the hope is then that those learned features will also generalize to the mains outside of your training data and so one of the common domains where people are trying to apply variational autoencoders is for example in reinforcement learning because the whole problem in reinforcement learning is that you have very sparse rewards and it takes a really long time to train anything so by using this variational autoencoder as some sort of a feature extractor the hope is that you can actually run your agent on the representation on the compressed representation instead of on the full input space and so when using this in practice there is actually a very clear trade-off if you disentangle the latent space too little then your network is sort of overfitting because you give it too much freedom it can just learn how to reconstruct your input training data but it won't generalize to unseen data in new cases on the other hand if you disentangle too much then you actually lose a lot of the high definition detail in your input and this can actually hurt performance in a lot of applications so personally I find this a really interesting idea and I'm very curious if this will lead to some types of networks that can learn to extract very useful low dimensional information from very high dimensional spaces because in the end we want to train agents that are able to understand the world by compressing a whole lot of information and then learning useful behavior on that latent space all right congratulations if you made it to the end of this video I hope you learned something don't forget to subscribe and I'd love to see you again in the next episode of archived insights [Music]"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Variational Autoencoders (VAEs)",
        "Video Title":"Understanding Variational Autoencoders (VAEs) | Deep Learning",
        "URL":"https:\/\/www.youtube.com\/watch?v=HBYQvKlaE0A",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/HBYQvKlaE0A\/hqdefault.jpg",
        "ID":"HBYQvKlaE0A",
        "Publish Time":"2024-04-09T15:14:08Z",
        "Channel":"DeepBean",
        "Channel ID":"UCvKxZESX9YYpmAGETmcrAmQ",
        "Transcript":"today we're going to be looking at variational autoencoders or vaes the vae is a type of generative model that means simply that it's capable of generating completely synthetic examples of a particular type of thing and this thing might be faces or chairs or whatever it is depending on the data set that you provide the idea is that the model will somehow have leared the distribution of the data and how it's able to do things such as sample from that distribution to create completely artificial examples or even things like anomaly detection or in painting now more concretely assuming that we have some true distribution of the data P star we want to build some probabilistic model P parameterized by Theta that will approximate P Star as closely as possible in other words if we have a data set of faces then P of X should register a high value for images of faces and a low value for images of non-faces now in statistical terms P of x is what we call the likelihood of data x given the model parameters Theta so what we're trying to do is to adjust Theta to achieve the maximum likelihood of observing the data points actually included in our data set D in other words it's a case of Maximum likelihood estimation now this is a visual example with some one-dimensional X so imagine we have some examples represented by the Dots here and what we want to do is maximize P of X as much as possible Over The observed data points and this means concurrently minimizing it everywhere else now if we have some sufficiently complex function with parameters Theta then depending on Theta this function can look like pretty much anything but we want to find the Theta that maximizes the likelihood of the data that we actually observe now given a sufficiently large data set vaes can do this and in this video I'll explain how so to get an idea of how this works let's take a simpler example say circles we want our model to learn the distribution of circles so here we first assume that there are some underlying factors of variation that are used to generate this particular type of thing that we call circles and we can call them position and radius so using just these two variables we can account for all the variation within the domain of circles and we'll call these underlying factors the latent variable Z they're called latent variables because we never see them that is to say we never directly see the position or radius of a circle we just see the circle itself represented by some pixels in an image nevertheless we get the sense that the structure of the data is in fact captured by these latent variables and the data representation is just their Shadow for example if you're telling your friend how to draw one of these circles you'd presumably specify its radius and position rather than listing out every pixel coordinate that is filled and every pixel coordinate that is empty now this intuition also lies behind why the the number of latent variables is usually a lot lower than the number of data variables which in this case is our pixels the latent representation is in a sense a maximally efficient compression of the data so in a sense what we have here is a representation learning problem how do we find an efficient latent representation that captures the factors of variation inherent in the data so now that we have some latent variables Z in addition to our data variables X and these coexist with one another what we have is a situation where for every data point there is both a data representation and a latent representation furthermore we should be able to infer the latent variables from the data variables and conversely to generate the data variables given the latent variables where these occur via some conditional distributions the importance of theta here is that it ensures that for example we generate images of circles instead of squares or triangles say and inversely that we extract the underlying features of circles during inference instead of the underlying features of anything else so really Theta is important in capturing the data set that we want so essentially what we're trying to find is a joint distribution P of Z and X Now using the chain rule of probability we can write this as the probability of Z independent of X and the conditional probability of x given Z now we don't really have any prior knowledge of how Z should be distributed so in practice we just make up some distribution as our P of Z for continuous latent variables this is typically a multivariate unit gaussian as we'll see later but in theory it could be anything we choose before moving on it's worth mentioning that given this factorization we can see that the vae is encouraged to map the full data distribution D to a latent distribution P of Z so if we choose a unit gaussian as our P of Z then sampling from this unit gasan will produce realistic synthetic examples in data space all we have to do now is optimize Theta but for that we have to maximize our original Target P of X and now this can only be found by integrating The Joint probability distribution over all possible values of Z in other words by marginalizing over Z this is why P of X can now be called the marginal likelihood of X in this context now remember that we want to optimize Theta such that P of X is high for data points that belong in our data set so ideally if we could calculate P of x given Theta we could compute the gradient of this with respect to Theta say over a batch of X and optimize Theta by gradient descent or something like that however there is just one slight problem and it's that this integral here is intractable in other words it's infeasible to compute both because it has no analytic solution and because it cannot be efficiently estimated for example if we were to use numerical integration techniques we'd require a number of samples that increases exponentially with the number of latent variables so we can't do this in cases where there's an appreciable number of factors of variation in the data so how do we optimize Theta well the vae works around this by realizing that we don't actually need to compute P of X itself we just need some way of increasing it as much as we can for the examples in our data set and it does this within a basian framework so let's quickly recap Bas rule say we have some model and it has some parameters H which in a sense form a hypothesis bases rule can now be used to estimate the probability of a particular hypothesis H given some observations that we call the evidence e so here P of H is the probability of that specific hypothesis in the absence of any evidence this is called the prior probability because it's the probability of H prior to us having seen any observations P of e given H is the conditional probability of observing the evidence given that the hypothesis is true this is called the likelihood it's the likelihood of observing the evidence given the hypothesis the denominator P of e is the probability of observing the evidence under all possible hypotheses this is called the marginal likelihood because it's been marginalized over all possible hypotheses it's also called the model evidence because it reflects the degree to which the data supports the overall model irrespective of the parameters now finally P of H given e is called the posterior probability and this is the probability that our hypothesis is correct in light of our observations e so if we observe some water droplets falling on our head we might assign high probability to the hypothesis that it's raining whereas in the absence of those observations we may not now we can adapt basis rule to our situation now our hypothesis concerns the values of Z and the model evidence is the likelihood that our distribution parameters Theta are appropriate to the data X that we observe so ideally we'd want to maximize the model evidence and for this we'd have to find some Theta for which the right hand side here is maximized however again the problem is that because the marginal likelihood is intractable the posterior is also intractable since the marginal likelihood is required in the calculation of the posterior nevertheless this simple relationship means that if we know one then we know the other given that the joint probability here is actually tractable this means that we can actually shift our attention from trying to directly calculate P of x to instead trying to approximate the posterior instead and it turns out that we can do this using the method of of variational inference now variational inference is essentially the process of approximating some Target distribution p with an approximation Q parameterized by fi and if P represents a basian distribution and this is often called variational b or variational basian inference now this Q should represent some sufficiently flexible family of distributions such that by optimizing fi we can push these two distributions as closely together as possible now before moving on it's worth mentioning that since we are sharing the parameters fi over all our data points X this is a form of amortized variational inference now in traditional variational inference we optimize our parameters f for every data point independently and as you can imagine this gets very computationally expensive especially if we have many data points in contrast in amortised inference we share the parameters across all our observations Which is far more efficient and allows us to deal with unseen examples without any reoptimization now this is called amortised inference since the term amortization generally refers to the spreading of costs and we are essentially spreading computational cost over multiple data points now one common objective used for variational inference is the cback lier Divergence or KL Divergence from P to Q This is defined as the expected information loss suffered when we adopt Q as our model while the true distribution is p hence it's a kind of relative information entropy that we incur by diverging from our true distribution P to the approximation Q one important property of the KL Divergence is that it can never be negative and this is because it does not make sense for the expected information loss to be negative if p is equal to Q then this is just zero so let's find the K Divergence from our Target to our approximation now in the right hand side we can split the quotient in the log and then separate out log P of X through this derivation here now for that specific term we don't have an expectation sign since P of X is independent of Z so the expected value of log P of X over Q is just log P of X now we can immediately see that this KL Divergence actually cannot be computed directly and this is because the marginal likelihood of X is intractable as we've already seen however we can begin to see how this formulation helps us deal with the intractability of P of X so first let's reshuffle and we can see that the log likelihood is now the sum of the KL Divergence and this extra term here we know that the KL Divergence from P to Q is non negative so we know that our log likelihood here on the left hand side is always greater than or equal to this extra term hence our term here essentially sets a lower bound on the value of the log likelihood since in a basian context P of X is called the model evidence we call this term the evidence lower bound or the elbow now for the sake of completeness it's worth mentioning that you can also alternatively derive the elbow using a mathematical result called Genson inequality this result basically states that the expectation of a concave function is always less than or equal to that function applied on the expectation and by concave function we just mean any function with the line connecting any two points on the curve always lies below the curve how is this useful well if we take our log likelihood and express the marginal likelihood as an integral we can then express it as this expectation value over here now because the log function is concave we can use the instance's inequality to arrive at this result here which as we can see is our uh evidence lower bound so anyway using this result we can think of the K Divergence from P to Q as essentially the gap between the elbow and the actual log likelihood so in other words a lower K Divergence increases the tightness of this bound now we can see from this picture that maximizing the elbow by optimizing fi and Theta will simultaneously do two things one it will maximize the log P of X and two it will minimize the the K Divergence from the true posterior P to the approximation Q in other words both the generative model and the inference model are simultaneously optimized all this without having to explicitly calculate P of X itself now the next question is how can we maximize the elbow of a large set of data points well one natural approach would be stochastic gradient descent with respect to parameters F and Theta given a data set D we ultimately want to maximize the elbow over all the data points in D and an efficient way of doing this is just to sample mini batches and perform gradient descent over the parameter space where the loss function is just the negative of the elbow now the trouble is we can easily differentiate the elbow function with respect to Theta to obtain the gradient but it's problematic to differentiate it with respect to fi and this is because fi itself defines the distribution under which the expectation is taken to see why this is an issue let's analyze the gradient first let's find the gradient with respect to Theta from here we can expand the expectation into an integral over Z then bring the gradient into the integral then we realize that the gradient of the expectation is just the expectation of the gradient now expectations are easy to estimate because we could just use some simple procedure like naive Monte Carlo sampling we just extract number of samples from the distribution then find the mean also notice that we've removed the log Q term here because it doesn't depend on Theta so this is all quite straightforward in contrast the gradient with respect to fi is a little tricky to see this we can first expand the expectation as before then bring the gradient into the integral but now we have to include Q within the gradient term as well since it also depends on fi now from here we can apply the product rule of differentiation then split this into two integrals now it's easy to see that the first term can be expressed as an expectation over Q therefore it can be easily estimated however this second term is not an expectation and so cannot be estimated easily and we cannot directly compute it either since in the general case we don't have an analytic solution to the FI gradient of Q now it turns out the solution here is to replace Q with with an equivalent distribution that is not parameterized by F and to do that we simply Express Z as some function G of f x and a random variable ETA which is sampled from P of ETA now the trick is to Define G such that F and X influence Z deterministically and all the stochasticity of the posterior distribution comes from this EA variable whose distribution remains constant throughout training we have essentially externalized the randomness by transferring it from Z to Ita now this change of random variables is called the reparameterization trick and it allows us to efficiently estimate the gradient of the elbow so now the expectation is taken not over q but over P of theta which allows us to estimate it using Monte Carlo sampling since now the gradient of the expectation is the expectation of the gradient this whole scheme of estimating the AL using the repot parameterization trick is called the stochastic gradient variational base estimator now before moving on it's worth reexpressing our elbow here into a form that enables some further Insight so if we just Shuffle around the terms here we can express the elbow in a way that explicitly contains a KL Divergence term this time between q and the prior of Z now we can see that we can again estimate our first expectation term using Monte Carlo sampling and of course we can do that with our second expectation term as well which represents the KL Divergence however with some design choices we can actually give the KL Divergence an analytic form which means we can directly calculate rather than estimate it which is lot more efficient in practice this is often ensured by setting both q and P of Z as gaussians each gaussian is parameterized by mean mu and standard deviation Sigma with the prior commonly defined as a multivariate unit giian as for Q the MU and sigma values are computed using some deterministic model on X and then Z is sampled using an ETA that is itself sampled from a unit gaussian since the gaussian distribution has an analytic form we can now Express the K Divergence as an analytic function of our me and standard deviation of our Q we start with our gaussian probability distribution function then simply plug that into the K Divergence formula and work out the mathematics using some identities we arrive at a pretty simple function which is commonly used to train vaes with gaussian posteriors and unit gaussian priors hence now we only need to estimate the log likelihood now that's all well and good but our next challenge is to figure out how to to efficiently estimate the log probability here for our data point x well we've seen that we have to estimate the likelihood term in the loss function and this generally entails extracting L samples from the distribution of our stochastic variable ITA using Monte Carlo sampling now to estimate the loss over the entire data set we can extract mini batches of M samples each and average the loss over each data point in the mini batch so here n is just the total number of of data points in the data set now as it turns out if we make our mini batch size M big enough then we can actually get away with just extracting one sample per data point in other words we set L to one and you only have to average over the mini batch this is called single sample Monte Carlo estimation and needless to say it massively decreases computational cost so now that we've gotten that out of the way how do we actually compute the log likelihood itself well it all depends on the type of distribution by which you choose to model P of x given Z for example if x is expressed in real numbers we might wish to use a gaan while if x takes a binary form as in binarized mest then we might wish to use a bar distribution let's look at the gaussian case first so here we're assuming that the generative model outputs a mean mu and standard deviation Sigma for every data variable so we take our gaussian probability density we can work through the mathematics to get this decomposed expression for the thing that we're trying to maximize now we can see that the second term is constant and other than that what we want to do to increase the overall log probability is minimize this mean squared difference between X and the mean of the distribution and also minimize this standard deviation Sigma now as it turns out there are a few difficulties faced by simultaneously optimizing mu and Sigma for example you can get a very very high log probability by generating some mu values that are very close to X and then pushing Sigma down to some very low value now that your mu is close to Target this Behavior can lead to numerical instability in the objective function and it may also lower performance by encouraging the model to focus on particular data variables rather than focusing on all variables equally for example by generating some pixels in well while performing poorly on the rest of the image for this reason practitioners typically forgo trying to learn Sigma and simply set it to one for all the output data variables this means that the first term in the expression vanishes and we're left just trying to minimize the mean squared error between X and Y mu with Y mu now being the only output of the generative model which is now a deterministic model on Z now let's look at the beri case this is the probability density of a univariant beri distribution where p is the probability that x equals to 1 and 1 minus p is the probability that x equals to zero so it's a bit like a coin toss with a weighted coin now imagine that X and P are vectors of length D so it's like we have D differently weighted coins this is now the multivariate beri distribution which is a product of all the individual coin tosses now if we take the log of that we event come to this expression where the probabilities of our coin tosses are just given by our generative model now this is exactly the form of the negative binary cross entropy between the output distribution given by P or Y and the ground truth distribution given by X when x equals 1 we want y to be pushed towards one and when x equals 0 we want it to be pushed towards zero hence if we want to maximize the log probability all we have to do is minimize this binary cross entropy so here is a quick summary of the loss functions typically used to maximize the log likelihood for the gaussian and beri cases in each case the latent input is passed through a deterministic model and in the beri case a binary sampling operation then takes place now I think it's worth mentioning that this is just common practice at the time of releasing this video there is some research going on on trying to learn Sigma for the gan case and who knows eventually that may become the dominant Paradigm now before we conclude let's stick to our gaussian case and sketch out the operation of the overall model where we also assume a gaan prior and posterior so in this scheme we have one deterministic model parameterized by fi that Maps some input X to two outputs mu and sigma then some stochastic variable ITA is sampled from a fixed distribution and these are then used to sample the latent variable Z then another deterministic model This Time parameterized by Theta Maps Z to some output y now to ensure maximum flexibility in the deterministic models we would ideally like to use some highly expressive and Powerful class of functions and it turns out that deep neural networks are perfectly suited for the task now visualizing it this way helps us to interpret the vae as essentially a coding device the inference model Q Can Be Imagined as an encoder model that reduces the data X to its latent representation Z which amounts to a low dimensional code of the original data then the generative model or the decoder model P attempts to reconstruct the original x given this code and the overall model is penalized according to the Reconstruction error between the input and the Reconstruction which is our mean squared error for real valued inputs therefore our model is an autoencoder a model that learns to reconstruct some input through a low-dimensional bottleneck that nevertheless captures all the important information in the input the primary architectural difference is that instead of deterministically mapping X to Z like an ordinary Auto incoder this time we sample Z from a stochastic model on X that has been learned through variational inference hence what we have is a variational auto encoder the other key difference is in the use of the kale Divergence Sloss function here which essentially acts as a regularization term to ensure that the latent space exhibits some desirable properties to illustrate this let's imagine we have trained an auto encoder on some generic shapes so we have some circles squares and triangles now a traditional autoencoder is just encouraged to learn a low dimensional code of the data so it may cluster shapes in the latent space like so furthermore the mapping is is fully deterministic so a point in X space is mapped to a point in zspace and vice versa now looking at this if we try to generate new data from some random point then it will most likely generate nonsense because the decoder model has not learned to map this particular point in latent space to an output that represents the data set and this is due to the fact that the latent space of an ordinary a encoder can be to a large extent arbitrarily organized a vae addresses this issue by mapping a point in X space to a distribution in zspace and by pushing each of these distributions as closely as possible to the prior of Z which in our case is a unit gaussian this encourages our distributions to overlap and fill all the space close to the origin what that means is that now when we sample from the prior we will always get sensible outputs to put this in more concrete terms our regularized latent space has been trained to exhibit two important properties the first is continuity that's to say smooth transitions in latent space will correspond to smooth transitions in data space and this is because the decoder is encouraged to reconstruct the same output regardless of which Z is sampled from the posterior during training this means that all points in the vicinity of some reference point are encouraged to reach construct similar looking outputs hence a point between two reference points could be expected to produce a reconstruction that looks something like a mix of the reconstructions produced by the reference points now the second property is completeness this just means that there is a complete mapping between the prior in latent space and the target distribution in data space so whenever we sample from the prior we should get sensible or meaningful output puts from the decoder now we can use these intuitions to conceptualize the delicate balance that the vae has to strike between the Reconstruction and regularization losses in particular we can imagine some weighted loss where the Reconstruction term is weighted by Alpha and the KL term is weighted by Beta now if Alpha is a lot greater than beta then too much emphasis is placed on reconstruction and we may recover our old problem with discontinuous distributions that we have had with the traditional Auto encoder on the other hand if beta is a lot greater than Alpha then too much emphasis is placed on regularization and every posterior is going to almost match the prior now this would give our vae almost zero reconstructive capacity since now it would produce the same reconstructed output for every input so in a way we can conceptualize the posterior distributions like a star there is some gravitational force pulling them all together and this is the KL Divergence term but there is also some outward pressure pushing them apart and this is the Reconstruction loss term what we want is to strike a balance between the two however that does not necessarily mean that Alpha and beta have to be exactly the same there is some benefit to having a beta greater than Alpha as we can see in methods such as the beta vae so there we have it to summarize the vae is a latent variable model that is able to capture the underlying factors of variation in some data set and map points in data space to distributions in a well regularized latent space they have been used in a ton of applications and they serve as a key milestone in the field of representation learning now if you want to learn more make sure to check out the links that I've put in the description"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Variational Autoencoders (VAEs)",
        "Video Title":"Variational Autoencoders | Generative AI Animated",
        "URL":"https:\/\/www.youtube.com\/watch?v=qJeaCHQ1k2w",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/qJeaCHQ1k2w\/hqdefault.jpg",
        "ID":"qJeaCHQ1k2w",
        "Publish Time":"2024-09-09T18:14:09Z",
        "Channel":"Deepia",
        "Channel ID":"UCJLoMm3XdBoMnQrKoboRkPA",
        "Transcript":"Generative Artificial Intelligence is a buzzword you'll hear everywhere these days, but its true meaning is often misunderstood. Many people mistakenly think it's a completely new type of AI, often confusing it with Chat-GPT or other large language models. In fact, generative AI differs from traditional AI in that it doesn't process existing data, but is used to create new data from scratch. In this video, we're going back to the roots of generative AI to explore one of the fundamental techniques for creating never-before-seen images: Variational Autoencoders, or VAEs. These models have not only laid the groundwork for understanding generative processes, but are also key components in modern architectures such as Stable Diffusion. But why do we even need such models? Why can't traditional deep learning models, like standard autoencoders, generate new images on their own? Let's dive in and find out. Autoencoders are based on a simple but powerful concept: they take data, compress it into a low-dimensional representation, and then reconstruct it back into its original form. While an image usually contains millions of values, the low-dimensional representation is usually a vector of 10 to 100 numbers. These representations live in a space with a more compact and interpretable form, usually called the latent space. If you're not familiar with these concepts, check out the previous video where we cover the basics of autoencoders. Now if we have a trained autoencoder capable of reconstructing images from their latent representations, a natural idea for generating new data would be to randomly sample points from this latent space and run them through the decoder. But here's the catch: in most cases, this approach will not produce useful results-mainly because the latent space is disorganized and irregular-so large areas of it will not produce meaningful decoded images. Another approach could be to take an encoded image, and then sample points that are in the neighborhood of its latent representation. This should give us images that resemble the original, right? Well, not exactly. The latent space is so poorly structured that even nearby points often don't correspond to meaningful variations of the original image, and if we sample points closer to the original image we just end up with the same reconstruction. So what's the solution? Well ideally, we want a latent space that's nicely organized - where sampling points lead to coherent, new images. Let's see how VAEs do it. Well first, let's start by giving you some context. The man behind the creation of variational autoencoders is Diederik Kingma, who first introduced them in 2013 in his groundbreaking paper, \"autoencoding variational Bayes\". Well, if you haven't heard of Kingma, he's also the mind behind the Adam optimizer, which is a widely used optimization algorithm for training neural networks-and a big part of why deep learning has made such huge progress in the past decade. Kingma's research focuses primarily on deep learning and Bayesian statistics, which are precisely the two core elements of variational autoencoders. Before we can dive into the theory behind VAEs, we will quickly cover some basic Bayesian notations and properties. If these are new to you, don't worry\u2014I'll break them down step by step. Let's say we have a random variable X which can take any value between 0 and 10. When an event occurs according to the probability distribution of X, we refer to this as \"sampling\" from X. Now, here is what we call the probability density function of X, which basically tells us the probability of a sample being any of the values between 0 and 10. We usually note p of x the density of X. An interesting quantity describing the distribution of X is its expectation, usually noted E of X. It is basically the average value that you can expect when sampling from X, and it can be computed using the integral of the probability density function. Now, let\u2019s consider two random variables, X and Z. What you see here in 3D is what we call the joint probability distribution of X and Z. This represents the probability of each possible pair of events occurring together. Of course each variable also has its own individual probability distribution, known as the marginal distribution. As before, we denote the marginal distribution of X as p of x, and the marginal distribution of Z as p of Z. Note that these are two separate densities, although we write both densities with a p. An interesting property of the joint distribution is that we can use it to compute each of the marginal distributions. This process, known as \"marginalization,\" involves integrating the joint distribution with respect to the other variable. For example, to find the probability of sampling a specific value according to X, we integrate the joint distribution over all possible values of Z. And similarly, to determine the probability of sampling a value according to Z, we integrate over all values of X. The final concept we need to cover is conditional probabilities. The term \"p of x given z\" represents the likelihood of the data x given the latent variable z. As you can see it is expressed using the joint distribution and the marginal distribution of Z. Essentially, it\u2019s like taking a slice of the joint distribution and normalizing it by the probability of sampling that value of Z. For instance if we want to compute p of x given that z equals to 3, we simply look at this slice and normalize its values by the probability that z equals to 3. Finally, \"p of z given x\" represents the probability that the latent variable z was present when we sampled the data x. We compute it just like we did for the other conditional distribution; we look at the slice and normalize the values by the probability that x equals 7. Now that we've got the basics down, let's explore the ideas and theory behind VAEs. So, our goal is to generate new data from a given distribution, p of x, which represents our dataset\u2014for example, images. The problem is, we don\u2019t know the exact shape or properties of p of x; we only have access to some samples, for instance images from our training split. To make working with p of x easier, we introduce another distribution, p of z, called the latent distribution. This distribution represents latent variables\u2014vectors in a lower-dimensional space that capture the core features of the data. Since these two distributions live in different spaces, we need mappings to connect them. The first mapping is called the posterior distribution, and it gives us the probability that a latent vector z was generated by a particular image x. The second mapping is the likelihood distribution, which, given a latent z, tells us the probability of reconstructing an image x from it. Now here\u2019s the idea: if we can sample latent vectors from the posterior distribution, those latents are likely to have been generated by images from our original data distribution, p of x. So, if we can reconstruct these latent vectors back into images, we\u2019ll effectively generate new samples from our original data distribution. However, there\u2019s one issue: we don\u2019t know the exact form of the latent distribution p of z either, which makes all the computations intractable. To solve this, we assume that the latent distribution is in fact a normal distribution. This assumption allows us to compute the likelihood, p of x given z, which measures how probable it is to reconstruct an image from a latent vector. But we\u2019re still missing an important part of the puzzle: the posterior distribution, p of z given x. This is where the variational part of the Variational Autoencoder comes in. Since we don\u2019t know the true posterior, we approximate it using a Gaussian distribution, which we\u2019ll call q of z given x. This Gaussian will have parameters - mu and sigma - that we need to learn, which is an optimization process usually known as variational Bayes. And here\u2019s where the magic happens: we\u2019ll train a deep encoder to estimate these parameters mu and sigma from the images. Then, we\u2019ll use a decoder to reconstruct images from latent variables that are sampled from the approximate posterior. But how do we actually train this whole autoencoder to approximate the posterior, and then reconstruct images from it ? Using Bayes' formula and some basic properties, we can derive this training objective for our autoencoder. For those of you who want to know how to derive this equation from scratch, we explain all the derivations in a bonus section at the end of this video. While this whole equation may seem a little daunting, it's actually quite intuitive. The first part is essentially a data consistency term, while the second part acts as a regularization. So this data consistency term simply measures how well our model can reconstruct an image x from its encoded version z. To ensure that z is taken from the approximate posterior, we take a real image and simply encode it. Conveniently, with all our assumptions, this likelihood term reduces to a simple L2 reconstruction loss, also known as the mean square error. So to estimate it, we simply take our latent and reconstruct it using the decoder, then measure the L2 between the reconstruction and the original image, just as we do for regular autoencoders. Now, let's look at the KL divergence part of the equation. If you remember our previous video on latent space visualization, you know that the KL divergence is a kind of distance between two probability distributions. So this term measures how close our approximate posterior is to the prior distribution p of z. Since we chose p of z to be a normal distribution, this means that when we optimize the ELBO, we constrain the approximate posterior to take the shape of the normal distribution as well! To recap, the ELBO is the training objective of our variational autoencoder, and it is designed to ensure that the generated samples come from our original data distribution p of x. It can be thought of as a regularized reconstruction loss, with the first term representing the usual L2 loss, and the second term imposing a normal distribution shape on our latent space. Now that we've got the theory down, let's move on to the practicalities. So, we have a theoretical objective to optimize, but how does an autoencoder actually implement these different distributions in practice? Let\u2019s quickly recap what an autoencoder does: it takes an input, compresses it into a lower-dimensional representation, and then decodes it back into the input space. In a standard autoencoder, this compressed representation is a single point in the latent space. Now with variational autoencoders, things work a bit differently. Instead of mapping the input to a single point, the encoder converts the input into a probability distribution, that we chose to be a Gaussian. Essentially, the encoder transforms the input data into the parameters of this Gaussian, namely the mean mu and variance sigma. Now, instead of representing our image as a single point in the latent space, it\u2019s represented as a Gaussian distribution. From this latent distribution, we sample points at random, and the decoder converts these sampled points back into the input space. We can then compute the different parts of the elbo loss, and backpropagate it through the network. But wait, can we really do that? How do we even backpropagate through the sampling operation here? Well, the thing is, we can't. This is where the \"reparameterization trick\" comes in. Instead of sampling directly from the Gaussian distribution, we introduce a random variable, often called epsilon, to handle the randomness outside of the network. This trick is actually pretty simple, but very powerful! First, we sample a random point from a standard normal distribution: this is our variable epsilon. Then we scale it by the variance of our approximated posterior distribution, and shift it by its mean. It is just as if we sampled directly from the posterior distribution, except that by using the reparameterization trick, the process becomes differentiable with respect to the mean and variance. This trick allows us to backpropagate through the entire network, enabling us to train the VAE end-to-end using standard gradient-based optimization techniques like Adam. In a typical VAE training, the process begins much like training a standard autoencoder. We start by taking an image and passing it through the network, which then generates a reconstructed version of that image. We then compare this reconstruction with the original image to measure how well the network is performing. But unlike a standard autoencoder, a VAE also outputs the parameters mu and sigma of the approximate posterior distribution. So we then calculate the Kullback-Leibler divergence between this posterior distribution and the prior distribution, which is just the standard normal distribution. The nice thing is that for two Gaussian distributions, the KL divergence has the following closed form expression. So we just compute this quantity, backpropagate the combined loss, update the network's weights, and repeat this process for each image in our dataset. Now, let's take a closer look at how the latent space of our VAE evolves as the training progresses. At the very beginning of the training, the images all get encoded to the same region of the latent space. Then, after a few gradient steps, the regularisation pushes the latents to spread over a normal distribution. After 2 or 3 epochs, the Kullback-Leibler divergence is so small that the latent space hardly changes, but the reconstruction loss still needs to be optimised. After the training is complete, the latent space of our VAE looks like this, which is very close to the shape of a 2D normal distribution. Additionaly, our VAE can now accurately reconstruct any digit it encounters: as you can see on the right of the screen, reconstructions are very close to the original images. But that's not all\u2014the trained VAE also allows us to generate entirely new handwritten digits that weren't part of our original dataset. By sampling a random latent vector, we can create a wide variety of new digits, some realistic and some more creative. And we can also generate new images that resemble existing ones. To do this, we first encode our reference image into its latent representation, then sample nearby latent vectors. The closer the sampled vector is to the reference vector, the more similar the resulting image will be. As we move further away in the latent space, the generated images become increasingly different from the original image. Another interesting feature of VAEs is their ability to blend two images seamlessly. To do this, we first encode both images into their latent representations using the encoder. Once we have these latent vectors, we can create a convex interpolation between them. By adjusting the interpolation parameter alpha, we smoothly transition from one image to the other. This shows that the latent space learnt by the VAE is sort of continuous, enabling us to navigate through it and still generate coherent results. This is very different from simple autoencoders, where changes in the latent space often lead to meaningless or distorted results. Variational autoencoders, however, maintain a compact and smooth latent space, ensuring that most points within the normal distribution in the latent space will lead to plausible samples. Variational autoencoders are great, but of course they have their weaknesses, the most obvious being that they tend to produce very blurry images. For example, if we train a fairly large variational autoencoder on the CelebA dataset, it will still struggle to reconstruct the images efficiently. This dataset is composed of portraits of celebrities, with images around 5 times bigger than the previous digit images. As you can see with these four reconstructions, the faces are somewhat realistic, but the background is just a blurry mess. Note that this is not caused by the architecture of the network, but by the regularization term in the loss. And of course, if we randomly sample new data points, the problem persists. When VAEs first came out, they were compared to the state of the art generative models, which at the time were gans. As you can see, the images produced by gans are much sharper, which made them more popular for a lot of tasks. And it hurts even more when you compare VAEs to the current state of the art generative models, diffusion models, that are used in all image generation products such as midjourney and stable diffusion. Another limitation of basic VAEs is the lack of conditioning to impose specific constraints on the generated data: with Kingma's original work, we can't really generate a specific image. Fortunately, many research papers have tried to address these challenges, resulting in several advanced VAE models designed to overcome these limitations. Some of the more notable variants include conditional VAEs, which allow for class-specific image generation; beta VAEs, which introduce a tunable parameter to control the trade-off between reconstruction quality and disentanglement; and vector quantized VAEs, which provide a discrete latent space for sharper reconstructions. We'll explore each of these models in future videos, so stay tuned if you want to know more. Thank you so much for watching this video! If you enjoyed it, please like and subscribe to help me grow the channel. The next video will be about contrastive learning. See you in the next video! So let's note q of z given x the approximate posterior distribution. Our goal is to sample new data from the original data distribution, so we need to pick a sample x that maximizes this probability. Well let's start by marginalizing the log density of the data distribution. We then introduce our approximate posterior using this simple trick. This allows us to rewrite our expression as the expectation of this ratio over our posterior. Now, because the log function is concave, we can apply Jensen's inequality, which gives us this final inequality where the log and expectation have been swapped. The right-hand side of this inequality is known as the Evidence Lower Bound, or ELBO. It is indeed a lower bound to the quantity on the left, which is called the evidence. So, if we want to maximize the evidence, we can focus on maximizing the ELBO\u2014something that\u2019s much simpler to do! We can replace the joint probability in this expression using Bayes' formula. The ELBO then gets broken down in two main pieces. The first one is just the expectation of the likelihood over the posterior distribution. The second term can actually be rewritten as the Kullback-Leibler divergence between the posterior and the prior distribution p of z. And just like that, we end up on the loss function used to train the variational autoencoder."
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Variational Autoencoders (VAEs)",
        "Video Title":"Variational Autoencoders - EXPLAINED!",
        "URL":"https:\/\/www.youtube.com\/watch?v=fcvYpzHmhvA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/fcvYpzHmhvA\/hqdefault.jpg",
        "ID":"fcvYpzHmhvA",
        "Publish Time":"2019-06-17T15:14:12Z",
        "Channel":"CodeEmporium",
        "Channel ID":"UC5_6ZD6s8klmMu9TXEB_1IA",
        "Transcript":"over the last decade deep learning has taken the field of AI by storm using neural networks we can now solve a host of problems to name a few one problem that we can solve is object detection feed the network and image and they will be able to identify locations of important objects in that image another problem we can solve is language translation feed a neural network in English sentence it'll spit out the equivalent in French another problem that we can solve is audio classification feed the neural network a sound wave and it will determine the object that produced that sound so if it hears uh it spits out dog and if it hears it spits out cat you can see these problems are quite different they have completely different input and output variables however all of them have one thing in common in all cases the neural network will process the input sample and it'll spit out some result that gives us some additional information about the input so take the case of object detection we give an input image and after processing it by the network we now know what objects are present and where they are located in the image that's additional information in the language translation case we give an input sentence in English and after processing it by the neural network we now know how to say the same sentence in another language like French that's additional information too now in the audio classification case we feed an audio sample as input and after processing it by the network we now know what animal made that sound the identity of that animal is again additional information however there are a category of networks that are a bit different in the sense that they don't merely provide additional information about some input sample but they also try to create or generate some sample image or audio or text themselves and this class of neural networks is called generative models appropriately named in this video we're going to go through a particular type of generative model called a variational auto encoder or a VA II the explanation will be twofold I'll start with an easy to understand intuition on VA ease once we have a firm understanding of them then we'll compare it to other types of generative models that have been hogging the spotlight recently generative adversarial networks Gans technique or not you'll be walking out with newfound knowledge and generative modeling and variational autoencoders I'm also going to throw in some technical jargon for you extra curious viewers this is code Emporium so let's get started let's start out with a broad concept generative modeling generative models are also just neural networks themselves normal neural network models usually take some sample as input and this sample is like raw data it could be like an image text or audio generative models on the other hand produce a sample as an output because of this flip I think you can see how and why this is so interesting with this technology there is so much potential for example you can train a model to understand how dogs work by feeding it hundreds of dog images then during test time we can just ask the model for an image and it'll spit out a dog image the cool thing is every time that we ask our model to generate a dog it'll generate a different dog every time so you can create an unlimited gallery of your favorite animal dago's sweet but what does this generative model black box look like let's take a look at this variational auto encoder as an example as mentioned before variational autoencoders are a type of generative model they are based off another type of architecture called auto-encoders these auto-encoders consists of two parts an encoder and a decoder the encoder takes an input sample and converts its information into some vector basically a set of numbers and we have a decoder which takes this vector and X man's it out to reconstruct the input sample now you may be thinking why are we doing this what is the point of trying to generate an output that is the same as the input and the answer to that is there is no point while using auto-encoders we don't tend to care about the output itself but rather the vector constructed in the middle this vector is important because it is a representation of the input image or audio and it's in a form that the computer understands so another question what is so great about this vector on its own I'd say the vector itself has limited use but we can feed it to complex architectures to solve some really cool problems here's an example of a paper that uses auto-encoders to infer location of an individual based on his or her tweet this architecture that they use consists of three stacked autoencoders to represent the input text from the tweet this is then piped to two output layers one of them is used to determine the state in the United States where the tweet was made and the other is to estimate the latitude and longitude positions of the user where the tweet was made I'll link the paper below in case your extra curious this is just one of the many interesting examples of what you can actually do with these auto-encoders however something we cannot do with auto-encoders is generate data now why is this the case let's go back to the auto encoder architecture it consists of an encoder and a decoder during training time we feed the images input and make the model learn the encoder and decoder parameters required to reconstruct the image again during testing time we only need the decoder part because this is the part that generates the image to do this we need to input some vector however we have no idea about the nature of this vector if we just give it some random values more likely than not we will end up with an image that looks like garbage so that's pointless now we need some method to determine this hidden vector here's some more intuition the idea behind a term this vector is through sampling from a distribution I'll explain these basic concepts of sampling and distribution but I'll also translate that into more technical terms for those of you who are more advanced in probability theory so distribution and sampling think of distribution as a pool a pool of numbers vectors consider the case where we want to build a generative model to generate different animals to accomplish this our generative model needs to learn to create a pool for cats a pool for dogs and another pool for giraffes like so when I say the dog pool I don't actually mean a pool that consists of dog images but instead it consists of some vector representation of these images and they are only understood by the computer so in a nutshell think of distribution as a pool of vectors now onto sampling sampling is a verb in English sampling means just closing your eyes reaching into a pool and picking one vector if you know where the pool is then you can go to the pool and randomly pick the vector so when we say I sample from the distribution of dog images it's equivalent to saying that we picked a random vector from the dog pool pretty simple now the problem with general auto-encoders is that we as human beings don't really know where these pools are imagine this box represents all possible values for the vector the hidden vector the cat pool can be here the dog pool can be here and the giraffe pool can be somewhere here each of these pools is learned by the model during training time so when we feed hundreds of images of animals our model will find patterns linking similar dogs cats and drafts and come up with these pools now these pools or more technically these distributions are learned internally by the auto encoder but there is no way for humans to know about these pools to make use of them for generating images during time we are basically sampling from a random distribution in other words it's equivalent to blindfolding ourselves and picking a value from this huge box that only consists of valid vectors in very specific locations and just garbage vectors everywhere else this is a very high chance that we'll pick a non relevant garbage vector from which we get a non relevant garbage output accordingly so the big takeaway we cannot generate dog images with an auto encoder because we don't know how to assign values to the vector during the generation phase we clearly have a problem here but what if we did know where to pick these vectors from then that would solve our problem right variational autoencoders does just that we first define a region we want to constrain this universe that is constrained the region from which we want to pick the vectors and within this region the goal of the variational auto encoder is to find the pool's the dog pool the cat pool and the giraffe pool and this is done during the training phase during the testing phase all we need to do now to generate an image is randomly sample a vector from this known region and then pass this vector to the generator part of our variational auto encoder this will generate an image a neat property about this region is that it's continuous so we can just alter some values in the vector to still get valid looking images say we train a variational auto encoder to print or generate handwritten digits from 0 through 9 the VA II will learn the pools such that they are within a defined region now these pools will represent the ten digits from 0 to 9 so we'll have to learn 10 pools the region now in which these pools are learned is continuous so I can just randomly sample a vector from this continuous region and just change its values ever so slightly the results of just changing this vector actually leads to very trippy and a psychedelic looking generating images when they're placed next to each other this is the simple intuition behind variational auto-encoders if you understood this then Congrats now let's revise some differences between the general auto-encoders and variational autoencoders just to make sure you have a clear understanding what each does from a more technical perspective though so first of all why do each exist the goal of general auto-encoders is to learn a hidden representation of the input while the goal of a variational autoencoder although it also learns a hidden representation of the input it also is used to generate new information general autoencoders cannot generate new data here's another question what are they optimizing autoencoders the general autoencoders learn to transform an input into some vector by minimizing reconstruction loss now during training and autoencoder make sure what is thrown into it is also spit out in other words it tries to minimize the difference from the original and the reconstructed images hence it seeks to minimize the reconstruction loss variational autoencoders on the other hand generate images by minimizing the sum of reconstruction loss and a latent loss now reconstruction loss is the same as what we defined for autoencoders with latent loss we ensure that all the pools learned by the network are within the same region that is in the middle that I defined here for more technical context we assume the pools follow a normal or Gaussian distribution hence during testing time they are actually sampled from the mixture of these gaussians now that we have a clear understanding of VA ease let's see how this compares with a more famous generative model generative adversarial networks so first off how do they learn to generate data variational auto-encoders have two losses to optimize the first is reconstruction loss what goes into the network is also spit out making sure that there is as little difference as possible the second is latent loss that is making sure the latent vector takes only a specific set of values so we want to know which region to sample this vector from optimizing two losses our variational auto encoder will learn to generate images now generative adversarial networks are gans work a little differently like all the VA e has an encoder and decoder architecture ganz also have two components a generator and a discriminator the generator is responsible for generating images and the discriminator determines whether a given image is either real or fake by fake I mean whether it was actually created by the generator both generator and discriminators play a minimax game where one tries to outperform the other the generator will try to generate an image that fools the discriminator making it think that its image is real and the discriminator tries to correctly distinguish between the real and fake images caching the generator with its wits if one of them messes up then its architecture is slightly tweaked to improve performance while looking at thousands of images during training the generator and discriminator and networks improve each other until the generator becomes proficient at generating animal images and the discriminator becomes proficient at determining real images from fake images generated by the generator then during testing time we can just use the generator to spit out the images that we need another aspect that we can compare ganz and VA eases stability during training now training and ganz involves finding something called a Nash equilibrium that is a point in the game between the generator and discriminator where the game is set to terminate or that there is an end of game point however there is no concrete algorithm to actually determine this equilibrium end of game point yet on the other hand V II's offer a closed form objective and by closed form I mean that there is a nice little formula that we can use to determine the end of training phase in variational auto-encoders now here's a third aspect from which we can compare and vs how good are the generated images va E's work very well in theory but they tend to generate blurry images you can mostly attribute this to the fact that VA E's are looking to optimize two factors during the training phase the reconstruction loss that is making sure that the output is as close to the input as possible and the latent loss that is making sure that the latent vector can only take a fixed range of values these two factors often counter each other there's a trade off so the middle ground usually leads to blurry image generation Gann training on the other hand is more empirical and optimized by way of trial and error they just work you can write down the losses theoretically but most of the intuition is based on the fact that we had the results before the actual theory for simple spatial data like images Gans produce really high quality results I made a video on the evolution of Gans since its inception in 2014 so be sure to check that out after this one and that's a brief comparison with Gans there are certainly deeper concepts that I didn't cover such as the need for the repair motorisation trick in variational auto-encoders or explicitly deriving the two losses of a variational auto encoder the reconstruction and latent loss however there are plenty of good blog posts out there outlining these concepts and I've linked some of these resources below I hope you got the base intuition of variational auto-encoders so that you can now more easily understand any learning resource you pick up from here on I may make a more mathy technical video on variation on O encoders later if most of you guys requested but I'll leave it at this for now thank you guys so much for watching subscribe to code Emporium in CS dojo for more videos on machine learning deep learning and artificial intelligence see you in the next one buh bye"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Diffusion Models for Image Generation",
        "Video Title":"Diffusion Models for AI Image Generation",
        "URL":"https:\/\/www.youtube.com\/watch?v=x2GRE-RzmD8",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/x2GRE-RzmD8\/hqdefault.jpg",
        "ID":"x2GRE-RzmD8",
        "Publish Time":"2025-01-30T12:01:16Z",
        "Channel":"IBM Technology",
        "Channel ID":"UCKWaEZ-_VweaEx1j62do_vQ",
        "Transcript":"If I drop red dye into this beaker of water, the laws of physics say that the particles will diffuse throughout the beaker until the system reaches equilibrium. Now, what if I wanted to somehow reverse this process to get back to the clear water? Keep this idea in mind because this concept of physical diffusion is what motivates the approach for text to image generation with diffusion models. Diffusion models power popular image tools like DALL-E-3 and sample diffusion where you can go from a prompt like a turtle wearing sunglasses playing basketball, to a hyper realistic image of just that. At a high level, diffusion models are a type of deep neural network that learn to add noise to a picture and then learn how to reverse that process to reconstruct a clear image. I know this might sound abstract, so to unpack this more, I'm going to walk through three important concepts that each build off each other. Starting first with Forward Diffusion. Going back to the beaker, think of how the drop of dye diffused and spread out throughout the glass until the water was no longer clear. Similarly with Forward diffusion, we're going to add noise to a training image over a series of time steps until the model starts to lose its features and become unrecognizable. Now this noise is added by what's called a Markov chain, which basically means that the current state of the image only depends on the most recent state. So as an example, let's start with an image of a person. My beautiful stick figure here and labeled this image X at time T equals to zero. For simplicity, imagine that this image is made of just three RGB pixels and we can represent the color of these pixels on our x, y, z plane here. Where the coordinates of each of our pixels correspond to their R, G, and B values. So as we move to the next timestep, T equals to one... We now add random Gaussian noise to our image. Think of Gaussian noise as looking a bit like those specks of TV static you get on your TV when you flip to a channel that has a weak connection. Now, mathematically adding Gaussian noise involves randomly sampling from a Gaussian distribution, a.k.a. a normal distribution or bell curve, in order to obtain numbers that will be added to each of the values of our RGB pixels. So to make this more concrete, let's look at this pixel in particular. The color coordinates of this pixel in the original image at time zero, start off at 255, 0, 0, corresponding to the color red. Pure red. Now as we add noise to the image going to timestep one, this involves randomly sampling values from our Gaussian distribution. And say we obtain a random values of -2, 2, and 0. Adding these together, what we get is a new pixel with color values 253, 2, 0 and we can represent this new color on our plane here. And show the change in this color with an arrow. So what just happened basically is that this pixel that was pure red in the original image at time zero has now become slightly less red in the direction of green at time t goes to one. So if we continue this process, so on and so forth, say we go two times, step two.. Adding more and more random Gaussian noise to our image. Again by randomly sampling values from our Gaussian distribution and using it to randomly adjust the color values of each of our pixels, gradually destroying any order or form or structure that can be found in the image. If we repeat this process many times, say over a thousand times steps, what happens is that shapes and edges in the image start to become more and more blurred, and over time, our person completely disappears. And what we end up with is completely white noise or a full screen and just TV static. So how quickly we go from a clear picture to an image of random noise is largely dictated by what's called the noise scheduler or the variance scheduler. This scheduling parameter controls the variance of our Gaussian distribution. Where a higher variance corresponds to larger probabilities of selecting a noise value that is higher in magnitude, thus resulting in more drastic jumps and changes at..for each color of each pixel. So after forward diffusion comes the opposite - reverse diffusion. This is similar to the process of if I took the beaker of red water and I somehow removed the red dye to get back to the clear water. Similarly for reverse diffusion, we're going to start with our image of random noise. And we're going to somehow remove the noise that was added to it in very structured and controlled manners in order to reconstruct a clear image. So to help me explain this more, there's this quote by the famous sculptor named Michelangelo, who once said, \"Every block of stone has a statue inside it and it's the job of the sculptor to discover it.\". In the same way, think of reverse diffusion as every image of random noise has a clear picture in it. And it's the job of the diffusion model to reveal it. So this can be done by training a type of convolutional neural network called a U-Net to learn this reverse diffusion process. So if we start with an image of completely random noise at a random time T, The model learns how to predict the noise that was added to this image at the previous time step. So say that this model predicts that the noise that was added to this image was a lot in the upper left hand corner here. And so the models objective here is to minimize the mean squared error between the predicted noise from the actual noise that was added to it during forward diffusion. We can then take this scale noise prediction and subtract it or remove it from our image at time t in order to obtain a prediction of what the slightly less noisy image looked like at time t minus one. So on our graph here for reverse diffusion, the model essentially learns how to backtrace its steps from each pixel's augmented colors back to its t noise colors. Now, if we repeat this process many times, over time, the model learns how to remove noise and very structured sequences in patterns in order to reveal more features of an image. Say slowly revealing an arm and a leg. It repeats this process until it gets back to one final noise prediction. One final noise removal and then finally, a clear picture. And our person has magically reappeared. So now that we've covered forward and reverse diffusion, it's time to introduce text into the picture by introducing a new concept called conditional fusion or guided diffusion. Up to this point, I've been describing unconditional diffusion because the image generation was done without any influence from outside factors. On the other hand, with conditional diffusion, the process will be guided by or conditioned on some text prompt. So the first step is we have to represent our text within embedding. Think of an embedding as a numeric representation or a numeric vector as able to capture the semantic meaning of natural language input. So as an example, an embedding model is able to understand that the word KING. Is more closely related to the word MAN than it is to the word WOMAN. So during training, these embeddings of these text descriptions are paired with their respective images that they describe in order to form a corpus of image and text pairs that are used to train this model to learn this conditional reverse diffusion process. In other words, learning how much noise to remove in which patterns at a given the current image, and now taking into account the different features of the embedded text. One method for incorporating these embeddings is what's called self attention guidance, which basically forces the model to pay attention to how specific portions of the prompt influenced the generation of certain regions or areas of the image. Another method is called the classifier free guidance. Think of this method as helping to amplify the effect that certain words in the prompt have on how the image is generated. So putting this all together, this means that the model is able to learn the relationship between the meaning of words and how they correlate with certain de-noising sequences that gradually reveal different features and shapes and edges in the picture. So once this process is learned, the model can be used to generate a completely new image. So first, the users text description has to be embedded. Then the model starts with an image of completely random noise. And it uses this text embedding along with the conditional reverse diffusion process it learned during training, to remove noise in the image and structure and patterns, you know, kind of like removing fog from the image until a new image has been generated. So the sophisticated architecture of these diffusion models allows them to pick up on complex patterns and also to create images that it's never seen before. In fact, the application of diffusion models spanned beyond just text to image use cases. Some other use cases involve image to image models, in painting missing components into an image, and even creating other forms of media like audio or video. In fact, diffusion models have been applied in different fields, everything from the marketing field to the medical field to even molecular modeling. Speaking of molecules, let's check on our beaker. If only I could. .. Well, would you look at that reverse diffusion! Anyways, thank you for watching. I hope you enjoyed this video and I will see you all next time. Peace."
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Diffusion Models for Image Generation",
        "Video Title":"Diffusion models explained in 4-difficulty levels",
        "URL":"https:\/\/www.youtube.com\/watch?v=yTAMrHVG1ew",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/yTAMrHVG1ew\/hqdefault.jpg",
        "ID":"yTAMrHVG1ew",
        "Publish Time":"2022-06-17T21:20:32Z",
        "Channel":"AssemblyAI",
        "Channel ID":"UCtatfZMf-8EkIwASXM4ts0A",
        "Transcript":"let's learn about the fusion models diffusion models are a fairly new innovation in the world of deep learning they are generative models that are being used in many different domains like audio generation or image generation you might have heard of them with their use in dali or imogen for example diffusion models can be used standalone like they did with glide or it can be used as part of a bigger and more complex model like it was done at dali 2 very recently their inner workings are quite complex so it could get a little bit confusing to understand how they work and how they are trained that's why in this video we are going to approach it step by step and explain the fusion models in five varying levels of difficulty starting from the easiest one to the more complex one level one diffusion models were inspired by non-equilibrium thermodynamics from physics and as you can understand from the name this field deals with systems that are not in thermodynamic equilibrium for example a drop of paint in a glass of water the density of the paint after you just dropped it is very high in one spot and zero in other parts of the water by the laws of physics the drop will diffuse into the water until it reaches an equilibrium in the physical world reversing this the fusion process is simply not possible but with diffusion models the goal is to learn a model that can reverse this process and bring the drop of paint to its original state in other words the drop being in one spot includes some information and as the diffusion process progresses we lose information in our case this information equals to clear images so working backwards from this diffuse paint is equivalent to working backwards to a proper image level two diffusion models work by replicating this diffusion process by adding noise to original images and later learning how to reverse this noise process the noise is applied to the images following a markov chain what is a markov chain a markov chain is a chain of events where the current time step only depends on the previous time step so that means there are no cross dependencies between time steps that do not immediately follow each other and this assumption of markov chains makes it tractable for the noise adding to be reversed later so at the end a diffusion model is a markov chain where in each time step we add a little bit of noise to our image until the image only consists of noise and later learning how to reverse this noise adding process after it is trained given only noise this model is able to generate high resolution images level three so now that we understand what diffusion models do is basically add noise to an image let's understand what it means to add noise to an image there are many different types of noise and the noise that is added in the fusion models is called the gaussian noise what is gaussian noise it is a noise that has the probability distribution of a gaussian or normal distribution given the different mean and variation values for this noise the location and the width of the distribution can change but the bell shape will stay the same adding gaussian noise to an image means changing the values of the pixels of that image slightly and the area of the probability distribution let's look at an example let's say for simplicity we have a 2 pixel image x-axis shows us the value of pixel 1 y-axis shows us the value of pixel 2 and the z-axis gives us the probability distribution if the values of our original images pixels are 120 and 90 out of zero to 255 our images will live in this point if we want to apply gaussian noise to this image we can draw the gaussian probability distribution where the mean is and the variance is determined by a constant let's say for now that it is 10. that means to add noise we select a random position inside this distribution it could be anywhere really close to the original point really far from the original point or somewhere in between the probability distribution tells us that this new point being closer to the original point is higher than it being further away from it so let's say if this point is selected randomly then the image in the next step of our markov chain will look like this and effectively we will have added gaussian noise to our image this is an example where the image only has two pixels but of course that does not really reflect reality normally images have many more pixels and when that happens this graph will have many more dimensions diffusion models add noise to the image in this way until it becomes nothing but noise this is done by adding just a little bit of noise for hundreds or even thousands of times so at the end we have a hundred or thousands long markov chain level four we learned what it means to add the noise but what does it mean to reverse or remove this noise reversing or removing the noise means recovering the values of these pixels so that the resulting image will resemble the original image in diffusion models this is achieved by using neural networks so let's look at our two pixel example again let's say this is where the image lives and this is the point it is fully noise during the forward diffusion process the image follows a path from the original position to gaussian noise position during the reverse diffusion we want to find a way to bring it back to its original position to do that we input the image to a convolutional neural network and we ask the network to produce the image in the previous step the type of convolutional network used in the original paper is called a unit it is called that because of its shape through the convolutions it makes a small representation of the image and then samples it back to the original dimensions this way the input and output dimensions of the networks has the same size okay that was a lot of information i hope you were able to follow along i based this video on this amazing article made by my colleague ryan o'connor in the assembly ai team and on top of everything we learned here today the article goes deeper into the math behind diffusion models you can find the link to the article in the description if you have any questions about how the fusion models work don't forget to leave them in the comment section below and if you like this video i would really appreciate it if you give it a like and subscribe to our channel to be one of the first people to know when we publish a new video thanks for watching and i will see you in the next video [Music] you"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Diffusion Models for Image Generation",
        "Video Title":"How AI Image Generators Work (Stable Diffusion \/ Dall-E) - Computerphile",
        "URL":"https:\/\/www.youtube.com\/watch?v=1CIpzeNxIhU",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/1CIpzeNxIhU\/hqdefault.jpg",
        "ID":"1CIpzeNxIhU",
        "Publish Time":"2022-10-04T13:16:35Z",
        "Channel":"Computerphile",
        "Channel ID":"UC9-y-6csu5WGm29I7JiwpnA",
        "Transcript":"generating images using diffusion what is that right so I should probably find out it's just things like Dolly and Dolly too yeah Imogen from Google stable diffusion now as well I've spent quite a long time messing about a stable diffusion I'm having quite a lot of fun with that so what I thought I'd do is I download the code I'd you know read the paper with work out what's going on and then we can talk about it I delved into this code and realized it's actually quite a lot to these these things right it's not so much that they're complicated it's just there's a lot of a lot of moving Parts um so let's just have a quick reminder of generative adversarial networks which are I suppose before now the the standard way for generating images and then we can talk about how it's different and why we're doing it using diffusion having a network or some you know deep Network train to Just Produce the same image over and over again not very interesting so we have some kind of random noise that we're using to make it different each time we have some kind of very large generator Network which is this is just I'm gonna give this black box big neural network right that turn that turns out an image that hopefully looks nice like at the like the thing we're trying to produce faces Landscapes people you know is this that how those Anonymous people on this person does not exist is this one yeah that's exactly how they work yeah if that's using I think style Gan right and it's that exact idea and that's trained on a large Corpus of faces and it just generates faces right at random right or at least mostly at random the way we train this is we have you know millions and millions of pictures of something that we're trying to produce so we produce we give it noise we produce an image and we have to tell is that good or is that bad right we need to give this network some instruction on if this image is actually looking like a face right otherwise it's not going to train it so what we do is we have another Network here which is sort of like the opposite and this says is it a real or is it a fake image and so we're giving this half a time we're giving it fake images and half a time we're giving it real faces so this trains and gets better at discriminating between the fake images produced here and the real images produced from the training set and in doing so this has to get better at faking them and so on and so forth and the hope is that they just get better and better and better all right now that that kind of works the the problem is that um they're very hard to train right you have a lot of problems with things like mode collapse where it just produces the same face if it produces a face that fools this every time there's not a lot of incentive for this network to do anything interesting right because it does solve the problem right it's beaten this let's move on right and so if you're not careful with your training process it's these kind of things can happen and I suppose intuitively it's quite difficult to go from this bit of noise to a really beautiful looking image in high resolution without there being some Oddities right and some things that go a bit more so what we're going to do is in diffusion models is try and simplify this process into a kind of iterative small step situation where the work that this network has to do is slightly smaller and you just run it a few times to try and make the process better right we'll start again on the paper so we can clean things up a bit so we've got an image right let's say it's an image of a rabbit right we add some noise so we've got a rabbit which is the same right and you add some noise to it now it's not speckly noise but I can't draw gaussian noise right and then we add another bit of noise right and the rabbit it's the same shape rabbit there's a bit more noise right and then we come over here and we come over here and we end up with just noise looks like nonsense and so the question is like how do we craft some kind of training algorithm some kind of what we call inference you know how do we actually deploy a network that can undo this process the first question is how much noise do we add why don't we just add loads of noise right so just delete all these images and doesn't really don't need to worry about that add loads of noise and then say like give me that and then you've got a pair of training examples you could use and the answer is it'll kind of work but that's about a very difficult job and you've sort of in the same problem with the Gant you're trying to do everything in one go right the intuition perhaps is that it's maybe slightly easier to go from this one to this one just remove a little bit of noise and then from this one to this one a little bit more noise well in traditional like image processing you do there are noise removal techniques rise yeah it's not difficult to do that is it no I mean it's it's difficult in a sense that you don't know what the original image was so what we're trying to do is train a network to undo this process that's the idea and if we can do that then we can start with random noise a bit like I can and we can just iterate this process and produce an image right now there's a lot of missing parts here right so we'll start building up the complexity a little bit okay so the first thing is is let's go back to our question of how much noise do we add right so we could add a small amount of noise and then the same amount again I've been the same amount again and we could keep adding it until we have essentially what looks like random noise over here right and that will be what we would call a linear schedule right for that is the same same amount of noise each time basically right and it's not interesting but it works the other thing you could do is you could add very little noise at the beginning and then ramp up the amount of noise you add later right and so there are different strategies depending on what paper you read about the best approach for adding noise but it's called the schedule right so the idea is you have a schedule that says right given this image so this is an image at uh at time T equals naught this is T equals one blah blah blah T equals some capital T which is like the final number of steps you've got right and this represents essentially all the noise and this represents some amount of noise and you can change how much each step has right and then the nice thing is you can then very easily produce because gaussians add together very nicely so you can say well I want T equals seven and you don't have to produce all the images you can just jump straight to t7 add the exact right amount of noise and then hand that back to the network so when you train this you can give it image random images from your training set with random amounts of noise added based on this schedule right varying randomly between 1 and T right and you can say okay here's a really noisy image Undo It here's a little less noisy image undo it right so what you do is you take your noise image image right I'm going to keep going with this rabbit it's taller than it was before right you take your noisy image at some time let's say t equals five right you have a giant unit shaped Network we've talked about encoder decoder networks before there's nothing particularly surprising about this one and then you also put in the time right because if we're running a funny schedule where your at different times have different amounts of noise you need to tell the network where it is so that it knows okay I'm gonna have to remove a lot of noise this time or just a little bit of noise what do we produce here so we could go for the whole hog and we just say we'll just produce the original rabbit image but then you've got a situation where you have to go from here all the way back to the rabbit that's a little bit difficult right mathematically it works out a little bit easier if we just try and predict the noise we want to know what is the noise that was added to this image that you could use to get back to the original image so this is all the noise from t1234 and five so you just get noise basically out here like this right with no rabbit that's the hope and then theoretically you could take that away from this and you get the rabbit back right now if you did that from here you would find that it's a little bit iffy right because you know you're predicting the noise all the way back to this rabbit is maybe quite difficult but if you did it from here it may be not quite so difficult we want to predict the noise so what we could do is predict the noise at let's say time T equals five and to say give me the noise it takes us back to T equals four right and then T equals three and T equals two the problem if you do that is that you're very stuck doing the exact time steps of the schedule used right if you used a thousand time steps for training now you've got to use a thousand time steps of inference right you can't speed it up so what we might try and do instead is say well okay whatever time step you're at you've got some amount of noise remove it all predict me all the noise in the image and just give me back that noise that I can take away and get back to the original image and so that's what we do so during training we pick a random Source image we pick a random time step and we add based on our schedule that amount of noise right so we have a noisy image a Time step T we put that into the network and we say what was the noise that we've just added to that image right now we haven't given it the original image right so that's what's Difficult about this we we have the original image without any noise on it that we're not showing it and we added some noise and we want that noise back right so we can do that very easily we've got millions of images in our or billions of images in our data set right we can add random bits of noise and we can say what was that noise right and over time it starts to build up a picture of what that noise is so it sounds like a really good kind of plug-in for Photoshop or something right it's going to be noise removal plug-in how does that turn into creating new images yeah so actually in some sense that's the clever bit right is how we use this network that produces noise to undo the noise right we've got a network which given an image with some noise added to it and a Time step that represents how much noise that is roughly or where we are in the noising process we have a network which produces an estimate for what that noise is in total and theoretically if we take that noise away from this we get back to the original image now that is not a perfect process right this network is not going to be perfect and so if you give it an incredibly noisy image and you take away what it predicts you'll get like a sort of maybe like a vague shape and so what we want to do is take it a little bit more slowly okay so we take this noise and we subtract it from our image right to get an estimate of what the original image was right T naught okay so we take this and we take this and we do subtraction and we get another image which is our estimate for T equals naught right and it's not going to look very good the first time but then we add a bunch of this noise back again and we get to a t that's slightly less than this one so maybe this was like T10 T equals 10. maybe we add like nine tenths of a noise back and we get to what we roughly T equals nine right so now we have a slightly less noisy image and we can repeat this process so we put the slightly less noisy image in we predict how to get back to T naught and we add back most but not all of the noise and then we repeat the process right and so each time we Loop this we get a little bit closer to the original image it was very difficult to predict the noise at T equals 10. it's slightly easier to predict the noise at T equals nine and very easy at T equals one because it's both mostly the image with a little bit of noise on it and so if we just sort of feel our way towards it by taking off little bits of noise at a time we can actually produce an image right so you start off with a noisy image you predict all the noise and remove it and then add back most of it right and so then you get and so at each step you have an estimate for what the original image was and you have a next image which is just a little bit less noisy than the one before and you Loop this a number of times right and that's basically how the image generation process works so you take your noisy image you Loop it and you gradually remove noise until you end up back at what the network thinks was the original image and you're doing this by predicting the noise and taking it away rather than spitting out an image with less noise right and that mathematically works out a lot easier to train and it's a lot more stable than again there's an elephant in the room here there is you're kind of talking about how to make random images effectively how do we direct this so that's where the complexity starts ramping up right we've got a structure where we can train a network to produce random images but it's not guided there's no way of saying I want a frog rabbit hybrid right which I've done and it's very weird so how do we do that the answer is we base condition this network that's the word we would use we'll basically give access to the text as well all right so let's actually infer on an image on my piece of paper right I bear in mind the output is going to be hand drawn by me so it's going to be terrible you start off with a random noise image right so this is just an image that you've generated by taking random gaussian noise mathematically this is centered around zero so you have negative and positive numbers you don't go from zero to two five five because it's just easier for the network to train you put in your time step so you generate a you put in a times that let's say you're going to do 50 iterations right so we put in a Time step that's maybe right at the end of our schedule but it says like time step equals you know 50 which is our most noised image right and then you pass it through the network and say estimate me the noise and we also take our string which is frogs frogs on stilts I'll have to have to try that later oh look right what's this one anyway we could spend let's say another 20 30 minutes producing fogs on stills we embed this right by using our GPT style Transformer embedding and we'd stick that in as well and then it produces an estimate of how much noise it thinks is in that image so that estimate on T equals 50 is going to be a bit average right it's not going to produce you a frog on a stilt picture it's going to produce you like a gray image or a brown image or something like that because that is a very very difficult problem to solve however if you subtract this noise from this image you get your first estimate for what your first image is right and when you add back a bunch of noise and you get to T equals 49 right so now we've got slightly less noise and maybe they're like the biggest outline of a frog on a stilt right and this is T equals 49 you take your embedding and you put this in as well right and you get another maybe slightly better estimate of the noise in the image and then we Loop right it's a for Loop right we've done those before you take this output you subtract it you add noise back and you repeat this process and you keep adding this text embedding now there's one final trick that they use to make things a little bit better if you do this you will get a picture that maybe looks slightly frog-like maybe there's a stilt in it right but it won't look anything like the images you see on the internet that have been produced by these tools because they do another trick to make the output even more tied to the text what you do is something called classifier free guidance so you actually put this image in twice once you include the embeddings of the text and once you don't right so this method this network is maybe slightly better when it has a text estimating the noise so you actually put in two images right this one's with the embedding and this one's no embedding right and this one is maybe slightly more random noise and this one's slightly more frog-like right or it's better better it's slightly moving towards the right thing and we can calculate the difference between these two noises and amplify that signal right and then feed that back so what we essentially do is we say okay if this network wasn't given any information on what was in the image and then this version of a network was what's the difference between those two predictions and can we amplify that when we loot this to really Target this kind of output right and the idea is basically you're really forcing this network or this this Loop to really point in direction of the of the scene we want right um and that's called classify free guidance and it is somewhat of a hack at the end of the network but it does work right if you turn it off which I've done it doesn't it produces you vague sort of structures that kind of look right it's not it's not terrible I mean I think I did like a muppet cooking in the kitchen and it just produced me a picture of a generic kitchen with no Muppet in it right but if you do this then you suddenly are really targeting what you want standard question got to ask it is this something people can play with without just going to one of these websites and typing some words well yeah I mean that's the thing is is that um is that it costs hundreds of thousands of dollars to try one of these networks because of how many images they use and how much processing power they use um the good news is that there are ones like stable diffusion that are um that are available to use for free right and you can use them through things like Google colab Now I I did this through Google collab um and it works really really well um and maybe we'll talk about that in another video where we delve into the code and see all of these bits happening within the code right I blew through my uh free Google allowance very very quickly I had to pay my eight pounds for uh for premium Google access so um you know eight pounds eight pounds thank you yeah so you know never let it be said I don't spare expense I I know I spare no expense on um on on computer file uh getting access to proper compute Hardware but um could beasts do something like that it could yeah almost of our servers could I'm just a bit lazy and haven't set them up to do so um but actually the code is quite easy to run that the the sort of the entry-level version of a code you literally can just like basically call one python function and it will produce you an image I'm using a code which is perhaps a little bit more detailed it's got the full loop in it and I can go in and inject things and change things so I can understand it better and we'll talk through that next you know perhaps next time the only other interesting thing about the current neural networks is that the weights here and here and here are shared so they are the same because otherwise this one here would always be the time to make one sandwich but you've got two people doing it so they make twice as many sandwiches each time they make a sandwich same with the computer we could either make the computer processor faster or"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Diffusion Models for Image Generation",
        "Video Title":"What are Diffusion Models?",
        "URL":"https:\/\/www.youtube.com\/watch?v=fbLgFrlTnGU",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/fbLgFrlTnGU\/hqdefault.jpg",
        "ID":"fbLgFrlTnGU",
        "Publish Time":"2022-04-20T14:39:48Z",
        "Channel":"Ari Seff",
        "Channel ID":"UCIxertsVDip8QHpnhinkAow",
        "Transcript":"imagine we take an image and add a bit of gaussian noise to it then do this again if we repeat this enough times eventually we'll have an unrecognizable picture of static a sample of Pure Noise now what if we could figure out how to undo this process that is start from noise image gradually remove the noise and end up with a coherent image this is the basic idea behind diffusion models an approach gaining Traction in generative modeling they've had success particularly in the domain of image generation and they are starting to rival and in some cases surpass other kinds of generative models you may be familiar with on certain tasks for example recent diffusion models have outperformed generative adversarial networks known as Gans in perceptual quality metrics and they've also shown impressive performance in various conditional settings such as converting text descriptions to images in painting and manipulation in this video we'll try to understand the basic mechanism behind diffusion models and how they can be adapted to different generative settings we'll start with a sample from some Target data distribution like an image from a training set let's call this x0 now let's define a forward diffusion process that gradually adds noise to the image over Big T time steps our model will be tasked with starting at X bigt and undoing this noise through what we'll call the reverse process the forward process which we'll denote with Q takes the form of a Markov chain where the distribution at a particular time step only depends on the sample from the immediately previous step so we can write out the distribution of corrupted samples conditioned on the initial data point x0 as the product of successive single step conditionals in the case of continuous data each transition is parameterized as a diagonal gaussian beta T here is the variance at a particular time step T typically these variances are treated as hyperparameters and follow a fixed schedule for a particular training run beta generally increases with time and is restricted to be between 0o and one meaning that this coefficient radical 1 minus beta T will likewise be non zero but less than one bringing the mean of each new Gan closer to zero in the limit as T approaches Infinity Q will approach a gaussian centered at zero with identity covariance losing all information about the original sample in practice the total number of steps Big T is on the order of a thousand using a large albeit finite number of steps allows us to set the individual variances beta T to be very small while still approximately maintaining the same limiting distribution but why do we want to use a small step size what's the benefit well it means that learning to undo the steps of the forward process won't be too difficult let's consider a simple case in one dimension suppose we were given the distribution of a forward process sample at time T minus one and it resembled a mixture of gaussians with two modes we then observe XT and want to infer the posterior distribution over XT minus one that is we'd like to determine where did the chain likely come from in order to arrive at XT what was the previous step of the chain if the noise step that is Q of XT given XT minus1 is allowed to be large then we will be quite uncertain about the location of XT minus one who knows where we jumped from but if the forward noise step is restricted to be small there is much less ambiguity about XT minus one we could then be justified in modeling the posterior of the forward step that is Q of XT minus1 given XT with a unimodal gaussian eliminating the contribution from the mode to the right and in fact it can be shown theoretically that in the limit of infinitesimal Step sizes the true reverse process will have the same functional form as the forward process so diffusion models leverage this observation parameterizing each learned reverse step to also be a unimodal diagonal gaussian aside from the sample at time T the model also takes t as input in order to account for the forward process variant schedule different time steps are associated with different noise levels and the model can learn to undo these individually like the forward process the reverse process is set up as a Markoff chain and we can write out the joint probability of a sequence of samples as a product of conditionals and the marginal probability of X Big T so what is p of X bigt here exactly well it's the same as Q of x bigt the Pure Noise distribution so at inference time in order to actually generate a sample we start from a gaussian and begin sampling from the Learned individual steps of the reverse process P of XT minus one given XT until producing an x0 okay great so we've defined these forward and reverse diffusion processes the forward process is designed to essentially push a sample off the data manifold turning it into noise and the reverse process is trained to produce a trajectory back to the data manifold resulting in a reasonable sample but what objective will we actually be optimizing is it some Maximum likelihood objective where we directly maximize the density assigned to x0 by the model well not exactly if we try to calculate P of x0 we see that we have to marginalize over all the possible trajectories all the ways we could have arrived at x0 when starting from a noise sample this unfortunately is contractable but it turns out we can maximize a lower bound to do this let's view X1 through X bigt as latent variables and x0 as an observed variable allowing us to interpret a diffusion model as a kind of latent variable generative model if we think back to another latent variable model you may be familiar with variational autoencoders commonly known as vaes we might get a hint about our training objective as a quick reminder in a vae we have an encoder that produces a distribution over Laten Z given a data input X and A decoder that reconstructs the data by producing a distribution over data x given a latent input Z so we can think of the forward process in diffusion models as analogous to the encoder producing latence from data and the reverse process as analogous to the decoder producing data from latens now unlike a vae encoder the forward process here is typically fixed it's the reverse process that we focus solely on learning this means that only a single Network needs to be trained unlike in a vae where two networks are trained jointly so we can now borrow the basic training objective used by vaes and a number of other lat variable models when we have a model with observations X and Laten variable Z we can derive What's called the variational lower bound also known as the evidence lower bound a lower bound on the marginal log likelihood Ood P Theta of X we won't walk through the full derivation here but the end result is a likelihood term also known as a reconstruction term subtracted by a k Divergence term the likelihood term encourages the model to maximize the expected density assigned to the data while the K Divergence encourages the approximate posterior qz given X to be similar to the prior on the latent variable P of Z as we saw earlier x0 will serve as the observation in the diffusion model framework while X1 through Big T will take the place of the latent variable Z here let's substitute these in all right now let's simplify a bit we can expand the K Divergence to combine the two terms into a single expectation and finally we can refactor the chain probabilities into their individual steps now there's a nice property of the forward process q that we didn't touch on earlier any arbitrary step of the forward process can be sampled directly in closed form this is just because the sum of independent gaussian steps is still a gaussian so at training time any term of this objective can be obtained without having to simulate an entire chain likewise we can optimize this objective by randomly sampling pairs of XT minus one and XT and maximizing the conditional density assigned by the reverse step to XT minus one however because different trajectories may visit different samples at time T minus one on the way to hitting XT the setup can have high variance limiting training efficiency to help with this we can rearrange the objective as follows let's examine each component P of X bigt is fixed it's just the start of the reverse process the Pure Noise distribution and as we saw earlier the whole forward process Q is also treated as fixed so we just have to worry about these two terms to the right here we have a sum of k divergences each between a reverse step and a forward process posterior conditioned on x0 one can prove with base rule that when we treat the original sample x0 as known like it is during training these Q terms are actually just gaussians since the reverse step is already parameterized as a gaussian each K Divergence now is simply comparing two gaussians and can be evaluated in closed form this helps reduce variance in the training process because instead of aiming to reconstruct Monte Carlos samples the targets for the reverse step become the true posteriors of the forward process given X there are a couple different ways we could imagine implementing the reverse step P Theta in the paper denoising diffusion probabilistic models ddpm for short the authors elect to set the reverse process variances to time specific constants as they found learning them led to unstable training and lower quality samples so the reverse step network is solely tasked with learning the means they then suggest a reparameterization that aims to have the network predict the noise that was added rather than the gaussian mean first we can rewrite sampling from an arbitrary forward step by using an auxiliary noise variable Epsilon Epsilon here has a constant distribution independent of the forward time step T and the reverse step model can be designed to Simply predict this Epsilon the authors also found that a simpler version of the variational bound that discards the term weights that appear in the original bound led to better sample quality so compared to the original variational lower bound their objective downweight steps that have very small noise at early time steps of the forward process allowing training to focus on more challenging greater noise steps like other generative Frameworks diffusion models can be made to sample conditionally given some variable of interest like a class label or a sentence description one way to do this is to just feed the conditioning variable y as an additional input during training in theory the model should learn to use y as a helpful hint about what it should be Recon constructing in practice some work has shown that further guiding the diffusion process with a separate classifier can help in this setup we take a trained classifier and push the reverse diffusion process in the direction of the gradient of the target label probability with respect to the current noise image and we can do this not just with single word labels but also with higher dimensional text descriptions as well of course one drawback of this technique is the Reliance upon a second Network an alternative approach eliminates this Reliance instead using special training of the diffusion model itself to guide the sampling in the paper classifier free diffusion Guidance the conditioning label Y is set to a null label with some probability during training then at inference time the reconstructed samples are artificially pushed further towards the Y conditional Direction and away from the null label even though no new information is being given to the model they found this to produce higher quality samples under human evaluation compared to classifier guidance in painting is another conditional generation problem where diffusion models have had success the naive way to perform inpainting with diffusion models is to take a model trained in the standard way and an inference time replace known regions of an image with a sample from the forward process after each reverse step now this works okay but can lead to Edge artifacts the model is not being made aware of the full surrounding context only a hazy version of it instead better results come from fine-tuning a model specifically for this task we can randomly remove sections of training images and have the model attempt to fill them in conditioned on the full clear context we can compare diffusion models to some other prominent deep generative models for sampling tasks diffusion models are somewhat limited by the slow markof chain this contrasts for example with Gans which can generate images in a single forward pack ongoing work aims to speed up sampling in diffusion models as we saw earlier diffusion models allow us to calculate a variational lower bound on the log likelihood similar to vaes in practice this lower bound can be quite good and even competitive on density estimation benchmarks which have long been dominated by autoaggressive models going Beyond lower bounds a continuous time formulation of diffusion models can give rise to what's called a probability flow OD this enables approximating log likelihood via numerical integration there's a close connection between denoising diffusion models and what are called score matching models and often these are now grouped together into a single class of models score here refers to the gradient of the log of the target probability density with respect to the data a score network is trained to estimate this value then a Markov chain is set up to actually produce samples from the learn distribution Guided by by this gradient well it turns out the score can actually be shown to be equivalent to the noise that's predicted in the denoising diffusion objective up to a scaling Factor so we can think of undoing the noise in a diffusion model approximately as trying to follow the gradient of the data log density diffusion models are really gaining momentum and it's been exciting to see their progress check out the links in the description to learn more thanks for watching"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Text-to-Image Models: DALL-E, Stable Diffusion",
        "Video Title":"How AI Image Generators Work (Stable Diffusion \/ Dall-E) - Computerphile",
        "URL":"https:\/\/www.youtube.com\/watch?v=1CIpzeNxIhU",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/1CIpzeNxIhU\/hqdefault.jpg",
        "ID":"1CIpzeNxIhU",
        "Publish Time":"2022-10-04T13:16:35Z",
        "Channel":"Computerphile",
        "Channel ID":"UC9-y-6csu5WGm29I7JiwpnA",
        "Transcript":"generating images using diffusion what is that right so I should probably find out it's just things like Dolly and Dolly too yeah Imogen from Google stable diffusion now as well I've spent quite a long time messing about a stable diffusion I'm having quite a lot of fun with that so what I thought I'd do is I download the code I'd you know read the paper with work out what's going on and then we can talk about it I delved into this code and realized it's actually quite a lot to these these things right it's not so much that they're complicated it's just there's a lot of a lot of moving Parts um so let's just have a quick reminder of generative adversarial networks which are I suppose before now the the standard way for generating images and then we can talk about how it's different and why we're doing it using diffusion having a network or some you know deep Network train to Just Produce the same image over and over again not very interesting so we have some kind of random noise that we're using to make it different each time we have some kind of very large generator Network which is this is just I'm gonna give this black box big neural network right that turn that turns out an image that hopefully looks nice like at the like the thing we're trying to produce faces Landscapes people you know is this that how those Anonymous people on this person does not exist is this one yeah that's exactly how they work yeah if that's using I think style Gan right and it's that exact idea and that's trained on a large Corpus of faces and it just generates faces right at random right or at least mostly at random the way we train this is we have you know millions and millions of pictures of something that we're trying to produce so we produce we give it noise we produce an image and we have to tell is that good or is that bad right we need to give this network some instruction on if this image is actually looking like a face right otherwise it's not going to train it so what we do is we have another Network here which is sort of like the opposite and this says is it a real or is it a fake image and so we're giving this half a time we're giving it fake images and half a time we're giving it real faces so this trains and gets better at discriminating between the fake images produced here and the real images produced from the training set and in doing so this has to get better at faking them and so on and so forth and the hope is that they just get better and better and better all right now that that kind of works the the problem is that um they're very hard to train right you have a lot of problems with things like mode collapse where it just produces the same face if it produces a face that fools this every time there's not a lot of incentive for this network to do anything interesting right because it does solve the problem right it's beaten this let's move on right and so if you're not careful with your training process it's these kind of things can happen and I suppose intuitively it's quite difficult to go from this bit of noise to a really beautiful looking image in high resolution without there being some Oddities right and some things that go a bit more so what we're going to do is in diffusion models is try and simplify this process into a kind of iterative small step situation where the work that this network has to do is slightly smaller and you just run it a few times to try and make the process better right we'll start again on the paper so we can clean things up a bit so we've got an image right let's say it's an image of a rabbit right we add some noise so we've got a rabbit which is the same right and you add some noise to it now it's not speckly noise but I can't draw gaussian noise right and then we add another bit of noise right and the rabbit it's the same shape rabbit there's a bit more noise right and then we come over here and we come over here and we end up with just noise looks like nonsense and so the question is like how do we craft some kind of training algorithm some kind of what we call inference you know how do we actually deploy a network that can undo this process the first question is how much noise do we add why don't we just add loads of noise right so just delete all these images and doesn't really don't need to worry about that add loads of noise and then say like give me that and then you've got a pair of training examples you could use and the answer is it'll kind of work but that's about a very difficult job and you've sort of in the same problem with the Gant you're trying to do everything in one go right the intuition perhaps is that it's maybe slightly easier to go from this one to this one just remove a little bit of noise and then from this one to this one a little bit more noise well in traditional like image processing you do there are noise removal techniques rise yeah it's not difficult to do that is it no I mean it's it's difficult in a sense that you don't know what the original image was so what we're trying to do is train a network to undo this process that's the idea and if we can do that then we can start with random noise a bit like I can and we can just iterate this process and produce an image right now there's a lot of missing parts here right so we'll start building up the complexity a little bit okay so the first thing is is let's go back to our question of how much noise do we add right so we could add a small amount of noise and then the same amount again I've been the same amount again and we could keep adding it until we have essentially what looks like random noise over here right and that will be what we would call a linear schedule right for that is the same same amount of noise each time basically right and it's not interesting but it works the other thing you could do is you could add very little noise at the beginning and then ramp up the amount of noise you add later right and so there are different strategies depending on what paper you read about the best approach for adding noise but it's called the schedule right so the idea is you have a schedule that says right given this image so this is an image at uh at time T equals naught this is T equals one blah blah blah T equals some capital T which is like the final number of steps you've got right and this represents essentially all the noise and this represents some amount of noise and you can change how much each step has right and then the nice thing is you can then very easily produce because gaussians add together very nicely so you can say well I want T equals seven and you don't have to produce all the images you can just jump straight to t7 add the exact right amount of noise and then hand that back to the network so when you train this you can give it image random images from your training set with random amounts of noise added based on this schedule right varying randomly between 1 and T right and you can say okay here's a really noisy image Undo It here's a little less noisy image undo it right so what you do is you take your noise image image right I'm going to keep going with this rabbit it's taller than it was before right you take your noisy image at some time let's say t equals five right you have a giant unit shaped Network we've talked about encoder decoder networks before there's nothing particularly surprising about this one and then you also put in the time right because if we're running a funny schedule where your at different times have different amounts of noise you need to tell the network where it is so that it knows okay I'm gonna have to remove a lot of noise this time or just a little bit of noise what do we produce here so we could go for the whole hog and we just say we'll just produce the original rabbit image but then you've got a situation where you have to go from here all the way back to the rabbit that's a little bit difficult right mathematically it works out a little bit easier if we just try and predict the noise we want to know what is the noise that was added to this image that you could use to get back to the original image so this is all the noise from t1234 and five so you just get noise basically out here like this right with no rabbit that's the hope and then theoretically you could take that away from this and you get the rabbit back right now if you did that from here you would find that it's a little bit iffy right because you know you're predicting the noise all the way back to this rabbit is maybe quite difficult but if you did it from here it may be not quite so difficult we want to predict the noise so what we could do is predict the noise at let's say time T equals five and to say give me the noise it takes us back to T equals four right and then T equals three and T equals two the problem if you do that is that you're very stuck doing the exact time steps of the schedule used right if you used a thousand time steps for training now you've got to use a thousand time steps of inference right you can't speed it up so what we might try and do instead is say well okay whatever time step you're at you've got some amount of noise remove it all predict me all the noise in the image and just give me back that noise that I can take away and get back to the original image and so that's what we do so during training we pick a random Source image we pick a random time step and we add based on our schedule that amount of noise right so we have a noisy image a Time step T we put that into the network and we say what was the noise that we've just added to that image right now we haven't given it the original image right so that's what's Difficult about this we we have the original image without any noise on it that we're not showing it and we added some noise and we want that noise back right so we can do that very easily we've got millions of images in our or billions of images in our data set right we can add random bits of noise and we can say what was that noise right and over time it starts to build up a picture of what that noise is so it sounds like a really good kind of plug-in for Photoshop or something right it's going to be noise removal plug-in how does that turn into creating new images yeah so actually in some sense that's the clever bit right is how we use this network that produces noise to undo the noise right we've got a network which given an image with some noise added to it and a Time step that represents how much noise that is roughly or where we are in the noising process we have a network which produces an estimate for what that noise is in total and theoretically if we take that noise away from this we get back to the original image now that is not a perfect process right this network is not going to be perfect and so if you give it an incredibly noisy image and you take away what it predicts you'll get like a sort of maybe like a vague shape and so what we want to do is take it a little bit more slowly okay so we take this noise and we subtract it from our image right to get an estimate of what the original image was right T naught okay so we take this and we take this and we do subtraction and we get another image which is our estimate for T equals naught right and it's not going to look very good the first time but then we add a bunch of this noise back again and we get to a t that's slightly less than this one so maybe this was like T10 T equals 10. maybe we add like nine tenths of a noise back and we get to what we roughly T equals nine right so now we have a slightly less noisy image and we can repeat this process so we put the slightly less noisy image in we predict how to get back to T naught and we add back most but not all of the noise and then we repeat the process right and so each time we Loop this we get a little bit closer to the original image it was very difficult to predict the noise at T equals 10. it's slightly easier to predict the noise at T equals nine and very easy at T equals one because it's both mostly the image with a little bit of noise on it and so if we just sort of feel our way towards it by taking off little bits of noise at a time we can actually produce an image right so you start off with a noisy image you predict all the noise and remove it and then add back most of it right and so then you get and so at each step you have an estimate for what the original image was and you have a next image which is just a little bit less noisy than the one before and you Loop this a number of times right and that's basically how the image generation process works so you take your noisy image you Loop it and you gradually remove noise until you end up back at what the network thinks was the original image and you're doing this by predicting the noise and taking it away rather than spitting out an image with less noise right and that mathematically works out a lot easier to train and it's a lot more stable than again there's an elephant in the room here there is you're kind of talking about how to make random images effectively how do we direct this so that's where the complexity starts ramping up right we've got a structure where we can train a network to produce random images but it's not guided there's no way of saying I want a frog rabbit hybrid right which I've done and it's very weird so how do we do that the answer is we base condition this network that's the word we would use we'll basically give access to the text as well all right so let's actually infer on an image on my piece of paper right I bear in mind the output is going to be hand drawn by me so it's going to be terrible you start off with a random noise image right so this is just an image that you've generated by taking random gaussian noise mathematically this is centered around zero so you have negative and positive numbers you don't go from zero to two five five because it's just easier for the network to train you put in your time step so you generate a you put in a times that let's say you're going to do 50 iterations right so we put in a Time step that's maybe right at the end of our schedule but it says like time step equals you know 50 which is our most noised image right and then you pass it through the network and say estimate me the noise and we also take our string which is frogs frogs on stilts I'll have to have to try that later oh look right what's this one anyway we could spend let's say another 20 30 minutes producing fogs on stills we embed this right by using our GPT style Transformer embedding and we'd stick that in as well and then it produces an estimate of how much noise it thinks is in that image so that estimate on T equals 50 is going to be a bit average right it's not going to produce you a frog on a stilt picture it's going to produce you like a gray image or a brown image or something like that because that is a very very difficult problem to solve however if you subtract this noise from this image you get your first estimate for what your first image is right and when you add back a bunch of noise and you get to T equals 49 right so now we've got slightly less noise and maybe they're like the biggest outline of a frog on a stilt right and this is T equals 49 you take your embedding and you put this in as well right and you get another maybe slightly better estimate of the noise in the image and then we Loop right it's a for Loop right we've done those before you take this output you subtract it you add noise back and you repeat this process and you keep adding this text embedding now there's one final trick that they use to make things a little bit better if you do this you will get a picture that maybe looks slightly frog-like maybe there's a stilt in it right but it won't look anything like the images you see on the internet that have been produced by these tools because they do another trick to make the output even more tied to the text what you do is something called classifier free guidance so you actually put this image in twice once you include the embeddings of the text and once you don't right so this method this network is maybe slightly better when it has a text estimating the noise so you actually put in two images right this one's with the embedding and this one's no embedding right and this one is maybe slightly more random noise and this one's slightly more frog-like right or it's better better it's slightly moving towards the right thing and we can calculate the difference between these two noises and amplify that signal right and then feed that back so what we essentially do is we say okay if this network wasn't given any information on what was in the image and then this version of a network was what's the difference between those two predictions and can we amplify that when we loot this to really Target this kind of output right and the idea is basically you're really forcing this network or this this Loop to really point in direction of the of the scene we want right um and that's called classify free guidance and it is somewhat of a hack at the end of the network but it does work right if you turn it off which I've done it doesn't it produces you vague sort of structures that kind of look right it's not it's not terrible I mean I think I did like a muppet cooking in the kitchen and it just produced me a picture of a generic kitchen with no Muppet in it right but if you do this then you suddenly are really targeting what you want standard question got to ask it is this something people can play with without just going to one of these websites and typing some words well yeah I mean that's the thing is is that um is that it costs hundreds of thousands of dollars to try one of these networks because of how many images they use and how much processing power they use um the good news is that there are ones like stable diffusion that are um that are available to use for free right and you can use them through things like Google colab Now I I did this through Google collab um and it works really really well um and maybe we'll talk about that in another video where we delve into the code and see all of these bits happening within the code right I blew through my uh free Google allowance very very quickly I had to pay my eight pounds for uh for premium Google access so um you know eight pounds eight pounds thank you yeah so you know never let it be said I don't spare expense I I know I spare no expense on um on on computer file uh getting access to proper compute Hardware but um could beasts do something like that it could yeah almost of our servers could I'm just a bit lazy and haven't set them up to do so um but actually the code is quite easy to run that the the sort of the entry-level version of a code you literally can just like basically call one python function and it will produce you an image I'm using a code which is perhaps a little bit more detailed it's got the full loop in it and I can go in and inject things and change things so I can understand it better and we'll talk through that next you know perhaps next time the only other interesting thing about the current neural networks is that the weights here and here and here are shared so they are the same because otherwise this one here would always be the time to make one sandwich but you've got two people doing it so they make twice as many sandwiches each time they make a sandwich same with the computer we could either make the computer processor faster or"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Text-to-Image Models: DALL-E, Stable Diffusion",
        "Video Title":"Understanding Text-to-Image Models: Dall-E, Stable Diffusion, and more",
        "URL":"https:\/\/www.youtube.com\/watch?v=sa9Oc26XXdc",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/sa9Oc26XXdc\/hqdefault.jpg",
        "ID":"sa9Oc26XXdc",
        "Publish Time":"2023-03-01T21:03:04Z",
        "Channel":"Vanderbilt Data Science",
        "Channel ID":"UCJUwHSLYEkp8QyTL6Czm0ow",
        "Transcript":"welcome everybody and welcome to the second of our workshops on understanding AI models today we're going to be talking about understanding text to image models I'm Jesse Spencer Smith I'm Chief data scientist at the data Science Institute and I have with me uh some of the data scientists and and postdocs from the data Science Institute including uh Professor Bell Chara Bell uh we have Dr petrelante Abby pagelante is a postdoc with us and we have umang a childhood data scientist with the data Science Institute so we're going to be here to be able to answer any questions that you have we're going to have a presentation for about the first hour it's going to be interactive and then for the last half hour for those of us who can who can still be here we're going to do some Hands-On with some pretty interesting tools much like the rest of uh the AI space things are moving extremely rapidly uh with image generation it started out a little bit more slowly as you're going to see but really over the last couple of weeks has has taken off but it's not all good so we have a lot of bad with the good and we're going to talk about why that's the case and where that comes from but first I'd like to start with uh some ideas from you who here and you can just do a with a virtual show of hands or actual show of hands who here has used one of these text to image or image to image models this will be either dolly or stable diffusion or something like mid-journey or dream something along those lines who have some experience excellent Dr yah so yeah and Brooke excellent and Abby I know that that you have as as well so Dr yeah tell us a little bit about what you've done uh with these and it's nice to see you thank you so much for hosting this session um so with mid-journey it's mostly Justin for creativity um I have a sixth grader and a first grader and we usually just use it to kind of like explore Concepts and I think through ART and I think it's an interesting way to teach kids about coding or prompting just the concept of prompting and having them articulate what they want and to be able to manipulate the subsequence drawings based on features they want and the limitations um like too many fingers um why is that what is it not meant so things like that it's been really fascinating fantastic and at the very uh in about the next half hour we're going to hit something that I think you're going to find very interesting which is a development in the last week uh in these models what a great great use of that um anybody else uh Brooke you you've been using this quite a bit how have you been using uh uh the models you say Brooke sorry um I did yeah I've been using it I am the communications marketing PR person for the department now um and I've been using it for images for our stories um for different Graphics like for upcoming women in data science conference which you'll see in a second um and um just for various you know graphic design usage okay uh fantastic uh and uh Abby you've you've used these as well let me ask you Abby what are some problems that you've run across like like you know just for rendering images yeah I mean I think there is a lot of problems I mean I think um like Dr York kind of alluded to uh it's really hard to get it to give good looking people a lot of the times um if you want to make people you have to put stuff in the prom in the negative prompt like disfigured too many fingers bad Anatomy scary eyes and sometimes it still just wants to generate those things anyway and you say negative prompt what does that mean um a negative prompt so yeah stable diffusion has positive prompts and negative prompts so positive prompts are things that you want the image to look like and negative prompts are where you can emphasize things that you don't want the image to look like and so a lot of times people put in negative prompts kind of things that they find stable diffusions doing that don't really fit into a pot like you can't really put like normal eyes into a positive prompt um but you want to kind of emphasize to not do something in the negative prompt excellent but I think it has a whole host of other problems that we will maybe talk to about sort of presumptions about what people and things should look like that definitely make it very hard to generate certain realistic images um yeah fantastic actually we're going to start with that so let's talk about where the models are trained from then we're going to dive into a problem that we had here at the DSi this Friday for generating images from uh for women in data science and for another just you know just generating images um and and and how this can it's pretty Insidious how this can sort of creep in and it can reinforce stereotypes when we least intended um but let's talk about where these models come from we'll get into the more technical detail in in just a little bit but uh does anybody know where the images come from that these models are trained on because it knows about all sorts of different things styles of art uh it knows about uh uh different images what is it trained on do you suppose doctor yeah yeah please isn't that part of the controversy of uh supposedly on open set data and images that are openly available but I think there are some places like Getty Etc that are saying that um they scraped some of their artists information and um the artists are not pleased absolutely so it actually is scraped so a lot of the training information is scraped from the web so first pass think about images on the web already you know we start getting into some some issues so what appears on the web well you have news images and you have other images as well but some of these are held you know with copyright some of these are intellectual property so that's part of it that definitely goes into into the training of of many of these models then what also goes into the training is something which is maybe not so obvious as a problem all of the all of the uh because these aren't just pictures these also people you know generated pictures as well drawings and sketches and and paintings so think about all of the presumptions and biases that go into the things that we create so much like with large language models which are also trained on text found on the internet here you have also models trained on what's in the internet so any bias which is apparent and bias in the the more lay term right not biases in terms of deep learning but bias in terms of the the lay term usage that is existing in the images on the internet are going to be learned and encoded by these models so why is why is that a problem well let's think about uh generally what these models do and we're going to try to chance we're going to get a chance to uh to try this out uh and uh let's see what what the models uh actually uh do for us so in this case uh you can see uh our women in data science 2023 upcoming conference and this is the prompt brick that I think that used which was diverse group of young women sitting around computers Asian women black women cool pop art yes so first of all we had to say diverse group what happened when you didn't say diverse groups it wasn't diverse and it was it was not diverse in a weird kind of way right and so but it was and this is where the biases start coming in many of those images that were generated there were of Asian women and also very heterogeneous very homogeneous as well so get a better representation then you get closer but uh technically can anyone identify problems with this image not even think about bias but just technically and Abby you're probably gonna you're gonna peg this one right away what do you see um I see a a bit of a weird looking hand on a thing I think yeah I always look at the hands and stable diffusion images because they're like never quite right so it can't right so it doesn't do the hands um and and you mentioned a way of fixing that problem at least so what's one way of problem fixing that problem so I think yeah the common thing is to put a negative prompt where you try to specify um the correct number of fingers and like normal length Fingers um you can put it also in the positive prompt and say like a group of people with normal hands um or something but I think that doesn't tend to work quite as well okay excellent now what about some more subtle issues here and we'll see another image that we also generated you'll see it right away more subtle issues here I don't know if this is just an extension of the hand the Fingers um but the the hand is not quite connected to the hand on the keyboard um so that's interesting um and eyes are sometimes a problem these ones look reasonable they're not bad but you're right with the hand absolutely Plus where's the screen so generates kind of like these different elements kind of make sense but it's like what are you looking at exactly the screen's over here then there's the images themselves you know not bad for such a simple prompt but we're the plunging necklines coming from I mean seriously it's like we say we didn't say anything about that right and and it went there so here's another image I and so this is almost the same image for another thing for our newsletter and so this was very close but this was supposed to be digital art cool techie why it's so this is where it went um and and is it the case that sort of all images are going here well let's go ahead and say let's do that same prompt but let's say men around no we don't all right so we don't want to be plunging necklines on on the men here but so and you can see there's a difference sort of in the in in the look as well and so this is a recognized problem right so this is a a known problem contrastive language Vision AI models pre-trained on scraped multimodal data exhibit sexual objective uh objectification bias definitely so notice that that was nowhere in the prompt it didn't come anywhere as a matter of fact if you have you know people around a computer you know you would expect you to go One Direction but it but it went in another so um so how can you fix this well one way is possibly you could ban terms could and but this gets more against like trying to block the generation of porn because you could certainly generate that as well so the problem here is and this is a article from the 24th um mid-journey tried to block porn Generation by Banning any words about the human product reproductive system primarily uh on the female side and so if you try to generate images that have to do with Fallopian tubes good luck because mid Journeys banned those now so not not a great approach um Brooke tell us about what what happened when you tried to address these uh these problems with the images by by using the term not sexist yeah I try to use the term not sexist unsexes a couple different variations of that and anytime it has scx s-e-x like sex at all even if it says not sexist it's gonna banana completely like any variation of that yeah it just says can't unable to generate words band so positive ways to to address this perhaps dressed for office okay a little bit better here so but still look at this image right here still even when you say dress for office it's still pushing in that in direction do we see that when we also do men not quite also you'll notice something else interesting here do you notice anything different uh other uh different about this so this is a group of men a group of women tech let's see what was that it was uh the same prompt group of young uh diverse men sitting at computers coding digital art cool Tucky dressed for office what are some differences you notice between these two the color palette color palette how about how they're and this is open for everybody how they're how they're sort of dressed which looks a little bit more professional or which is the most professional looking image certainly the men the ones with the ties yep so there are ties here but you don't see any instances of that here [Music] so that's interesting the girls seem to have headphones that jumped out at me a little bit well that's interesting yeah and none over here interesting anything else that anyone else would like to to comment on yeah I think going along with the colors just kind of yeah like the the art style is almost it's a more serious art style for the men and it leans more into that like pop art cartoonish so a couple of things that we learned from this one the bias is inherent in the learning of the images and of the wording as well how do these two actually interact and how do you even get to images from text that's what we'll be talking about today two that up until state of the art let's call it two weeks ago this is about as good as you could get with control so it had to only be text if you wanted to do more control you had to do it either with positive prompts or negative prompts and you can see how squishy that is you push in One Direction and it sort of goes in another Direction it's it's almost like whack-a-mole trying to to you know to to get everything in the in the direction that you want so what we're going to be talking about today is uh and our goal for the infant today is that by the end of this first hour you have a good sense of how these models work where they come from uh how they can be used uh then we're going to be talking about uh the different varieties and how they've evolved so where they came from where they've evolved to and where this most recent iteration has come from in this last week this brand new iteration that offers much more control of these models and then we'll talk about what is likely to come next before we jump in questions or observations before we start tackling this excellent then let's start closer to the beginning so in 2021 there are a couple of uh of nearly a concurrent uh Publications and not releases of models but Publications about models and then open availability of models uh around the same time these were all around this idea of being able to generate images directly from text so if we take a look at uh at what these were were able to uh to generate you know it's it's not bad here's an example of of uh of models that were that were that were generated you'll notice that um well first of all that they weren't great right so you see some some garbage here they're very sort of simplistic they're not necessarily photorealistic but still pretty stunning because we had never been able to say something like hey just give to a deep learning model an illustration of a baby hedgehog and a Christmas sweater walking a a dog that was something we were never able to do before so these produces simplistic ma simplistic images but they could do it text was more difficult with these and could see that uh you know especially with this one they're cherry picking here right and this is the best they could get the other thing with these models is that they were huge huge models they did this in a particular way um how many of you were here last week for uh understanding chat GPT so Ben I said yeah you were here a few other folks were here so uh on Becky oh fantastic so in those you know we talked about these large language models what they did here was they took those self same large language models but trained them not just on text but also to train them on images together a multimodal model and so they could actually put in as part of the prompt and it was almost like a translate translate this text into an image and then this is what it would generate the outside was huge models could not be run on any sort of standard Hardware it couldn't be round on something less than a huge server Farm which is why only the largest companies had it and why they didn't well other reasons why they didn't open source it but you know this requires many computers of significant power all combined together to enter to generate even a simple image and they recognized early on that there was problems here as well with uh with the possibility of generating a very problematic images so uh nearly at the at the same time um uh shortly after that's you'll notice that this first one was uh this was first one was a 2021 then in April of 2022 was this next generation of models with worked in a very different kind of way interesting thing about this you can think of this as being two different models yoked together so the clip models clip models were trained on providing text summaries of images so given an image this is a dog standing on a sidewalk in front of a house the clip model would actually generate the description of what was in the image and the trick here was to take that clip model that was trained on image and also trained with text but to reverse it so now once you've reversed it now you could have a textual representation and then from that you could generate the image and this used a a a process where you were simply you were running it sort of uh in reverse again the model which which came out of this called doll E2 hugely computationally expensive because it was operating at the pixel level so the problem is this here's a random set of pixels how do we move the pixels and so when we compare it to clip the description becomes better and better and closer to the text that we put in you're solving this interesting inverse problem of trying to move this random image closer to the image closer to the description that you had so dolly far more successful so now we're getting into things that are are much more realistic than the very first one and now you can do things like oil painting by Matisse of a humanoid robot playing chess not bad so part of the part of the change here is now you're training on many millions of images whereas the first model where it's only trained on some now this is trained on millions of images so now it is learn things like Style um so it understands Matisse uh it understands uh uh a photo of a white fur monster standing in a purple room so now it understands the difference between drawing and photo and the type of representation so down here you have a van Gogh style painting of an American football player and understands what American football is you know they actually even has pads on right here so now getting far far better looking images still very difficult to generate and requiring large-scale computers uh to be able to uh to do this so a non-trivial problem so again now we're also in April 2022 nearly at the same time that Dolly 2 came out another group came out and this was a group that became stability AI came up with a way of of solving this problem in a different way recall that all of these images were created going from Pixel to a final image so the main idea here is that you're already going to be operating in in a latent space so you're going to take this image and then you're going to be clip is going to look at it and it's going to come up with a representation of this just like with the large language models and that's the latent space so what's happening in this image there's going to be a representation of this within the Deep learning model that's going to be your first step so you get up into the deep learning model their idea was why do all this work in the pixel space start right up here in the in the semantic space and do that work here so let's talk about how stable diffusion generates its images in why it's able to do as good a job if not better than these huge models but to do so much more efficiently so let's take a look at the architecture that we have here first so what you're looking at here is the architecture for a stable diffusion model you have input here which can be image you have text input as well and the very first thing that happens is that this gets represented in the Deep learning model and what's called a stable diffusion encoding block so much like in the large language models you take the text and the very first thing you do is you represent it mathematically as what's called a semantic in a semantic space you're representing these not just as a collection of pixels but as a semantic representation of what's in the image all the different parts that are in the image so you start there so you're no longer working with just the the individual images right the individual pixels now what you're doing is you're squeezing that representation down so at first it represents everything in the image maybe this blade of grass that's right behind the dog is not important but it's represented there somehow so what happens is and this is called a unet the representation gets squeezed down you'll notice that this is getting smaller and smaller 30 by 32 16 by 16. that's the size of the representation so it's getting smaller and you're adding noise and now what's happening is that you're getting rid of the noise and you're getting larger again getting a richer and richer so what happens is if you start with just the text then this is just noise and the text is driving everything you could put text and an image but you're adding noise squeezing it down and then the text is causing it to balloon back out again but now including more of the semantics in the text that you had but all of this is happening in the Deep learning a latent space so it happens much faster than trying to move all these millions of pixels around so it's happening at the very very end then it generates an image based on the latent information that's been added into this model so you don't really until you get to the output you ever really need to be getting all the way back to the level of the pixel where it gets big again so in short you have the text you might have image you may just have noise it squeezes that down until it's the smallest possible representation it removes that gets it larger again and then generates a new image based on the text that's driven it so very high level if you're interested in getting more in depth we're actually going to have a workshop over the summer where we can dive into the mathematics of this is as well but questions about this so far um can you explain more about time encoder here yes so uh so these encoder blocks first of all are are pre-trained so these are pre-trained on millions of images so its representation of what an image is is well informed by all of the images that it's been trained on so millions of images along with text because remember clip can generate that text from the images so it's learn the the association between image and text so the text that you have right here is represented in that same space along with the images so think of um think of a of a you know projecting into another space where you might have a total a totally different set of basis functions in this case your basis functions are going to be all of the all of the semantic space around the images so what does that look like well it depends on what it's been trained on but know that the training of this the pre-training of this is expensive so this is expensive to do the training but the inferencing where we just put in text and we put in the image is actually far less expensive that actually just looks like Matrix multiplies all the way down so you take the image and you take the text and you say please represent this in this large representational space that you've learned fantastic now that we're there squeeze it down to a smaller representation so you can think of that you know saving only the most important things now take it down again so now you only get to say the most important things and so this image that I may have started with they've had a whole bunch of things but what's the most important part of the image in order to save most of the information in the image the semantic information what's most required and now let's begin to grow it out again as we get to down to the lower lower layers and that's where the text encoding drives that Recreation of the text does that answer your question Sanjay oh and you're still in here it was I was wondering about how time and go to work system I can kind of oh the type oh there are things together as you described but it's not that clear how time and good works in encoder decoder so stable effusion actually can also work with video um and so that's going to be a crucial part for this um I am unsure and Abby tell me if you know differently if the time encoder is essential for the image only I am also not positive about that I don't think it's necessary for the image only but I will I will check um because I I know you I know I think I know your interest in this absolutely um I I will check on that I don't think it's necessary for that because normally those steps that I was talking about are captured naturally in the architecture so I I don't think and and to get really into what's happening here this is a reversible Markov uh process for the injection and the removal of noise that's what's happening with the with the unit not unlike what happens with unet more broadly other questions Okay so uh this made a huge uh this made a huge difference um so Abby can you share a little bit about what you're able to do with stable diffusion models and maybe you can sort of share your your interface take about five minutes if you could sort of share what you did and what we did for our AI Valentines uh a few weeks ago and muted sorry um yeah I can share um in one moment you're going to be a co-host and you should be able to share there you go and while you're getting started up I'll describe what we did for uh for our AI Valentines uh first of all did anybody here attend the AI Valentines anybody stop by it's mostly undergraduates so for the AI Valentines we um first of all use large language models to generate love poems and the style of different poets you could describe your Valentine and then it would write a poem in in the style of a poet about your Valentine uh one of my favorites was uh I I had one generated for my wife in the style of Sylvia Plath which turned out to be a dark dark but actually rather good Valentine's Day poem it ended nicely um so you know interesting juxtapositions there the other thing we did was you could generate Valentine images based on stable diffusion models so you could describe what you wanted to have and the fun thing was that we also had uh you could do um we at uh uh temporary tattoos that you could generate with these as well so we had a we had a lot of fun generating temporary tattoos um and Doctor yeah absolutely we will add you to the to the mailing list and we're going to break out those two those tattoos again because those were a lot of fun but Abby uh we were just talking about the doll e taking tons and tons of compute how did we actually generate the images for Valentines um yeah okay so I'm pulling up the um this is what we were using for AI Valentine's to generate um images which is a nice thing that somebody else made I did not make this um for uh running stable diffusion and basically getting to like play with all of these different um okay if things that you can mess around with to generate different images and stable diffusion and there's both um text to image modes here and image to image mode here let me ask you Abby where is this running though is this running on a huge Farm of uh of gpus no so this is running on just uh one little GPU via Google collab so free Google collab and it's running on pre Google so we've moved from Dolly where it requires a farm even inference it requires significant compute to being able to generate these images just based on a single two generations old GPU so here you did a picture of a galaxy this is especially funny so Dr petellante just completed her PhD about six months ago or so in astronomy so this is especially funny um yeah and you saw I mean just how quick that was right I enter a picture of a galaxy and I mean that in my professional opinion looks like a Galaxy and it generated it in like five seconds so it's pretty incredible so huge difference between stable diffusion models and diffusion type models versus the original version was much much smaller extremely fast inference on can you talk a little bit of a few of the problems that you had though um yeah so I had a lot of problems I think in um in text to image things are a little bit more open even in you know in your idea of what you want to generate um you don't sometimes you have an exact image in your mind but a lot of times you kind of are a bit more open to what it gets um I think image to image is very challenging in particular if you have an image and you want to make the resulting image look similar to the input image it's very hard to get it to not just kind of completely change that image and so uh I've had a lot of challenges getting image to image we talked about you know creating people who look good we also yet have issues with it changing the races and genders of people in what way well in in kind of a two-way where it definitely prefers making people white particularly if you have things like a group of beautiful people or successful people or like you know the more kind of positive words the more that it kind of thinks that's what it's supposed to make um but also just in it it really doesn't like to create diverse groups of people it seems like if it is going to make somebody in the image of the race its presumption is that people of a race only hang out together and so that if I'm going to generate one person of a particular race everybody in the image should be of that race because like diverse groups do not exist um and so even in an image to image if it changes the race of one person odds are it'll change it of everybody to match which is not usually that was a challenge wasn't it so actually you know because people were giving us their images and and so you that was actually a fair amount of the work that you were trying to do with people when they were creating uh images yeah so the models are becoming more and more powerful but we're still not addressing the issue which is what they're trained on so you can think of this as in some ways this technology is US holding up a mirror to ourselves this is when you train a powerful general purpose learner on all images and all texts this this is what it's seeing right now so um are there any other images that you'd like to or any of the other capabilities you'd like to uh to show us I see that you have the positive prompt here and then the negative prompt no scary eyes not just figured not ugly um yeah so this was uh an image that I was trying to um just change like the background so there was a group of people um they did not initially all look like this and I was mainly trying to just change the background of this image so they were not originally in front of a brick wall I was just trying to say let's put them in front of like a more interesting background maybe that would be a use case you'd want um and yeah so I mean one thing you will notice is despite me putting in the negative prompts and these double parentheses are just supposed to give extra emphasis to that thing um and despite me putting in the prompt not to kind of make them look weird that you will notice if you look at the faces lots of them look very strange um it is definitely a challenge and then you'll also notice in the positive prompt um again into to try to sort of create a realistic looking image I have all these words like you can specify the camera you've used um you can say HD 4K image quality I repeat again a high quality 4K HD photograph so all of these things that kind of trying to you know direct uh that kind of style to be in a particular you know high quality photograph um but it's still a challenge it really is uh very hard to get people to look good in stable diffusion thank you thank you very much Abby anything else uh we should know about um I don't think so not right now this is great thank you so much so let's go ahead and see if we can get a sense of of why controlling these is is so difficult let's let's take a look again at this architecture you notice right here with this architecture that that text comes in only at the beginning so that text is only in the beginning and it gets encoded and then all of this work is done being informed by the text but but that's all it's not driving it along the process at all so different tricks that we'll put in try to emphasize it and so that representation becomes heavier and heavier with both the positive and the negative but is it going to make it to the very end you know maybe or maybe not and then you also mentioned something else that was interesting which was you were trying to replace the background right and you you couldn't do it because you were having to do that with text alone so did I mention that things are are happening at a Breakneck pace well let's go and take a look at a publication that came out February 10th do you know about this one Abby I don't I did I've been out this week so I've not had a chance to tell you so I don't even think I've told you about this yet yeah I don't think that I've seen it so this is adding additional additional adding conditional control to text and image diffusion models the idea here is that if you try to train a model to be in a particular to follow a particular style or uh to to uh to perform in a particular way you tend you you risk disrupting all of the learning so Abby there were some models that were free trained and fine-tuned on specific types of of styles can you describe some of the specialized styles that these diffusion models stable diffusion models were trained on yeah so for instance like people um fine-tuned models with the specific goal of creating like um images that looked like Van Gogh had painted them so you could take any image and put in like a little token for Van Gogh and then you'll always get something that looks like Van Gogh painted it or that same idea but with different art styles you know pop art or comic or pixel art or whatever it is fantastic so one of the things that you gain when you get a smaller model is that it's faster you can inference it on almost anything and it's open source stable diffusion completely open source you know you can build your own uh and you can train it as you wish as well but the problem with that additional training sometimes it forgets what it was originally trained on so that means you might actually get less you might get worse performance generally out of it even though it gets better and better with Van Gogh remember with Dolly you could simply say Van Gogh and it was going to give you that but that's because dolly is huge you know huge model and has been trained massively on all this and so it can do better at giving to particular styles so that's one problem how do you train the models in additional Styles without losing everything you had two how do you control the generation of the image um in a particular way so you want to replace the background you want it to match another image in some way let's say that you want to do image segmentation and you want to say you're going to place other people or you want to change just the background so lead people as they are but change the background impossible in current approaches that was until this was released uh what is it two weeks one week ago now a little bit over a week ago now so with this the idea is you never actually touch that underlying model instead you have a dual version of the model that you can do additional training on so what does that look like well it's a very clever idea and I think it's one that we're going to be seeing also in the large language space quite possibly and that is you have a dual model you have your stable diffusion model and then you have your control net model so this model starts out as just a plain copy of what you had before but now what you do is you can allow yourself to do very heavy training of just the encoder blocks so this means if you you know given whatever kind of training Paradigm you want you can heavily heavily train these at inference time you still have benefit of everything that was learned before because these are all locked but now these are trained and so if you want to go a particular style and we're going to see you can do many things other than just style you can do heavy training here and remember heavy is not all that heavy because these are small models but heavy has in many uh many epochs and on lots of data and get very different Behavior out of the model while maintaining the quality of the image that you that you get and never unlearning what you had before so we're not going to get too much into the details here but let's go ahead and and take a look at what sort of capabilities uh this this gives you uh with these uh what are called control net uh stable diffusion models so first off I'd like to note is this is completely open source so love to see that all open source you know description of what they're doing you can go in you can analyze the the code the other thing is that they've done some significant pre-training on these models so let's say that you start with a picture like this you can use a type of filter a number of different filters like candy Edge but canny Edge does is just take edges and then you can generate pictures from that so this is one way of getting exactly the structure that you want I want this picture and here's what I want out of it now generate new images and now you can do whatever you want to describing other parts so these are not from the original image at all these were trained and these were generated only on the candy Edge so this is very fine game grain frying grain control that guides the generation of images you don't have to put in you know no weird eyes or no no weird feet although they're not feet they're whatever they're called on birds all right so that's that's one example all right how about a cute dog so now we're going to do candy edge with a cute dog all right you're going to get the same cute dog but now the fur may change the coloring may change all of that can change because all you're capturing here is candy Edge Sanjay did you have a question no I don't have I don't have a question I just trying to understand what's going on here which is makes sense yeah yeah so before and so here we have two things right so we have this is the text this is the image that you're putting in so this is all that the all that the diffusion model is seeing plus it's seeing the text prompt cute dog so if we were to go over here what's going in here is the canny edge so nothing from the original image what's the prompt cute dog let's take a look at another one so the other thing that you could do is there's another way of doing filtering uh ml LSD lines so this is just a way of picking out parts of an image that have to do with lines so this is only getting sort of the outlines so here's the original image this is the this is all that's going in and then the prompt is room so here let me go and zoom into this just a little bit so the text is room and then this is what it's generating from it so you'll notice it's respecting the different things that it sees this appears to be a couch or something like it and it's capturing that Faithfully in each one this could be a window and it's capturing that there's a window there it does something a little bit different here changes the flooring changes the materials changes the lighting but the architecture is the same okay let's do um another one again with uh with line Maps this time so now you have a building and it says building so the line gives you buildings of entirely different styles but this but the building itself does not change between the two if you were to put building in here and try to put a building like this you'd get something in the old style diffusion models without control net you'd get something that might look very very different it's not respecting anything about the architecture here it's respecting the lines in the architecture because that's what you have put in another type of filter um and if I'm not mistaken this is oil painting of handsome old man Masterpiece that might be the most interesting man alive I'm not too sure from the commercials from before but again very little information here and this is what it generates based on that same pose same you know orientation of head all of that stays the same even though everything else might change so and again here's the original image this is what gets fed in which is really just the line drawing and then here's the different images that you can get out all right let's do scribbles this one is pretty shocking to me here's an image where it was a scribble map which is just like almost like a child's drawing of it so you take a photograph of something get a child to have this is all that that the control stable diffusion model sees and then you say turtle so now you get a turtle where the background may change entirely this one's underwater this one's on top of water but it's filling in with coloring and with lighting and with background that makes sense for turtles the pose of the head is the same the leg is in the same position but it's filling in only where it needs to Okay so Dr yah um where is your mind going with this especially thinking of of your kids what would you like to be able to do with with this I think the example of the turtle is just absolutely remarkable and it's it's funny because I'm I'm always just so in awe and impressed with these tools but I I suspect the people that just know the inner workings of it or maybe there's some any but it's it's unbelievable this is just remarkable that we're able to do these things and I just can't get over how impressive this all is I'm pretty odd too for your kids what would you like your kids to be able to do with this so this is a generated scribble map I think and I'm I'm probably too much of a let's make everything into a lesson but really to understand um more of like well what's the difference between the uh scenery like why did it pick that why didn't it pick it from the desert um and for them to just kind of really Intuit what is going on in the background um and uh what they else they get out it'd be like well if you wanted um uh if you do a camel how do you think it would render it so yeah so that's where we get to interactive scribbles so given an interactive scribble like this and then the prompt which is dog in a room so you start with this and then it generates these let's do this live so here I have running on hugging face a control net taken directly from there but it's running on my own uh so it's on a beefy GPU but only one GPU and what I'm going to do here is I'm going to open a drawing canvas um oh is it are you still just coming up with it or is it it may be spinning up okay now I don't want to load I want to open a drawing canvas I'm hoping that you will do this okay I'm going to try to restart this earlier this morning chat GPT was down so I don't know if they might be having some general issues but Twitter was collectively just losing its mind uh this one I so it's having to rebuild um what the interesting thing about this is and hopefully this will come up in just a moment but the idea here is that you interactively draw some kind of a picture you give it a prompt and then it will generate different versions of this for you uh in uh real time so let's go and see if we are up and running yet still building okay but let's go and finish taking a look at what happens with fake scribbles this case it's creating scribbles from an image and you say bag and then these are the different images that you get notice how photo realistic these are so and sort of right out of the back it's because it's being constrained controlled by that strong input of the drawings it highly constrains what diffusion models can do same thing with shoes so this is one way of going from you know one to the other but now you can do things like human pose so here's a person posing totally natural pose I do that all the time you know so I'll pose right you use pose extraction and then you ask for Chef in the kitchen with exactly the same pose we can do the same thing here using a different type of pose model seeing okay so this is a pose of a person you know totally different but you've captured the pose right and this says astronaut on the moon so the astronaut on the moon has the same pose as what was in the picture it's the only thing that's being carried over now is the pose let's do image segmentation so here you start with this image here but remember you're not using the image you do image segmentation of this is house this is part of house this is grass this is something else driveway this may be bushes so render an image where this part is house this is like outer part of house driveway grass otherwise other than that you get to do whatever you want to so Abby you can imagine in that picture you do the image segmentation and people and not people all right again with the segmentation Maps and here you're saying River you can also use depth so here's a 3D image if you capture only the depth then you can actually create other images that have that recreate that same depth normal map yet another way of pulling out information again 3D information from normal map this is just giving you you know that that distance so now you have other Abraham Lincoln's generated just from the depth map and here you have you know a depth map just from that particular person so you actually are capturing pose because of that anime line drawing so um this in the last week so a great deal of control that's possible oh looks like I won't be able to do that interactively um in the next half hour we're going to be trying some of this interactive so we're going to be trying uh the uh uh we'll be trying some of the uh interactive using dream Studio I emailed this to you hopefully you had a chance to sign in so you can try the old style non-controllenet version to see what that looks like and uh and you'll be able to get online I'll send you the link for control net it may take a while for it to render I'm hoping mine will be up as well so let's go ahead and conclude these models which have the ability to generate text from image and image to image are really quite new we've only had them around for about the last two years and really they've only come into their loan into their own in the last year a year ago we're able to start working with models more interactively on our own Hardware with the Advent of diffusion models now with the Advent just last week of control net diffusion models we have the possibility of finally controlling the models and doing both image segmentation semantic segmentation and also transfer of only very specific parts of images that we want to have carried over to the new models so these are great movements uh in in development but you'll notice that nothing that I've talked about addressed the images and the problems that we talked about at the beginning of the hour so where is that Improvement going to come from well those of you who are part of the workshop last week might remember reinforcement learning from Human feedback that was something that really moved these large language models into something that was more useful it mitigated the harm and bias but didn't remove it but mitigated it my prognostication is that we're going to start seeing that same type of reinforcement learning from Human feedback in these types of models as well steering at a deep level the learning that these models have second prompt engineering is going to come become more and more important in these types of models as well Brooke when you and I were trying to fix the images and you know not sexist you know not working because they removed that you know professional clothing helped so that type of prompt engineering is going to be important as well but also control net may offer some ways of controlling that as well control net works not just at that image level but could also worked at the text level as well so can we modify based on that we don't know yet that's open area for research but what is certain is that whatever happens is going to be happening quickly you shouldn't just take the model outputs and and leave them as is be aware that we're holding up this mirror to ourselves and we may not like what we see so be careful when you're using these but absolutely explore and use them as broadly as possible to enrich the lives of your children to use them in education but just use them responsibly all right let's finish up this this first part thank you very much for all of your time let's move into some interactive work with this then anyone that needs to take off we've had a number of folks already had to leave for the at the one o'clock hour those of you are still here uh let me go inside a send a link to this control net and you can get this to work it's going to take a little while for it to bring up but this is the control net that you can try so that is control net but here is a link you only you'll need to sign into this but this is the link to the base diffusion diffusion model so I would try the base diffusion model first that second link that I just sent you go ahead and sign up and I will actually share with you some of the things that that I can create here while you're pulling that up and that I'd love to see some of the things you're creating so first of all questions before I start generating here and I'm going to ask you to to tell me what to what to create using diffusion what questions do you have with tools like chat EBT where you can essentially communicate your thoughts be a prose how far away are we from tools where you can use natural language to communicate with Photoshop say like make that tree go away or make her shirt red move these two people closer together we are there actually uh the problem is that it's an imprecise knife so I think what's going to be happening next is especially with control net being open source very rapidly these companies are going to do something along the lines of you know indicate which part of the image you want to edit right and then using a control net to only do operations within that or even better yet um something like you know the outlines so click on what you want to edit and then have it choose either the outline or the semantic right so you can say I want to only edit the house point to the house this house then we'll select the house by using semantic segmentation then use control net to only operate on the house right and then okay for the house do you want to change the structure you want to change the you know because then it's going to be are you going to be doing like an outline are you going to be doing you know these are all different things that are much more much better controlled with control net um like I said because whoever creates those tools first is going to have a huge Advantage so um Adobe is going to go there soon they're not going to feel like they need to do it immediately plus they don't want to kill their own business right but other companies absolutely are going to see an opening are going to say like hey are you tired of you know wading through Photoshop here you know yeah that's amazing Brooke um well like in canva and some other software you can already just eliminate the background which is the click of the button yeah which is super convenient so can be able to try to use like the magic generator to try to like get an image out so it's pretty amazing what shall I try here you'll notice the prompt is at the at the bottom of the screen here it was a dream of a distant Galaxy um what are what are your suggestions about what I should try to create here maybe a friendly alien oh sure okay so let's do a friendly alien and do we want to have like a high quality do you want that or do you want it like in a style uh [Music] um I mean if you do it like photorealistic it might be interesting but you could also decide typically the stuff I do I usually try to make it like pop art or like digital art something that's going to be a little bit more you know yeah we'll do both did you notice how it sort of thought was Fuzzy and then got Sharper that's diffusion so what you were looking at there was its first guess based at that high level representational and then it went to the pixel level so what you were looking at was diffusion there friendly okay a little bit okay this is friendly maybe friendly if I woke up and saw this above me no this one I would scream okay so let's go and try uh so we can try pop art and notice it getting fuzzy and then it will how fascinating safety filter I have no idea why all right give me something else to try and notice that this was really blunt right this is like text and then you're gonna get an image out of it what else should I try think along the same lines or no it could be something brand new how about an illustration of something that you might want to have for a paper or for we could try that Dr yahwe what do you what do you think oh goodness um we are doing an evaluation to see how well um chat GPT can do on maintenance or certification board examination so how about a doctor studying on a computer at home this is potentially really disruptive because these are all self-taste exams and you can feed multiple choice questions to it we've done with test data set and it does extremely well like with the US Emily and stuff so I don't know if that have any implications for these self-paced exams now oh man I'm looking we're having a conversation shortly aren't we yeah I'm really looking forward to that okay Doctor studying on a computer at home I knew it mail mail occasion yeah so let's go and change this um and so here we here see and here we are having to say a female doctor so that's why I'm wondering if reinforcement learning from Human feedback could be you know what you know let's you know yes this is the base of what you've been trained on but let's yeah let's make the first pass so I don't have to do this hands aren't too bad what else should we do with this but these are only for medical doctors so just oh I yeah and and you notice the cues here are like they all have stethoscopes on which is interesting because heaven knows you know that's that's you first thing you do before you start working on computers you've gone you put on that stethoscope um let's let's try another one should is there any any alterations we should try on this one or would you like to try something did you have something oh Dr yeah go ahead oh style of Van Gogh I think is always fun so something else I'll point out which is funny you notice that I'm sort of naturally typing a sentence here so the folks who work and really get into prompts refer to these types of prompts as Boomer props because we tend to we can't help ourselves we write sentences whereas you'll notice that Abby's prompts were like just words and Concepts and the double parens around them too and it's just like so this is a boomer prompt does it work less well um it might in some cases not bad and again the hands are like the hands here are getting a little bit freaky but uh and and I guess Van Gogh just didn't draw that many stethoscopes we had to give those up or laptops I love that the laptop exactly so uh Sanju what uh what would you like uh to try actually um I was wondering how this works with some number so for example say that you know for my paper I want to present really complicated descriptions uh for example like what about you know action between X and Y on G but my first challenge here I know this is only for images but I was wondering how this works with a whole bunch of values with images you know it has if you could put something in the chat for me to for me to try I I'll I'll try that um it has problems with words and it has problems with um uh numbers I I believe it will have problems with it with numbers because remember it's working in the latent space so and it's not that large of a model and so it doesn't have a lot to borrow on where it's learned the words on a page um but it might be able to do some but this is where controlnet comes in right so you could you can absolutely imagine you know training your own special control net which is able to take text and put it into an image in a particular way like here's the text the text has got to be there but it might be on a page which is at an orientation you know or at an angle um so you can imagine I might be able to do that so can you think of if you want to put something in the chat I will copy and paste it from the chat and then uh put it in here let me see okay I'm doing and you know interesting thing about this this is also this one's a bit larger this model but absolutely could run on a beefy GPU um this would not run as well on like a free collab but maybe a paid collab this one could run on so again order of magnitude and not and I don't mean in the incorrect sense usually when people say order magnitude they mean 10 I mean order of magnitude in terms of 100 times smaller than the Dali models and also in terms of speed as well so we're moving into something that's much faster to generate a linear version okay okay let's see what it does oh I have no idea what it's going to do hmm oh well there's one so do any of these make I noticed that it's making up the words here right and it's like it's it's word like but it's not actual words I love the sort of illustrations it's like getting in the Navy this is fascinating so when I looked at you know one picture can have different kind of variations so say that um in say a set package we have one kind of picture so we want to have some kind of variation for that picture so I was wondering if there's some kind of applicability here um so when I when we provide an image password from one step package whether they can produce both with different kind of versions we can use so yeah so here it is possible to put an image in here as well so if anybody has an image I think that we can share that on here but I'm not sure so it can also do that prompt and image to image um but uh we just have to have an image to sort of notice that I was doing stable diffusion 1.5 it actually goes up to two two has some problems compared to 1.5 actually more restrictions but but these are things you can try out so if you follow that link has anyone followed that link and tried some things already that you'd like to share and Sanju that's interesting what you're mentioning there and I'm thinking and this is something that that we've not even begun to explore yet because this this model is is brand new and the training of this is not bad um I'm going to be really interested in seeing uh you know they did training of all these different types of models I'm wondering what training of this looks like and I've not had a chance to look in here to see yeah they do so they see they talk about training it here so some of the things you're talking about may you might be able to train them right so you know train on a large number of uh of illustrations uh and especially if if you have wording that takes you from one to the other or modifies it in some way could probably be trained what should I try uh next year and I'm going to see if this has come up yet oh please come up ah this is killing me give me a quick say I'm going to see if I can um I'm going to restart this space foreign to restart and get started because I would love to be able to show you it was so fun doing the real-time generation I did a bunny I did a terrible job drawing the bunny uh but it filled it in beautifully um you know sort of the the pose and the direction the gays Direction um I tried in that one the original prompt that I used diverse women and coding sitting at computers coding very different than what you get mid-journey really okay oh here we go okay great y'all can you all see my interactive scribble screen all right I will try to draw something so one thing to keep in mind with this you'll need to reduce that size it starts really big so I can try a dog okay so here here's my picture of a dog this is a tail that is the um here's snout legs and you can sort of get the the sensor that that maybe data scientist was a very good profession for me since I can't draw but let's see what it does with that so let's go and do the uh and we can do a cute dog and then what we're going to do is we're going to go ahead and run it you notice that it's processing up here a little bit terrifying but my drawing was pretty terrible it's actually pretty so interesting comment that you made um well you joked about you know being a data scientist and not an artist and I think um an interesting conversation I've had with my friend of mine a colleague of mine who and you can be both I'm not I'm sorry yes but go ahead is just the concept of um these tools potentially like democratizing Talent um whereas it used to be something unique and special about this person and she's not you know lamenting it but she was like oh yeah now this is something everyone can do um I I just think that's interesting I think you're absolutely right and it's and these are really deep issues right I mean this is um so the one is yes it can augment so first of all there's the issue of being trained on artists work without their permission and without compensation so that's a thing but then there's also you know that question of you know to be able to do this used to be require a huge different set of skills that now everyone has so can it augment your creativity right can you do things very different than what you could than you could before I know somebody who has been um trying to train uh models on his artwork he's an architect actually he doesn't have time to you know but he's been training the models on his on his artwork to generate new kinds of things but with different feelings you know different and so that is absolutely something that you can do uh and with especially with control net that you were just seeing you can actually get very fine-grained control of not just shapes but conceptually as well semantic segmentation and it really opens the door to other kinds of things as well you know so you have the blue period Well you know what if you have the slightly manic period I mean quite literal you can say okay now here's the feeling that I want imbued and everything that I have and then generate based on that so whole new fields that are going to be opening up but then you know then you also have to I mean there's there's sort of an existential question that comes around this around this too you know who is creating a if if this can create things better than I can then I than we have the way we've done in the past and what is my worth you know these are deep questions that uh that we will be dealing with uh uh yeah in addition to making you know the cute puppies so we're gonna we're gonna be wrestling with this uh as we as we move forward all right any any uh requests on on things I should try drawing and see how well it can generate so in the control net that you have here you might have to wait for many many minutes before you can generate but do know that if you want to pay uh this is hugging face if you want to pay them for compute then you can clone this and then run this on your own um so I'll definitely be doing that I've done that here I'll probably take it down to a less expensive GPU I put a pretty beefy one on it so it'd be quick for you all um but uh but know that that's possible to do well to do as well it's not that expensive eventually of course this will absolutely make its way into applications but it's about a week old so uh this is going to be the way for you to get access to it right now all right folks we're at the end of the time thank you very much for all of your questions your comments and uh and uh uh suggestions here know that uh like to point out a couple of things real quickly at the at the end here uh if you're interested in uh in diving a little bit deeper into any of this we're having a workshop on March 22 I'm sorry we're having a a symposium March 27 March 28th March 27th is going to be Workshops the 28th is going to be all day AI Revolutions in Academia and in research speakers and panels um on on the impact and this is still coming together we were asked to do this very recently um so do not be surprised if you actually get asked to be on a panel because uh uh we're going to be interested on on many viewpoints of uh and many different uses second we have our AI summer this is going to be May 8th through June 2nd where we're going to be talking about prompting prompt engineering and all these different types of models and how to train them and how to use them even from scratch so this will be an intense of four weeks but it'll be a great four weeks for you to get a good background we offer this every summer and every winter and not a one of them has been the same because the things keep on changing so quickly so if you've attended this before you probably want to attend again finally I'll also mention that I do offer a semester class semester-along class um on the standard University undergraduate calendar on AI models Transformer models so if you would like to sit in or if you have students you would like to attend um it's through the data Science Institute but they are more than welcome to participate in that class where we get very much into the theory in the mathematics of what make these models tick all right I think that is it thank you all so much real pleasure talking with you all today thank you so much you guys are the coolest people I know this is amazing see you all soon"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Text-to-Image Models: DALL-E, Stable Diffusion",
        "Video Title":"DALL-E vs Stable Diffusion: Which AI Text to Image Generator is Better ? A Practical Test",
        "URL":"https:\/\/www.youtube.com\/watch?v=UeykRura7sA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/UeykRura7sA\/hqdefault.jpg",
        "ID":"UeykRura7sA",
        "Publish Time":"2024-09-16T21:44:37Z",
        "Channel":"MaxonShire",
        "Channel ID":"UC9DIS5jutRK2uCSreTJFWOA",
        "Transcript":"hello and welcome back to a new test in this video we will perform a comparison test between the free version of Dolly 3 via Microsoft Bing image Creator and stable diffusion via dream Studio to see which AI image generator is better and processes the details of the prompts accurately we will execute four prompts on each website simple for a one point modder for two points Advan for three points and a sophisticated prompt for Four Points please note that the purpose of this test is to show how reliable is the AI tool in generating the expected output based on the given promps therefore we will ignore other factors such as pricing the speed of the processing Etc let's start our test first the simplest prompt a tree and a park during Autumn let's execute it on both platforms of course we will try to pick the accurate one out of the four outputs now both platforms generated acceptable results but Dolly added lots of unwanted details such as humans animals while stable diffusion generated exactly the expected elements and the overall quality is a bit better therefore one point goes to stable diffusion next let's move on to the second prompt which is a bit more detailed and worth two points a mountain range covered in snow under a clear blue sky with pine trees lining the slopes after comparing we can see that both platforms generated similar results with all the expected objects and details mentioned prompt are in both platforms with good quality and therefore two points goes to both platforms now let's process the advance prompt a medieval night in silver armor standing on a cliff overlooking a vast Battlefield at dawn with mist rising from the valley and Banners flying in the distance now the medieval Knight wearing a silver armor standing on a cliff appears in both platforms and the Mystic Valley appears as well however I can't see any flying banners on the images generated by stable diffusion therefore three points goes to Dolly while stable diffusion gets two points for overall good performance finally let's process the most advanced and sophisticated prompt An Elegant ballroom with chandeliers marble floors and floorlength windows revealing a vibrant Garden outside filled with people in 18th century attired dancing gracefully under soft golden lighting all the objects in detail such as chandeliers Marble Floors floor length Windows revealing a vibrant Garden outside 18th century people dancing are all present in both systems therefore Both Deserve full for points now to final result both platforms of almost similar accuracy and quality with a score of 9 out of 10 and that means that you can use both platforms with great confidence and switch between them when you run out of credits please note that the paid plans might produce a slightly better results but I don't think that they will be significantly much better in my opinion the main advantages of the paid plans are having more credits to use in robust generation before this test we run a similar test between idiogram versus Adobe Firefly please like And subscribe and then watch that video from the follow-up video window that will appear on your screen"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Text-to-Image Models: DALL-E, Stable Diffusion",
        "Video Title":"Prompt Engineering Tutorial: Text-to-Image (Midjourney, Stable Diffusion, DALL\u00b7E 3 &amp; More!)",
        "URL":"https:\/\/www.youtube.com\/watch?v=6RAStep_3OI",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/6RAStep_3OI\/hqdefault.jpg",
        "ID":"6RAStep_3OI",
        "Publish Time":"2024-07-15T16:30:08Z",
        "Channel":"Learn With Shopify",
        "Channel ID":"UC7geKfz2-IH0rsgRBtHTm0g",
        "Transcript":"can you tell which image was created using prompt engineering hint hint it's the better one if you found this video chances are you've already tried AI image generators but they never seem to give you what you want well don't worry because in the time it takes for you to watch this video you will be a prompt Pro but before we get started I want to introduce you to Sammy here he is isn't he cute but he could be a lot cuter I created these images by inputting the simple prompt a dog sitting by a lake into mid Journey my my intention for the image is to create an atmospheric background for a camping supply website and building the possibilities are endless with AI image generators on one side I can write a prompt that sells the calm peaceful Tranquility of retired life in the wilderness but change a few ingredients of the prompt and now I have a creepy haunted lake that still gives me nightmares to this day nightmares aside in this video I'm going to use our boy Sammy here at my camping supply store to demonstrate for you why prompt engineering is so important when it comes to text to image AI I'm not only going to show you the breakdown on what to include in your prompt to help AI work with you I'm going to show you how to use resources to help you such as Gemini and chat GPT what settings to look for in your AI generator so that you're set up for Success the first time and many more I'll be primarily using mid Journey today but I'll leave in differences you might see in other apps like Dolly 3 and stable diffusion [Music] welcome back to learn with Shopify I'm your host Rachel fiser and I will be summarizing so many articles YouTube videos and Reddit posts that I watched and read so trust me watch this video it will save you so much time and turn you into a master prompt engineer this is part three of four-part series that we have created here on lar the Shopify about prompt engineering part one is hosted by Michelle and provides you with a general overview about what is prompt engineering and how to use it in all fields part two is about textto text prompt engineering for copy hosted by Bridget and part four is about textto video prompt engineering hosted by Charisma if you want to watch our prompt engineering Master Class you can find the video links in the description sections below let's do this what is prompt engineering you ask well prompt engineering is the skill of creating inputs for AI tools that produce optimal results a good way that I like to look at it is the difference between making a box cake and baking a homemade cake both Avenues give you cake but one gives you more high quality product because the recipes are a little different this video will be about creating prompts for text image AI generators we are going to be mostly using mid journey I do want to say there's two versions of mid Journey the first one you can access through Discord and the second one is on their website I'm going to show you both throughout the video so the prompt a dog sitting by a lake is our box cake example of a prompt it gives us a picture that definitely qualifies as a dog sitting by a lake but let's see what happens when we increase the quality of ingredients we are using how do we do that we do that by increasing the specificity creativity and details of our prompt let me just say the amount of videos I watch that claim to tell me how to prompt engineer and just ended up providing me with a bunch of adjectives was one too many okay I will talk about it a bit but I will not make your ears bleed and I Will Not Waste 10 minutes of your life to summarize what they all basically say is that AI image generators won't ask you what you want they will just show you what you asked for so you have to be descriptive and provide key details such as who what your subject is what they're doing adjectives about your subject even style lighting inspiration Etc now I've already mentioned a few key details in that list but let's break that down step by step and flesh out our box cake prompt example together the first step before writing a prompt is intention ask yourself what is this image for and what do I want the audience to feel the biggest problem people have when prompt engineering is lack of specificity so knowing what you want going into it is already going to bring you that much closer to your goal in image so let's say I want this image of a dog sitting by a lake for my website that sells camping equipment which by the way if you don't have a website yet you can have one in minutes Shopify makes it super easy to build an amazing website and we even have a free trial on offer so check that out the link in the description below when people look at Lil Sammy enjoying that Lake I want them to feel enticed to join him I want them to feel peaceful at ease Serene and long to be by that very same Lake keeping that in mind let's continue so what is the subject of your image for us it's easy a dog how would you describe the subject we can do better than just a dog because Sammy is more than that hint hint a thesaurus will be your best friend but the first steps are easy I'm just going to pick a breed such as a golden retriever cuz I'm partial to them cuz spiritually I am one how about a middle-aged scruffy golden retriever but let's not stop at his appearance what feeling does he give off perhaps he looks Serene at ease lazy and cuddly we are not going to include all these words for now just the ones that dep pict what we want in the most accurate way details are important but it's equally as important to be concise if you make your prompt overly complicated a lot of generators may get confused so with that in mind I'm going to keep it simple and just start with middle-aged for now what is the action he is lying lazily by the lake watching the birds patiently waiting for his owner to join him it's important to make your subject active even if Sammy is being a lazy good boy it doesn't mean they have to be moving but it does mean they have to interact with their environment in some way by including action it tells the AI how your subject interacts with other elements in the image and typically results in a more cohesive product again the more concise detail the better what does the lighting look like if you want your image to be lit well it better be lit I'm sorry I had to a lot of people forget about how important lighting is and how easily it can transform a mediocre flat image and elevate it into something outstanding it affects the mood atmosphere texture warmth and focus of the image which is why it must be included in your prompt if you want a high quality image so since it's a camping scene we definitely need a fire which will provide that cozy Ambiance we're looking for but I also want a little more magic what's more magical than golden hour at night so let's say we have a middle-aged golden retriever laying by a calm lake watching the birds fly across the sunset at Golden hour just to the left of him sits a smoldering campfire casting a warm glow across his fur o I can smell the marshmallows already I can also add detail about whether he is back lit by the sun in soft focus to diffuse it or in harsh light if I want maybe longer shadows in the image the more you know about lighting the more you can guide AI don't worry we got you covered check out the description below for a link to a PDF of a quick list of lighting terms to use what lens am I seeing this image through lenses this is something that people skip over all the time in their prompts all images are seen through a certain set of lenses which includes Our Eyes by changing the lens we can dramatically change the image so let's take our box cake prompt look at what happens when we change the lens to a wideangle lens or a fisheye lens completely different images we also dropped a quick list in the description below of some lenses you can use but if you really want to get crazy you can put in your favorite make or model of a lens like if you were doing a face portrait try Panavision Primo 70 mil that lens is worth more than my car and you can get it for free with AI to be honest a lot of things are worth more than my car at this point for this prompt I'm going to use a wide angle lens because I want to capture the landscape because of course the whole reason people love to go camping is because of those beautiful views I'll show you how I add that in towards the end all right moving on angle angle is another ingredient you don't want to miss where is the camera in relationship to the subject matter right do we want to be looking up at the subject at a low angle puts the character in a more dominant hero position or do we want to be eye level which is like a natural perspective again the Quick List is in the description below we got you I'm going to using an eye level because I want the viewer to feel like it's their campsite that they are looking at what style do I want this image to be animated or in a comic style or do I want it to be cinematic or hyperrealistic do I want this to be a digital image or an oil painting when it comes to style feel free to use specific medium terms like I mentioned before but also remember that a lot of AI models are trained from images on the internet so the best way to get it to understand what you want is to include references it would understand so feel free to include references to Van go or Da Vinci or Monae for example sabled Fusion not only loves descriptive words but has a feature where you can upload a photo as a reference with your prompt however make sure it's an image that is in the public domain or is one you own the same thing goes with D3 with the additional caveat that they will decline a request for a reference that refers to an artist who is currently living ethically I would say avoiding terms and references to artists living and working is a good rule of thumb to go by when using AI image generators especially if you plan on using your image for commercial use you don't really want to get into copyright trouble but AI image generators are great for using a pinch but we also want to respect work created by real life artists finally don't forget to include your desired aspect ratio and resolution of your image aspect ratio affects the height and width and is important so that you don't accidentally create a photo that is too small for where you need it and therefore becomes like a blurry mess when you blow it up somewhere else similarly resolution affects the quality and detail of the image the higher the resolution the more detail the lower the resolution the less detail you get the idea now in mid Journey the first variation of your picture is a smaller resolution once you find the one you like you can upscale it to a higher resolution pretty handy once again we have a breakdown of some aspect ratio prompts in the description below also fun thing about M Journey too is after you've picked your photo you can also expand it so you can either have it expand the view so it can become bigger or you can shrink it down so if you want to like narrow in on the dog you can do that or if you want a wider view of the landscape you can do that too having answered all those questions to the best of our ability let's see what our prompt finally looks like a middle-aged golden retriever Lying by a calm lake watching the birds fly by Across the sunset at Golden hour just the left of him sits smoldering campfire casting a warm glow across his fur a green camping tent sits in the foreground by the fire a hyperrealistic wide angle comma eye Lev perspective period D- AR 16x9 let's see what happens when we put this into mid Journey already there's a huge difference in detail as you can see we have a lot more going on in this image but our prompt isn't quite perfect yet also if you're wondering where all of your images are going mid Journey puts all your outputs into your own web page with a reference code and prompts so if you're ever wondering what your prompt was for that really great image you liked way long ago it's all there chances are if you found this video you've wasted time with bad prompts and spend all your available credits on your image generator of choice well friend I have a great resource for you you and you can thank me later and that is the website Lex Lex will allow you to view unlimited images and prompts without using up any free credits so for instance let's input our box cake prompt a dog laying by a lake already you can see tons of images showing this all ranging in Styles details lighting Etc hover over when you like and it will give you the prompt that was used to generate that image that way you can see what kind of details have been included and How It Was Written which is helpful if you're learning additionally the prompts themselves May feature terms referring to style texture and lighting that you can click on which will then bring you to another page of images specific to that term which is great to learn about lighting and all that stuff you can also directly copy The Prompt used and apply it to any other AI generator it may not create the exact same image but especially if you find when you like on Lexica it could bring you that much closer to what you're looking for so put that in your back pocket but now that we understand the breakdown of what makes a good prompt let's see what happens when we ask gbt and Google Gemini to do the work for us because after all who else should we asked to help us craft the perfect prompt for AI generators than AI so I straight up asked chat gbt to create a prompt for chat GPT that would tell it to work for me as a prompt engineer the initial result was a little indirect but it did end up working a bit however I tweaked it here and there and I used it for both Gemini and chat gbt so here's what you can type in you will act as my professional prompt engineer for the AI generator software mid Journey or dolly or stable Fusion whatever you want I'm creating an image for my business website Instagram page and I need an image that will capture Beauty peace calm whatever kind of adjectives you want to put in there here is the initial prompt you're going to put in the box cake prompt we talked about I would like you to provide three variations all high resolution realistic set in a wide angle lens or whatever other preferences you have just don't forget to include aspect ratio okay I did this twice with each Avenue once with our box cake example and once with our homemade cake example okay so side by side I think I like the Gemini responses a little better just because I mean look at the way it brings everything down for you yeah but let's start with the prompts we got when we asked Chachi PT and Gemini to enhance the box cake prompt we used okay so the dog laying by a lake for the Chachi PT one I'm going to delete this variation and just start the prompt at a dog lounging because again we want to keep it active at the beginning side note for all the prompts though the only thing I'm going to edit is the aspect ration so I changed it to D- AR 16 by 9 at the end of The Prompt so it will read better that just reads better on Mid Journey which is what I'm using okay now let's see the chat gbt image okay now the Gemini one okay so side by side these look pretty awesome but let's see what happens when we take the prompt chat gbt and Gemini enhanced from our fancier one the one we made together let's see if that makes anything better is more detail better let's see okay so great now let's compare the image at the beginning of the video created with our box kick example alone with the ones that the enhanced prompts from Chachi BT and Gemini okay so as you can see while the first image is kind of flat it doesn't really tell a story it doesn't have really any Nuance even though the prompts themselves see a little technical finessing already you can see the difference having a clear intention and detailed descriptions can have on the output of your image the reality is even with a perfectly pristine prompt you may still be running into some issues don't worry it might not be the prompts fault you may be underutilizing some key features some common features that you should look for and play with are negative prompts weight and seed number so just like how regular prompts tell AI what you want negative prompts tell AI what you don't want in your image let's use our box cake example for instance we want the image to be warm Serene calm so I can put words that are opposite to that such as cold ominous depressing into the negative prompt and AI will emphasize the positive qualities we mentioned in our initial prompt so it plays off of opposites essentially Sable diffusion has a separate Feld for negative prompts but for Mid Journey use dash dash no followed by what you don't want and it will pretty much work the same you can also highlight an area of the image you don't want or change something like add a hot air balloon like we did here because who doesn't like a hot air balloon weight Pro PRP strength or prompt adherence they're all the same they're just under different names determines how heavily the AI relies on the prompt to create the image the lower the weight the more creative II will get the higher the more specific it will be to the idea however if you put that setting too high you may lose some of the variety AI can provide you with or it'll take it like too literally if it's too low it might leave out details such as our boy Sammy which we don't want we love Sammy so it's important to check where your prompt strength is set so that you're getting the desired output from your prompt this is something you have to play around with a little bit in midly but I recommend about a little less than 3\/4 of the way up for your first prompt this gives AI enough room to play while also remaining true to what you're asking for if you find the images that you're getting just need like small little tweaks then check out the seed number a seed number is a series of numbers that AI attaches to the image it makes from all the values determined by your settings and the prompt this allows for an infinite amount of variations from one single prompt so if you use the same seed number with the same prompt and the same values it will turn out the same image every time that's how it's supposed to work if you want to see more variation or even just slight changes having a look at the seed number is a great place to start copying and pasting it next to the prompt you use and then changing one number at a time is a great way to see all that little variations you want now the sign of a real professional prompt engineer is someone who uses natural language enhancements in their prompts this specifically includes dashes and brackets they add more Nuance to your prompt by adding emphasis indicating range Styles and altogether make your prompt easier for the generator to read so dashes are great to include in mid Journey Dolly and stbl fusion software prompts because they separate elements making it easier for the AI to interpret an instance you would use this would be something like a quiet Lakeside dash dash a lush forest in the background so there I'm separating the image of the lake and the description of the forest so it doesn't get lumped together another way to use this would be right before your preferred aspect ratio just like we did earlier so Dash D AR 16 by9 or whatever one you want to use you can also use it at the end of your prompt for like dash dash wide angle all that stuff brackets can help communicate where you want the focus to be in an image or add emphasis to the use of certain color theme noun Etc so say I want the lake to be extra Whimsical or Serene let's put brackets around either one of those terms this effect applies to Mid Journey dolly three stable diffusion but remember to use them sparingly if you use too many you might confuse the AI it's like I don't know which one's more important here are some specific examples you can play around with when using mid Journey so say you found a prompt that gives you what you're looking for but you want more options type in-- repeat space and insert number let's use seven for example and it will use the prompt to create seven different images input-- chaos space number the number being between 1 and 1,000 and you've chaos mode this will diversify the variations it creates and give you some more avanguard ideas the lower the number the less variation the higher the number you get the idea it's the same old game you can interchange the word chaos with words like weird or stylized or creepy or raw and they will pretty much do the same thing with consideration of the word you use but if you really want to get wild don't be afraid to combine the goats I know I know it's crazy but why not type in Das Das chaos next to dash dash weird and dive deep down into the rabbit hole and see just how endless the possibilities are with all that being said let's now add some final enhancements the first prompt we made together all the way back at the beginning of the video feels so long ago a middle-aged golden retriever Lying by a calm lake watching the birds fly across the sunset at bracket golden hour close bracket just to the left of him comma sits a smoldering campfire casting a warm glow across his fur period a green camping tent sits in foreground by the fire period Das eye Lev Das hyperrealistic Das wide angle lens Das Das AR 16 by9 and finally at long last here is the final image so let's take the first one and compare them side by side now this image isn't perfect it's great but it's not perfect however I just saved myself a ton of time by inputting a high quality prompt the first time from here the distance between this image and the one I'm after is much smaller than when I started with our box Kake example as generative AI continues to evolve so will our ability to communicate with it and hey since you made it to the end of this video you're already that much better at it if you want to see more like this video let us know in the comments and make sure to like And subscribe we love having you here so why don't you stay thanks for being here today I've been your host Rachel Fischer and I'll see you next time [Music]"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Evaluating Generative Models: Inception Score, FID",
        "Video Title":"Inception Score for Evaluating Generative Models",
        "URL":"https:\/\/www.youtube.com\/watch?v=EQhuB5zhQwo",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/EQhuB5zhQwo\/hqdefault.jpg",
        "ID":"EQhuB5zhQwo",
        "Publish Time":"2024-04-20T16:51:55Z",
        "Channel":"AI Focus",
        "Channel ID":"UCsb8Jq2kjD8_Lx0M8E6_EdQ",
        "Transcript":"in this video I introduce Inception SC for evaluating generative models Inception SC is a magic to evaluate train generative model G that includes distribution PG over image X this figure shows the basic idea how can we use Inception Network to evaluate generate model by computing the Inception scroll the Inception scroll is well related with the human judgment the given image X the model G predict conditional probability P of for class label Y where 1,000 matches the number of classes in the image net in s scroll is defined as exponential of expectation where d k is a k diverges and py is a marginal probability to generate diverse images the marginal probability py show have a high entropy a lar of Divergence between conditional probability p and marginal probability py will result in a high Inception score to Compu Inception SC from samples XI using monol approximation we construct a empirical marginal distribution P had from the trinum data set where a is number of image sampled from the model log Inception scow is defined as a expectation of K Divergence over the T imagees here where conditional probability is a probability of the label Y which is generated by the trend model G on the Tesla image the XI for more details please check with this references Than You by"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Evaluating Generative Models: Inception Score, FID",
        "Video Title":"What is an Inception Score (IS) in AI Art?",
        "URL":"https:\/\/www.youtube.com\/watch?v=eKfO2RFF2QE",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/eKfO2RFF2QE\/hqdefault.jpg",
        "ID":"eKfO2RFF2QE",
        "Publish Time":"2023-06-22T14:27:37Z",
        "Channel":"Eye on Tech",
        "Channel ID":"UC-e2aGRMGMl67MDJoqcj19Q",
        "Transcript":"[Music] we've all had fun asking AI platforms to generate images some more successful than others but aside from Human judgment there's a way to score how accurate an AI image is to The Prompt it was given it's called the Inception score or is a mathematical equation that's just too complex for us to get into right now but basically it scores images on a scale from zero being the worst to theoretically Infinity the best the is based on two factors quality or how good the image is and diversity or the variety and randomness of the images it produces for instance say the AI is producing images of cats quality images should have a clearly identifiable cut like if you'd run a picture or taken a photo of a cat if the image is not clearly identifiable as a cat the is will be low diversity would mean producing a different cat breed or cat pose every time if the AI produces the same kind of cat over and over the is will be low there are some limitations of the algorithm though it only works on small square image sizes a limited sample size will produce an artificially high score and unusual images that weren't part of the training set will produce an artificially low score [Music] thank you"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Evaluating Generative Models: Inception Score, FID",
        "Video Title":"Effectively Unbiased FID and Inception Score and Where to Find Them",
        "URL":"https:\/\/www.youtube.com\/watch?v=xdLVzkUfvm0",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/xdLVzkUfvm0\/hqdefault.jpg",
        "ID":"xdLVzkUfvm0",
        "Publish Time":"2020-07-17T05:09:30Z",
        "Channel":"ComputerVisionFoundation Videos",
        "Channel ID":"UC0n76gicaarsN_Y9YShWwhw",
        "Transcript":"my name is mein jin chong i'm from uic and i'd like to present our work we prove that fid has a bias which depends on the generator for the graph the y axis is fid the x-axis is 1 over n fid changes as n changes which means bias exists the slope depends on the generator this bias is serious it confuses comparisons in the figure comparing fids at different points leads to different rankings we show how to extrapolate with variance control to get unbiased f id infinity so both sequence samples point more evenly to give us bad estimates of the unbiased score these figures show up extrapolated predictions of fid100k are very good meaning we can rely on predicted fid infinity everything we show for fid is true for inception score except that in second score is negatively biased in conclusion never use fid and inception score to compare models use fid-infinity and such principle infinity"
    },
    {
        "Domain":"Deep Learning",
        "Sub Domain":"Generative Models in Deep Learning",
        "Topic":"Evaluating Generative Models: Inception Score, FID",
        "Video Title":"Evaluating GANs with Inception Score and FID",
        "URL":"https:\/\/www.youtube.com\/watch?v=eGbhEDDb8NA",
        "Thumbnail":"https:\/\/i.ytimg.com\/vi\/eGbhEDDb8NA\/hqdefault.jpg",
        "ID":"eGbhEDDb8NA",
        "Publish Time":"2024-11-23T14:43:41Z",
        "Channel":"Giuseppe Canale",
        "Channel ID":"UCrBZKEFFdXiRjm2yuvyEjkw",
        "Transcript":""
    }
]