{
    "domain": [
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Introduction to Vectors and Matrices",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 188,
            "page_oevrview": "Inner product. An important special case of matrix-matrix multiplication is the multiplication of a row vector with a column vector. If a and b are n-vectors, then the inner product a T b = a1b1 + a2b2 + \u00b7 \u00b7 \u00b7 + anbn can be interpreted as the matrix-matrix product of the 1 \u00d7 n matrix a T and the n\u00d71 matrix b."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Introduction to Vectors and Matrices",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 132,
            "page_oevrview": "The left- and right hand sides of the equation above involve very different steps and operations, but the final result of each is the same m-vector. Matrix-vector multiplication also distributes across the matrix argument: For any m \u00d7 n matrices A and B, and any n-vector u, we have (A + B)u = Au + Bu."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Introduction to Vectors and Matrices",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 9,
            "page_oevrview": "The growing power of computers, together with the development of high level computer languages and packages that support vector and matrix computation, have made it easy to use the methods described in this book for real applications."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Introduction to Vectors and Matrices",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Calculus by Gilbert Strang.pdf",
            "page_no": 446,
            "page_oevrview": "11 Vectors and Matrices MATRIX MULTIPLICATION To understand the power of matrices, we must multiply them. The product of A- 1 with Au is a matrix times a vector. But that multiplication can be done another way. First A-&#39; multiplies A, a matrix times a matrix. The product A -&#39;A is another matrix (a very special matrix)."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Introduction to Vectors and Matrices",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Calculus by Gilbert Strang.pdf",
            "page_no": 442,
            "page_oevrview": "The matrix-vector multiplication Au is defined so that all these equations are the same: [:: ::I[:] [ a1x + b1y Au by rows: = ] (each is a2x + b2y a dot product) Au by columns: [:I [;I = x[~l]+ y[:l] (combination of column vectors) A is the coeficient matrix. The unknown vector is u."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Vector Operations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 13,
            "page_oevrview": "Two vectors a and b are equal, which we denote a = b, if they have the same size, and each of the corresponding entries is the same. If a and b are n-vectors, then a = b means a1 = b1, . . . , an = bn."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Vector Operations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 34,
            "page_oevrview": ". . , n. Vector addition x+y of two n-vectors takes n additions, ie, xi + yi for i = 1, . . . , n. Computing the inner product x T y = x1y1 + \u00b7 \u00b7 \u00b7 + xnyn of two n-vectors takes 2n \u2212 1 flops, n scalar multiplications and n\u22121 scalar additions."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Vector Operations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 36,
            "page_oevrview": "The associated vector of differences is the (n \u2212 1)-vector d given by d = (x2 \u2212 x1, x3 \u2212 x2, . . . , xn \u2212 xn\u22121). Express d in terms of x using vector operations (eg, slicing notation, sum, difference, linear combinations, inner product). The difference vector has a simple interpretation when x represents a time series."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Vector Operations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Advanced Calculus by Lynn H Loomis.pdf",
            "page_no": 42,
            "page_oevrview": "For example, define T: e([a, b]) ~ IR by T(f) = f: f(t) dt. Then the laws of the integral calculus say that T(f + g) = T(f) + T(g) and T(cf) = cT(f). Thus T &quot;preserves&quot; the vector operations. Or we can say that T &quot;commutes&quot; with the vector operations, since plus followed by T equals T followed by plus."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Vector Operations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Advanced Calculus by Lynn H Loomis.pdf",
            "page_no": 55,
            "page_oevrview": "If W is a vector space and A is an arbitrary set, then the set V = WA of all W-valued functions on A is a vector space in &quot;exactly the same way that ~A is. Addition is the natural addition of functions, (f + g) (a) = f(a) + g(a), and, similarly, (xf)(a) = x(j(a) for every function f and scalar x."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Matrix Operations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 187,
            "page_oevrview": "You can multiply two matrices A and B provided their dimensions are compatible, which means the number of columns of A equals the number of rows of B. Suppose A and B are compatible, eg, A has size m \u00d7 p and B has size p \u00d7 n."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Matrix Operations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 188,
            "page_oevrview": "Inner product. An important special case of matrix-matrix multiplication is the multiplication of a row vector with a column vector. If a and b are n-vectors, then the inner product a T b = a1b1 + a2b2 + \u00b7 \u00b7 \u00b7 + anbn can be interpreted as the matrix-matrix product of the 1 \u00d7 n matrix a T and the n\u00d71 matrix b."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Matrix Operations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 127,
            "page_oevrview": "6.3.3 Scalar-matrix multiplication Scalar multiplication of matrices is defined in a similar way as for vectors, and is done by multiplying every element of the matrix by the scalar. For example (\u22122) \uf8ee \uf8f0 1 6 9 3 6 0 \uf8f9 \uf8fb = \uf8ee \uf8f0 \u22122 \u221212 \u221218 \u22126 \u221212 0 \uf8f9 \uf8fb ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Matrix Operations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Linear Algebra Done Right by Sheldon Axler.pdf",
            "page_no": 87,
            "page_oevrview": "Thus the entry in row , column , of is computed by taking row of and column of , multiplying together corresponding entries, and then summing. You may have learned this definition of matrix multiplication in an earlier course, although you may not have seen this motivation for it."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Matrix Operations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Linear Algebra Done Right by Sheldon Axler.pdf",
            "page_no": 102,
            "page_oevrview": "The matrix of with respect to this basis is the -by-1 matrix \u2133( ) = \u239b\u239c\u239c\u239c\u239c \u239d 1 \u22ee \u239e\u239f\u239f\u239f\u239f \u23a0 , where 1 , \u2026 , are the scalars such that = 1 1 + \u22ef + . The matrix \u2133( ) of a vector \u2208 depends on the basis 1 , \u2026 , of , as well as on . However, the basis should be clear from the context and thus it is not included in the notation."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Vector Spaces and Bases",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 51,
            "page_oevrview": "Every vector space V possesses a basis B. The preceding exam ples show that there can be many bases of a vector space V , ie, there is no unique basis. However, all bases possess the same number of elements, the basis vectors. \u2662 basis vector We only consider finite-dimensional vector spaces V ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Vector Spaces and Bases",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 52,
            "page_oevrview": "The dimension of a vector space is not necessarily the number of elements in a vector. For instance, the vector space V = span[ 0 1 ] is one-dimensional, although the basis vector possesses two elements. \u2662 Remark."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Vector Spaces and Bases",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 56,
            "page_oevrview": "We consider a basis {b1, . . . , bn} of an n-dimensional vector space V . In the following, the order of the basis vectors will be important. Therefore, we write B = (b1, . . . , bn) (2.89) ordered basis and call this n-tuple an ordered basis of V . Remark (Notation)."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Vector Spaces and Bases",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Linear Algebra Done Right by Sheldon Axler.pdf",
            "page_no": 58,
            "page_oevrview": "2.35 definition: dimension, dim \u2022 The dimension of a finite-dimensional vector space is the length of any basis of the vector space. \u2022 The dimension of a finite-dimensional vector space is denoted by dim . 2.36 example: dimensions \u2022 dim = because the standard basis of has length ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Vector Spaces and Bases",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Linear Algebra Done Right by Sheldon Axler.pdf",
            "page_no": 213,
            "page_oevrview": "In general, given a basis 1 , \u2026 , of and a vector \u2208 , we know that there is some choice of scalars 1 , \u2026 , \u2208 such that = 1 1 + \u22ef + . Computing the numbers 1 , \u2026 , that satisfy the equation above can be a long computation for an arbitrary basis of ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Special Matrices in Linear Algebra",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 28,
            "page_oevrview": "These special matrices are also called row/column vectors. row vector column vector Figure 2.4 By stacking its columns, a matrix A can be represented as a long vector a. re-shape A \u2208 R 4\u00d72 a \u2208 R 8 Rm\u00d7n is the set of all real-valued (m, n)-matrices."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Special Matrices in Linear Algebra",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 106,
            "page_oevrview": "For the smallest cases, we already know when a matrix is invertible. If A is a 1 \u00d7 1 matrix, ie, it is a scalar number, then A = a =\u21d2 A \u22121 = 1 a . Thus a 1 a = 1 holds, if and only if a \u0338= 0. For 2 \u00d7 2 matrices, by the definition of the inverse (Definition 2.3), we know that AA\u22121 = I."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Special Matrices in Linear Algebra",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 104,
            "page_oevrview": "For this reason matrix decomposition is also matrix factorization often referred to as matrix factorization. Matrix decompositions are used to describe a matrix by means of a different representation using factors of interpretable matrices."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Special Matrices in Linear Algebra",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 124,
            "page_oevrview": "An n \u00d7 n identity matrix is sparse, since it has only n nonzeros, so its density is 1/n. The zero matrix is the sparsest possible matrix, since it has no nonzero entries. Several special sparsity patterns have names; we describe some important ones below. Like sparse vectors, sparse matrices arise in many applications."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Special Matrices in Linear Algebra",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 139,
            "page_oevrview": "Several important geometric transformations or mappings from points to points can be expressed as matrix-vector products y = Ax, with A a 2 \u00d7 2 (or 3 \u00d7 3) matrix. In the examples below, we consider the mapping from x to y, and focus on the 2-D case (for which some of the matrices are simpler to describe). Scaling."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Eigenvalues and Eigenvectors in Linear Algebra",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 111,
            "page_oevrview": "In the linear algebra literature and software, it is often a conven tion that eigenvalues are sorted in descending order, so that the largest eigenvalue and associated eigenvector are called the first eigenvalue and its associated eigenvector, and the second largest called the second eigen value and its associated eigenvector, and so on."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Eigenvalues and Eigenvectors in Linear Algebra",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 112,
            "page_oevrview": "Geometrically, the eigenvector corresponding to a nonzero eigenvalue points in a direction that is stretched by the linear mapping. The eigenvalue is the factor by which it is stretched. If the eigenvalue is negative, the direction of the stretching is flipped."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Eigenvalues and Eigenvectors in Linear Algebra",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 113,
            "page_oevrview": "From our definition of the eigen vector x \u0338= 0 and eigenvalue \u03bb of A, there will be a vector such that Ax = \u03bbx, ie, (A \u2212 \u03bbI)x = 0. Since x \u0338= 0, this requires that the kernel (null space) of A \u2212 \u03bbI contains more elements than just 0. This means that A \u2212 \u03bbI is not invertible and therefore det(A \u2212 \u03bbI) = 0."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Eigenvalues and Eigenvectors in Linear Algebra",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Linear Algebra Done Right by Sheldon Axler.pdf",
            "page_no": 178,
            "page_oevrview": "Hence ( , ) is the set of all eigenvectors of corresponding to , along with the 0 vector. For \u2208 \u2112( ) and \u2208 , the set ( , ) is a subspace of because the null space of each linear map on is a subspace of . The definitions imply that is an eigenvalue of if and only if ( , ) \u2260 {0}."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Eigenvalues and Eigenvectors in Linear Algebra",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Linear Algebra Done Right by Sheldon Axler.pdf",
            "page_no": 154,
            "page_oevrview": "12 Suppose = \u2295 , where and are nonzero subspaces of . Define \u2208 \u2112( ) by ( + ) = for each \u2208 and each \u2208 . Find all eigenvalues and eigenvectors of . 13 Suppose \u2208 \u2112( ). Suppose \u2208 \u2112( ) is invertible. (a) Prove that and \u22121 have the same eigenvalues."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Characteristic Equation and Determinant",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Linear Algebra Done Right by Sheldon Axler.pdf",
            "page_no": 377,
            "page_oevrview": "The equation in 9.62 shows that this new definition is equivalent to our previous definition when = (8.26). 9.63 definition: characteristic polynomial Suppose \u2208 \u2112( ). The polynomial defined by \u21a6 det( \u2212 ) is called the characteristic polynomial of ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Characteristic Equation and Determinant",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Linear Algebra Done Right by Sheldon Axler.pdf",
            "page_no": 378,
            "page_oevrview": "Let be the operator on dim such that the matrix of (with respect to the standard basis of dim ) is . For all \u2208 we have ( ) = det( \u2212 ) = det( \u2212 ) = det( \u2212 ). Thus is the characteristic polynomial of . The case = (first sentence of this proof) now implies that 0 = ( ) = ( ) = ( )."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Characteristic Equation and Determinant",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Linear Algebra Done Right by Sheldon Axler.pdf",
            "page_no": 326,
            "page_oevrview": "8.30 characteristic polynomial is a multiple of minimal polynomial Suppose = and \u2208 \u2112( ). Then the characteristic polynomial of is a polynomial multiple of the minimal polynomial of . Proof The desired result follows immediately from the Cayley\u2013Hamilton theo rem (8.29) and 5.29."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Characteristic Equation and Determinant",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 105,
            "page_oevrview": "Determinants are important concepts in linear algebra. A determinant is a mathematical object in the analysis and solution of systems of linear equations. Determinants are only defined for square matrices A \u2208 Rn\u00d7n , ie, matrices with the same number of rows and columns."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Characteristic Equation and Determinant",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 109,
            "page_oevrview": "Determinants are invariant to transposition, ie, det(A) = det(A \u22a4 ). If A is regular (invertible), then det(A \u22121 ) = 1 det(A) . Similar matrices (Definition 2.22) possess the same determinant. There fore, for a linear mapping \u03a6 : V \u2192 V all transformation matrices A\u03a6 of \u03a6 have the same determinant."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Eigen Decomposition and Matrix Diagonalization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 122,
            "page_oevrview": ". . , pn , ie, the pi form a basis of Rn . Theorem 4.20 (Eigendecomposition). A square matrix A \u2208 Rn\u00d7n can be factored into A = P DP \u22121 , (4.55) where P \u2208 Rn\u00d7n and D is a diagonal matrix whose diagonal entries are the eigenvalues of A, if and only if the eigenvectors of A form a basis of Rn ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Eigen Decomposition and Matrix Diagonalization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 124,
            "page_oevrview": "(4.61) Diagonal matrices D can efficiently be raised to a power. Therefore, we can find a matrix power for a matrix A \u2208 Rn\u00d7n via the eigenvalue decomposition (if it exists) so that A k = (P DP \u22121 ) k = P DkP \u22121 . (4.62) Computing Dk is efficient because we apply this operation individually to any diagonal element."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Eigen Decomposition and Matrix Diagonalization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 123,
            "page_oevrview": "\u2662 Geometric Intuition for the Eigendecomposition We can interpret the eigendecomposition of a matrix as follows (see also Figure 4.7): Let A be the transformation matrix of a linear mapping with respect to the standard basis ei (blue arrows). P \u22121 performs a basis change from the standard basis into the eigenbasis."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Eigen Decomposition and Matrix Diagonalization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Linear Algebra Done Right by Sheldon Axler.pdf",
            "page_no": 329,
            "page_oevrview": "Thus with respect to this basis, the matrix of | ( , ), which equals ( \u2212 )| ( , ) + | ( , ), looks like the desired form shown above for . The generalized eigenspace decomposition (8.22) shows that putting together the bases of the ( , )&#39;s chosen above gives a basis of ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Eigen Decomposition and Matrix Diagonalization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Linear Algebra Done Right by Sheldon Axler.pdf",
            "page_no": 180,
            "page_oevrview": "5.58 enough eigenvalues implies diagonalizability Suppose is finite-dimensional and \u2208 \u2112( ) has dim distinct eigenvalues. Then is diagonalizable. Proof Suppose has distinct eigenvalues 1 , \u2026 , dim . For each , let \u2208 be an eigenvector corresponding to the eigenvalue ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Singular Value Decomposition (SVD)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 59,
            "page_oevrview": "The singular value decomposition (SVD) provides another way to factorize a matrix, into singular vectors and singular values. The SVD allows us to discover some of the same kind of information as the eigendecomposition. However, 44."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Singular Value Decomposition (SVD)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 60,
            "page_oevrview": "CHAPTER 2. LINEAR ALGEBRA the SVD is more generally applicable. Every real matrix has a singular value decomposition, but the same is not true of the eigenvalue decomposition. For example, if a matrix is not square, the eigendecomposition is not defined, and we must use a singular value decomposition instead."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Singular Value Decomposition (SVD)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 164,
            "page_oevrview": "Using the SVD of X, we can express the variance of X as: Var[x] = 1 m \u2212 1X\ue03eX (5.88) = 1 m \u2212 1 (U\u03a3W\ue03e)\ue03eU\u03a3W\ue03e (5.89) = 1 m \u2212 1 W\u03a3\ue03e U \ue03eU\u03a3W \ue03e (5.90) = 1 m \u2212 1W\u03a3 2W\ue03e , (5.91) where we use the fact that U \ue03eU = I because the U matrix of the singular value decomposition is defined to be orthogonal."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Singular Value Decomposition (SVD)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 125,
            "page_oevrview": "Let A \u2208 Rm\u00d7n be a rectangular matrix of rank r \u2208 [0, min(m, n)]. The SVD of A is a decomposition of the form SVD singular value decomposition A = U V m \u03a3 \u22a4 n m m m n n n (4.64) with an orthogonal matrix U \u2208 Rm\u00d7m with column vectors ui , i = 1, . . ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Singular Value Decomposition (SVD)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 131,
            "page_oevrview": ". . , r. Hence, right-multiplying with V \u22a4 yields A = U\u03a3V \u22a4 , which is the SVD of A. Example 4.13 (Computing the SVD) Let us find the singular value decomposition of A = 1 0 1 \u22122 1 0 . (4.81) The SVD requires us to compute the right-singular vectors vj , the singular values \u03c3k, and the left-singular vectors ui ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Principal Component Analysis (PCA)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 263,
            "page_oevrview": "PCA is discussed in greater detail as a tool for unsupervised learning in Chapter 12. Here we describe its use as a dimension reduction technique for regression. An Overview of Principal Components Analysis PCA is a technique for reducing the dimension of an n \u00d7 p data matrix X."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Principal Component Analysis (PCA)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 265,
            "page_oevrview": "The Principal Components Regression Approach The principal components regression (PCR) approach involves construct- principal components regression ing the frst M principal components, Z1,...,ZM, and then using these components as the predictors in a linear regression model that is ft us ing least squares."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Principal Component Analysis (PCA)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 264,
            "page_oevrview": "It is the dimension along which the data vary the most, and it also defnes the line that is closest to all n of the observations. The distances from each observation to the principal component are represented using the black dashed line segments. The blue dot represents (pop, ad)."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Principal Component Analysis (PCA)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 63,
            "page_oevrview": "PCA is defined by our choice of the decoding function. Specifically, to make the decoder very simple, we choose to use matrix multiplication to map the code back into Rn . Let g(c) = Dc, where D \u2208 R n\u00d7l is the matrix defining the decoding. Computing the optimal code for this decoder could be a difficult problem."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Principal Component Analysis (PCA)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 163,
            "page_oevrview": "To achieve full independence, a representation learning algorithm must also remove the nonlinear relationships between variables. PCA learns an orthogonal, linear transformation of the data that projects an input x to a representation z as shown in figure 5.8."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Low-Rank Approximation Techniques in Matrix Factorization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 142,
            "page_oevrview": "CP decomposition The SVD low-rank approximation is frequently used in machine learn ing for computational efficiency reasons. This is because it reduces the amount of memory and operations with nonzero multiplications we need to perform on potentially very large matrices of data (Trefethen and Bau III, 1997)."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Low-Rank Approximation Techniques in Matrix Factorization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 138,
            "page_oevrview": "We can interpret the approximation of A by a rank-k matrix as a form of lossy compression. Therefore, the low-rank approximation of a matrix appears in many machine learning applications, eg, image processing, noise filtering, and regularization of ill-posed prob lems."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Low-Rank Approximation Techniques in Matrix Factorization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 135,
            "page_oevrview": "These applications harness various important properties of the SVD, its relation to the rank of a matrix, and its ability to approximate matrices of a given rank with lower-rank matrices. Substituting a matrix with its SVD has often the advantage of making calculation more robust to nu merical rounding errors."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Low-Rank Approximation Techniques in Matrix Factorization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 456,
            "page_oevrview": ". . , m. We can express this approximation in compact notation as \u02c6f(x) = f(z) + Df(z)(x \u2212 z). For x near z, \u02c6f(x) is a very good approximation of f(x). As in the scalar case, the Taylor approximation is sometimes written with a second argument as \u02c6f(x; z) to show the point z around which the approximation is made."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Linear Algebra",
            "topic": "Low-Rank Approximation Techniques in Matrix Factorization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 218,
            "page_oevrview": "The first step requires 2n 3 flops (see \u00a75.4), the second step requires 2n 2 flops, and the third step requires n 2 flops. The total number of flops is then 2n 3 + 3n 2 \u2248 2n 3 , so the order is n 3 , cubic in the number of variables, which is the same as the number of equations."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Limits and Continuity",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Calculus by Gilbert Strang.pdf",
            "page_no": 94,
            "page_oevrview": "At a jump, or an infinite limit, or an infinite oscillation, there is no way across the discontinuity except to start again on the other side. The function x&quot; is continuous for n &gt; 0. It is not continuable for n &lt; 0. The function x0 equals 1 for every x, except that 00 is not defined."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Limits and Continuity",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Calculus by Gilbert Strang.pdf",
            "page_no": 96,
            "page_oevrview": "For 0 &lt; x &lt; 1 the function f (x) = x never reaches its minimum (zero). If we close the interval by defining f (0) = 3 (discontinuous) the minimum is still not reached. Because of the jump, the intermediate value F = 2 is also not reached. The idea of continuity was inescapable, after Cauchy defined the idea of a limit."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Limits and Continuity",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Calculus by Gilbert Strang.pdf",
            "page_no": 97,
            "page_oevrview": "(c) If f (1)= 1 and f (2)= -2, then somewhere f(x)= 0. (d) If f (1)= 1 and f (2) = -2 and f is continuous on [I, 21, then somewhere on that interval f(x) = 0. 36 The functions cos x and 2x are continuous. Show from the property that cos x = 2x at some point between 0 and 1."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Limits and Continuity",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Advanced Calculus by Lynn H Loomis.pdf",
            "page_no": 138,
            "page_oevrview": "It is understood here that ~ is universally quantified over the domain A of f. We say that f is continuous if f is continuous at every point a in its domain. If the absolute value of a number is replaced by the norm of a vector, the limit theorems that we sampled in Section 1 hold verbatim for normed linear spaces."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Limits and Continuity",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Advanced Calculus by Lynn H Loomis.pdf",
            "page_no": 222,
            "page_oevrview": "Here d will, in general, depend both on y and c; if either y or c is changed, the corresponding d may have to be changed. Thus lJ in the definition of continuity depends both on E and on the point y at which continuity is being asserted."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Continuous vs. Discontinuous Functions",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Calculus by Gilbert Strang.pdf",
            "page_no": 94,
            "page_oevrview": "EXAMPLE2 The absolute value Ixl is continuous. Its slope jumps (not continuable). EXAMPLE3 Any rational function P(x)/Q(x) is continuous except where Q = 0. EXAMPLE4 The function that jumps between 1 at fractions and 0 at non-fractions is discontinuous everywhere."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Continuous vs. Discontinuous Functions",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Calculus by Gilbert Strang.pdf",
            "page_no": 96,
            "page_oevrview": "Examples show why we require closed intervals and continuous functions. For 0 &lt; x &lt; 1 the function f (x) = x never reaches its minimum (zero). If we close the interval by defining f (0) = 3 (discontinuous) the minimum is still not reached. Because of the jump, the intermediate value F = 2 is also not reached."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Continuous vs. Discontinuous Functions",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Calculus by Gilbert Strang.pdf",
            "page_no": 97,
            "page_oevrview": "(c) If f (1)= 1 and f (2)= -2, then somewhere f(x)= 0. (d) If f (1)= 1 and f (2) = -2 and f is continuous on [I, 21, then somewhere on that interval f(x) = 0. 36 The functions cos x and 2x are continuous. Show from the property that cos x = 2x at some point between 0 and 1."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Continuous vs. Discontinuous Functions",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Advanced Calculus by Lynn H Loomis.pdf",
            "page_no": 250,
            "page_oevrview": "But we can show that S includes the space e([a, b], W) of all continuous functions from [a, b] to W, and the integral of a continuous function is thus uniquely defined. Lemma 10.1. e([a, b], W) c S. Proof. A continuous function f on [a, b] is uniformly continuous (Theorem 5.1)."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Continuous vs. Discontinuous Functions",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Advanced Calculus by Lynn H Loomis.pdf",
            "page_no": 193,
            "page_oevrview": "Corollary. If F(x, y) is a continuous real-valued function on the unit square [0, 1] X [0, 1], and if aF jay exists and is a uniformly continuous function on the square, then UF(x, y) dx is a differentiable function of y and its derivative is U (aFjay) (x, y) dx. Proof."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Partial Derivatives and Gradients",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 152,
            "page_oevrview": "The gradient is then the collection of these partial derivatives. Definition 5.5 (Partial Derivative). For a function f : Rn \u2192 R, x \u2192 f(x), x \u2208 Rn partial derivative of n variables x1, . . . , xn we define the partial derivatives as \u2202f \u2202x1 = lim h\u21920 f(x1 + h, x2, . . . , xn) \u2212 f(x) h . . . \u2202f \u2202xn = lim h\u21920 f(x1, . . ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Partial Derivatives and Gradients",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 153,
            "page_oevrview": "\u2662 Example 5.7 (Gradient) For f(x1, x2) = x 2 1x2 + x1x 3 2 \u2208 R, the partial derivatives (ie, the deriva tives of f with respect to x1 and x2) are \u2202f(x1, x2) \u2202x1 = 2x1x2 + x 3 2 (5.43) \u2202f(x1, x2) \u2202x2 = x 2 1 + 3x1x 2 2 (5.44) and the gradient is then df dx = \u2202f(x1, x2) \u2202x1 \u2202f(x1, x2) \u2202x2 = 2x1x2 + x 3 2 x 2 1 + 3x1x 2 2 \u2208 R 1\u00d72 ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Partial Derivatives and Gradients",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 154,
            "page_oevrview": "If f(x1, x2) is a function of x1 and x2, where x1(s, t) and x2(s, t) are themselves functions of two variables s and t, the chain rule yields the partial derivatives \u2202f \u2202s = \u2202f \u2202x1 \u2202x1 \u2202s + \u2202f \u2202x2 \u2202x2 \u2202s , (5.51) \u2202f \u2202t = \u2202f \u2202x1 \u2202x1 \u2202t + \u2202f \u2202x2 \u2202x2 \u2202t , (5.52) Draft (2024-01-15) of \u201cMathematics for Machine Learning\u201d."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Partial Derivatives and Gradients",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 455,
            "page_oevrview": "Finding gradients. The gradient of a function can be found by evaluating the partial derivatives using the common functions and rules for derivatives of scalar valued functions, and assembling the result into a vector. In many cases the result can be expressed in a more compact matrix-vector form."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Partial Derivatives and Gradients",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 457,
            "page_oevrview": "The partial derivative with respect to xj , at z, is \u2202h \u2202xj (z) = 2f1(z) \u2202f1 \u2202xj (z) + \u00b7 \u00b7 \u00b7 + 2fm(z) \u2202fm \u2202xj (z). Arranging these to form the row vector Dh(z), we see we can write this using matrix multiplication as Dh(z) = 2f(z) T Df(z). The gradient of h is the transpose of this expression, \u2207h(z) = 2Df(z) T f(z)."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Chain Rule, Product Rule, and Quotient Rule",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 151,
            "page_oevrview": "Product rule: (f(x)g(x))\u2032 = f \u2032 (x)g(x) + f(x)g \u2032 (x) (5.29) Quotient rule: f(x) g(x) \u2032 = f \u2032 (x)g(x) \u2212 f(x)g \u2032 (x) (g(x))2 (5.30) Sum rule: (f(x) + g(x))\u2032 = f \u2032 (x) + g \u2032 (x) (5.31) Chain rule: g(f(x))\u2032 = (g \u25e6 f) \u2032 (x) = g \u2032 (f(x))f \u2032 (x) (5.32) Here, g \u25e6 f denotes function composition x \u2192 f(x) \u2192 g(f(x))."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Chain Rule, Product Rule, and Quotient Rule",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 153,
            "page_oevrview": "(5.45) 5.2.1 Basic Rules of Partial Differentiation Product rule: (fg) \u2032 = f \u2032g + fg\u2032 , Sum rule: (f + g) \u2032 = f \u2032 + g \u2032 , Chain rule: (g(f))\u2032 = g \u2032 (f)f \u2032 In the multivariate case, where x \u2208 Rn , the basic differentiation rules that we know from school (eg, sum rule, product rule, chain rule; see also Section 5.1.2) still apply."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Chain Rule, Product Rule, and Quotient Rule",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 154,
            "page_oevrview": "If f(x1, x2) is a function of x1 and x2, where x1(s, t) and x2(s, t) are themselves functions of two variables s and t, the chain rule yields the partial derivatives \u2202f \u2202s = \u2202f \u2202x1 \u2202x1 \u2202s + \u2202f \u2202x2 \u2202x2 \u2202s , (5.51) \u2202f \u2202t = \u2202f \u2202x1 \u2202x1 \u2202t + \u2202f \u2202x2 \u2202x2 \u2202t , (5.52) Draft (2024-01-15) of \u201cMathematics for Machine Learning\u201d."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Chain Rule, Product Rule, and Quotient Rule",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Calculus by Gilbert Strang.pdf",
            "page_no": 167,
            "page_oevrview": "The derivative of 2x + 1 is remembered-without z or u or f or g. EXAMPLE 7 sin J&amp; is a chain of z = sin y, y = &amp;,u = 1 -x (three functions). With that triple chain you will have the hang of the chain rule: The derivative of sin fi is (cos JK) (2&amp;) (- This is (dz/dy)(dy/du)(du/dx)."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Chain Rule, Product Rule, and Quotient Rule",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Calculus by Gilbert Strang.pdf",
            "page_no": 80,
            "page_oevrview": "The right side becomes u(x) times dvldx-we can multiply the two limits-plus v(x) times duldx. That proves the product rule-definitely useful. We could go immediately to the quotient rule for u(x)/v(x). But start with u = 1. The derivative of l/x is - 1/x2 (known). What is the derivative of l/v(x)?"
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Hessian Matrix and Jacobian Matrix in for Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 102,
            "page_oevrview": "When our function has multiple input dimensions, there are many second derivatives. These derivatives can be collected together into a matrix called the Hessian matrix. The Hessian matrix H(f)(x) is defined such that H(f)(x)i,j = \u2202 2 \u2202xi\u2202xj f(x). (4.6) Equivalently, the Hessian is the Jacobian of the gradient."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Hessian Matrix and Jacobian Matrix in for Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 101,
            "page_oevrview": "4.3.1 Beyond the Gradient: Jacobian and Hessian Matrices Sometimes we need to find all of the partial derivatives of a function whose input and output are both vectors. The matrix containing all such partial derivatives is known as a Jacobian matrix."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Hessian Matrix and Jacobian Matrix in for Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 14,
            "page_oevrview": "CONTENTS Calculus dy dx Derivative of y with respect to x \u2202y \u2202x Partial derivative of y with respect to x \u2207xy Gradient of y with respect to x \u2207X y Matrix derivatives of y with respect to X \u2207Xy Tensor containing derivatives of y with respect to X \u2202f \u2202x Jacobian matrix J \u2208 R m\u00d7n of f : R n \u2192 Rm \u2207 2xf(x) or H(f)(x) The Hessian matrix of f at input point x \ue05a f(x)dx Definite integral over the entire domain of x \ue05a S f(x)dx Definite integral with respect to x over the set S Probability and Information Theory a\u22a5b The random variables a and b are independent a\u22a5b | c They are conditionally independent given c P(a) A probability distribution over a discrete variable p(a) A probability distribution over a continuous vari able, or over a variable whose type has not been specified a \u223c P Random variable a has distribution P Ex\u223cP[f(x)] or Ef(x) Expectation of f(x) with respect to P(x) Var(f(x)) Variance of f(x) under P(x) Cov(f(x), g(x)) Covariance of f(x) and g(x) under P(x) H(x) Shannon entropy of the random variable x DKL(P\ue06bQ) Kullback-Leibler divergence of P and Q N (x; \u00b5, \u03a3) Gaussian distribution over x with mean \u00b5 and covariance \u03a3 xiii."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Hessian Matrix and Jacobian Matrix in for Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 456,
            "page_oevrview": ". . fm(x) \uf8f9 \uf8fa \uf8fb , where fi is a scalar-valued function of x = (x1, . . . , xn). Jacobian. The partial derivatives of the components of f(x) with respect to the components of x, evaluated at z, are arranged into an m\u00d7n matrix denoted Df(z), called the derivative matrix or Jacobian of f at z."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Hessian Matrix and Jacobian Matrix in for Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 457,
            "page_oevrview": "The Jacobian or derivative matrix of f at z is given by Df(z) = Dg(h(z))Dh(z). (This is matrix multiplication; compare it to composition formula for scalar-valued functions of scalars given above.) This chain rule is described on page 184. C.2 Optimization Derivative condition for minimization."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Gradient Descent Optimization Algorithm",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 234,
            "page_oevrview": "Gradient descent exploits the fact that f(x0) decreases fastest if one moves from x0 in the direction of the negative gradient \u2212((\u2207f)(x0))\u22a4 of f at x0. We assume in this book that the functions are differentiable, and refer the reader to more general settings in Section 7.4."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Gradient Descent Optimization Algorithm",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 237,
            "page_oevrview": "Approximating the gradient is still useful as long as it points in roughly the same direction as the true gradient. stochastic gradient Stochastic gradient descent descent (often shortened as SGD) is a stochastic ap proximation of the gradient descent method for minimizing an objective function that is written as a sum of differentiable functions."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Gradient Descent Optimization Algorithm",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 235,
            "page_oevrview": "\u2662 7.1.1 Step-size As mentioned earlier, choosing a good step-size is important in gradient descent. If the step-size is too small, gradient descent can be slow. If the The step-size is also called the learning rate. step-size is chosen too large, gradient descent can overshoot, fail to con verge, or even diverge."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Gradient Descent Optimization Algorithm",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 309,
            "page_oevrview": "8.3.1 Stochastic Gradient Descent Stochastic gradient descent (SGD) and its variants are probably the most used optimization algorithms for machine learning in general and for deep learning in particular. As discussed in section 8.1.3, it is possible to obtain an unbiased estimate of the gradient by taking the average gradient on a minibatch of m examples drawn iid from the data generating distribution."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Gradient Descent Optimization Algorithm",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 192,
            "page_oevrview": "Convex optimization converges starting from any initial parameters (in theory\u2014in practice it is very robust but can encounter numerical problems). Stochastic gradient descent applied to non-convex loss functions has no such convergence guarantee, and is sensitive to the values of the initial parameters."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Advanced Optimizers (Adam, RMSprop, etc.)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 323,
            "page_oevrview": "RMSProp uses an exponentially decaying average to discard history from the extreme past so that it can converge rapidly after finding a convex bowl, as if it were an instance of the AdaGrad algorithm initialized within that bowl."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Advanced Optimizers (Adam, RMSprop, etc.)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 324,
            "page_oevrview": "RMSProp also incorporates an estimate of the (uncentered) second-order moment, however it lacks the correction factor. Thus, unlike in Adam, the RMSProp second-order moment estimate may have high bias early in training."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Advanced Optimizers (Adam, RMSprop, etc.)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 322,
            "page_oevrview": "AdaGrad performs well for some but not all deep learning models. 8.5.2 RMSProp The RMSProp algorithm (Hinton, 2012) modifies AdaGrad to perform better in the non-convex setting by changing the gradient accumulation into an exponentially weighted moving average."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Advanced Optimizers (Adam, RMSprop, etc.)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 78,
            "page_oevrview": "\uf0a1 The optimizer specifies the exact way in which the gradient of the loss will be used to update parameters: for instance, it could be the RMSProp opti mizer, SGD with momentum, and so on. Licensed to &lt;null&gt;"
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Advanced Optimizers (Adam, RMSprop, etc.)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 83,
            "page_oevrview": "\uf0a1 Optimizer\u2014Determines how the network will be updated based on the loss func tion. It implements a specific variant of stochastic gradient descent (SGD). A neural network that has multiple outputs may have multiple loss functions (one per output)."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Convex vs. Non-Convex Optimization in Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 242,
            "page_oevrview": "The distinction between convex func tions and convex sets are often not strictly presented in machine learning literature, but one can often infer the implied meaning from context. convex set Definition 7.2. A set C is a convex set if for any x, y \u2208 C and for any scalar \u03b8 with 0 \u2a7d \u03b8 \u2a7d 1, we have \u03b8x + (1 \u2212 \u03b8)y \u2208 C ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Convex vs. Non-Convex Optimization in Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 249,
            "page_oevrview": "The classical Legendre transform is defined on convex differentiable functions in RD. Remark. Convex differentiable functions such as the example f(x) = x 2 is a nice special case, where there is no need for the supremum, and there is a one-to-one correspondence between a function and its Legendre trans form."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Convex vs. Non-Convex Optimization in Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 245,
            "page_oevrview": "\u2662 In summary, a constrained optimization problem is called a convex opti- convex optimization mization problem if problem min x f(x) subject to gi(x) \u2a7d 0 for all i = 1, ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Convex vs. Non-Convex Optimization in Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 192,
            "page_oevrview": "<b>Convex optimization converges starting from any initial parameters</b> (in theory\u2014in practice it is very robust but can encounter numerical problems). Stochastic gradient descent applied to non-convex loss functions has no such convergence guarantee, and is sensitive to the values of the initial parameters."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Convex vs. Non-Convex Optimization in Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 108,
            "page_oevrview": "Convex optimization algorithms are able to provide many more guarantees by making stronger restrictions. Convex optimization algorithms are applicable only to convex functions\u2014functions for which the Hessian is positive semidefinite everywhere."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Definite and Indefinite Integrals",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Calculus by Gilbert Strang.pdf",
            "page_no": 196,
            "page_oevrview": "The theory of calculus will show that there are no others. The indefinite integral is the most general antiderivative (with no limits): indefinite integral f(x) =J v(x) dx = 4x -$x2 + C. (5) By contrast, the definite integral is a number. It contains no arbitrary constant C. More that that, it contains no variable x."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Definite and Indefinite Integrals",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Calculus by Gilbert Strang.pdf",
            "page_no": 195,
            "page_oevrview": "They both use the antideriva tive f(x). The definite one involves the limits 0 and 4, the indefinite one doesn&#39;t: The indefinite integral is a function f(x) = 4x -ix2."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Definite and Indefinite Integrals",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Calculus by Gilbert Strang.pdf",
            "page_no": 211,
            "page_oevrview": "This section takes two steps. First, we choose C. Second, we construct f(x). The object is to define the integral-in the most frequent case when a suitable f(x) is not directly known. The indefinite integral contains &quot;+ C.&quot; The constant is not settled because f (x) + C has the same slope for every C."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Definite and Indefinite Integrals",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Advanced Calculus by Lynn H Loomis.pdf",
            "page_no": 358,
            "page_oevrview": "We say that a contented function f is contented relative to the decomposition IRn = IRk X IRI if there exists a set AI C IRk of content zero (in IRk) such that i) for each fixed x E IRk, X ~ A&quot; the function f(x, .) is a contented function on IRI; ii) the function IRlf which assigns to x the number IRlf(x, .) is a contented function on IRk."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Calculus",
            "topic": "Definite and Indefinite Integrals",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Advanced Calculus by Lynn H Loomis.pdf",
            "page_no": 249,
            "page_oevrview": "The values of f at the subdividing points may be among these values or they may be different. For each step function f we define f: f(t) dt as Li&#39;= 1 ai Ilti, where f = ai on (t;-b ti) and Ilti = ti - ti_l."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Introduction to Probability",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 3,
            "page_oevrview": "It provides the possibility to derive confidence intervals and perform tests of hypotheses where traditional (normal approximation or large sample) methods are inappropriate."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Introduction to Probability",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 27,
            "page_oevrview": "The number P(A) is called the probability that A occurs. Property (i) expresses that the outcome of the experiment is always an element of the sample space, and property (ii) is the additivity property of a probability function."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Introduction to Probability",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 4,
            "page_oevrview": "Topics that follow are elementary probability theory, simulation, joint distributions, the law of large numbers, the central limit theorem, statistical modeling (in formal: why and how we can draw inference from data), data analysis, the bootstrap, estimation, simple linear regression, confidence intervals, and hy pothesis testing."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Introduction to Probability",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 18,
            "page_oevrview": "1.2 Probability Models A formal definition of probability begins with a sample space, often written S. This sample space is any set that lists all possible outcomes(or, responses) of some unknown experiment or situation. For example, perhaps S rain snow clear when predicting tomorrow&#39;s weather."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Introduction to Probability",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 15,
            "page_oevrview": "1.1 Probability: A Measure of Uncertainty Often in life we are confronted by our own ignorance. Whether we are pondering tonight&#39;s traffic jam, tomorrow&#39;s weather, next week&#39;s stock prices, an upcoming elec tion, or where we left our hat, often we do not know an outcome with certainty."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Probability Axioms and Sample Spaces in Statistics",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 24,
            "page_oevrview": "The outcomes are elements of a sample space \u2126, and subsets of \u2126 are called events.The events will be assigned a probability, a number between 0 and 1 that expresses how likely the event is to occur."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Probability Axioms and Sample Spaces in Statistics",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 29,
            "page_oevrview": "From the additivity property we can also find a way to compute probabilities of complements of events: from A \u222a Ac = \u2126, we deduce that P(Ac )=1 \u2212 P(A). 2.4 Products of sample spaces Basic to statistics is that one usually does not consider one experiment, but that the same experiment is performed several times."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Probability Axioms and Sample Spaces in Statistics",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 30,
            "page_oevrview": "In general, when we perform an experiment n times, then the corresponding sample space is \u2126=\u21261 \u00d7 \u21262 \u00d7\u00b7\u00b7\u00b7\u00d7 \u2126n, where \u2126i for i = 1,...,n is a copy of the sample space of the original exper iment."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Probability Axioms and Sample Spaces in Statistics",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 181,
            "page_oevrview": "P(A) is called the probability of A. probability The probability of a single event must lie in the interval [0, 1], and the total probability over all outcomes in the sample space \u2126 must be 1, ie, P(\u2126) = 1. Given a probability space (\u2126, A, P), we want to use it to model some real-world phenomenon."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Probability Axioms and Sample Spaces in Statistics",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 178,
            "page_oevrview": "Associated with the random variable is a function that measures the probability that a particular outcome (or probability set of outcomes) will occur; this is called the probability distribution. distribution Probability distributions are used as a building block for other con cepts, such as probabilistic modeling (Section 8.4), graphical models (Sec tion 8.5), and model selection (Section 8.6)."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Conditional Probability and Bayes' Theorem",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 36,
            "page_oevrview": "The multiplication formula is then used to calculate joint probabilities for what happens at both stages\u037e the law of total probability is used to compute the probabilities for what happens at the second stage\u037e and Bayes&#39; theorem is used to calculate the conditional probabilities for the first stage, given what has occurred at the second stage."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Conditional Probability and Bayes' Theorem",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 39,
            "page_oevrview": "Conditional probability satisfies its own law of total probability. Events are independent if they have no effect on each other&#39;s probabilities. For mally, this means that PABPAPB . If A and B are independent, and PA 0 and PB 0, then PAB PA and PBAPB . EXERCISES 1.5.1 Suppose that we roll four fair sixsided dice."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Conditional Probability and Bayes' Theorem",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 390,
            "page_oevrview": "The posterior density, or posterior probability function (whichever is relevant), is given by s fs ms (7.1.1) Sometimes this use of conditional probability is referred to as an application of Bayes&#39; theorem (Theorem 1.5.2)."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Conditional Probability and Bayes' Theorem",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 43,
            "page_oevrview": "Bayes&#39; rule. Suppose the events C1, C2, ..., Cm are disjoint and C1 \u222a C2 \u222a\u00b7\u00b7\u00b7\u222a Cm = \u2126. The conditional probability of Ci, given an arbitrary event A, can be expressed as: P(Ci | A) = P(A | Ci) \u00b7 P(Ci) P(A | C1)P(C1) + P(A | C2)P(C2) + \u00b7\u00b7\u00b7 + P(A | Cm)P(Cm) . This is the traditional form of Bayes&#39; formula."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Conditional Probability and Bayes' Theorem",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 42,
            "page_oevrview": "The law of total probability. Suppose C1, C2, ..., Cm are disjoint events such that C1 \u222a C2 \u222a\u00b7\u00b7\u00b7\u222a Cm = \u2126. The probability of an arbitrary event A can be expressed as: P(A) = P(A | C1)P(C1) + P(A | C2)P(C2) + \u00b7\u00b7\u00b7 + P(A | Cm)P(Cm). Figure 3.2 illustrates the law for m = 5."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Discrete Probability Distributions",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 56,
            "page_oevrview": "42 Section 2.3: Discrete Distributions Definition 2.3.3 For a discrete random variable X, its probability function is the function pX : R 1 [0 1] defined by pX x PX x . Hence, if x1 x2 are the distinct values such that PX xi pi for all i with i pi 1, then pX x pi x xi for some i 0 otherwise."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Discrete Probability Distributions",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 62,
            "page_oevrview": "For example, if we randomly select people to participate in an opinion poll so that each set of n individuals in a pop ulation of N has the same probability of being selected, then the number of people who respond yes to a particular question is distributed Hypergeometric NM n where M is the number of people in the entire population who would respond yes."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Discrete Probability Distributions",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 144,
            "page_oevrview": "We thus see that, for a discrete random variable X, once we know the probabilities that X x (or equivalently, once we know the probability function pX ), it is straightfor ward (at least in simple cases) to compute the expected value of X."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Discrete Probability Distributions",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 55,
            "page_oevrview": "In fact, the distribution function F of a discrete random variable X can be expressed in terms of the probability mass function p of X and vice versa. If X attains values a1, a2,..., such that p(ai) &gt; 0, p(a1) + p(a2) + \u00b7\u00b7\u00b7 = 1, then F(a) = ai\u2264a p(ai)."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Discrete Probability Distributions",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 54,
            "page_oevrview": "It suffices to list the possible values of X and their corre sponding probabilities. This information is contained in the probability mass function of X. Definition. The probability mass function p of a discrete random variable X is the function p : R \u2192 [0, 1], defined by p(a) = P(X = a) for \u2212 \u221e &lt;a&lt; \u221e."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Continuous Probability Distributions",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 74,
            "page_oevrview": "Definition. A continuous random variable has a normal distribu tion with parameters \u00b5 and \u03c32 &gt; 0 if its probability density function f is given by f(x) = 1 \u03c3 \u221a2\u03c0 e \u2212 1 2 x\u2212\u00b5 \u03c3 2 for \u2212 \u221e &lt;x&lt; \u221e. We denote this distribution by N(\u00b5, \u03c32)."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Continuous Probability Distributions",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 70,
            "page_oevrview": "This motivates the following definition. Definition. A continuous random variable has a uniform distribu tion on the interval [\u03b1, \u03b2] if its probability density function f is given by f(x) = 0 if x is not in [\u03b1, \u03b2] and f(x) = 1 \u03b2 \u2212 \u03b1 for \u03b1 \u2264 x \u2264 \u03b2. We denote this distribution by U(\u03b1, \u03b2)."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Continuous Probability Distributions",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 67,
            "page_oevrview": "Definition. A random variable X is continuous if for some function f : R \u2192 R and for any numbers a and b with a \u2264 b, P(a \u2264 X \u2264 b) = b a f(x) dx. The function f has to satisfy f(x) \u2265 0 for all x and \u221e \u2212\u221e f(x) dx = 1. We call f the probability density function (or probability density) of X."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Continuous Probability Distributions",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 66,
            "page_oevrview": "In particular, if ba with a small positive number, and if f is continuous at a, then we see that P a X a a a fx dx fa Thus, a density function evaluated at a may be thought of as measuring the probability of a random variable being in a small interval about a."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Continuous Probability Distributions",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 85,
            "page_oevrview": "All probabilities associated with X can be determined from FX . As x increases from to , FX x increases from 0 to 1. If X is discrete, then FX x yx PX y . If X is absolutely continuous, then FX x x fX t dt, and fX x FX x . We write x for the cdf of the standard normal distribution evaluated at x."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Central Limit Theorem and Law of Large Numbers",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 203,
            "page_oevrview": "14 The central limit theorem The central limit theorem is a refinement of the law of large numbers. For a large number of independent identically distributed random variables X1,...,Xn, with finite variance, the average X\u00afn approximately has a normal distribution, no matter what the distribution of the Xi is."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Central Limit Theorem and Law of Large Numbers",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 456,
            "page_oevrview": "The law of large numbers now states: if Y\u00afn is the average of n independent random variables with expectation \u00b5 and variance \u03c32, then for any \u03b5 &gt; 0: limn\u2192\u221e P |Y\u00afn \u2212 \u00b5| &gt; \u03b5 = 0. So, if a = \u00b5 and the variance \u03c32 is finite, then it is true."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Central Limit Theorem and Law of Large Numbers",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 9,
            "page_oevrview": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203 15 Exploratory data analysis: graphical summaries ............"
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Central Limit Theorem and Law of Large Numbers",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 214,
            "page_oevrview": "This approach leads to a famous result, known as the central limit theorem, discussed in Section 4.4. Sometimes we cannot even develop useful approximations for large n due to the difficulty of the problem or perhaps because n is just too small in a particular applica tion."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Central Limit Theorem and Law of Large Numbers",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 225,
            "page_oevrview": "The strong law of large numbers says that if Xn is iid, then the averages Mn X1 Xn n converge with probability 1 to E Xi . EXERCISES 4.3.1 Let U Uniform[5 10], and let ZI[5 7 U (ie, Z is the indicator function of [5 7 ) and Zn I [5 7 1 n 2 U . Prove that Zn Z with probability 1."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Point Estimation and Confidence Intervals",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 344,
            "page_oevrview": "We call this number a point estimate: if we are required to select one number, this is it. Had the measurements started a day earlier, however, the whole experiment would in essence be the same, but the results might have been different."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Point Estimation and Confidence Intervals",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 367,
            "page_oevrview": "Especially for p near 0 or 1 this method yields conservative confidence intervals. Define X\u02dc = X + (z\u03b1/2) 2 2 and \u02dcn = n + (z\u03b1/2) 2 , and \u02dcp = X/ \u02dc n\u02dc. The approximate 100(1 \u2212 \u03b1)% confidence interval is then given by p\u02dc\u2212 z\u03b1/2 p\u02dc(1 \u2212 p\u02dc) n\u02dc , p\u02dc+ z\u03b1/2 p\u02dc(1 \u2212 p\u02dc) n\u02dc ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Point Estimation and Confidence Intervals",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 364,
            "page_oevrview": "If we want to make a confidence interval for p, based on the number of suc cesses X in the sample, we need to find statistics L and U (see the definition of confidence intervals on page 343) such that P(L&lt;p&lt;U)=1 \u2212 \u03b1, where L and U are to be based on X only. In general, this problem does not have a solution."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Point Estimation and Confidence Intervals",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 340,
            "page_oevrview": "So C is a confidence interval for if, whenever we are sampling from P the probability that is in the interval is at least equal to For a given data set, such an interval either covers or it does not."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Point Estimation and Confidence Intervals",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 344,
            "page_oevrview": "For example, if we want an approximate 0.95confidence interval for in Example 6.3.3, then based on the observed x 0 79 we obtain 0 79 1 96 0 79 1 0 79 1000 [0 76475 0 81525] The margin of error in this case equals 0 025245 so we can conclude that we know the true proportion with reasonable accuracy based on our sample."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Hypothesis Testing: Null and Alternative Hypotheses",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 376,
            "page_oevrview": "The null hypothesis corresponds to the position of the defendant: just as he is presumed to be innocent until proven guilty, so is the null hypothesis presumed to be true until the data provide convincing evidence against it. The alternative hypothesis corresponds to the charges brought against the defendant."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Hypothesis Testing: Null and Alternative Hypotheses",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 380,
            "page_oevrview": "The null hypothesis H0 may be true, whereas the data lead to rejection of H0. On the other hand, the alternative hypothesis H1 may be true, whereas we do not reject H0 on the basis of the data. These wrong decisions are called type I and type II errors. Type I and II errors."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Hypothesis Testing: Null and Alternative Hypotheses",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 385,
            "page_oevrview": "26 Testing hypotheses: elaboration In the previous chapter we introduced the setup for testing a null hypothesis against an alternative hypothesis using a test statistic T . The notions of type I error and type II error were introduced."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Hypothesis Testing: Null and Alternative Hypotheses",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 471,
            "page_oevrview": "Sometimes the Neyman\u2013Pearson theorem leads to solutions to hypothesis test ing problems when the null or alternative hypotheses allow for more than one possible value for but in general we must resort to likelihood ratio tests for such problems."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Hypothesis Testing: Null and Alternative Hypotheses",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 480,
            "page_oevrview": "The purpose of a hypothesis testing procedure is to decide which of H0 or Ha is true based on the observed data s So in this problem, the action space is H0 Ha and the correct action function is A H0 0 Ha 0 An alternative, and useful, way of thinking of the two hypotheses is as subsets of ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "P-values and Significance Testing in Statistics",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 86,
            "page_oevrview": "Roughly speaking, we interpret the p-value as follows: a small p-value p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real association between the predictor and the response."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "P-values and Significance Testing in Statistics",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 94,
            "page_oevrview": "So it reports the partial efect of adding that variable to the model. For instance, as we discussed earlier, these p-values indicate that TV and radio are related to sales, but that there is no evidence that newspaper is associated with sales, when TV and radio are held fxed."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "P-values and Significance Testing in Statistics",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 11,
            "page_oevrview": "Contents xv 13.5 A Re-Sampling Approach to p-Values and False Discovery Rates . . . . . . . . . . . . . . . . ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "P-values and Significance Testing in Statistics",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 389,
            "page_oevrview": "This can be done even without obtaining a dataset and computing the value t of the test statistic. The p value, on the other hand, represents the strength of the evidence the observed value t bears against H0. But it does not specify all values of T that lead to rejection of H0 at a given level \u03b1."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "P-values and Significance Testing in Statistics",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 379,
            "page_oevrview": "The smaller the p-value, the stronger evidence the observed value t bears against H0. The phrase \u201cat least as extreme as the observed value t\u201d refers to a particular direction, namely the direction in which values of T provide stronger evidence against H0 and in favor of H1."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Bayesian vs. Frequentist Approaches in Statistical Inference",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 287,
            "page_oevrview": "The Bayesian approach differs from the standard (\u201cfrequentist\u201d) method for inference in its use of a prior distribution to express the uncertainty present before seeing the data, and to allow the uncertainty remaining after seeing the data to be expressed in the form of a posterior distribution."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Bayesian vs. Frequentist Approaches in Statistical Inference",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 430,
            "page_oevrview": "Bagging and boosting are non-Bayesian procedures that have some simi larity to MCMC in a Bayesian model. The Bayesian approach fixes the data and perturbs the parameters, according to current estimate of the poste rior distribution."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Bayesian vs. Frequentist Approaches in Statistical Inference",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 311,
            "page_oevrview": "However, the greedy, short-sighted CART algorithm (Section 9.2) tries to find the best split on either feature, and then splits the resulting strata."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Bayesian vs. Frequentist Approaches in Statistical Inference",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 387,
            "page_oevrview": "The Bayesian formulation in essence removes the am biguity, but at the price of a more involved model. The Bayesian approach to inference is sometimes presented as antagonistic to meth ods that are based on repeated sampling properties (often referred to as frequentist 373."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Probability and Statistics",
            "topic": "Bayesian vs. Frequentist Approaches in Statistical Inference",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf",
            "page_no": 11,
            "page_oevrview": "It has become apparent in recent years, however, that Bayesian methodology is widely used in applications. As such, we feel that it is important for students to be exposed to this, as well as to the frequentist approaches, early in their statistical education."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "Convex vs. Non-Convex Functions in Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 249,
            "page_oevrview": "The classical Legendre transform is defined on convex differentiable functions in RD. Remark. Convex differentiable functions such as the example f(x) = x 2 is a nice special case, where there is no need for the supremum, and there is a one-to-one correspondence between a function and its Legendre trans form."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "Convex vs. Non-Convex Functions in Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 242,
            "page_oevrview": "Figures 7.5 and 7.6 illustrate convex and nonconvex sets, respectively. Figure 7.6 Example of a nonconvex set. Convex functions are functions such that a straight line between any two points of the function lie above the function."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "Convex vs. Non-Convex Functions in Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 245,
            "page_oevrview": ". . , m hj (x) = 0 for all j = 1, . . . , n , (7.38) where all functions f(x) and gi(x) are convex functions, and all hj (x) = 0 are convex sets. In the following, we will describe two classes of convex optimization problems that are widely used and well understood."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "Convex vs. Non-Convex Functions in Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 108,
            "page_oevrview": "Convex optimization algorithms are able to provide many more guarantees by making stronger restrictions. Convex optimization algorithms are applicable only to convex functions\u2014functions for which the Hessian is positive semidefinite everywhere."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "Convex vs. Non-Convex Functions in Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 337,
            "page_oevrview": "In non-convex problems, the path taken by the optimization trajectory can be very complicated and visit many different regions. Including points in parameter space from the distant past that may be separated from the current point by large barriers in the cost function does not seem like a useful behavior."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "First-Order and Second-Order Conditions for Convexity",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 243,
            "page_oevrview": "(7.31) If we further know that a function f(x) is twice differentiable, that is, the Hessian (5.147) exists for all values in the domain of x, then the function f(x) is convex if and only if \u22072 x f(x) is positive semidefinite (Boyd and Vandenberghe, 2004)."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "First-Order and Second-Order Conditions for Convexity",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 249,
            "page_oevrview": "Let us derive this from first principles. For a convex differentiable function, we know that at x0 the tangent touches f(x0) so that f(x0) = sx0 + c . (7.56) Recall that we want to describe the convex function f(x) in terms of its gradient \u2207xf(x), and that s = \u2207xf(x0)."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "First-Order and Second-Order Conditions for Convexity",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 244,
            "page_oevrview": "Combining the preceding two facts, we see that \u03b1f1(x) + \u03b2f2(x) is convex for \u03b1, \u03b2 \u2a7e 0. This closure property can be extended using a sim ilar argument for nonnegative weighted sums of more than two convex functions. Draft (2024-01-15) of \u201cMathematics for Machine Learning\u201d. Feedback: https://mml-book.com."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "First-Order and Second-Order Conditions for Convexity",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Advanced Calculus by Lynn H Loomis.pdf",
            "page_no": 161,
            "page_oevrview": "Therefore, there is a ~ such that II[f(x) - f(l)]j(x - l)II &lt; m + e when Ix - II ~ ~. It follows that IIf(l + ~) - f(a) II ~ 11.f(l + ~) - f(l) II + IIf(l) - f(a) II ~ (m + e)~ + (m + e)(l - a) + e = (m + e)(l + ~ - a) + e, so that l + ~ EA, a contradiction."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "First-Order and Second-Order Conditions for Convexity",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Advanced Calculus by Lynn H Loomis.pdf",
            "page_no": 355,
            "page_oevrview": "For any E &gt; 0, let ~ be so small that IIJ&quot;,(x)-IJ&quot;,(y)1I &lt; 1 + E for IIx - yllao &lt; ~ for all x, y in a compact neighborhood of A. (It is possible to choose such a ~, since J(x) is a uniformly continuous function of x, so that J&quot;,(x)-IJ&quot;,(y) is close to the identity matrix when x is close to y; see Section 8, Chapter 4.)"
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "Constrained vs. Unconstrained Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 459,
            "page_oevrview": "This is the extension of the gradient condition for unconstrained optimization to the constrained case. Constrained nonlinear least squares. As an example, consider the constrained least squares problem minimize f(x) 2 subject to g(x) = 0, where f : R n \u2192 R m and g : R n \u2192 R p . Define h(x) = f(x) 2 ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "Constrained vs. Unconstrained Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 458,
            "page_oevrview": "The method of Lagrange multipliers is an extension of the derivative or gradient conditions for (unconstrained) minimization, that handles constrained optimization problems. Lagrange multipliers."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "Constrained vs. Unconstrained Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 430,
            "page_oevrview": ". . , p, in vector form g(\u02c6x) = 0, (19.3) ie, \u02c6x is feasible, which we already knew. The first set of equations can be written in vector form as 2Df(\u02c6x) T f(\u02c6x) + Dg(\u02c6x) T z\u02c6 = 0. (19.4) This equation is the extension of the condition (18.3) for the unconstrained non linear least squares problem (18.2)."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "Constrained vs. Unconstrained Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 109,
            "page_oevrview": "We introduce new variables \u03bbi and \u03b1j for each constraint, these are called the KKT multipliers. The generalized Lagrangian is then defined as L(x,\u03bb, \u03b1) = f(x) +\ue058 i \u03bbi g (i) (x) +\ue058 j \u03b1jh (j)(x). (4.14) We can now solve a constrained minimization problem using unconstrained optimization of the generalized Lagrangian."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "Constrained vs. Unconstrained Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 108,
            "page_oevrview": "A more sophisticated approach is to design a different, unconstrained opti mization problem whose solution can be converted into a solution to the original, constrained optimization problem. For example, if we want to minimize f(x) for 93."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "Lagrange Multipliers and KKT Conditions in Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 459,
            "page_oevrview": "C.3 Lagrange multipliers 449 KKT conditions. The KKT conditions (named for Karush, Kuhn, and Tucker) state that if \u02c6x is a solution of the constrained optimization problem, then there is a vector \u02c6z that satisfies \u2202L \u2202xi (\u02c6x, z\u02c6) = 0, i = 1, . . . , n, \u2202L \u2202zi (\u02c6x, z\u02c6) = 0, i = 1, . . . , p."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "Lagrange Multipliers and KKT Conditions in Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 458,
            "page_oevrview": "Lagrange multipliers. The Lagrangian function associated with the constrained problem is defined as L(x, z) = h(x) + z1g1(x) + \u00b7 \u00b7 \u00b7 + zpgp(x) = h(x) + g(x) T z, with arguments x (the original variable to be determined in the optimization prob lem), and a p-vector z, called the (vector of) Lagrange multipliers."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "Lagrange Multipliers and KKT Conditions in Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 430,
            "page_oevrview": "The method of Lagrange multipliers tells us that for any solution \u02c6x of (19.1), there is a set of Lagrange multipliers \u02c6z that satisfy \u2202L \u2202xi (\u02c6x, z\u02c6) = 0, i = 1, ."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "Lagrange Multipliers and KKT Conditions in Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 109,
            "page_oevrview": "We introduce new variables \u03bbi and \u03b1j for each constraint, these are called the KKT multipliers. The generalized Lagrangian is then defined as L(x,\u03bb, \u03b1) = f(x) +\ue058 i \u03bbi g (i) (x) +\ue058 j \u03b1jh (j)(x). (4.14) We can now solve a constrained minimization problem using unconstrained optimization of the generalized Lagrangian."
        },
        {
            "domain": "Foundational Mathematics",
            "sub_domain": "Optimization",
            "topic": "Lagrange Multipliers and KKT Conditions in Optimization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 110,
            "page_oevrview": "To gain some intuition for this idea, we can say that either the solution is on the boundary imposed by the inequality and we must use its KKT multiplier to influence the solution to x, or the inequality has no influence on the solution and we represent this by zeroing out its KKT multiplier."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Python Syntax and Data Structures",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 51,
            "page_oevrview": "The three most important types of sequences are lists, tuples, and strings. We introduce lists now. The following command instructs Python to join together the numbers 3, 4, and 5, and to save them as a list named x. When we type x, it gives us list back the list."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Python Syntax and Data Structures",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 65,
            "page_oevrview": "Entries in the diferent arrays can be combined to form a row. The pandas library can be used to create and work with data frame objects. Reading in a Data Set The frst step of most analyses involves importing a data set into Python."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Python Syntax and Data Structures",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 53,
            "page_oevrview": "For instance, method given an array x, the expression x.sum() sums all of its elements, using the sum() method for arrays. The call x.sum() automatically provides x as the .sum() frst argument to its sum() method."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Python Syntax and Data Structures",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 54,
            "page_oevrview": "In general, all current machine-learning systems use tensors as their basic data structure. Tensors are fundamental to the field\u2014so fundamental that Google&#39;s TensorFlow was named after them. So what&#39;s a tensor? At its core, a tensor is a container for data\u2014almost always numerical data. So, it&#39;s a container for numbers."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Python Syntax and Data Structures",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 55,
            "page_oevrview": "This is also called the tensor&#39;s ndim in Python libraries such as Numpy. \uf0a1 Shape\u2014This is a tuple of integers that describes how many dimensions the ten sor has along each axis. For instance, the previous matrix example has shape (3, 5), and the 3D tensor example has shape (3, 3, 5)."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Lists and Tuples in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Linear Algebra Done Right by Sheldon Axler.pdf",
            "page_no": 19,
            "page_oevrview": "Many mathematicians call a list of length an -tuple. Lists are often written as elements separated by commas and surrounded by parentheses. Thus a list of length two is an ordered pair that might be written as ( , ). A list of length three is an ordered triple that might be written as ( , , )."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Lists and Tuples in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Linear Algebra Done Right by Sheldon Axler.pdf",
            "page_no": 54,
            "page_oevrview": "This implies that each \u2212 equals 0 (because 1 , \u2026 , is linearly independent). Hence 1 = 1 , \u2026 , = . We have the desired uniqueness, completing the proof in one direction. For the other direction, suppose every \u2208 can be written uniquely in the form given by 2.29. This implies that the list 1 , \u2026 , spans ."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Lists and Tuples in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 51,
            "page_oevrview": "In Python, lists hold arbitrary objects, and are added using concatenation. In fact, concatenation is the concatenat ion behavior that we saw earlier when we entered &quot;hello&quot; + &quot; &quot; + &quot;world&quot;. This example refects the fact that Python is a general-purpose program ming language."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Lists and Tuples in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 63,
            "page_oevrview": "Its because they are diferent Python types, and are treated diferently by numpy. Slices can be used to extract objects from arbitrary sequences, such as strings, lists, and tuples, while the use of lists for indexing is more limited."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Lists and Tuples in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 61,
            "page_oevrview": "This has to do with slice notation in Python. slice Slice notation is used to index sequences such as lists, tuples and arrays. Suppose we want to retrieve the fourth through sixth (inclusive) entries of a string. We obtain a slice of the string using the indexing notation [3:6]."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Dictionaries and Sets in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 450,
            "page_oevrview": "We can also specify a set by giving conditions that its entries must satisfy, using the notation {x | condition(x)}, which means the set of x that satisfy the condition, which depends on x. We say that a set contains its elements, or that the elements are in the set, using the symbol \u2208, as in 2 \u2208 {1, 2, 6}."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Dictionaries and Sets in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 126,
            "page_oevrview": "An example is the friend relation on a set of n people, where (i, j) \u2208 R means that person i and person j are friends. (In this case the associated graph is called the &#39;social network graph&#39;.) 6.3.2 Matrix addition Two matrices of the same size can be added together."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Dictionaries and Sets in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 20,
            "page_oevrview": "Such a vector encodes a subset of a collection of n objects, with oi = 1 meaning that object i is contained in the subset, and oi = 0 meaning that object i is not in the subset."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Dictionaries and Sets in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Advanced Calculus by Lynn H Loomis.pdf",
            "page_no": 25,
            "page_oevrview": "If the set of indices is I, the indexed set is designated {Xi: i E l} or {Xi};EI (or {Xi};:&#39;l in case I = Z+). However, this notation suggests that we view the indexed set as being obtained by letting the index run through the index set I and collecting the indexed objects."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Dictionaries and Sets in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Advanced Calculus by Lynn H Loomis.pdf",
            "page_no": 18,
            "page_oevrview": "4. SETS It is present-day practice to define every mathematical object as a set of some kind or other, and we must examine this fundamental notion, however briefly. A set is a collection of objects that is itself considered an entity. The objects in the collection are called the elements or members of the set."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Functions, Loops, and Conditionals in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 69,
            "page_oevrview": "The loop ends either when the cell ends or when code is indented at the same level as the original for statement. We see that the fnal line above which prints the total is executed only once after the for loop has terminated. Loops can be nested by additional indentation."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Functions, Loops, and Conditionals in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 50,
            "page_oevrview": "To run a function called fun, we type fun(input1,input2), where the inputs (or arguments) input1 and input2 tell Python how to run the function. A function can have any number of inputs. For example, the argument print() function outputs a text representation of all of its arguments to print() the console."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Functions, Loops, and Conditionals in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 70,
            "page_oevrview": "Tasks such as this can often be accomplished using the zip() function that loops over a sequence of tuples. zip() In [98]: total = 0 for value, weight in zip([2,3,19], [0.2,0.3,0.5]): total += weight * value print(&#39;Weighted average is: {0}&#39;.format(total)) Weighted average is: 10.8 String Formatting In the code chunk above we also printed a string displaying the total."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Functions, Loops, and Conditionals in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 220,
            "page_oevrview": "It&#39;s similar to the transformation operated by a densely connected layer in a feedforward network. state_t = 0 for input_t in input_sequence: output_t = activation(dot(W, input_t) + dot(U, state_t) + b) state_t = output_t To make these notions absolutely unambiguous, let&#39;s write a naive Numpy implemen tation of the forward pass of the simple RNN. import numpy as np timesteps = 100 input_features = 32 output_features = 64 inputs = np.random.random((timesteps, input_features)) state_t = np.zeros((output_features,)) W = np.random.random((output_features, input_features)) U = np.random.random((output_features, output_features)) b = np.random.random((output_features,)) successive_outputs = [] for input_t in inputs: output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b) successive_outputs.append(output_t) state_t = output_t final_output_sequence = np.concatenate(successive_outputs, axis=0) Easy enough: in summary, an RNN is a for loop that reuses quantities computed during the previous iteration of the loop, nothing more. Of course, there are many different RNNs fitting this definition that you could build\u2014this example is one of the simplest RNN formulations. RNNs are characterized by their step function, such as the following function in this case (see figure 6.10): output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b) Listing 6.20 More detailed pseudocode for the RNN Listing 6.21 Numpy implementation of a simple RNN Number of timesteps in the input sequence Dimensionality of the input feature space Dimensionality of the output feature space Input data: random noise for the sake of the example Initial state: an all-zero vector Creates random weight matrices input_t is a vector of shape (input_features,)."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Functions, Loops, and Conditionals in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 159,
            "page_oevrview": "This is the role of the steps_per_epoch argument: after having drawn steps_per_epoch batches from the generator\u2014that is, after having run for Understanding Python generators A Python generator is an object that acts as an iterator: it&#39;s an object you can use with the for \u2026 in operator."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Object-Oriented Programming (OOP) in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 53,
            "page_oevrview": "We can fnd out the number of rows and columns by looking at its shape attribute. shape In [16]: x.shape Out[16]: (2, 2) A method is a function that is associated with an object. For instance, method given an array x, the expression x.sum() sums all of its elements, using the sum() method for arrays."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Object-Oriented Programming (OOP) in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 51,
            "page_oevrview": "In Python, lists hold arbitrary objects, and are added using concatenation. In fact, concatenation is the concatenat ion behavior that we saw earlier when we entered &quot;hello&quot; + &quot; &quot; + &quot;world&quot;. This example refects the fact that Python is a general-purpose program ming language."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Object-Oriented Programming (OOP) in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 22,
            "page_oevrview": "A central problem in all statistical learning situations involves choosing the best method for a given application. Hence, in Chapter 5 we intro duce cross-validation and the bootstrap, which can be used to estimate the accuracy of a number of diferent methods in order to choose the best one."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Object-Oriented Programming (OOP) in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 16,
            "page_oevrview": "This solution is to allow computers to learn from experience and understand the world in terms of a hierarchy of concepts, with each concept defined in terms of its relation to simpler concepts."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Object-Oriented Programming (OOP) in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 115,
            "page_oevrview": "In order to solve the classification task, the learning algorithm only has to define a single function mapping from a vector input to a categorical output. When some of the inputs may be missing, rather than providing a single classification function, the learning algorithm must learn a set of functions."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Classes, Objects, Inheritance, and Encapsulation in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 51,
            "page_oevrview": "In Python, lists hold arbitrary objects, and are added using concatenation. In fact, concatenation is the concatenat ion behavior that we saw earlier when we entered &quot;hello&quot; + &quot; &quot; + &quot;world&quot;. This example refects the fact that Python is a general-purpose program ming language."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Classes, Objects, Inheritance, and Encapsulation in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 126,
            "page_oevrview": "Every python object has its own notion of namespace, also accessible with dir(). This will include both the attributes of the object as well as any methods associated with it. For instance, we see &#39;sum&#39; in the listing for an array. In [6]: A = np.array([3,5,11]) dir(A) Out[6]: ... &#39;strides&#39;, &#39;sum&#39;, &#39;swapaxes&#39;, ..."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Classes, Objects, Inheritance, and Encapsulation in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 22,
            "page_oevrview": "1. Introduction 11 Organization of This Book Chapter 2 introduces the basic terminology and concepts behind statisti cal learning."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Classes, Objects, Inheritance, and Encapsulation in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 362,
            "page_oevrview": "Because even given the progress made so far, most of the fundamental questions in AI remain unanswered. Many haven&#39;t even been properly asked yet. Licensed to &lt;null&gt;"
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Python Virtual Environments: venv and Conda",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 344,
            "page_oevrview": "The pooling layers let you spatially downsample the data, which is required to keep feature maps to a reasonable size as the number of features grows, and to allow subsequent convolution layers to \u201csee\u201d a greater spatial extent of the inputs."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Python Virtual Environments: venv and Conda",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 143,
            "page_oevrview": "Even though the convnet will be basic, its accuracy will blow out of the water that of the densely connected model from chapter 2. The following lines of code show you what a basic convnet looks like. It&#39;s a stack of Conv2D and MaxPooling2D layers. You&#39;ll see in a minute exactly what they do. from keras import layers from keras import models model = models.Sequential() model.add(layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(28, 28, 1))) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;)) model.add(layers.MaxPooling2D((2, 2))) model.add(layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;)) Importantly, a convnet takes as input tensors of shape (image_height, image_width, image_channels) (not including the batch dimension)."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Python Virtual Environments: venv and Conda",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 384,
            "page_oevrview": "INDEX 361 training, convnets on small datasets (continued) data preprocessing 135\u2013138 downloading data 131\u2013133 relevance for small-data problems 130\u2013131 using data augmentation 138\u2013142 interrupting 249 language models 276\u2013278 models 192\u2013195 training loop 11, 46 training sets 97\u2013100 hold-out validation 98\u201399 iterated K-fold validation with shuffling 100 K-fold validation 99 train_labels variable 27, 68 translation-invariant patterns 123, 321 transposition 43 Turing test 5 Turing, Alan 5 two-branch networks 59 Tyka, Mike 280, 306 U Ubuntu installing Keras on 343\u2013344 installing Python scientific suite on 341 installing Theano on 343 setting up GPU support 342\u2013343 underfitting 104\u2013110 adding dropout 109\u2013110 adding weight regularization 107\u2013108 reducing network size 104\u2013107 unfreezing layers 154 Unix workstation 65 unsupervised learning 94 V VAEs (variational autoencod ers), generating images with 296\u2013304 concept vectors for image editing 297\u2013298 sampling from latent spaces of images 296\u2013297 validation scores 100 validation sets 97\u2013100 hold-out validation 98\u201399 iterated K-fold validation with shuffling 100 K-fold validation 99 overfitting 97 overview 73 validation_data argument 74, 137 validation_steps argument 137 values handling missing 102 normalizing 101\u2013102 vanishing gradient problem 202 Vapnik, Vladimir 15 vector data 35, 319 vector regression 96 vectorization 101 vectorized data 69 vectorized implementations 38 vectorizing text 180 vectors (1D tensors) 31 versatility 23 vi 350 video data 37, 319 visual concepts 160 visualizing convnet filters 167\u2013172 convnet learning 160\u2013176 heatmaps of class activation 172\u2013176 intermediate activations 160\u2013166 volumetric data 319 W weight decay 107 weight regularization, adding 107\u2013108 weight sharing of layers 246\u2013247 weight-initialization schemes 22 weights argument, VGG16 58, 145 weights, layers 10 Welling, Max 298 width_shift range 139 word embeddings 184\u2013195 defining models 191 downloading GloVe word embeddings 190 evaluating models 192\u2013195 learning embedding layers 185\u2013187 loading GloVe embeddings in models 191 preprocessing embeddings 190\u2013191 tokenizing data 189\u2013190 training models 192\u2013195 using pretrained word embeddings 188 word vectors 184 Word2vec algorithm 188 word-embedding space 185 word_index 69 workflow of machine learning 111\u2013115, 318\u2013319 assembling datasets 111\u2013112 choosing evaluation protocol 112 choosing measure of success 112 defining problems 111\u2013112 developing models 113\u2013114 preparing data 112\u2013113 regularizing models 114\u2013115 tuning hyperparameters 114\u2013115 workflows 18 workstations, setting up 65\u201367 Jupyter notebooks 65 running jobs in cloud 66 running Keras 66 selecting GPUs 66\u201367 writing callbacks 251\u2013252 X Xception 244, 248 XGBoost library 19, 337 Y yield operator 136 Z Zisserman, Andrew 143 zoom_range 139 Licensed to &lt;null&gt;"
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Python Virtual Environments: venv and Conda",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 31,
            "page_oevrview": "In this context an environment is the equivalent of a Databricks workspace. Within each catalog we have the ability to manage both data and models. This architecture allows assets in the prod catalog to be accessed from the development environment, provided appropriate permissions are granted."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Python Virtual Environments: venv and Conda",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 22,
            "page_oevrview": "We then show how these methods can be used to ft non-linear additive models for which there is more than one input."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Package Management with Pip and Conda",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 364,
            "page_oevrview": "For instance: $ sudo apt-get install python3-pip python3-dev When you&#39;re installing packages using pip, keep in mind that by default, it targets Python 2. To target Python 3, you should use pip3: $ sudo pip3 install tensorflow-gpu Licensed to &lt;null&gt;"
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Package Management with Pip and Conda",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 382,
            "page_oevrview": "INDEX 359 O object detection 94 objective function 10, 60 Occam&#39;s razor principle 107 octaves 281\u2013282 one-hot encoding of characters 181\u2013183 of words 181\u2013183 overview 79, 84, 101 online documentation, Keras 338 optimization 22, 50, 104, 113, 263\u2013264 optimizer argument 11, 29, 58, 73 optimizers 60 output classes 77 overview 95 tensor 237 overfitting adding dropout 109\u2013110 adding weight regularization 107\u2013108 reducing network size 104\u2013107 using recurrent dropout to fight 216\u2013217 P padding 125\u2013126 parameterized layers 10 parameters adjusting 249 overview 97 partitions 99 passwd() command 349 PCA (principal component analysis) 255 Pichai, Sundar 22 pip 350 plot_model 258 plotting code 156 pointwise convolutions 243 pooling 1D, for sequence data 226 predict method 76, 83, 147 prediction error 95\u201396 predictions 83 preparing data 112\u2013113 preprocessing data 101\u2013103, 135\u2013138 for neural networks 101\u2013102 overview 135\u2013138 embeddings 190\u2013191 pretrained convnets 143\u2013159 feature extraction 143\u2013152 with data augmentation 149\u2013152 without data augmentation 147\u2013149 fine-tuning 152\u2013158 with small datasets 159 pretrained networks 130, 143 pretrained word embeddings 184 probabilistic modeling 14 probability distribution 80 problems, defining 111\u2013112 processing sequences with convnets 225\u2013231 1D convolution for sequence data 225\u2013226 1D pooling for sequence data 226 combining with recurrent neural networks to pro cess long sequences 228\u2013231 implementing 1D convnets 226\u2013227 program subroutines 334 program synthesis 331 PyCharm 65 pydot library 257 pydot-ng 341 Python installing scientific suite on Ubuntu 341 overview 19 python-pip package 341 Q question-answering model 238 R random forests 16\u201317 randomly shuffle data 100 randomness 272 rank 31 recurrent dropout 207, 216 recurrent layers, bidirectional 207 recurrent neural networks 196\u2013224, 319, 321\u2013322 basic machine-learning approach 213\u2013215 bidirectional 219\u2013222 combining with convnets 228\u2013231 first recurrent baseline 215\u2013216 generative, history of 271 GRU layers 202\u2013204 LSTM layers 202\u2013204 non-machine-learning baselines 212\u2013213 preparing data for 210\u2013212 recurrent layers in Keras 198\u2013202 stacking recurrent layers 217\u2013219 using recurrent dropout to fight overfitting 216\u2013217 ReduceLROnPlateau callbacks 250\u2013251 regression 60, 85\u201391, 320 regularization loss function 300 regularizing models 114\u2013115 reinforcement learning 95\u201396 relu (rectified linear unit) 71 representations extracting 28 overview 6 reshaping tensors 42\u201343 residual connections 235 response map 124 return_sequences argument 198 reusability 23 reverse-mode differentiation 52 RGB (red-green-blue) format 6 RMSProp optimizer 53, 73, 77, 135, 155, 222 RNN (recurrent neural network) 196 rotation_range 139 S samples axis 34 samples dimension 34 sampling from language models 276\u2013278 Licensed to &lt;null&gt;"
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Package Management with Pip and Conda",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 52,
            "page_oevrview": "Previously, our train ing images, for instance, were stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Package Management with Pip and Conda",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 275,
            "page_oevrview": "For instance, as we saw in Figure 6.23, one can easily obtain a model with R2 = 1 when p&gt;n. Reporting this fact might mislead others into thinking that a statistically valid and useful model has been obtained, whereas in fact this provides absolutely no evidence of a compelling model."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Python Programming",
            "topic": "Package Management with Pip and Conda",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 22,
            "page_oevrview": "Within each Python lab, we present the results that we obtained when we performed the lab at the time of writing this book. However, new versions of Python are continuously released, and over time, the packages called in the labs will be updated. Therefore, in the future, it is possible that the results shown in."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "NumPy Library",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 52,
            "page_oevrview": "The name numpy is an abbreviation for numerical Python. To access numpy, we must frst import it. import In [7]: import numpy as np In the previous line, we named the numpy module np; an abbreviation for module easier referencing."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "NumPy Library",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 61,
            "page_oevrview": "The function np.arange() returns a sequence of numbers spaced out by np.arange() step. If step is not specifed, then a default value of 1 is used. Let&#39;s create a sequence that starts at 0 and ends at 10. In [53]: seq2 = np.arange(0, 10) seq2 Out[53]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) Why isn&#39;t 10 output above?"
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "NumPy Library",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 53,
            "page_oevrview": "For instance, method given an array x, the expression x.sum() sums all of its elements, using the sum() method for arrays. The call x.sum() automatically provides x as the .sum() frst argument to its sum() method."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "NumPy Library",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 55,
            "page_oevrview": "This is also called the tensor&#39;s ndim in Python libraries such as Numpy. \uf0a1 Shape\u2014This is a tuple of integers that describes how many dimensions the ten sor has along each axis. For instance, the previous matrix example has shape (3, 5), and the 3D tensor example has shape (3, 3, 5)."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "NumPy Library",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 63,
            "page_oevrview": "An element-wise product is done with the * operator in Numpy, Keras, Theano, and TensorFlow. dot uses a different syntax in TensorFlow, but in both Numpy and Keras it&#39;s done using the standard dot operator: import numpy as np z = np.dot(x, y) In mathematical notation, you&#39;d note the operation with a dot (.): z=xy Mathematically, what does the dot operation do?"
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Linear Algebra Operations using NumPy",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 62,
            "page_oevrview": "BLAS are low-level, highly parallel, efficient tensor-manipulation routines that are typically implemented in Fortran or C. So, in Numpy, you can do the following element-wise operation, and it will be blaz ing fast: import numpy as np z=x+y z = np.maximum(z, 0.)"
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Linear Algebra Operations using NumPy",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 64,
            "page_oevrview": "The first dimension of x must be the same as the 0th dimension of y! This operation returns a vector of 0s with the same shape as y. x and y are Numpy matrices. The first dimension of x must be the same as the 0th dimension of y! This operation returns a matrix of 0s with a specific shape."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Linear Algebra Operations using NumPy",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 61,
            "page_oevrview": "If you want to write a naive Python imple mentation of an element-wise operation, you use a for loop, as in this naive implementation of an element-wise relu operation: def naive_relu(x): assert len(x.shape) == 2 x = x.copy() for i in range(x.shape[0]): for j in range(x.shape[1]): x[i, j] = max(x[i, j], 0) return x x is a 2D Numpy tensor."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Linear Algebra Operations using NumPy",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 39,
            "page_oevrview": "The notation f : R n \u2192 R means that f is a function that maps real n-vectors to real numbers, ie, it is a scalar-valued function of n-vectors. If x is an n-vector, then f(x), which is a scalar, denotes the value of the function f at x. (In the notation f(x), x is referred to as the argument of the function.)"
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Linear Algebra Operations using NumPy",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 451,
            "page_oevrview": "Vector operations In the table below, x and y are n-vectors and a is a scalar. ax n x + y n x T y 2n x 2n x \u2212 y 3n rms(x) 2n std(x) 4n (x, y) 6n The convolution a\u2217b of an n-vector a and m-vector b can be computed by a special algorithm that requires 5(m + n) log2 (m + n) flops."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Pandas DataFrames for Data Analysis",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 65,
            "page_oevrview": "Entries in the diferent arrays can be combined to form a row. The pandas library can be used to create and work with data frame objects. Reading in a Data Set The frst step of most analyses involves importing a data set into Python."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Pandas DataFrames for Data Analysis",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 72,
            "page_oevrview": "We can use the pd.plotting.scatter_matrix() function to create a scat- pd.plotting. scatter_ matrix() terplot matrix to visualize all of the pairwise relationships between the columns in a data frame. In [110]: pd.plotting.scatter_matrix(Auto); We can also produce scatterplots for a subset of the variables."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Pandas DataFrames for Data Analysis",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 76,
            "page_oevrview": "66 2. Statistical Learning This has used the frst column in the fle as an index for the data frame. This means that pandas has given each row a name corresponding to the appropriate university. Now you should see that the frst data column is Private. Note that the names of the colleges appear on the left of the table."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Pandas DataFrames for Data Analysis",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 165,
            "page_oevrview": "It is a simple example of a representation that attempts to disentangle the unknown factors of variation underlying the data. In the case of PCA, this disentangling takes the form of finding a rotation of the input space (described by W) that aligns the principal axes of variance with the basis of the new representation space associated with z."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Pandas DataFrames for Data Analysis",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 63,
            "page_oevrview": "To give the problem a unique solution, we constrain all of the columns of D to have unit norm. In order to turn this basic idea into an algorithm we can implement, the first thing we need to do is figure out how to generate the optimal code point c \u2217 for each input point x."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Reading and Writing Data with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 65,
            "page_oevrview": "This can be read in as follows: In [74]: Auto = pd.read_csv(&#39;Auto.data&#39;, delim_whitespace=True) Both Auto.csv and Auto.data are simply text fles. Before loading data into Python, it is a good idea to view it using a text editor or other software, such as Microsoft Excel."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Reading and Writing Data with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 75,
            "page_oevrview": "(a) Use the pd.read_csv() function to read the data into Python. Call the loaded data college. Make sure that you have the directory set to the correct location for the data. (b) Look at the data used in the notebook by creating and running a new cell with just the code college in it."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Reading and Writing Data with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 51,
            "page_oevrview": "In Python, lists hold arbitrary objects, and are added using concatenation. In fact, concatenation is the concatenat ion behavior that we saw earlier when we entered &quot;hello&quot; + &quot; &quot; + &quot;world&quot;. This example refects the fact that Python is a general-purpose program ming language."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Reading and Writing Data with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 56,
            "page_oevrview": "First, we load the MNIST dataset: from keras.datasets import mnist (train_images, train_labels), (test_images, test_labels) = mnist.load_data() Next, we display the number of axes of the tensor train_images, the ndim attribute: &gt;&gt;&gt; print(train_images.ndim) 3 Here&#39;s its shape: &gt;&gt;&gt; print(train_images.shape) (60000, 28, 28) And this is its data type, the dtype attribute: &gt;&gt;&gt; print(train_images.dtype) uint8 So what we have here is a 3D tensor of 8-bit integers."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Reading and Writing Data with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 231,
            "page_oevrview": "On this plot, you can clearly see the yearly periodicity of temperature. Listing 6.28 Inspecting the data of the Jena weather dataset Listing 6.29 Parsing the data Licensed to &lt;null&gt;"
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Web Scraping and APIs in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 22,
            "page_oevrview": "These include stepwise selection, ridge regression, principal components regression, and the lasso. The remaining chapters move into the world of non-linear statistical learning."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Web Scraping and APIs in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 65,
            "page_oevrview": "Entries in the diferent arrays can be combined to form a row. The pandas library can be used to create and work with data frame objects. Reading in a Data Set The frst step of most analyses involves importing a data set into Python."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Web Scraping and APIs in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 51,
            "page_oevrview": "In Python, lists hold arbitrary objects, and are added using concatenation. In fact, concatenation is the concatenat ion behavior that we saw earlier when we entered &quot;hello&quot; + &quot; &quot; + &quot;world&quot;. This example refects the fact that Python is a general-purpose program ming language."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Web Scraping and APIs in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 382,
            "page_oevrview": "INDEX 359 O object detection 94 objective function 10, 60 Occam&#39;s razor principle 107 octaves 281\u2013282 one-hot encoding of characters 181\u2013183 of words 181\u2013183 overview 79, 84, 101 online documentation, Keras 338 optimization 22, 50, 104, 113, 263\u2013264 optimizer argument 11, 29, 58, 73 optimizers 60 output classes 77 overview 95 tensor 237 overfitting adding dropout 109\u2013110 adding weight regularization 107\u2013108 reducing network size 104\u2013107 using recurrent dropout to fight 216\u2013217 P padding 125\u2013126 parameterized layers 10 parameters adjusting 249 overview 97 partitions 99 passwd() command 349 PCA (principal component analysis) 255 Pichai, Sundar 22 pip 350 plot_model 258 plotting code 156 pointwise convolutions 243 pooling 1D, for sequence data 226 predict method 76, 83, 147 prediction error 95\u201396 predictions 83 preparing data 112\u2013113 preprocessing data 101\u2013103, 135\u2013138 for neural networks 101\u2013102 overview 135\u2013138 embeddings 190\u2013191 pretrained convnets 143\u2013159 feature extraction 143\u2013152 with data augmentation 149\u2013152 without data augmentation 147\u2013149 fine-tuning 152\u2013158 with small datasets 159 pretrained networks 130, 143 pretrained word embeddings 184 probabilistic modeling 14 probability distribution 80 problems, defining 111\u2013112 processing sequences with convnets 225\u2013231 1D convolution for sequence data 225\u2013226 1D pooling for sequence data 226 combining with recurrent neural networks to pro cess long sequences 228\u2013231 implementing 1D convnets 226\u2013227 program subroutines 334 program synthesis 331 PyCharm 65 pydot library 257 pydot-ng 341 Python installing scientific suite on Ubuntu 341 overview 19 python-pip package 341 Q question-answering model 238 R random forests 16\u201317 randomly shuffle data 100 randomness 272 rank 31 recurrent dropout 207, 216 recurrent layers, bidirectional 207 recurrent neural networks 196\u2013224, 319, 321\u2013322 basic machine-learning approach 213\u2013215 bidirectional 219\u2013222 combining with convnets 228\u2013231 first recurrent baseline 215\u2013216 generative, history of 271 GRU layers 202\u2013204 LSTM layers 202\u2013204 non-machine-learning baselines 212\u2013213 preparing data for 210\u2013212 recurrent layers in Keras 198\u2013202 stacking recurrent layers 217\u2013219 using recurrent dropout to fight overfitting 216\u2013217 ReduceLROnPlateau callbacks 250\u2013251 regression 60, 85\u201391, 320 regularization loss function 300 regularizing models 114\u2013115 reinforcement learning 95\u201396 relu (rectified linear unit) 71 representations extracting 28 overview 6 reshaping tensors 42\u201343 residual connections 235 response map 124 return_sequences argument 198 reusability 23 reverse-mode differentiation 52 RGB (red-green-blue) format 6 RMSProp optimizer 53, 73, 77, 135, 155, 222 RNN (recurrent neural network) 196 rotation_range 139 S samples axis 34 samples dimension 34 sampling from language models 276\u2013278 Licensed to &lt;null&gt;"
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Web Scraping and APIs in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 362,
            "page_oevrview": "Because even given the progress made so far, most of the fundamental questions in AI remain unanswered. Many haven&#39;t even been properly asked yet. Licensed to &lt;null&gt;"
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Handling Missing Data with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 352,
            "page_oevrview": "For most learning methods, the imputation approach (3) is necessary. The simplest tactic is to impute the missing value with the mean or median of the nonmissing values for that feature. (Note that the above procedure for generalized additive models is analogous to this.)"
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Handling Missing Data with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 330,
            "page_oevrview": "Surrogate splits exploit correlations between predictors to try and alleviate the effect of missing data. The higher the cor relation between the missing predictor and the other predictors, the smaller the loss of information due to the missing value. The general problem of missing data is discussed in Section 9.6."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Handling Missing Data with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 351,
            "page_oevrview": "Data are said to be missing completely at random (MCAR) if the distribution of R doesn&#39;t depend on the observed or missing data: Pr(R|Z, \u03b8) = Pr(R|\u03b8). (9.32) MCAR is a stronger assumption than MAR: most imputation methods rely on MCAR for their validity."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Handling Missing Data with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 125,
            "page_oevrview": "The network will learn from exposure to the data that the value 0 means missing data and will start ignoring the value. Note that if you&#39;re expecting missing values in the test data, but the network was trained on data without any missing values, the network won&#39;t have learned to ignore missing values!"
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Handling Missing Data with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 124,
            "page_oevrview": "For now, we&#39;ll review the basics that are common to all data domains. 4.3.1 Data preprocessing for neural networks Data preprocessing aims at making the raw data at hand more amenable to neural networks. This includes vectorization, normalization, handling missing values, and feature extraction."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Detecting and Removing Outliers with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 113,
            "page_oevrview": "If we believe that an outlier has occurred due to an error in data collec tion or recording, then one solution is to simply remove the observation. However, care should be taken, since an outlier may instead indicate a defciency with the model, such as a missing predictor. 5."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Detecting and Removing Outliers with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 114,
            "page_oevrview": "Comparing the left-hand panels of Figures 3.12 and 3.13, we observe that removing the high leverage observation has a much more substantial impact on the least squares line than removing the outlier. In fact, high leverage observations tend to have a sizable impact on the estimated regression line."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Detecting and Removing Outliers with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 5,
            "page_oevrview": ". . . . 27 2.2 Assessing Model Accuracy . . . . . . . . . . . . . . . . . . 27 2.2.1 Measuring the Quality of Fit ."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Detecting and Removing Outliers with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 132,
            "page_oevrview": "At training time, we zero out at random a fraction of the values in the matrix: layer_output *= np.random.randint(0, high=2, size=layer_output.shape) At test time, we scale down the output by the dropout rate."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Processing with Python",
            "topic": "Detecting and Removing Outliers with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 133,
            "page_oevrview": "This made me realize that randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce over fitting.\u201d1 The core idea is that introducing noise in the output values of a layer can break up happenstance patterns that aren&#39;t significant (what Hinton refers to as con spiracies), which the network will start memorizing if no noise is present."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Visualization with Python",
            "topic": "Matplotlib for Data Visualization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 58,
            "page_oevrview": "We will use the subplots() function from matplotlib.pyplot to create a fgure and the axes onto which we plot our data. For many more examples of how to make plots in Python, readers are encouraged to visit matplotlib.org/stable/gallery/. In matplotlib, a plot consists of a fgure and one or more axes."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Visualization with Python",
            "topic": "Matplotlib for Data Visualization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 59,
            "page_oevrview": "This can be achieved by passing additional arguments to subplots(). Below, we create a 2 \u00d7 3 grid of plots in a fgure of size determined by the figsize argument. In such situations, there is often a relationship between the axes in the plots. For example, all plots may have a common x-axis."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Visualization with Python",
            "topic": "Matplotlib for Data Visualization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 130,
            "page_oevrview": "First we see the syntax for defning a function: def funcname(...). The function has arguments ax, b, m where ax is an axis object for an exisiting plot, b is the intercept and m is the slope of the desired line."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Visualization with Python",
            "topic": "Matplotlib for Data Visualization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 56,
            "page_oevrview": "More precisely, it&#39;s an array of 60000 matrices of 28 \u00d7 8 integers. Each such matrix is a grayscale image, with coeffi cients between 0 and 255."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Visualization with Python",
            "topic": "Matplotlib for Data Visualization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 280,
            "page_oevrview": "Inspecting and monitoring deep-learning models using Keras callbacks and TensorBoard 257 Note that Keras also provides another, cleaner way to plot models as graphs of layers rather than graphs of TensorFlow operations: the utility keras.utils.plot_model."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Visualization with Python",
            "topic": "Seaborn for Data Visualization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 219,
            "page_oevrview": "Under suitable smoothness conditions on f, the value of b that minimizes the MISE as n goes to infinity is given by b = C(f)n\u22121/3 where C(f)=61/3 \u221e \u2212\u221e f (x) 2 dx \u22121/3 (see for instance [29] or [12]). A simple data-based choice for b is obtained by estimating the constant C(f)."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Visualization with Python",
            "topic": "Seaborn for Data Visualization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf",
            "page_no": 267,
            "page_oevrview": "Boxplot of different datasets from Exercise 17.2."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Visualization with Python",
            "topic": "Seaborn for Data Visualization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 87,
            "page_oevrview": "High-dimensional data is often hard to analyze or visualize. However, high-dimensional data quite often pos sesses the property that only a few dimensions contain most information, and most other dimensions are not essential to describe key properties of the data."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Visualization with Python",
            "topic": "Seaborn for Data Visualization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 259,
            "page_oevrview": "Each column represents a particular feature of interest about the example, and we index the features as d = 1, . . . , D. Recall that data is represented as vectors, which means that each example (each data point) is a D-dimensional vector."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Data Visualization with Python",
            "topic": "Seaborn for Data Visualization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 479,
            "page_oevrview": "In the original space, every word is represented by a one-hot vector, so every pair of words is at Euclidean distance \u221a 2 from each other. In the embedding space, words that frequently appear in similar contexts (or any pair of words sharing some \u201cfeatures\u201d learned by the model) are close to each other."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Version Control and Reproducibility",
            "topic": "Git Basics: Cloning, Branching, Merging",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 89,
            "page_oevrview": "It allows for maintaining a clear history of changes, safe rollback to a previous version of the code, multiple contributors to work on their own branches of the project before merging to the main branch, etc."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Version Control and Reproducibility",
            "topic": "Git Basics: Cloning, Branching, Merging",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 65,
            "page_oevrview": "Conversely, a model that gets a perfect score is suspicious, as most problems have noise in the data that&#39;s at least somewhat hard to predict."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Version Control and Reproducibility",
            "topic": "Git Basics: Cloning, Branching, Merging",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 31,
            "page_oevrview": "Furthermore, just as environments can have different names, the branch naming and management strategies in Git can also vary. The Git workflow we present involves a dev branch merged into \u201cmain,\u201d and subsequently the main branch is merged into a \u201crelease\u201d branch."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Version Control and Reproducibility",
            "topic": "GitHub for Collaborative Projects: Pull Requests and Commit History",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 89,
            "page_oevrview": "It allows for maintaining a clear history of changes, safe rollback to a previous version of the code, multiple contributors to work on their own branches of the project before merging to the main branch, etc."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Version Control and Reproducibility",
            "topic": "Experiment Tracking with Jupyter Notebooks and Python Scripts",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 88,
            "page_oevrview": "They&#39;re widely used in the data-science and machine-learning communities. A notebook is a file generated by the Jupyter Notebook app (https://jupyter.org), which you can edit in your browser. It mixes the ability to execute Python code with rich text-editing capabilities for annotating what you&#39;re doing."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Version Control and Reproducibility",
            "topic": "Experiment Tracking with Jupyter Notebooks and Python Scripts",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 368,
            "page_oevrview": "A Jupyter notebook is a web app that allows you to write and annotate Python code interactively. It&#39;s a great way to experiment, do research, and share what you&#39;re working on. Many deep-learning applications are very computationally intensive and can take hours or even days when running on a laptop&#39;s CPU cores."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Version Control and Reproducibility",
            "topic": "Experiment Tracking with Jupyter Notebooks and Python Scripts",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 275,
            "page_oevrview": "You run this experiment and process the information it generates. This inspires your next idea. The more iterations of this loop you&#39;re able to run, the more refined and powerful your ideas become."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Version Control and Reproducibility",
            "topic": "Experiment Tracking with Jupyter Notebooks and Python Scripts",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 22,
            "page_oevrview": "The reader may choose to work through the labs at their own pace, or the labs may be the focus of group sessions as part of a classroom environment. Within each Python lab, we present the results that we obtained when we performed the lab at the time of writing this book."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Version Control and Reproducibility",
            "topic": "Experiment Tracking with Jupyter Notebooks and Python Scripts",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 449,
            "page_oevrview": "The callbacks argument allows for several tasks to be carried out at various points while training a model. Here our ErrorTracker() callback will enable us to com pute validation error while training and, fnally, the test error. We now ft the model for 50 epochs."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Version Control and Reproducibility",
            "topic": "Logging Hyperparameters and Results for Machine Learning Experiments",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 451,
            "page_oevrview": "At various time points, the hyperparameter optimization algorithm can choose to begin a new experiment, to \u201cfreeze\u201d a running experiment that is not promising, or to \u201cthaw\u201d and resume an experiment that was earlier frozen but now appears promising given more information."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Version Control and Reproducibility",
            "topic": "Logging Hyperparameters and Results for Machine Learning Experiments",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 443,
            "page_oevrview": "The generalization error typically follows a U-shaped curve when plotted as a function of one of the hyperparameters, as in figure 5.3. At one extreme, the hyperparameter value corresponds to low capacity, and generalization error is high because training error is high."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Version Control and Reproducibility",
            "topic": "Logging Hyperparameters and Results for Machine Learning Experiments",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 136,
            "page_oevrview": "Since the validation set is used to \u201ctrain\u201d the hyperparameters, the validation set error will underestimate the generalization error, though typically by a smaller amount than the training error. After all hyperparameter optimization is complete, the generalization error may be estimated using the test set."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Version Control and Reproducibility",
            "topic": "Logging Hyperparameters and Results for Machine Learning Experiments",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 287,
            "page_oevrview": "\uf0a1 The hyperparameter space is typically made of discrete decisions and thus isn&#39;t continuous or differentiable. Hence, you typically can&#39;t do gradient descent in hyperparameter space. Instead, you must rely on gradient-free optimization techniques, which naturally are far less efficient than gradient descent."
        },
        {
            "domain": "Programming Fundamentals",
            "sub_domain": "Version Control and Reproducibility",
            "topic": "Logging Hyperparameters and Results for Machine Learning Experiments",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 120,
            "page_oevrview": "In essence, this tuning is a form of learning: a search for a good configuration in some parameter space. As a result, tuning the configura tion of the model based on its performance on the validation set can quickly result in overfitting to the validation set, even though your model is never directly trained on it."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Data Preprocessing for Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 124,
            "page_oevrview": "For now, we&#39;ll review the basics that are common to all data domains. 4.3.1 Data preprocessing for neural networks Data preprocessing aims at making the raw data at hand more amenable to neural networks. This includes vectorization, normalization, handling missing values, and feature extraction."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Data Preprocessing for Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 29,
            "page_oevrview": "The measurement is used as a feedback signal to adjust the way the algorithm works. This adjustment step is what we call learning. A machine-learning model transforms its input data into meaningful outputs, a pro cess that is \u201clearned\u201d from exposure to known examples of inputs and outputs."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Data Preprocessing for Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 10,
            "page_oevrview": "CONTENTS ix Running deep-learning jobs in the cloud: pros and cons 66 What is the best GPU for deep learning? 66 3.4 Classifying movie reviews: a binary classification example 68 The IMDB dataset 68 \u25a0 Preparing the data 69 Building your network 70 \u25a0 Validating your approach 73 Using a trained network to generate predictions on new data 76 \u25a0 Further experiments 77 \u25a0 Wrapping up 77 3.5 Classifying newswires: a multiclass classification example 78 The Reuters dataset 78 \u25a0 Preparing the data 79 Building your network 79 \u25a0 Validating your approach 80 Generating predictions on new data 83 \u25a0 A different way to handle the labels and the loss 83 \u25a0 The importance of having sufficiently large intermediate layers 83 \u25a0 Further experiments 84 \u25a0 Wrapping up 84 3.6 Predicting house prices: a regression example 85 The Boston Housing Price dataset 85 \u25a0 Preparing the data 86 \u25a0 Building your network 86 \u25a0 Validating your approach using K-fold validation 87 \u25a0 Wrapping up 91 3.7 Chapter summary 92 4 Fundamentals of machine learning 93 4.1 Four branches of machine learning 94 Supervised learning 94 \u25a0 Unsupervised learning 94 Self-supervised learning 94 \u25a0 Reinforcement learning 95 4.2 Evaluating machine-learning models 97 Training, validation, and test sets 97 \u25a0 Things to keep in mind 100 4.3 Data preprocessing, feature engineering, and feature learning 101 Data preprocessing for neural networks 101 \u25a0 Feature engineering 102 4.4 Overfitting and underfitting 104 Reducing the network&#39;s size 104 \u25a0 Adding weight regularization 107 \u25a0 Adding dropout 109 4.5 The universal workflow of machine learning 111 Defining the problem and assembling a dataset 111 Choosing a measure of success 112 \u25a0 Deciding on an Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Data Preprocessing for Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 468,
            "page_oevrview": "Formatting images to have the same scale is the only kind of preprocessing that is strictly necessary. Many computer vision architectures require images of a standard size, so images must be cropped or scaled to fit that size. Even this rescaling is not always strictly necessary."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Data Preprocessing for Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 113,
            "page_oevrview": "Machine learning is essentially a form of applied statistics with increased emphasis on the use of computers to statistically estimate complicated functions and a decreased emphasis on proving confidence intervals around these functions; we therefore present the two central approaches to statistics: frequentist estimators and Bayesian inference."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Encoding Categorical Variables: One-Hot Encoding and Label Encoding",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 102,
            "page_oevrview": "For a more detailed explanation of one-hot encoding, see section 6.1. In this case, one-hot encoding of the labels consists of embedding each label as an all-zero vector with a 1 in the place of the label index."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Encoding Categorical Variables: One-Hot Encoding and Label Encoding",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 204,
            "page_oevrview": "It consists of associating a unique integer index with every word and then turning this integer index i into a binary vector of size N (the size of the vocabulary); the vector is all zeros except for the ith entry, which is 1."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Encoding Categorical Variables: One-Hot Encoding and Label Encoding",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 206,
            "page_oevrview": "% len(word_index)) A variant of one-hot encoding is the so-called one-hot hashing trick, which you can use when the number of unique tokens in your vocabulary is too large to handle explicitly."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Encoding Categorical Variables: One-Hot Encoding and Label Encoding",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 410,
            "page_oevrview": "The output is the class label, represented by a vector Y = (Y0, Y1,...,Y9) of 10 dummy variables, with a one in the position corresponding to the label, and zeros elsewhere. In the machine learning community, this is known as one-hot encoding."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Encoding Categorical Variables: One-Hot Encoding and Label Encoding",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 362,
            "page_oevrview": "However, the sklearn implementa tion of decision trees does not take advantage of this approach; instead it simply treats the one-hot-encoded levels as separate variables. In [6]: accuracy_score(High, clf.predict(X)) Out[6]: 0.7275 With only the default arguments, the training error rate is 21%."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Categorical Encoding with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 102,
            "page_oevrview": "One-hot encoding is a widely used for mat for categorical data, also called categorical encoding. For a more detailed explanation of one-hot encoding, see section 6.1. In this case, one-hot encoding of the labels consists of embedding each label as an all-zero vector with a 1 in the place of the label index."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Categorical Encoding with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 107,
            "page_oevrview": "\uf0a1 There are two ways to handle labels in multiclass classification: \u2013 Encoding the labels via categorical encoding (also known as one-hot encod ing) and using categorical_crossentropy as a loss function \u2013 Encoding the labels as integers and using the sparse_categorical_crossentropy loss function \uf0a1 If you need to classify data into a large number of categories, you should avoid creating information bottlenecks in your network due to intermediate layers that are too small."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Categorical Encoding with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Categorical Encoding with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 115,
            "page_oevrview": "In order to solve the classification task, the learning algorithm only has to define a single function mapping from a vector input to a categorical output. When some of the inputs may be missing, rather than providing a single classification function, the learning algorithm must learn a set of functions."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Categorical Encoding with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 19,
            "page_oevrview": "Autoencoders are trained to preserve as much information as possible when an input is run through the encoder and then the decoder, but are also trained to make the new representation have various nice properties. Different kinds of autoencoders aim to achieve different kinds of properties."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Feature Scaling Techniques: Min-Max Scaling, Standardization, Log Transformations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 279,
            "page_oevrview": "The standardization of each original feature is typically the first step in feature engineering. Note that the constant feature f1(x) = 1 is not standardized. (In fact, it cannot be standardized since its standard deviation across the data set is zero.) Winsorizing features."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Feature Scaling Techniques: Min-Max Scaling, Standardization, Log Transformations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 280,
            "page_oevrview": "If the feature value also includes the value 0 (so the logarithm is undefined) a common variation on the log transformation is to use \u02dcxk = log(xk + 1). This compresses the range of values that we encounter. As an example, suppose the original features record the number of visits to websites over some time period."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Feature Scaling Techniques: Min-Max Scaling, Standardization, Log Transformations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 283,
            "page_oevrview": "In document analysis applications word count features are typically replaced with term frequency inverse document frequency (TFIDF) values, which scale the raw count values by a function of the frequency with which the word appears across the given set of documents, usually in such a way that uncommon words are given more weight."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Feature Scaling Techniques: Min-Max Scaling, Standardization, Log Transformations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 194,
            "page_oevrview": "This transformation can be ft and then applied to arbitrary data. In the frst line below, the parameters for the scaling are computed and stored in scaler, while the second line actually constructs the standardized set of features."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Feature Scaling Techniques: Min-Max Scaling, Standardization, Log Transformations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 267,
            "page_oevrview": "The x-axis displays the shrinkage factor of the coefcient estimates, defned as the 2 norm of the shrunken coefcient estimates divided by the 2 norm of the least squares estimate. in the regression model, the bias decreases, but the variance increases. This results in a typical U-shape for the mean squared error."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Feature Scaling with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Feature Scaling with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 125,
            "page_oevrview": "4.3.2 Feature engineering Feature engineering is the process of using your own knowledge about the data and about the machine-learning algorithm at hand (in this case, a neural network) to make the algorithm work better by applying hardcoded (nonlearned) transfor mations to the data before it goes into the model."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Feature Scaling with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 126,
            "page_oevrview": "It usually requires understanding the problem in depth. Before deep learning, feature engineering used to be critical, because classical shallow algorithms didn&#39;t have hypothesis spaces rich enough to learn useful features by themselves. The way you presented the data to the algorithm was essential to its suc cess."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Feature Scaling with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 194,
            "page_oevrview": "This transformation can be ft and then applied to arbitrary data. In the frst line below, the parameters for the scaling are computed and stored in scaler, while the second line actually constructs the standardized set of features."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Feature Scaling with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 182,
            "page_oevrview": "In general, we can per negative binomial form a regression by modeling the response Y as coming from a particular member of the exponential family, and then transforming the mean of the response so that the transformed mean is a linear function of the predictors via (4.42)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Text Data Preprocessing: Tokenization and TF-IDF Vectorization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 203,
            "page_oevrview": "All text-vectorization processes consist of applying some tokenization scheme and then associating numeric vectors with the generated tokens. These vectors, packed into sequence tensors, are fed into deep neural networks. There are multiple ways to associate a vector with a token."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Text Data Preprocessing: Tokenization and TF-IDF Vectorization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 206,
            "page_oevrview": "Vectorization modes other than one-hot encoding are supported by this tokenizer. Builds the word index Stores the words as vectors of size 1000. If you have close to 1000 words (or more), you&#39;ll see many hash collisions, which will decrease the accuracy of this encoding method."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Text Data Preprocessing: Tokenization and TF-IDF Vectorization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 124,
            "page_oevrview": "For now, we&#39;ll review the basics that are common to all data domains. 4.3.1 Data preprocessing for neural networks Data preprocessing aims at making the raw data at hand more amenable to neural networks. This includes vectorization, normalization, handling missing values, and feature extraction."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Text Data Preprocessing: Tokenization and TF-IDF Vectorization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 60,
            "page_oevrview": "They predominantly: Operate on in-memory indexes. Focus solely on vector embeddings, often requiring a secondary storage mechanism for the actual data objects. Are typically immutable; post-index creation changes necessitate a complete rebuild of the index."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Text Data Preprocessing: Tokenization and TF-IDF Vectorization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 59,
            "page_oevrview": "This representation captures the semantic essence of the prompt and is used to search for relevant information in the database. Information retrieval Using the prompt&#39;s vector representation, RAG queries the external data sources or databases."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Text Preprocessing with NLTK and Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 124,
            "page_oevrview": "4.3.1 Data preprocessing for neural networks Data preprocessing aims at making the raw data at hand more amenable to neural networks. This includes vectorization, normalization, handling missing values, and feature extraction."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Text Preprocessing with NLTK and Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 211,
            "page_oevrview": "The rationale behind using pretrained word embed dings in natural-language processing is much the same as for using pretrained conv nets in image classification: you don&#39;t have enough data available to learn truly powerful features on your own, but you expect the features that you need to be fairly generic\u2014that is, common visual features or semantic features."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Text Preprocessing with NLTK and Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Text Preprocessing with NLTK and Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 462,
            "page_oevrview": "In the preprocessing used to form these data, sequences were padded with 0s in the beginning if they were not long enough, hence we remove this padding by restricting to entries where padded_sample &gt; 0. We then provide the frst 12 words of the sample review."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Text Preprocessing with NLTK and Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 182,
            "page_oevrview": "In [2]: from ISLP import confusion_table from ISLP.models import contrast from sklearn.discriminant_analysis import \\ (LinearDiscriminantAnalysis as LDA, QuadraticDiscriminantAnalysis as QDA) from sklearn.naive_bayes import GaussianNB from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import StandardScaler."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Feature Engineering for Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 125,
            "page_oevrview": "4.3.2 Feature engineering Feature engineering is the process of using your own knowledge about the data and about the machine-learning algorithm at hand (in this case, a neural network) to make the algorithm work better by applying hardcoded (nonlearned) transfor mations to the data before it goes into the model."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Feature Engineering for Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 126,
            "page_oevrview": "At this point, your features are making the problem so easy that no machine learning is required; a simple rounding opera tion and dictionary lookup are enough to recover the approximate time of day. That&#39;s the essence of feature engineering: making a problem easier by expressing it in a simpler way."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Feature Engineering for Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 41,
            "page_oevrview": "With joint feature learning, whenever the model adjusts one of its internal features, all other fea tures that depend on it automatically adapt to the change, without requiring human intervention. Everything is supervised by a single feedback signal: every change in the model serves the end goal."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Feature Engineering for Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 61,
            "page_oevrview": "Feature Engineering and Selection Features are how data is presented to a model, serving to inform that model on things it may not infer by itself."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Feature Engineering for Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 62,
            "page_oevrview": "For example, even if a particular deep learning model was trained on images that did not contain any forks, it may give a useful embedding to be used by a model that is trained to detect them, because a fork is an object, and that model was trained to detect similar human-made objects."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Feature Selection Methods: Filter, Wrapper, and Embedded Methods",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 61,
            "page_oevrview": "Feature Engineering and Selection Features are how data is presented to a model, serving to inform that model on things it may not infer by itself."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Feature Selection Methods: Filter, Wrapper, and Embedded Methods",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 62,
            "page_oevrview": "These tables are called embeddings, and they allow data scientists to perform transfer learning because they can be used in domains on which they were not trained. Transfer Learning Transfer learning is the technique of using information gained from solving one problem in solving a different problem."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Feature Selection Methods: Filter, Wrapper, and Embedded Methods",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 39,
            "page_oevrview": "Feature Engineering and Selection EDA leads naturally into feature engineering and feature selection. Feature engineer\u2010 ing is the process of taking raw data from the selected datasets and transforming it into \u201cfeatures\u201d that better represent the underlying problem to be solved."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Feature Selection Methods: Filter, Wrapper, and Embedded Methods",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 494,
            "page_oevrview": "User embeddings and item embeddings can then be conveniently visualized when they are first reduced to a low dimension (two or three), or they can be used to compare users or items against each other, just like word embeddings. One way to obtain these embeddings is by performing a singular value decomposition of the matrix R of actual targets (such as ratings)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Feature Selection Methods: Filter, Wrapper, and Embedded Methods",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 271,
            "page_oevrview": "This is an example of a general strategy in machine learning called model averaging. Techniques employing this strategy are known as ensemble methods. The reason that model averaging works is that different models will usually not make all the same errors on the test set."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Feature Selection with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Feature Selection with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 126,
            "page_oevrview": "It usually requires understanding the problem in depth. Before deep learning, feature engineering used to be critical, because classical shallow algorithms didn&#39;t have hypothesis spaces rich enough to learn useful features by themselves. The way you presented the data to the algorithm was essential to its suc cess."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Feature Selection with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 125,
            "page_oevrview": "4.3.2 Feature engineering Feature engineering is the process of using your own knowledge about the data and about the machine-learning algorithm at hand (in this case, a neural network) to make the algorithm work better by applying hardcoded (nonlearned) transfor mations to the data before it goes into the model."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Feature Selection with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 63,
            "page_oevrview": "using tools such as Auto-sklearn or AutoML applications that cross-reference features against a given target to estimate which features, derivatives, or combinations are likely to yield the best results, leaving out all the features that would probably not make that much of a difference."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Feature Engineering and Preprocessing",
            "topic": "Implementing Feature Selection with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 62,
            "page_oevrview": "\u2022 More features mean a loss of some stability. \u2022 The sheer number of features can raise privacy concerns. Automated feature selection can help by using heuristics to estimate how critical some features will be for the predictive performance of the model."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Introduction to Supervised Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 120,
            "page_oevrview": "The term supervised learning originates from the view of the target y being provided by an instructor or teacher who shows the machine learning system what to do. In unsupervised learning, there is no instructor or teacher, and the algorithm must learn to make sense of the data without this guide."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Introduction to Supervised Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 155,
            "page_oevrview": "MACHINE LEARNING BASICS 5.7 Supervised Learning Algorithms Recall from section 5.1.3 that supervised learning algorithms are, roughly speaking, learning algorithms that learn to associate some input with some output, given a training set of examples of inputs x and outputs y. In many cases the outputs y may be difficult to collect automatically and must be provided by a human \u201csupervisor,\u201d but the term still applies even when the training set targets were collected automatically."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Introduction to Supervised Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 121,
            "page_oevrview": "Traditionally, people refer to regression, classification and structured output problems as supervised learning. Density estimation in support of other tasks is usually considered unsupervised learning."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Introduction to Supervised Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 12,
            "page_oevrview": "Broadly speaking, supervised statistical learning involves building a statistical model for pre dicting, or estimating, an output based on one or more inputs. Problems of this nature occur in felds as diverse as business, medicine, astrophysics, and public policy."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Introduction to Supervised Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 35,
            "page_oevrview": "Many classical statistical learn ing methods such as linear regression and logistic regression (Chapter 4), as logistic well as more modern approaches such as GAM, boosting, and support vec- regression tor machines, operate in the supervised learning domain. The vast majority of this book is devoted to this setting."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Simple Linear Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 79,
            "page_oevrview": "3.1 Simple Linear Regression Simple linear regression lives up to its name: <b>it is a very straightforward simple linear regression approach for predicting a quantitative response Y on the basis of a sin gle predictor variable X</b>. It assumes that there is approximately a linear relationship between X and Y ."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Simple Linear Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 89,
            "page_oevrview": "We will see that R2 flls this role. 3.2 Multiple Linear Regression Simple linear regression is a useful approach for predicting a response on the basis of a single predictor variable. However, in practice we often have more than one predictor."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Simple Linear Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 138,
            "page_oevrview": "It is claimed in the text that in the case of simple linear regression of Y onto X, the R2 statistic (3.17) is equal to the square of the correlation between X and Y (3.18)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Simple Linear Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 72,
            "page_oevrview": "In this procedure, \u201cregress b on a\u201d means a simple univariate regression of b on a with no intercept, producing coefficient \u02c6\u03b3 = a, b/a, a and residual vector b \u2212 \u03b3\u02c6a. We say that b is adjusted for a, or is \u201corthogonalized\u201d with respect to a."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Simple Linear Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 63,
            "page_oevrview": ". . , Xp), and want to predict a real-valued output Y . The linear regression model has the form f(X) = \u03b20 + p j=1 Xj\u03b2j . (3.1) The linear model either assumes that the regression function E(Y |X) is linear, or that the linear model is a reasonable approximation."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Multiple Linear Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 90,
            "page_oevrview": "Then the multiple linear regression model takes the form Y = \u03b20 + \u03b21X1 + \u03b22X2 + \u00b7\u00b7\u00b7 + \u03b2pXp + , (3.19) where Xj represents the jth predictor and \u03b2j quantifes the association between that variable and the response. We interpret \u03b2j as the average efect on Y of a one unit increase in Xj , holding all other predictors fxed."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Multiple Linear Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 89,
            "page_oevrview": "We will see that R2 flls this role. 3.2 Multiple Linear Regression Simple linear regression is a useful approach for predicting a response on the basis of a single predictor variable. However, in practice we often have more than one predictor."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Multiple Linear Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 98,
            "page_oevrview": "This pro nounced non-linear pattern suggests a synergy or interaction efect between interaction the advertising media, whereby combining the media together results in a bigger boost to sales than using any single medium."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Multiple Linear Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 71,
            "page_oevrview": "(3.22) Therefore, expected prediction error and mean squared error differ only by the constant \u03c3 2 , representing the variance of the new observation y0. 3.2.3 Multiple Regression from Simple Univariate Regression The linear model (3.1) with p &gt; 1 inputs is called the multiple linear regression model."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Multiple Linear Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 344,
            "page_oevrview": "Thus if there are r linearly independent basis functions in the model, and K knots were selected in the forward process, the formula is M(\u03bb) = r+cK, where c = 3. (When the model is restricted to be additive\u2014details below\u2014 a penalty of c = 2 is used)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Linear Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 127,
            "page_oevrview": "The sklearn package has a particular notion sklearn for this type of task: a transform. A transform is an object that is created with some parameters as arguments. The object has two main methods: fit() and transform()."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Linear Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 16,
            "page_oevrview": "The approach was frst successfully applied to problems in astronomy. Linear regression is used for predicting quantitative values, such as an individual&#39;s salary."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Linear Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 183,
            "page_oevrview": "The syntax of sm.GLM() is similar to that of sm.OLS(), except that we must pass in the argument family=sm.families.Binomial() in order to tell statsmodels to run a logistic regression rather than some other type of generalized linear model."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Linear Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 122,
            "page_oevrview": "In the case of linear regression, the output is a linear function of the input. Let y\u02c6 be the value that our model predicts y should take on. We define the output to be y\u02c6 = w\ue03ex (5.3) where w \u2208 R n is a vector of parameters. Parameters are values that control the behavior of the system."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Linear Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 124,
            "page_oevrview": "Because there is only one feature, the weight vector w contains only a single parameter to learn, w1. (Left)Observe that linear regression learns to set w1 such that the line y = w1x comes as close as possible to passing through all the training points."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Polynomial Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 302,
            "page_oevrview": "7.4.1 Piecewise Polynomials Instead of ftting a high-degree polynomial over the entire range of X, piece wise polynomial regression involves ftting separate low-degree polynomials piecewise polynomial regression over diferent regions of X."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Polynomial Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 298,
            "page_oevrview": "In Section 7.7, we show that these approaches can be seamlessly in tegrated in order to model a response Y as a function of several predictors X1,...,Xp. 7.1 Polynomial Regression Historically, the standard way to extend linear regression to settings in which the relationship between the predictors and the response is non linear has been to replace the standard linear model yi = \u03b20 + \u03b21xi + i with a polynomial function yi = \u03b20 + \u03b21xi + \u03b22x2 i + \u03b23x3 i + \u00b7\u00b7\u00b7 + \u03b2dxd i + i, (7.1) where i is the error term."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Polynomial Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 335,
            "page_oevrview": "Report the regression output, and plot the resulting data and polynomial fts. (b) Plot the polynomial fts for a range of diferent polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Polynomial Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 301,
            "page_oevrview": "Note that the feature vector model parameters \u03b8 still appear only linearly. Example 9.3 (Polynomial Regression) We are concerned with a regression problem y = \u03d5 \u22a4 (x)\u03b8+\u03f5, where x \u2208 R and \u03b8 \u2208 RK. A transformation that is often used in this context is \u03d5(x) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 \u03d50(x) \u03d51(x) . . ."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Polynomial Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 305,
            "page_oevrview": "When we go to higher-degree The case of M = N \u2212 1 is extreme in the sense that otherwise the null space of the corresponding system of linear equations would be non-trivial, and we would have infinitely many optimal solutions to the linear regression problem. polynomials, we notice that they fit the data better and better."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Polynomial Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 320,
            "page_oevrview": "In [7]: plot_wage_fit(age_df, poly_age, &#39;Degree-4 Polynomial&#39;); With polynomial regression we must decide on the degree of the polyno mial to use. Sometimes we just wing it, and decide to use second or third degree polynomials, simply to obtain a nonlinear ft. But we can make such a decision in a more systematic way."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Polynomial Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 297,
            "page_oevrview": "\u2022 Polynomial regression extends the linear model by adding extra pre dictors, obtained by raising each of the original predictors to a power."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Polynomial Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 226,
            "page_oevrview": "We can repeat this procedure for increasingly complex polynomial fts. To automate the process, we again use a for loop which iteratively fts polynomial regressions of degree 1 to 5, computes the associated cross validation error, and stores it in the ith element of the vector cv_error."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Polynomial Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Polynomial Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 39,
            "page_oevrview": "A kernel function is a computationally tractable operation that maps any two points in your initial space to the distance between these points in your target representation space, completely bypassing the explicit computation of the new rep resentation."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Ridge and Lasso Regression for Regularization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 256,
            "page_oevrview": "As with ridge regression, when the least squares estimates have exces sively high variance, the lasso solution can yield a reduction in variance at the expense of a small increase in bias, and consequently can gener ate more accurate predictions."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Ridge and Lasso Regression for Regularization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 252,
            "page_oevrview": "We say that the lasso yields sparse models\u2014that is, sparse models that involve only a subset of the variables. As in ridge regression, selecting a good value of \u03bb for the lasso is critical; we defer this discussion to Section 6.2.3, where we use cross-validation."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Ridge and Lasso Regression for Regularization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 254,
            "page_oevrview": "In contrast, if s is small, then p j=1 |\u03b2j | must be small in order to avoid violating the budget. Similarly, (6.9) indicates that when we perform ridge regression, we seek a set of coefcient estimates such that the RSS is as small as possible, subject to the requirement that p j=1 \u03b22 j not exceed the budget s."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Ridge and Lasso Regression for Regularization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 88,
            "page_oevrview": "Each method applies a simple transformation to the least squares estimate \u03b2\u02c6 j , as detailed in Table 3.4. Ridge regression does a proportional shrinkage. Lasso translates each coefficient by a constant factor \u03bb, truncating at zero."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Ridge and Lasso Regression for Regularization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 87,
            "page_oevrview": "The lasso estimate is defined by \u03b2\u02c6lasso = argmin \u03b2 N i=1 yi \u2212 \u03b20 \u2212 p j=1 xij\u03b2j 2 subject to p j=1 |\u03b2j | \u2264 t. (3.51) Just as in ridge regression, we can re-parametrize the constant \u03b20 by stan dardizing the predictors; the solution for \u03b2\u02c6 0 is \u00afy, and thereafter we fit a model without an intercept (Exercise 3.5)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Regularized Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 281,
            "page_oevrview": "6.5.2 Ridge Regression and the Lasso We will use the sklearn.linear_model package (for which we use skl as shorthand below) to ft ridge and lasso regularized linear models on the Hitters data. We start with the model matrix X (without an intercept) that we computed in the previous section on best subset regression."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Regularized Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 464,
            "page_oevrview": "There are several solvers for logistic regression; here we use liblinear which works well with the sparse input format. In [79]: logit = LogisticRegression(penalty=&#39;l1&#39;, C=1/lam_max, solver=&#39;liblinear&#39;, warm_start=True, fit_intercept=True) The path of 50 values takes approximately 40 seconds to run."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Regularized Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 22,
            "page_oevrview": "These include stepwise selection, ridge regression, principal components regression, and the lasso. The remaining chapters move into the world of non-linear statistical learning. We frst introduce in Chapter 7 a number of non-linear meth ods that work well for problems with a single input variable."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Regularized Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Regularized Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 131,
            "page_oevrview": "As an alternative to L2 regularization, you can use one of the following Keras weight regularizers. from keras import regularizers regularizers.l1(0.001) regularizers.l1_l2(l1=0.001, l2=0.001) Listing 4.6 Adding L2 weight regularization to the model Listing 4.7 Different weight regularizers available in Keras Figure 4.7 Effect of L2 weight regularization on validation loss L1 regularization Simultaneous L1 and L2 regularization Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Logistic Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 146,
            "page_oevrview": "The logistic regression model leaves the marginal density of X as an arbi trary density function Pr(X), and fits the parameters of Pr(G|X) by max imizing the conditional likelihood\u2014the multinomial likelihood with proba bilities the Pr(G = k|X)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Logistic Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 138,
            "page_oevrview": "4.4 Logistic Regression The logistic regression model arises from the desire to model the posterior probabilities of the K classes via linear functions in x, while at the same time ensuring that they sum to one and remain in [0, 1]."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Logistic Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 318,
            "page_oevrview": "This involves repeatedly fitting a weighted linear regression of a working response variable on the covariates; each regression yields a new value of the parameter estimates, which in turn give new work ing responses and weights, and the process is iterated (see Section 4.4.1)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Logistic Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 147,
            "page_oevrview": "For the Default data, logistic regression models the probability of default. For example, the probability of default given balance can be written as Pr(default = Yes|balance). The values of Pr(default = Yes|balance), which we abbreviate p(balance), will range between 0 and 1."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Logistic Regression",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 149,
            "page_oevrview": "The logistic function will always produce an S-shaped curve of this form, and so regardless of the value of X, we will obtain a sensible prediction. We also see that the logistic model is better able to capture the range of probabilities than is the linear regression model in the left-hand plot."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Logistic Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 464,
            "page_oevrview": "There are several solvers for logistic regression; here we use liblinear which works well with the sparse input format. In [79]: logit = LogisticRegression(penalty=&#39;l1&#39;, C=1/lam_max, solver=&#39;liblinear&#39;, warm_start=True, fit_intercept=True) The path of 50 values takes approximately 40 seconds to run."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Logistic Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 183,
            "page_oevrview": "The syntax of sm.GLM() is similar to that of sm.OLS(), except that we must pass in the argument family=sm.families.Binomial() in order to tell statsmodels to run a logistic regression rather than some other type of generalized linear model."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Logistic Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 188,
            "page_oevrview": "These are the average of each predictor within each class, and are used by LDA as estimates of \u00b5k. These suggest that there is a tendency for the previous 2 days&#39; returns to be negative on days when the market increases, and a tendency for the previous days&#39; returns to be positive on days when the market declines."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Logistic Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Logistic Regression with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 270,
            "page_oevrview": "This means you can call a model on an input tensor and retrieve an out put tensor: y = model(x) If the model has multiple input tensors and multiple output tensors, it should be called with a list of tensors: y1, y2 = model([x1, x2]) When you call a model instance, you&#39;re reusing the weights of the model\u2014exactly like what happens when you call a layer instance."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Decision Trees and Random Forests",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 354,
            "page_oevrview": "We can think of this process as decorrelating the trees, thereby making the average of the resulting trees less variable and hence more reliable. The main diference between bagging and random forests is the choice of predictor subset size m."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Decision Trees and Random Forests",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 338,
            "page_oevrview": "Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision tree methods. decision tree Tree-based methods are simple and useful for interpretation."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Decision Trees and Random Forests",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 355,
            "page_oevrview": "Random forests (m&lt;p) lead to a slight improvement over bagging (m = p). A single classifcation tree has an error rate of 45.7 %. tive model. Notably, each tree is built on a bootstrap data set, independent of the other trees."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Decision Trees and Random Forests",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 39,
            "page_oevrview": "1.2.4 Decision trees, random forests, and gradient boosting machines Decision trees are flowchart-like structures that let you classify input data points or pre dict output values given inputs (see figure 1.11). They&#39;re easy to visualize and inter pret."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Decision Trees and Random Forests",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 40,
            "page_oevrview": "Applied to decision trees, the use of the gradient boosting technique results in models that strictly outperform random forests most of the time, while having similar proper ties. It may be one of the best, if not the best, algorithm for dealing with nonperceptual data today."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Decision Trees and Random Forests with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 361,
            "page_oevrview": "\u2022 In random forests, the trees are once again grown independently on random samples of the observations. However, each split on each tree is performed using a random subset of the features, thereby decorre lating the trees, and leading to a more thorough exploration of model space relative to bagging."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Decision Trees and Random Forests with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 367,
            "page_oevrview": "Bagging and random forests cannot overft by increasing the number of trees, but can underft if the number is too small. Growing a random forest proceeds in exactly the same way, except that we use a smaller value of the max_features argument."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Decision Trees and Random Forests with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 338,
            "page_oevrview": "Each of these approaches involves producing multiple trees which are then combined to yield a single consensus prediction. We will see that combining a large number of trees can often result in dramatic improvements in prediction accuracy, at the expense of some loss in interpretation."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Decision Trees and Random Forests with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 39,
            "page_oevrview": "In particular, the Random Forest algorithm introduced a robust, practical take on decision-tree learning that involves building a large number of specialized decision trees and then ensembling their outputs."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Decision Trees and Random Forests with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Naive Bayes Classifiers",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 37,
            "page_oevrview": "One of the best-known algorithms in this category is the Naive Bayes algorithm. Naive Bayes is <b>a type of machine-learning classifier based on applying Bayes&#39; theo rem while assuming that the features in the input data are all independent</b> (a strong, or \u201cnaive\u201d assumption, which is where the name comes from)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Naive Bayes Classifiers",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 288,
            "page_oevrview": "The easiest way to pool the predictions of a set of classifiers (to ensemble the classifiers) is to average their predictions at inference time: preds_a = model_a.predict(x_val) preds_b = model_b.predict(x_val) preds_c = model_c.predict(x_val) preds_d = model_d.predict(x_val) final_preds = 0.25 * (preds_a + preds_b + preds_c + preds_d) This will work only if the classifiers are more or less equally good."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Naive Bayes Classifiers",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 119,
            "page_oevrview": "\uf0a1 Multilabel classification\u2014A classification task where each input sample can be assigned multiple labels. For instance, a given image may contain both a cat and a dog and should be annotated both with the \u201ccat\u201d label and the \u201cdog\u201d label. The number of labels per image is usually variable."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Naive Bayes Classifiers",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 167,
            "page_oevrview": "The naive Bayes classifer takes a diferent tack for estimating f1(x),..., fK(x). Instead of assuming that these functions belong to a particular family of distributions (eg multivariate normal), we instead make a single assumption: Within the kth class, the p predictors are independent."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Naive Bayes Classifiers",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 168,
            "page_oevrview": "Essentially, the naive Bayes assumption introduces some bias, but reduces variance, leading to a classifer that works quite well in practice as a result of the bias-variance trade-of."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Naive Bayes with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 37,
            "page_oevrview": "Naive Bayes is a type of machine-learning classifier based on applying Bayes&#39; theo rem while assuming that the features in the input data are all independent (a strong, or \u201cnaive\u201d assumption, which is where the name comes from)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Naive Bayes with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Naive Bayes with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 211,
            "page_oevrview": "Instead of learning word embeddings jointly with the problem you want to solve, you can load embedding vectors from a precomputed embedding space that you know is highly structured and exhibits useful properties\u2014that captures generic aspects of language structure."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Naive Bayes with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 167,
            "page_oevrview": "The naive Bayes classifer takes a diferent tack for estimating f1(x),..., fK(x). Instead of assuming that these functions belong to a particular family of distributions (eg multivariate normal), we instead make a single assumption: Within the kth class, the p predictors are independent."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Naive Bayes with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 173,
            "page_oevrview": "In three of the scenarios, the Bayes decision boundary is linear, and in the remaining scenarios it is non-linear. For each scenario, we pro duced 100 random training data sets. On each of these training sets, we ft each method to the data and computed the resulting test error rate on a large test set."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Support Vector Machines (SVM)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 374,
            "page_oevrview": "SVMs have been shown to perform well in a variety of settings, and are often considered one of the best \u201cout of the box\u201d classifers. The support vector machine is a generalization of a simple and intu itive classifer called the maximal margin classifer, which we introduce in Section 9.1."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Support Vector Machines (SVM)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 386,
            "page_oevrview": "9.3.2 The Support Vector Machine The support vector machine (SVM) is an extension of the support vector support vector machine classifer that results from enlarging the feature space in a specifc way, using kernels."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Support Vector Machines (SVM)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 389,
            "page_oevrview": "Left: The support vector classifer and LDA are compared. Right: The support vector classifer is compared to an SVM using a radial basis kernel with \u03b3 = 10\u22123, 10\u22122, and 10\u22121. behavior, in the sense that only nearby training observations have an efect on the class label of a test observation."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Support Vector Machines (SVM)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 156,
            "page_oevrview": "The SVM predicts that the positive class is present when w\ue03e x + b is positive. Likewise, it predicts that the negative class is present when w\ue03e x + b is negative. One key innovation associated with support vector machines is the kernel trick."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Support Vector Machines (SVM)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 157,
            "page_oevrview": "Support vector machines are able to mitigate this by learning an \u03b1 vector that contains mostly zeros. Classifying a new example then requires evaluating the kernel function only for the training examples that have non-zero \u03b1i. These training examples are known 142."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing SVM with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 394,
            "page_oevrview": "In [2]: from sklearn.svm import SVC from ISLP.svm import plot as plot_svm from sklearn.metrics import RocCurveDisplay We will use the function RocCurveDisplay.from_estimator() to produce RocCurve Display.from_ estimator() several ROC plots, using a shorthand roc_curve."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing SVM with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 395,
            "page_oevrview": "For linear kernels, we can extract the coefcients of the linear decision boundary as follows: In [8]: svm_linear.coef_ Out[8]: array([[1.173 , 0.7734]]) Since the support vector machine is an estimator in sklearn, we can use the usual machinery to tune it."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing SVM with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 398,
            "page_oevrview": "In [22]: fig, ax = subplots(figsize=(8,8)) plot_svm(X_train, y_train, svm_rbf, ax=ax) We can see from the fgure that there are a fair number of training errors in this SVM ft. If we increase the value of C, we can reduce the number of training errors."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing SVM with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing SVM with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 38,
            "page_oevrview": "SVMs proceed to find these boundaries in two steps: 1 The data is mapped to a new high-dimensional representation where the decision boundary can be expressed as a hyperplane (if the data was two dimensional, as in figure 1.10, a hyperplane would be a straight line)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Ensemble Learning: Bagging, Boosting, and Stacking",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 350,
            "page_oevrview": "8.2 Bagging, Random Forests, Boosting, and Bayesian Additive Regression Trees 343 8.2 Bagging, Random Forests, Boosting, and Bayesian Additive Regression Trees An ensemble method is an approach that combines many simple \u201cbuilding ensemble block\u201d models in order to obtain a single and potentially very powerful model."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Ensemble Learning: Bagging, Boosting, and Stacking",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 356,
            "page_oevrview": "In this case, the boosted stump ensemble is ftting an additive model, since each term involves only a single variable. More generally d is the interaction depth, and controls interaction depth the interaction order of the boosted model, since d splits can involve at most d variables."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Ensemble Learning: Bagging, Boosting, and Stacking",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 360,
            "page_oevrview": "We see that the test error for boosting approaches that of BART, but then begins to increase as the number of iterations increases. Furthermore, the training error for boosting decreases as the number of iterations increases, indicating that boosting has overft the data."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Ensemble Learning: Bagging, Boosting, and Stacking",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 271,
            "page_oevrview": "Throughout this book, we will see many examples of sparsity regularization used in a variety of contexts. 7.11 Bagging and Other Ensemble Methods Bagging (short for bootstrap aggregating) is a technique for reducing gen eralization error by combining several models (Breiman, 1994)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Ensemble Learning: Bagging, Boosting, and Stacking",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 272,
            "page_oevrview": "Bagging is a method that allows the same kind of model, training algorithm and objective function to be reused several times. Specifically, bagging involves constructing k different datasets."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Ensemble Methods with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 289,
            "page_oevrview": "In recent times, one style of basic ensemble that has been very successful in prac tice is the wide and deep category of models, blending deep learning with shallow learn ing. Such models consist of jointly training a deep neural network with a large linear model."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Ensemble Methods with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Ensemble Methods with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 288,
            "page_oevrview": "A smarter way to ensemble classifiers is to do a weighted average, where the weights are learned on the validation data\u2014typically, the better classifiers are given a higher weight, and the worse classifiers are given a lower weight."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Ensemble Methods with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 271,
            "page_oevrview": "This is an example of a general strategy in machine learning called model averaging. Techniques employing this strategy are known as ensemble methods. The reason that model averaging works is that different models will usually not make all the same errors on the test set."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Ensemble Methods with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 341,
            "page_oevrview": "The gradient flows through many layers provided that the Jacobian of the linear transformation has reasonable singular values. Moreover, linear functions consistently increase in a single direction, so even if the model&#39;s output is very far from correct, it is clear simply from computing the gradient which direction its output should move to reduce the loss function."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Model Evaluation Metrics",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 45,
            "page_oevrview": "During development, data scientists select meaningful evaluation metrics for the use case, and those metrics or their custom logic are used in this step. REGISTER MODEL Upon completion of model training, the model artifact is registered to the prod catalog."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Model Evaluation Metrics",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 67,
            "page_oevrview": "Model Evaluation Evaluating LLMs is a challenging and evolving domain, primarily because LLMs often demonstrate uneven capabilities across different tasks. An LLM might excel in one benchmark, but slight variations in the prompt or problem can drastically affect its performance."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Model Evaluation Metrics",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 36,
            "page_oevrview": "EVALUATION Model quality is evaluated by testing on held-out data. The results of these tests are logged to the MLflow Tracking server. At this point it can be determined if a newly developed model outperforms that of the current model in production."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Model Evaluation Metrics",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 65,
            "page_oevrview": "With this in mind, it&#39;s important to evaluate a model in context and have some ability to compare it to what existed before the model\u2014whether a previous model or a rules based process\u2014to get an idea of what the outcome would be if the current model or decision process were replaced by the new one."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Model Evaluation Metrics",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 103,
            "page_oevrview": "The metrics to be monitored can be of two varieties: \u2022 Statistical metrics like accuracy, ROC AUC, log loss, etc. As the model designer has probably already chosen one of these metrics to pick the best model, it is a first-choice candidate for monitoring."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Model Evaluation with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Model Evaluation with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 120,
            "page_oevrview": "You train on the training data and evaluate your model on the validation data. Once your model is ready for prime time, you test it one final time on the test data. You may ask, why not have two sets: a training set and a test set? You&#39;d train on the training data and evaluate on the test data. Much simpler!"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Model Evaluation with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 122,
            "page_oevrview": "This method is helpful when the performance of your model shows significant variance based on your train test split. Like hold-out validation, this method doesn&#39;t exempt you from using a dis tinct validation set for model calibration. Schematically, K-fold cross-validation looks like figure 4.2."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Model Evaluation with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 111,
            "page_oevrview": "Model Evaluation Once the logging system is in place, it periodically fetches data from the production environment for monitoring. Everything goes well until one day the data drift alert is triggered: the incoming data distribution is drifting away from the training data dis\u2010 tribution."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Model Evaluation with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 75,
            "page_oevrview": "For example, when a model is developed using scikit-learn (Python) and pro\u2010 duction is a Java-based environment that expects PMML or ONNX as input, conversion is obviously required."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Cross-Validation Techniques for Model Evaluation",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 290,
            "page_oevrview": "Recall that cross-validation provides an estimate of the generalization error by repeatedly splitting the dataset into training and validation sets. We can apply this idea one more time, ie, for each split, we can perform another round of cross-validation."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Cross-Validation Techniques for Model Evaluation",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 269,
            "page_oevrview": "Cross-validation iterates through (ideally) all combinations of assignments of chunks to R and V; see Figure 8.4. This procedure is repeated for all K choices for the validation set, and the performance of the model from the K runs is averaged."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Cross-Validation Techniques for Model Evaluation",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 270,
            "page_oevrview": "Cross-validation approximates the expected generalization error EV [R(f, V)] \u2248 1 K K k=1 R(f (k) , V (k) ), (8.13) where R(f (k) , V (k) ) is the risk (eg, RMSE) on the validation set V (k) for predictor f (k) ."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Cross-Validation Techniques for Model Evaluation",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 110,
            "page_oevrview": "It consists of splitting the available data into K partitions (typically K = 4 or 5), instanti ating K identical models, and training each one on K \u2013 1 partitions while evaluating on the remaining partition. The validation score for the model used is then the average of the K validation scores obtained."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Cross-Validation Techniques for Model Evaluation",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 122,
            "page_oevrview": "This method is helpful when the performance of your model shows significant variance based on your train test split. Like hold-out validation, this method doesn&#39;t exempt you from using a dis tinct validation set for model calibration. Schematically, K-fold cross-validation looks like figure 4.2."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Cross-Validation with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 225,
            "page_oevrview": "5.3.2 Cross-Validation In theory, the cross-validation estimate can be computed for any general ized linear model. In practice, however, the simplest way to cross-validate in Python is to use sklearn, which has a diferent interface or API than statsmodels, the code we have been using to ft GLMs."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Cross-Validation with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 226,
            "page_oevrview": "218 5. Resampling Methods package, we provide a wrapper, sklearn_sm(), that enables us to easily use sklearn_sm() the cross-validation tools of sklearn with models ft by statsmodels. The class sklearn_sm() has as its frst argument a model from statsmodels."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Cross-Validation with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 215,
            "page_oevrview": "When we examine real data, we do not know the true test MSE, and so it is difcult to determine the accuracy of the cross-validation estimate. However, if we examine simulated data, then we can compute the true test MSE, and can thereby evaluate the accuracy of our cross-validation results."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Cross-Validation with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 110,
            "page_oevrview": "It consists of splitting the available data into K partitions (typically K = 4 or 5), instanti ating K identical models, and training each one on K \u2013 1 partitions while evaluating on the remaining partition. The validation score for the model used is then the average of the K validation scores obtained."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Cross-Validation with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 122,
            "page_oevrview": "This method is helpful when the performance of your model shows significant variance based on your train test split. Like hold-out validation, this method doesn&#39;t exempt you from using a dis tinct validation set for model calibration. Schematically, K-fold cross-validation looks like figure 4.2."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Hyperparameter Tuning Techniques",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 286,
            "page_oevrview": "The process of optimizing hyperparameters typically looks like this: 1 Choose a set of hyperparameters (automatically). 2 Build the corresponding model. 3 Fit it to your training data, and measure the final performance on the valida tion data. 4 Choose the next set of hyperparameters to try (automatically)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Hyperparameter Tuning Techniques",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 287,
            "page_oevrview": "\uf0a1 The hyperparameter space is typically made of discrete decisions and thus isn&#39;t continuous or differentiable. Hence, you typically can&#39;t do gradient descent in hyperparameter space. Instead, you must rely on gradient-free optimization techniques, which naturally are far less efficient than gradient descent."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Hyperparameter Tuning Techniques",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 356,
            "page_oevrview": "Hyperparameter tuning, however, is a simple search proce dure; and in that case we know what the engineer wants to achieve: it&#39;s defined by the loss function of the network being tuned. It&#39;s already common practice to set up basic AutoML systems that take care of most model knob tuning."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Hyperparameter Tuning Techniques",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 443,
            "page_oevrview": "11.4.1 Manual Hyperparameter Tuning To set hyperparameters manually, one must understand the relationship between hyperparameters, training error, generalization error and computational resources (memory and runtime)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Hyperparameter Tuning Techniques",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 444,
            "page_oevrview": "Usually these hyperparameters are switches that specify whether or not to use some optional component of the learning algorithm, such as a preprocessing step that normalizes the input features by subtracting their mean and dividing by their standard deviation."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Grid Search and Random Search with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 448,
            "page_oevrview": "Grid search wastes an amount of computation that is exponential in the number of non-influential hyperparameters, while random search tests a unique value of every influential hyperparameter on nearly every trial. Figure reproduced with permission from Bergstra and Bengio (2012). 433."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Grid Search and Random Search with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 449,
            "page_oevrview": "Typically, a grid search involves picking values approximately on a logarithmic scale, eg, a learning rate taken within the set {.1, .01, 10\u22123 , 10\u22124 , 10\u22125}, or a number of hidden units taken with the set {50, 100, 200, 500, 1000, 2000}. Grid search usually performs best when it is performed repeatedly."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Grid Search and Random Search with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 450,
            "page_oevrview": "The main reason why random search finds good solutions faster than grid search is that there are no wasted experimental runs, unlike in the case of grid search, when two values of a hyperparameter (given values of the other hyperparameters) would give the same result."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Grid Search and Random Search with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Supervised Learning",
            "topic": "Implementing Grid Search and Random Search with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 382,
            "page_oevrview": "INDEX 359 O object detection 94 objective function 10, 60 Occam&#39;s razor principle 107 octaves 281\u2013282 one-hot encoding of characters 181\u2013183 of words 181\u2013183 overview 79, 84, 101 online documentation, Keras 338 optimization 22, 50, 104, 113, 263\u2013264 optimizer argument 11, 29, 58, 73 optimizers 60 output classes 77 overview 95 tensor 237 overfitting adding dropout 109\u2013110 adding weight regularization 107\u2013108 reducing network size 104\u2013107 using recurrent dropout to fight 216\u2013217 P padding 125\u2013126 parameterized layers 10 parameters adjusting 249 overview 97 partitions 99 passwd() command 349 PCA (principal component analysis) 255 Pichai, Sundar 22 pip 350 plot_model 258 plotting code 156 pointwise convolutions 243 pooling 1D, for sequence data 226 predict method 76, 83, 147 prediction error 95\u201396 predictions 83 preparing data 112\u2013113 preprocessing data 101\u2013103, 135\u2013138 for neural networks 101\u2013102 overview 135\u2013138 embeddings 190\u2013191 pretrained convnets 143\u2013159 feature extraction 143\u2013152 with data augmentation 149\u2013152 without data augmentation 147\u2013149 fine-tuning 152\u2013158 with small datasets 159 pretrained networks 130, 143 pretrained word embeddings 184 probabilistic modeling 14 probability distribution 80 problems, defining 111\u2013112 processing sequences with convnets 225\u2013231 1D convolution for sequence data 225\u2013226 1D pooling for sequence data 226 combining with recurrent neural networks to pro cess long sequences 228\u2013231 implementing 1D convnets 226\u2013227 program subroutines 334 program synthesis 331 PyCharm 65 pydot library 257 pydot-ng 341 Python installing scientific suite on Ubuntu 341 overview 19 python-pip package 341 Q question-answering model 238 R random forests 16\u201317 randomly shuffle data 100 randomness 272 rank 31 recurrent dropout 207, 216 recurrent layers, bidirectional 207 recurrent neural networks 196\u2013224, 319, 321\u2013322 basic machine-learning approach 213\u2013215 bidirectional 219\u2013222 combining with convnets 228\u2013231 first recurrent baseline 215\u2013216 generative, history of 271 GRU layers 202\u2013204 LSTM layers 202\u2013204 non-machine-learning baselines 212\u2013213 preparing data for 210\u2013212 recurrent layers in Keras 198\u2013202 stacking recurrent layers 217\u2013219 using recurrent dropout to fight overfitting 216\u2013217 ReduceLROnPlateau callbacks 250\u2013251 regression 60, 85\u201391, 320 regularization loss function 300 regularizing models 114\u2013115 reinforcement learning 95\u201396 relu (rectified linear unit) 71 representations extracting 28 overview 6 reshaping tensors 42\u201343 residual connections 235 response map 124 return_sequences argument 198 reusability 23 reverse-mode differentiation 52 RGB (red-green-blue) format 6 RMSProp optimizer 53, 73, 77, 135, 155, 222 RNN (recurrent neural network) 196 rotation_range 139 S samples axis 34 samples dimension 34 sampling from language models 276\u2013278 Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Introduction to Unsupervised Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 120,
            "page_oevrview": "Roughly speaking, unsupervised learning involves observing several examples of a random vector x, and attempting to implicitly or explicitly learn the proba bility distribution p(x), or some interesting properties of that distribution, while supervised learning involves observing several examples of a random vector x and an associated value or vector y, and learning to predict y from x, usually by estimating p(y | x)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Introduction to Unsupervised Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 121,
            "page_oevrview": "Density estimation in support of other tasks is usually considered unsupervised learning. Other variants of the learning paradigm are possible. For example, in semi supervised learning, some examples include a supervision target but others do not."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Introduction to Unsupervised Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 113,
            "page_oevrview": "Most machine learning algorithms can be divided into the categories of supervised learning and unsupervised learning; we describe these categories and give some examples of simple learning algorithms from each category. Most deep learning algorithms are based on an optimization algorithm called stochastic gradient descent."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Introduction to Unsupervised Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 117,
            "page_oevrview": "4.1.2 Unsupervised learning This branch of machine learning consists of finding interesting transformations of the input data without the help of any targets, for the purposes of data visualization, data compression, or data denoising, or to better understand the correlations present in the data at hand."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Introduction to Unsupervised Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 118,
            "page_oevrview": "Self-supervised learning can be reinterpreted as either supervised or unsupervised learning, depending on whether you pay attention to the learning mechanism or to the context of its application."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "K-Means Clustering",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 479,
            "page_oevrview": "The main challenge is to figure out how many prototypes to use and where to put them. Methods differ according to the number and way in which prototypes are selected. 13.2.1 K-means Clustering K-means clustering is a method for finding clusters and cluster centers in a set of unlabeled data."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "K-Means Clustering",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 482,
            "page_oevrview": "The two steps of the alternating EM algorithm are very similar to the two steps in K means: \u2022 In the E-step, each observation is assigned a responsibility or weight for each cluster, based on the likelihood of each of the correspond ing Gaussians."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "K-Means Clustering",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 500,
            "page_oevrview": "K-means cluster ing is due to Lloyd (1957) and MacQueen (1967). Kohonen (1989) intro duced learning vector quantization. The tangent distance method is due to Simard et al. (1993). Hastie and Tibshirani (1996a) proposed the discrim inant adaptive nearest-neighbor technique."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "K-Means Clustering",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 79,
            "page_oevrview": "Typical applications use values of k that range from a handful to a few hundred or more, with values of N that range from hundreds to billions. Part of the task of clustering a collection of vectors is to determine whether or not the vectors can be divided into k groups, with vectors in each group near each other."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "K-Means Clustering",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 86,
            "page_oevrview": "However, depending on the initial choice of representatives, the algorithm can, and does, converge to different final partitions, with different objective values. The k-means algorithm is a heuristic, which means it cannot guarantee that the partition it finds minimizes our objective J clust."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing K-Means with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 165,
            "page_oevrview": "The k-means algorithm works by initializing k different centroids {\u00b5(1) , . . . , \u00b5 (k)} to different values, then alternating between two different steps until convergence. In one step, each training example is assigned to cluster i, where i is the index of the nearest centroid \u00b5 (i) ."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing K-Means with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 138,
            "page_oevrview": "It can be used to estimate generalization error of a learning algorithm A when the given dataset D is too small for a simple train/test or train/valid split to yield accurate estimation of generalization error, because the mean of a loss L on a small test set may have too high variance."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing K-Means with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 158,
            "page_oevrview": "More generally, k-nearest neighbors is a family of techniques that can be used for classification or regression. As a non-parametric learning algorithm, k-nearest neighbors is not restricted to a fixed number of parameters."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing K-Means with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 479,
            "page_oevrview": "To use K-means clustering for classification of labeled data, the steps are: \u2022 apply K-means clustering to the training data in each class sepa rately, using R prototypes per class; \u2022 assign a class label to each of the K \u00d7 R prototypes; \u2022 classify a new feature x to the class of the closest prototype."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing K-Means with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 469,
            "page_oevrview": "Our software (referenced in the Computational Considerations on page 455) allows several strategies; here we describe the default. The user supplies the number Rk of subclasses per class. Within class k, a k-means clustering model, with multiple random starts, is fitted to the data."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Hierarchical Clustering",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 479,
            "page_oevrview": "One chooses the desired number of cluster centers, say R, and the K-means procedure iteratively moves the centers to minimize the total within cluster variance.1 Given an initial set of centers, the K means algorithm alternates the two steps: \u2022 for each center we identify the subset of training points (its cluster) that is closer to it than any other center; \u2022 the means of each feature for the data points in each cluster are computed, and this mean vector becomes the new center for that cluster."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Hierarchical Clustering",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 482,
            "page_oevrview": "Observations close to the center of a cluster will most likely get weight 1 for that cluster, and weight 0 for every other clus ter. Observations half-way between two clusters divide their weight accordingly. \u2022 In the M-step, each observation contributes to the weighted means (and covariances) for every cluster."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Hierarchical Clustering",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 17,
            "page_oevrview": "528 14.5 Principal Components, Curves and Surfaces . ."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Hierarchical Clustering",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 79,
            "page_oevrview": "The goal of clustering is to group or partition the vectors (if possible) into k groups or clusters, with the vectors in each group close to each other."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Hierarchical Clustering",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 82,
            "page_oevrview": "Formally, we can express these index sets in terms of the group assignment vector c as Gj = {i | ci = j}, which means that Gj is the set of all indices i for which ci = j. Group representatives. With each of the groups we associate a group represen tative n-vector, which we denote z1, . . . , zk."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing Hierarchical Clustering with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 479,
            "page_oevrview": "One chooses the desired number of cluster centers, say R, and the K-means procedure iteratively moves the centers to minimize the total within cluster variance.1 Given an initial set of centers, the K means algorithm alternates the two steps: \u2022 for each center we identify the subset of training points (its cluster) that is closer to it than any other center; \u2022 the means of each feature for the data points in each cluster are computed, and this mean vector becomes the new center for that cluster."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing Hierarchical Clustering with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 482,
            "page_oevrview": "Observations close to the center of a cluster will most likely get weight 1 for that cluster, and weight 0 for every other clus ter. Observations half-way between two clusters divide their weight accordingly. \u2022 In the M-step, each observation contributes to the weighted means (and covariances) for every cluster."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing Hierarchical Clustering with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 16,
            "page_oevrview": "463 13.3 k-Nearest-Neighbor Classifiers . . . . . . . . . . . . . . . 463 13.3.1 Example: A Comparative Study . . . . . ."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing Hierarchical Clustering with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing Hierarchical Clustering with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 39,
            "page_oevrview": "A kernel function is a computationally tractable operation that maps any two points in your initial space to the distance between these points in your target representation space, completely bypassing the explicit computation of the new rep resentation."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "DBSCAN Clustering Algorithm",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 81,
            "page_oevrview": "\u2022 Daily energy use patterns. The 24-vectors xi give the average (electric) en ergy use for N customers over some period (say, a month) for each hour of the day. A clustering algorithm partitions customers into groups, each with similar patterns of daily energy consumption."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "DBSCAN Clustering Algorithm",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 80,
            "page_oevrview": "Suppose the vector xi gives the quantities (or dollar values) of n items purchased by customer i over some period of time. A clustering algorithm will group the customers into k market segments, which are groups of customers with similar purchasing patterns."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "DBSCAN Clustering Algorithm",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf",
            "page_no": 79,
            "page_oevrview": "Typical applications use values of k that range from a handful to a few hundred or more, with values of N that range from hundreds to billions. Part of the task of clustering a collection of vectors is to determine whether or not the vectors can be divided into k groups, with vectors in each group near each other."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "DBSCAN Clustering Algorithm",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 166,
            "page_oevrview": "If we ask each clustering algorithm to find two clusters, one algorithm may find a cluster of cars and a cluster of trucks, while another may find a cluster of red vehicles and a cluster of gray vehicles."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "DBSCAN Clustering Algorithm",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 165,
            "page_oevrview": "It is a simple example of a representation that attempts to disentangle the unknown factors of variation underlying the data. In the case of PCA, this disentangling takes the form of finding a rotation of the input space (described by W) that aligns the principal axes of variance with the basis of the new representation space associated with z."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing DBSCAN with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing DBSCAN with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 211,
            "page_oevrview": "Instead of learning word embeddings jointly with the problem you want to solve, you can load embedding vectors from a precomputed embedding space that you know is highly structured and exhibits useful properties\u2014that captures generic aspects of language structure."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing DBSCAN with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 50,
            "page_oevrview": "It&#39;s a set of 60000 training images, plus 10000 test images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of \u201csolving\u201d MNIST as the \u201cHello World\u201d of deep learning\u2014it&#39;s what you do to verify that your algorithms are working as expected."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing DBSCAN with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 182,
            "page_oevrview": "In [2]: from ISLP import confusion_table from ISLP.models import contrast from sklearn.discriminant_analysis import \\ (LinearDiscriminantAnalysis as LDA, QuadraticDiscriminantAnalysis as QDA) from sklearn.naive_bayes import GaussianNB from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import StandardScaler."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing DBSCAN with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 188,
            "page_oevrview": "These suggest that there is a tendency for the previous 2 days&#39; returns to be negative on days when the market increases, and a tendency for the previous days&#39; returns to be positive on days when the market declines."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Principal Component Analysis (PCA)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 263,
            "page_oevrview": "PCA is discussed in greater detail as a tool for unsupervised learning in Chapter 12. Here we describe its use as a dimension reduction technique for regression. An Overview of Principal Components Analysis PCA is a technique for reducing the dimension of an n \u00d7 p data matrix X."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Principal Component Analysis (PCA)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 265,
            "page_oevrview": "The Principal Components Regression Approach The principal components regression (PCR) approach involves construct- principal components regression ing the frst M principal components, Z1,...,ZM, and then using these components as the predictors in a linear regression model that is ft us ing least squares."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Principal Component Analysis (PCA)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 264,
            "page_oevrview": "It is the dimension along which the data vary the most, and it also defnes the line that is closest to all n of the observations. The distances from each observation to the principal component are represented using the black dashed line segments. The blue dot represents (pop, ad)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Principal Component Analysis (PCA)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 63,
            "page_oevrview": "PCA is defined by our choice of the decoding function. Specifically, to make the decoder very simple, we choose to use matrix multiplication to map the code back into Rn . Let g(c) = Dc, where D \u2208 R n\u00d7l is the matrix defining the decoding. Computing the optimal code for this decoder could be a difficult problem."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Principal Component Analysis (PCA)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 163,
            "page_oevrview": "To achieve full independence, a representation learning algorithm must also remove the nonlinear relationships between variables. PCA learns an orthogonal, linear transformation of the data that projects an input x to a representation z as shown in figure 5.8."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing PCA with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 63,
            "page_oevrview": "PCA is defined by our choice of the decoding function. Specifically, to make the decoder very simple, we choose to use matrix multiplication to map the code back into Rn . Let g(c) = Dc, where D \u2208 R n\u00d7l is the matrix defining the decoding. Computing the optimal code for this decoder could be a difficult problem."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing PCA with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 165,
            "page_oevrview": "It is a simple example of a representation that attempts to disentangle the unknown factors of variation underlying the data. In the case of PCA, this disentangling takes the form of finding a rotation of the input space (described by W) that aligns the principal axes of variance with the basis of the new representation space associated with z."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing PCA with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 163,
            "page_oevrview": "To achieve full independence, a representation learning algorithm must also remove the nonlinear relationships between variables. PCA learns an orthogonal, linear transformation of the data that projects an input x to a representation z as shown in figure 5.8."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing PCA with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 289,
            "page_oevrview": "The CV score is provided for each possible number of components from 1 to 19 inclusive. The PCA() method complains if we try to ft an intercept only with n_components=0 so we also compute the MSE for just the null model with these splits. In [53]: Xn = np.zeros((X.shape[0], 1)) cv_null = skm.cross_validate(linreg,"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing PCA with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 263,
            "page_oevrview": "An Overview of Principal Components Analysis PCA is a technique for reducing the dimension of an n \u00d7 p data matrix X. The frst principal component direction of the data is that along which the observations vary the most."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "t-SNE and UMAP for High-Dimensional Data Visualization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 323,
            "page_oevrview": "Furthermore, dimensions in high-dimensional data are often correlated so that the data possesses an intrinsic lower-dimensional structure. Dimensionality reduction exploits structure and correlation and allows us to work with a more compact rep resentation of the data, ideally without losing information."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "t-SNE and UMAP for High-Dimensional Data Visualization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 87,
            "page_oevrview": "More specifically, we can project the original high-dimensional data onto a lower-dimensional feature space and work in this lower-dimensional space to learn more about the dataset and extract relevant patterns. For example, machine \u00a92024 MP Deisenroth, AA Faisal, CS Ong. Published by Cambridge University Press (2020)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "t-SNE and UMAP for High-Dimensional Data Visualization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 259,
            "page_oevrview": "Observe that we have dropped the Name column of Table 8.1 in the new numerical representation."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "t-SNE and UMAP for High-Dimensional Data Visualization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 278,
            "page_oevrview": "Because the embedding space is 128-dimensional, TensorBoard auto matically reduces it to 2D or 3D using a dimensionality-reduction algorithm of your choice: either principal component analysis (PCA) or t-distributed stochastic neighbor embedding (t-SNE)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "t-SNE and UMAP for High-Dimensional Data Visualization",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 55,
            "page_oevrview": "This is also called the tensor&#39;s ndim in Python libraries such as Numpy. \uf0a1 Shape\u2014This is a tuple of integers that describes how many dimensions the ten sor has along each axis. For instance, the previous matrix example has shape (3, 5), and the 3D tensor example has shape (3, 3, 5)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing t-SNE and UMAP with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing t-SNE and UMAP with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 270,
            "page_oevrview": "This means you can call a model on an input tensor and retrieve an out put tensor: y = model(x) If the model has multiple input tensors and multiple output tensors, it should be called with a list of tensors: y1, y2 = model([x1, x2]) When you call a model instance, you&#39;re reusing the weights of the model\u2014exactly like what happens when you call a layer instance."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing t-SNE and UMAP with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 278,
            "page_oevrview": "Because the embedding space is 128-dimensional, TensorBoard auto matically reduces it to 2D or 3D using a dimensionality-reduction algorithm of your choice: either principal component analysis (PCA) or t-distributed stochastic neighbor embedding (t-SNE)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing t-SNE and UMAP with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 341,
            "page_oevrview": "The gradient flows through many layers provided that the Jacobian of the linear transformation has reasonable singular values. Moreover, linear functions consistently increase in a single direction, so even if the model&#39;s output is very far from correct, it is clear simply from computing the gradient which direction its output should move to reduce the loss function."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing t-SNE and UMAP with Scikit-Learn",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 156,
            "page_oevrview": "(5.83) This function is nonlinear with respect to x, but the relationship between \u03c6(x) and f (x) is linear."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Association Rule Mining",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 104,
            "page_oevrview": "According to this model, a one-unit increase in X1 is associated with an average increase in Y of \u03b21 units. Notice that the presence of X2 does not alter this statement\u2014that is, regardless of the value of X2, a one unit increase in X1 is associated with a \u03b21-unit increase in Y ."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Association Rule Mining",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 343,
            "page_oevrview": "We can select a value of \u03b1 using a validation set or using cross-validation. We then return to the full data set and obtain the subtree corresponding to \u03b1. This process is summarized in Algorithm 8.1."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Association Rule Mining",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 340,
            "page_oevrview": "Given that a player is less experienced, the number of hits that he made in the previous year seems to play little role in his salary."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Association Rule Mining",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 496,
            "page_oevrview": "After computation of the metric, it is used in a nearest-neighbor rule at x0. This complicated formula is actually quite simple in its operation. It first spheres the data with respect to W, and then stretches the neighborhood in the zero-eigenvalue directions of B\u2217 (the between-matrix for the sphered data )."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Association Rule Mining",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 17,
            "page_oevrview": ". 507 14.3.5 Combinatorial Algorithms . . . . . . . . . . . . 507 14.3.6 K-means . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing Association Rule Mining in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 498,
            "page_oevrview": "Once this relation is defined, we can use it like a verb. Because (1, 2) \u2208 S, we say that 1 is less than 2. Because (2,1) \ue036\u2208 S, we can not say that 2 is less than 1. Of course, the entities that are related to one another need not be numbers. We could define a relation is_a_type_of containing tuples like (dog, mammal)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing Association Rule Mining in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 499,
            "page_oevrview": "They also learn about interactions between words, such as which word is likely to come after a sequence of words, by learning functions of these vectors. We can extend this approach to entities and relations by learning an embedding vector for each relation."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing Association Rule Mining in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 63,
            "page_oevrview": "One way we can encode these points is to represent a lower-dimensional version of them. For each point x (i) \u2208 Rn we will find a corresponding code vector c (i) \u2208 Rl. If l is smaller than n, it will take less memory to store the code points than the original data."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing Association Rule Mining in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 126,
            "page_oevrview": "Every python object has its own notion of namespace, also accessible with dir(). This will include both the attributes of the object as well as any methods associated with it. For instance, we see &#39;sum&#39; in the listing for an array. In [6]: A = np.array([3,5,11]) dir(A) Out[6]: ... &#39;strides&#39;, &#39;sum&#39;, &#39;swapaxes&#39;, ..."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Unsupervised Learning",
            "topic": "Implementing Association Rule Mining in Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 133,
            "page_oevrview": "We compute the VIF for each of the variables in the model matrix X, using the function variance_inflation_factor(). variance_ inflation_ factor() In [29]: vals = [VIF(X, i) for i in range(1, X.shape[1])] vif = pd.DataFrame({&#39;vif&#39;:vals}, index=X.columns[1:]) vif Out[29]: vif crim 1.767 zn 2.298 indus 3.987 chas 1.071 nox 4.369 rm 1.913 age 3.088 dis 3.954 rad 7.445 tax 9.002 ptratio 1.797 lstat 2.871 The function VIF() takes two arguments: a dataframe or array, and a vari able column index."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Markov Decision Processes (MDPs) in Reinforcement Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 496,
            "page_oevrview": "Reinforcement learning requires choosing a tradeoff between exploration and exploitation. Exploitation refers to taking actions that come from the current, best version of the learned policy\u2014actions that we know will achieve a high reward."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Markov Decision Processes (MDPs) in Reinforcement Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 121,
            "page_oevrview": "This means we can represent the dataset with a design matrix X \u2208 R150\u00d74 , where Xi,1 is the sepal length of plant i, Xi,2 is the sepal width of plant i, etc. We will describe most of the learning algorithms in this book in terms of how they operate on design matrix datasets."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Markov Decision Processes (MDPs) in Reinforcement Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 497,
            "page_oevrview": "Reinforcement learning involves interaction between the learner and the environment. This feedback loop means that it is not straightforward to evaluate the learner&#39;s performance using a fixed set of test set input values. The policy itself determines which inputs will be seen. Dudik et al."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Markov Decision Processes (MDPs) in Reinforcement Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 57,
            "page_oevrview": "This includes the parameters of the end formula itself, but it also includes all the transformations to go from the input data that will be fed to the model to the end formula that will yield a value plus the possible derived data (like a classification or a decision)."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Markov Decision Processes (MDPs) in Reinforcement Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 30,
            "page_oevrview": "This allows the models to be integrated with the business rules, as well as helps the SMEs to fully understand decision contexts and the potential impact of model changes."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Implementing MDPs with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Implementing MDPs with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 134,
            "page_oevrview": "What you&#39;re trying to model changes over time. In this case, the right move is to constantly retrain your model on data from the recent past, or gather data at a timescale where the problem is stationary."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Implementing MDPs with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 380,
            "page_oevrview": "17\u201318 evaluating models of 97\u2013100 choosing evaluation protocols 100 test sets 97\u2013100 training sets 97\u2013100 validation sets 97\u2013100 feature engineering 101\u2013103 feature learning 101\u2013103 history of 14\u201319 decision trees 16\u201317 gradient boosting machines 16\u201317 kernel methods 15\u201316 neural networks 14\u201315, 17 probabilistic modeling 14 random forests 16\u201317 learning representations from data 6\u20138 models, risk of anthropo morphizing 325\u2013327 overfitting and underfitting 104\u2013110 adding dropout 109\u2013110 adding weight regularization 107\u2013108 reducing network size 104\u2013107 workflow of 111\u2013115, 318\u2013319 assembling datasets 111\u2013112 choosing evaluation protocol 112 choosing measure of success 112 defining problems 111\u2013112 developing models 113\u2013114 preparing data 112\u2013113 regularizing models 114\u2013115 tuning hyperparameters 114\u2013115 See also non-machine learn ing MAE (mean absolute error) 87, 91, 212, 320 Matplotlib library 33, 74, 349 matrices (2D tensors) 31\u201332 maximum operation 40 max-pooling operations 127\u2013129 MaxPooling1D layer 226, 231 MaxPooling2D layer 120, 122, 127 mean_squared_error 73 memorization capacity 104 metrics 29 metrics, logging 249 Microsoft Cognitive Toolkit. See CNTK Mikolov, Tomas 188 Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Implementing MDPs with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 56,
            "page_oevrview": "Their goal is to find a syn\u2010 thetic representation of the data they are fed, and this data represents the world as it was at the time of collection. Therefore, machine learning models can be used to make predictions when the future looks like the past, because their synthetic repre\u2010 sentation is still valid."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Implementing MDPs with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 141,
            "page_oevrview": "PART III MLOps: Real-World Examples."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Q-Learning and Deep Q Networks (DQN) in Reinforcement Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 186,
            "page_oevrview": "Learning in deep neural networks requires computing the gradients of complicated functions. We present the back-propagation algorithm and its modern generalizations, which can be used to efficiently compute these gradients. Finally, we close with some historical perspective."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Q-Learning and Deep Q Networks (DQN) in Reinforcement Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 40,
            "page_oevrview": "At the same time that the scale and accuracy of deep networks has increased, so has the complexity of the tasks that they can solve. Goodfellow et al. (2014d) showed that neural networks could learn to output an entire sequence of characters transcribed from an image, rather than just identifying a single object."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Q-Learning and Deep Q Networks (DQN) in Reinforcement Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 214,
            "page_oevrview": "In many cases, the number of hidden units required by the shallow model is exponential in n. Such results were first proved for models that do not resemble the continuous, differentiable neural networks used for machine learning, but have since been extended to these models."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Q-Learning and Deep Q Networks (DQN) in Reinforcement Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 32,
            "page_oevrview": "You can think of a deep network as a multistage information-distillation operation, where information goes through successive filters and comes out increasingly purified (that is, useful with regard to some task). So that&#39;s what deep learning is, technically: a multistage way to learn data representa tions."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Q-Learning and Deep Q Networks (DQN) in Reinforcement Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 31,
            "page_oevrview": "1.1.4 The \u201cdeep\u201d in deep learning Deep learning is a specific subfield of machine learning: a new take on learning repre sentations from data that puts an emphasis on learning successive layers of increasingly meaningful representations."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Implementing Q-Learning with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "All of the following steps are the same. The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Implementing Q-Learning with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 382,
            "page_oevrview": "INDEX 359 O object detection 94 objective function 10, 60 Occam&#39;s razor principle 107 octaves 281\u2013282 one-hot encoding of characters 181\u2013183 of words 181\u2013183 overview 79, 84, 101 online documentation, Keras 338 optimization 22, 50, 104, 113, 263\u2013264 optimizer argument 11, 29, 58, 73 optimizers 60 output classes 77 overview 95 tensor 237 overfitting adding dropout 109\u2013110 adding weight regularization 107\u2013108 reducing network size 104\u2013107 using recurrent dropout to fight 216\u2013217 P padding 125\u2013126 parameterized layers 10 parameters adjusting 249 overview 97 partitions 99 passwd() command 349 PCA (principal component analysis) 255 Pichai, Sundar 22 pip 350 plot_model 258 plotting code 156 pointwise convolutions 243 pooling 1D, for sequence data 226 predict method 76, 83, 147 prediction error 95\u201396 predictions 83 preparing data 112\u2013113 preprocessing data 101\u2013103, 135\u2013138 for neural networks 101\u2013102 overview 135\u2013138 embeddings 190\u2013191 pretrained convnets 143\u2013159 feature extraction 143\u2013152 with data augmentation 149\u2013152 without data augmentation 147\u2013149 fine-tuning 152\u2013158 with small datasets 159 pretrained networks 130, 143 pretrained word embeddings 184 probabilistic modeling 14 probability distribution 80 problems, defining 111\u2013112 processing sequences with convnets 225\u2013231 1D convolution for sequence data 225\u2013226 1D pooling for sequence data 226 combining with recurrent neural networks to pro cess long sequences 228\u2013231 implementing 1D convnets 226\u2013227 program subroutines 334 program synthesis 331 PyCharm 65 pydot library 257 pydot-ng 341 Python installing scientific suite on Ubuntu 341 overview 19 python-pip package 341 Q question-answering model 238 R random forests 16\u201317 randomly shuffle data 100 randomness 272 rank 31 recurrent dropout 207, 216 recurrent layers, bidirectional 207 recurrent neural networks 196\u2013224, 319, 321\u2013322 basic machine-learning approach 213\u2013215 bidirectional 219\u2013222 combining with convnets 228\u2013231 first recurrent baseline 215\u2013216 generative, history of 271 GRU layers 202\u2013204 LSTM layers 202\u2013204 non-machine-learning baselines 212\u2013213 preparing data for 210\u2013212 recurrent layers in Keras 198\u2013202 stacking recurrent layers 217\u2013219 using recurrent dropout to fight overfitting 216\u2013217 ReduceLROnPlateau callbacks 250\u2013251 regression 60, 85\u201391, 320 regularization loss function 300 regularizing models 114\u2013115 reinforcement learning 95\u201396 relu (rectified linear unit) 71 representations extracting 28 overview 6 reshaping tensors 42\u201343 residual connections 235 response map 124 return_sequences argument 198 reusability 23 reverse-mode differentiation 52 RGB (red-green-blue) format 6 RMSProp optimizer 53, 73, 77, 135, 155, 222 RNN (recurrent neural network) 196 rotation_range 139 S samples axis 34 samples dimension 34 sampling from language models 276\u2013278 Licensed to &lt;null&gt;"
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Implementing Q-Learning with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 362,
            "page_oevrview": "Learn ing is a lifelong journey, especially in the field of AI, where we have far more unknowns on our hands than certitudes."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Implementing Q-Learning with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 481,
            "page_oevrview": "LVQ is an online algorithm\u2014observations are processed one at a time. The idea is that the training points attract prototypes of the correct class, and repel other prototypes. When the iterations settle down, prototypes should be close to the training points in their class."
        },
        {
            "domain": "Classical Machine Learning",
            "sub_domain": "Reinforcement Learning",
            "topic": "Implementing Q-Learning with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 480,
            "page_oevrview": "Simulated example with three classes and five prototypes per class. The data in each class are generated from a mixture of Gaussians. In the upper panel, the prototypes were found by applying the K-means clustering algo rithm separately in each class."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Perceptron and Multi-Layer Perceptron (MLP)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 20,
            "page_oevrview": "A multilayer perceptron is just a mathematical function mapping some set of input values to output values. The function is formed by composing many simpler functions. We can think of each application of a different mathematical function as providing a new representation of the input."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Perceptron and Multi-Layer Perceptron (MLP)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 183,
            "page_oevrview": "Chapter 6 Deep Feedforward Networks Deep feedforward networks, also often called feedforward neural networks, or multilayer perceptrons (MLPs), are the quintessential deep learning models. The goal of a feedforward network is to approximate some function f \u2217 ."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Perceptron and Multi-Layer Perceptron (MLP)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 234,
            "page_oevrview": "This table-filling strategy is sometimes called dynamic programming. 6.5.7 Example: Back-Propagation for MLP Training As an example, we walk through the back-propagation algorithm as it is used to train a multilayer perceptron. Here we develop a very simple multilayer perception with a single hidden layer."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Perceptron and Multi-Layer Perceptron (MLP)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 339,
            "page_oevrview": "Together, the chain of layers in the model forms one complex geometric transforma tion, broken down into a series of simple ones. This complex transformation attempts to map the input space to the target space, one point at a time."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Perceptron and Multi-Layer Perceptron (MLP)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 82,
            "page_oevrview": "The notion of layer compatibility here refers specifically to the fact that every layer will only accept input tensors of a certain shape and will return output tensors of a cer tain shape."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Implementing Neural Networks with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 85,
            "page_oevrview": "On GPU, TensorFlow wraps a library of well-optimized deep-learning operations called the NVIDIA CUDA Deep Neural Network library (cuDNN). 3.2.2 Developing with Keras: a quick overview You&#39;ve already seen one example of a Keras model: the MNIST example."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Implementing Neural Networks with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 61,
            "page_oevrview": "We have three tensor operations here: a dot product (dot) between the input tensor and a tensor named W; an addition (+) between the resulting 2D ten sor and a vector b; and, finally, a relu operation. relu(x) is max(x, 0)."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Implementing Neural Networks with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 69,
            "page_oevrview": "Let&#39;s say the initial value of the coefficient is 0.3. After the forward pass on a batch of data, the loss of the network on the batch is 0.5. If you change the coefficient&#39;s value to 0.35 and rerun the forward pass, the loss increases to 0.6. But if you lower the coefficient to 0.25, the loss falls to 0.4."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Implementing Neural Networks with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 407,
            "page_oevrview": "10.1 Single Layer Neural Networks A neural network takes an input vector of p variables X = (X1, X2,...,Xp) and builds a nonlinear function f(X) to predict the response Y . We have built nonlinear prediction models in earlier chapters, using trees, boosting and generalized additive models."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Implementing Neural Networks with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 345,
            "page_oevrview": "The name \u201cconvolutional neural network\u201d indicates that the network employs a mathematical operation called convolution. Convolution is a specialized kind of linear operation. Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Activation Functions in Deep Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 95,
            "page_oevrview": "In order to get access to a much richer hypothesis space that would benefit from deep representations, you need a non-linearity, or activation function. relu is the most popular activation function in deep learning, but there are many other candi dates, which all come with similarly strange names: prelu, elu, and so on."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Activation Functions in Deep Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 226,
            "page_oevrview": "It involves three distinct transformations. All three have the form of a SimpleRNN cell: y = activation(dot(state_t, U) + dot(input_t, W) + b) But all three transformations have their own weight matrices, which you&#39;ll index with the letters i, f, and k."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Activation Functions in Deep Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 305,
            "page_oevrview": "Computes the gradients of the dream with regard to the loss Normalizes the gradients (important trick) Sets up a Keras function to retrieve the value of the loss and gradients, given an input image This function runs gradient ascent for a number of iterations. Licensed to &lt;null&gt;"
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Activation Functions in Deep Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 210,
            "page_oevrview": "Their use as output units is compatible with the use of gradient-based learning when an appropriate cost function can undo the saturation of the sigmoid in the output layer. When a sigmoidal activation function must be used, the hyperbolic tangent activation function typically performs better than the logistic sigmoid."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Activation Functions in Deep Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 207,
            "page_oevrview": "The design process consists of trial and error, intuiting that a kind of hidden unit may work well, and then training a network with that kind of hidden unit and evaluating its performance on a validation set."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Backpropagation Algorithm for Training Neural Networks",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 219,
            "page_oevrview": "During training, forward propagation can continue onward until it produces a scalar cost J(\u03b8). The back-propagation algorithm (Rumelhart et al., 1986a), often simply called backprop, allows the information from the cost to then flow backwards through the network, in order to compute the gradient."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Backpropagation Algorithm for Training Neural Networks",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 193,
            "page_oevrview": "This means we use the cross-entropy between the training data and the model&#39;s predictions as the cost function. Sometimes, we take a simpler approach, where rather than predicting a complete probability distribution over y, we merely predict some statistic of y conditioned on x."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Backpropagation Algorithm for Training Neural Networks",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 274,
            "page_oevrview": "Each time we load an example into a minibatch, we randomly sample a different binary mask to apply to all of the input and hidden units in the network. The mask for each unit is sampled independently from all of the others."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Backpropagation Algorithm for Training Neural Networks",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 75,
            "page_oevrview": "Backpropaga tion starts with the final loss value and works backward from the top layers to the bot tom layers, applying the chain rule to compute the contribution that each parameter had in the loss value."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Backpropagation Algorithm for Training Neural Networks",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 34,
            "page_oevrview": "This is the training loop, which, repeated a suffi cient number of times (typically tens of iterations over thousands of examples), yields weight values that minimize the loss function. A network with a minimal loss is one for which the outputs are as close as they can be to the targets: a trained network."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Loss Functions in Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 266,
            "page_oevrview": "To define what it means to fit the loss function data well, we need to specify a loss function \u2113(yn, y\u02c6n) that takes the ground truth label and the prediction as input and produces a non-negative num ber (referred to as the loss) representing how much error we have made The expression on this particular prediction."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Loss Functions in Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 387,
            "page_oevrview": "Remark. The ideal loss function between binary labels is to count the num ber of mismatches between the prediction and the label. This means that for a predictor f applied to an example xn, we compare the output f(xn) with the label yn. We define the loss to be zero if they match, and one if they do not match."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Loss Functions in Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 166,
            "page_oevrview": "In order to train these models, we require the gradient of a loss function L with respect to all model parameters Aj , bj for j = 1, . . . , K. This also requires us to compute the gradient of L with respect to the inputs of each layer."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Loss Functions in Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 33,
            "page_oevrview": "The loss function <b>takes the predictions of the network and the true target (what you wanted the network to output) and computes a distance score, capturing how well the network has done on this specific example</b> (see figure 1.8)."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Loss Functions in Machine Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 83,
            "page_oevrview": "3.1.3 Loss functions and optimizers: keys to configuring the learning process Once the network architecture is defined, you still have to choose two more things: \uf0a1 Loss function (objective function)\u2014The quantity that will be minimized during training. It represents a measure of success for the task at hand."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Implementing Custom Loss Functions with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 83,
            "page_oevrview": "It represents a measure of success for the task at hand. \uf0a1 Optimizer\u2014Determines how the network will be updated based on the loss func tion. It implements a specific variant of stochastic gradient descent (SGD). A neural network that has multiple outputs may have multiple loss functions (one per output)."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Implementing Custom Loss Functions with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 315,
            "page_oevrview": "Adds a style loss component for each target layer Adds the total variation loss Function to fetch the values of the current loss and the current gradients This class wraps fetch_loss_and_grads in a way that lets you retrieve the losses and gradients via two separate method calls, which is required by the SciPy optimizer you&#39;ll use."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Implementing Custom Loss Functions with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 325,
            "page_oevrview": "Because the loss is taken care of in the custom layer, you don&#39;t specify an external loss at compile time (loss=None), which in turn means you won&#39;t pass target data during training (as you can see, you only pass x_train to the model in fit). from keras.datasets import mnist vae = Model(input_img, y) vae.compile(optimizer=&#39;rmsprop&#39;, loss=None) vae.summary() (x_train, _), (x_test, y_test) = mnist.load_data() x_train = x_train.astype(&#39;float32&#39;) / 255. x_train = x_train.reshape(x_train.shape + (1,)) x_test = x_test.astype(&#39;float32&#39;) / 255. x_test = x_test.reshape(x_test.shape + (1,)) vae.fit(x=x_train, y=None, shuffle=True, epochs=10, batch_size=batch_size, validation_data=(x_test, None)) Listing 8.26 Custom layer used to compute the VAE loss Listing 8.27 Training the VAE Instantiates the decoder model, which turns \u201cdecoder_input\u201d into the decoded image Applies it to z to recover the decoded z You implement custom layers by writing a call method."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Implementing Custom Loss Functions with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 266,
            "page_oevrview": "To define what it means to fit the loss function data well, we need to specify a loss function \u2113(yn, y\u02c6n) that takes the ground truth label and the prediction as input and produces a non-negative num ber (referred to as the loss) representing how much error we have made The expression on this particular prediction."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Implementing Custom Loss Functions with Python",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 387,
            "page_oevrview": "Remark. The ideal loss function between binary labels is to count the num ber of mismatches between the prediction and the label. This means that for a predictor f applied to an example xn, we compare the output f(xn) with the label yn. We define the loss to be zero if they match, and one if they do not match."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Vanishing and Exploding Gradient Problems in Deep Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 305,
            "page_oevrview": "<b>Vanishing gradients make it difficult to know which direction the parameters should move to improve the cost function, while exploding gradients can make learning unstable</b>. The cliff structures described earlier that motivate gradient clipping are an example of the exploding gradient phenomenon."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Vanishing and Exploding Gradient Problems in Deep Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 418,
            "page_oevrview": "The vanishing and exploding gradient problem for RNNs was independently discovered by separate researchers (Hochreiter, 1991; Bengio et al., 1993, 1994). One may hope that the problem can be avoided simply by staying in a region of parameter space where the gradients do not vanish or explode."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Vanishing and Exploding Gradient Problems in Deep Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 428,
            "page_oevrview": "The difficulty that arises is that when the parameter gradient is very large, a gradient descent parameter update could throw the parameters very far, into a region where the objective function is larger, undoing much of the work that had been done to reach the current solution."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Vanishing and Exploding Gradient Problems in Deep Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 269,
            "page_oevrview": "If this feed back signal has to be propagated through a deep stack of layers, the signal may become tenuous or even be lost entirely, rendering the network untrainable. This issue is known as vanishing gradients."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Vanishing and Exploding Gradient Problems in Deep Learning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 225,
            "page_oevrview": "This is due to the van ishing gradient problem, an effect that is similar to what is observed with non-recurrent networks (feedforward networks) that are many layers deep: as you keep adding layers to a network, the network eventually becomes untrainable."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Gradient Stabilization Techniques",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 378,
            "page_oevrview": "A possible resolution to this dilemma is to induce a tree T(x; \u0398m) at the mth iteration whose predictions tm are as close as possible to the negative gradient. Using squared error to measure closeness, this leads us to \u0398\u02dc m = arg min \u0398 N i=1 (\u2212gim \u2212 T(xi ;"
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Gradient Stabilization Techniques",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 383,
            "page_oevrview": "Coordinate functions estimated by boosting stumps for the sim ulated example used in Figure 10.9. The true quadratic functions are shown for comparison. 10.12 Regularization Besides the size of the constituent trees, J, the other meta-parameter of gradient boosting is the number of boosting iterations M."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Gradient Stabilization Techniques",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf",
            "page_no": 380,
            "page_oevrview": "Lines 2(a)\u2013(d) are repeated K times at each iteration m, once for each class using (10.38). The result at line 3 is K different (coupled) tree expansions fkM(x), k = 1, 2, . . . , K. These produce probabilities via (10.21) or do classification as in (10.20). Details are given in Exercise 10.9."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Gradient Stabilization Techniques",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 328,
            "page_oevrview": "In the method of conjugate gradients, we seek to find a search direction that is conjugate to the previous line search direction, ie it will not undo progress made in that direction. At training iteration t, the next search direction dt takes 313."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Gradient Stabilization Techniques",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 430,
            "page_oevrview": "Because the gradient of all the parameters (including different groups of parameters, such as weights and biases) is renormalized jointly with a single scaling factor, the latter method has the advantage that it guarantees that each step is still in the gradient direction, but experiments suggest that both forms work similarly."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Normalization Techniques: Batch, Layer, Instance, Group",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 283,
            "page_oevrview": "BATCH NORMALIZATION Normalization is a broad category of methods that seek to make different samples seen by a machine-learning model more similar to each other, which helps the model learn and generalize well to new data."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Normalization Techniques: Batch, Layer, Instance, Group",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 284,
            "page_oevrview": "It requires significantly fewer parameters and involves fewer computations, thus resulting in smaller, speedier models. And because it&#39;s a more representationally efficient way to perform convolution, it tends to learn better representations using less data, resulting in better-performing models."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Normalization Techniques: Batch, Layer, Instance, Group",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 124,
            "page_oevrview": "In the examples of classifying digits and predicting house prices, the data already came in vectorized form, so you were able to skip this step. VALUE NORMALIZATION In the digit-classification example, you started from image data encoded as integers in the 0\u2013255 range, encoding grayscale values."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Normalization Techniques: Batch, Layer, Instance, Group",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 335,
            "page_oevrview": "Batch normalization acts to standardize only the mean and variance of each unit in order to stabilize learning, but allows the relationships between units and the nonlinear statistics of a single unit to change."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Neural Networks in Deep Learning",
            "topic": "Normalization Techniques: Batch, Layer, Instance, Group",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 333,
            "page_oevrview": "Batch normalization can be applied to any input or hidden layer in a network. Let H be a minibatch of activations of the layer to normalize, arranged as a design matrix, with the activations for each example appearing in a row of the matrix."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Fundamentals of Convolution Operations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 345,
            "page_oevrview": "Convolution is a specialized kind of linear operation. Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers. In this chapter, we will first describe what convolution is."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Fundamentals of Convolution Operations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 371,
            "page_oevrview": "Convolutional layers are hard-coded to be invariant specifically to translation. Other operations besides convolution are usually necessary to implement a convolutional network. To perform learning, one must be able to compute the gradient with respect to the kernel, given the gradient with respect to the outputs."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Fundamentals of Convolution Operations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 362,
            "page_oevrview": "One key insight is that convolution and pooling can cause underfitting. Like any prior, convolution and pooling are only useful when the assumptions made by the prior are reasonably accurate. If a task relies on preserving precise spatial information, then using pooling on all features can increase the training error."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Fundamentals of Convolution Operations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 146,
            "page_oevrview": "The convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches, producing an output feature map. This output feature map is still a 3D tensor: it has a width and a height."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Fundamentals of Convolution Operations",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 150,
            "page_oevrview": "But the distance between two successive windows is a parameter of the convolution, called its stride, which defaults to 1. It&#39;s possible to have strided convolu tions: convolutions with a stride higher than 1."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "CNN Building Blocks: Pooling, Stride, Padding",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 150,
            "page_oevrview": "It&#39;s conceptually similar to convolution, except that instead of transforming local patches via a learned linear transformation (the con volution kernel), they&#39;re transformed via a hardcoded max tensor operation."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "CNN Building Blocks: Pooling, Stride, Padding",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 148,
            "page_oevrview": "They may differ for two reasons: \uf0a1 Border effects, which can be countered by padding the input feature map \uf0a1 The use of strides, which I&#39;ll define in a second Let&#39;s take a deeper look at these notions. UNDERSTANDING BORDER EFFECTS AND PADDING Consider a 5 \u00d7 5 feature map (25 tiles total)."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "CNN Building Blocks: Pooling, Stride, Padding",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 151,
            "page_oevrview": "In short, the reason to use downsampling is to reduce the number of feature-map coefficients to process, as well as to induce spatial-filter hierarchies by making succes sive convolution layers look at increasingly large windows (in terms of the fraction of the original input they cover)."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "CNN Building Blocks: Pooling, Stride, Padding",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 414,
            "page_oevrview": "How does a convolutional neural network build up this hierarchy? It com bines two specialized types of hidden layers, called convolution layers and pooling layers. Convolution layers search for instances of small patterns in the image, whereas pooling layers downsample these to select a prominent subset."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "CNN Building Blocks: Pooling, Stride, Padding",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 417,
            "page_oevrview": "This reduces the size of the image by a factor of two in each direction, and it also provides some location invariance: ie as long as there is a large value in one of the four pixels in the block, the whole block registers as a large value in the reduced image."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Popular CNN Architectures: AlexNet, ResNet, EfficientNet",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 269,
            "page_oevrview": "Parameter sharing has allowed CNNs to dramatically lower the number of unique model parameters and to significantly increase network sizes without requiring a corresponding increase in training data. It remains one of the best examples of how to effectively incorporate domain knowledge into the network architecture."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Popular CNN Architectures: AlexNet, ResNet, EfficientNet",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 345,
            "page_oevrview": "Chapter 9 Convolutional Networks Convolutional networks (LeCun, 1989), also known as convolutional neural networks or CNNs, are a specialized kind of neural network for processing data that has a known, grid-like topology."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Popular CNN Architectures: AlexNet, ResNet, EfficientNet",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 412,
            "page_oevrview": "The former system is based on scoring proposals generated by another machine translation system, while the latter uses a standalone recurrent network to generate the translations. These authors respectively called this architecture, illustrated in figure 10.12, the encoder-decoder or sequence-to-sequence architecture."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Popular CNN Architectures: AlexNet, ResNet, EfficientNet",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 166,
            "page_oevrview": "If this original dataset is large enough and general enough, then the spatial hierarchy of fea tures learned by the pretrained network can effectively act as a generic model of the visual world, and hence its features can prove useful for many different computer vision problems, even though these new problems may involve completely different classes than those of the original task."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Popular CNN Architectures: AlexNet, ResNet, EfficientNet",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 342,
            "page_oevrview": "Each type of network is meant for a specific input modality: a network architecture (dense, convolutional, recurrent) encodes assumptions about the structure of the data: a hypothesis space within which the search for a good model will proceed."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Implementing CNNs with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 456,
            "page_oevrview": "There are 32 channels in the frst hidden layer, in contrast to the three channels in the input layer. We use a 3 \u00d7 3 convolution fl ter for each channel in all the layers. Each convolution is followed by a max-pooling layer over 2 \u00d7 2 blocks."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Implementing CNNs with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 414,
            "page_oevrview": "It com bines two specialized types of hidden layers, called convolution layers and pooling layers. Convolution layers search for instances of small patterns in the image, whereas pooling layers downsample these to select a prominent subset."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Implementing CNNs with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 455,
            "page_oevrview": "448 10. Deep Learning mnist_model, mnist_dm, mnist_trainer, mnist_module, mnist_results, mlr_model, mlr_module, mlr_trainer) 10.9.3 Convolutional Neural Networks In this section we ft a CNN to the CIFAR100 data, which is available in the torchvision package. It is arranged in a similar fashion as the MNIST data."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Implementing CNNs with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 86,
            "page_oevrview": "The learning process is configured in the compilation step, where you specify the optimizer and loss function(s) that the model should use, as well as the metrics you want to monitor during training. Here&#39;s an example with a single loss function, which is by far the most common case: from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=&#39;mse&#39;, metrics=[&#39;accuracy&#39;]) Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the fit() method, similar to what you would do in Scikit-Learn and several other machine-learning libraries: model.fit(input_tensor, target_tensor, batch_size=128, epochs=10) Licensed to &lt;null&gt;"
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Implementing CNNs with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 85,
            "page_oevrview": "Any piece of code that you write with Keras can be run with any of these backends without having to change anything in the code: you can seamlessly switch between the two during development, which often proves useful\u2014for instance, if one of these backends proves to be faster for a specific task."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Transfer Learning with Pre-trained CNN Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 166,
            "page_oevrview": "Using a pretrained convnet 143 5.3 Using a pretrained convnet A common and highly effective approach to deep learning on small image datasets is to use a pretrained network. A pretrained network is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Transfer Learning with Pre-trained CNN Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 153,
            "page_oevrview": "Together, these three strategies\u2014training a small model from scratch, doing feature extraction using a pretrained model, and fine-tuning a pre trained model\u2014will constitute your future toolbox for tackling the problem of per forming image classification with small datasets."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Transfer Learning with Pre-trained CNN Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 318,
            "page_oevrview": "\uf0a1 Hence, deep learning allows style transfer to be formulated as an optimization process using a loss defined with a pretrained convnet. \uf0a1 Starting from this basic idea, many variants and refinements are possible. Licensed to &lt;null&gt;"
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Transfer Learning with Pre-trained CNN Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 340,
            "page_oevrview": "All the layers of the second network (with the upper layers initialized randomly) are then jointly trained to perform a different set of tasks (another subset of the 1000 ImageNet object categories), with fewer training examples than for the first set of tasks."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Convolutional Neural Networks (CNNs)",
            "topic": "Transfer Learning with Pre-trained CNN Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 399,
            "page_oevrview": "In this way, the network can learn to take into account input conditions (such as those it generates itself in the free-running mode) not seen during training and how to map the state back towards one that will make the network generate proper outputs after a few steps."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Recurrent Neural Networks (RNNs) for Sequence Modeling",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 219,
            "page_oevrview": "A recurrent neural network (RNN) adopts the same principle, albeit in an extremely simplified version: it processes sequences by iterating through the sequence elements and maintaining a state containing information relative to what it has seen so far."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Recurrent Neural Networks (RNNs) for Sequence Modeling",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 244,
            "page_oevrview": "Importantly, an RNN trained on reversed sequences will learn different representations than one trained on the original sequences, much as you would have different mental models if time flowed backward in the real world\u2014if you lived a life where you died on your first day and were born on your last day."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Recurrent Neural Networks (RNNs) for Sequence Modeling",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 238,
            "page_oevrview": "Advanced use of recurrent neural networks 215 solution with a space of complicated models, the simple, well-performing baseline may be unlearnable, even if it&#39;s technically part of the hypothesis space."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Recurrent Neural Networks (RNNs) for Sequence Modeling",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 389,
            "page_oevrview": "For the simplicity of exposition, we refer to RNNs as operating on a sequence that contains vectors x (t) with the time step index t ranging from 1 to \u03c4 . In practice, recurrent networks usually operate on minibatches of such sequences, with a different sequence length \u03c4 for each member of the minibatch."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Recurrent Neural Networks (RNNs) for Sequence Modeling",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 388,
            "page_oevrview": "Much as a convolutional network is a neural network that is specialized for processing a grid of values X such as an image, a recurrent neural network is a neural network that is specialized for processing a sequence of values x (1) , . . . , x (\u03c4) ."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Long Short-Term Memory (LSTMs) and Gated Recurrent Units (GRUs)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 425,
            "page_oevrview": "Instead of a unit that simply applies an element wise nonlinearity to the affine transformation of inputs and recurrent units, LSTM recurrent networks have \u201cLSTM cells\u201d that have an internal recurrence (a self-loop), in addition to the outer recurrence of the RNN. Each cell has the same inputs and outputs as an ordinary recurrent network, but has more parameters and a system of gating units that controls the flow of information."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Long Short-Term Memory (LSTMs) and Gated Recurrent Units (GRUs)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 427,
            "page_oevrview": "The main difference with the LSTM is that a single gating unit simultaneously controls the forgetting factor and the decision to update the state unit."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Long Short-Term Memory (LSTMs) and Gated Recurrent Units (GRUs)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 424,
            "page_oevrview": "The output of the cell can be shut off by the output gate. All the gating units have a sigmoid nonlinearity, while the input unit can have any squashing nonlinearity. The state unit can also be used as an extra input to the gating units. The black square indicates a delay of a single time step."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Long Short-Term Memory (LSTMs) and Gated Recurrent Units (GRUs)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 225,
            "page_oevrview": "The underlying Long Short-Term Memory (LSTM) algorithm was developed by Hochreiter and Schmidhuber in 1997;3 it was the culmi nation of their research on the vanishing gradient problem. This layer is a variant of the SimpleRNN layer you already know about; it adds a way to carry information across many timesteps."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Long Short-Term Memory (LSTMs) and Gated Recurrent Units (GRUs)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 238,
            "page_oevrview": "Instead of the LSTM layer introduced in the previous section, you&#39;ll use the GRU layer, developed by Chung et al. in 2014.5 Gated recurrent unit (GRU) layers work using the same principle as LSTM, but they&#39;re somewhat streamlined and thus cheaper to run (although they may not have as much representational power as LSTM)."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Implementing RNNs, LSTMs, and GRUs with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 242,
            "page_oevrview": "A bidirectional RNN exploits the order sensitiv ity of RNNs: it consists of using two regular RNNs, such as the GRU and LSTM layers you&#39;re already familiar with, each of which processes the input sequence in one direc tion (chronologically and antichronologically), and then merging their representa tions."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Implementing RNNs, LSTMs, and GRUs with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 345,
            "page_oevrview": "Three RNN layers are available in Keras: SimpleRNN, GRU, and LSTM. For most prac tical purposes, you should use either GRU or LSTM. LSTM is the more powerful of the two but is also more expensive; you can think of GRU as a simpler, cheaper alternative to it."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Implementing RNNs, LSTMs, and GRUs with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 225,
            "page_oevrview": "The underlying Long Short-Term Memory (LSTM) algorithm was developed by Hochreiter and Schmidhuber in 1997;3 it was the culmi nation of their research on the vanishing gradient problem. This layer is a variant of the SimpleRNN layer you already know about; it adds a way to carry information across many timesteps."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Implementing RNNs, LSTMs, and GRUs with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 425,
            "page_oevrview": "Instead of a unit that simply applies an element wise nonlinearity to the affine transformation of inputs and recurrent units, LSTM recurrent networks have \u201cLSTM cells\u201d that have an internal recurrence (a self-loop), in addition to the outer recurrence of the RNN. Each cell has the same inputs and outputs as an ordinary recurrent network, but has more parameters and a system of gating units that controls the flow of information."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Implementing RNNs, LSTMs, and GRUs with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 389,
            "page_oevrview": "RNNs may also be applied in two dimensions across spatial data such as images, and even when applied to data involving time, the network may have connections that go backwards in time, provided that the entire sequence is observed before it is provided to the network."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Attention Mechanisms in Sequence Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 435,
            "page_oevrview": "SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS mechanism for choosing an address is in its form identical to the attention mechanism which had been previously introduced in the context of machine translation (Bahdanau et al., 2015) and discussed in section 12.4.5.1. The idea of attention mechanisms for neural networks was introduced even earlier, in the context of handwriting generation (Graves, 2013), with an attention mechanism that was constrained to move only forward in time through the sequence."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Attention Mechanisms in Sequence Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 490,
            "page_oevrview": "The weights \u03b1 (t) are usually produced by applying a softmax function to relevance scores emitted by another portion of the model. The attention mechanism is more expensive computationally than directly indexing the desired h (t) , but direct indexing cannot be trained with gradient descent."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Attention Mechanisms in Sequence Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 433,
            "page_oevrview": "To read, they take a weighted average of many cells. To write, they modify multiple cells by different amounts. The coefficients for these operations are chosen to be focused on a small number of cells, for example, by producing them via a softmax function."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Attention Mechanisms in Sequence Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 424,
            "page_oevrview": "The network processes the input sequence X sequentially; each X feeds into the hidden layer, which also has as input the activation vector A\u22121 from the previous element in the sequence, and produces the current activation vector A."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Attention Mechanisms in Sequence Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 431,
            "page_oevrview": "The RNN processes this sequence from left to right with the same weights W (for the input layer), while the AR model simply treats all L elements of the sequence equally as a vector of L \u00d7 p predictors \u2014 a process called fattening in the neural network literature. fattening Of course the RNN also includes the hidden layer activations A which transfer information along the sequence, and introduces additional nonlin earity."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Transformers and Self-Attention Architecture",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 433,
            "page_oevrview": "(2014b) introduced the neural Turing machine, which is able to learn to read from and write arbitrary content to memory cells without explicit supervision about which actions to undertake, and allowed end-to-end training without this supervision signal, via the use of a content-based soft attention mechanism (see Bahdanau et al."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Transformers and Self-Attention Architecture",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 412,
            "page_oevrview": "They proposed to make C a variable-length sequence rather than a fixed-size vector. Additionally, they introduced an attention mechanism that learns to associate elements of the sequence C to elements of the output 397."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Transformers and Self-Attention Architecture",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 382,
            "page_oevrview": "It is able to understand entire scenes including many objects and relationships between objects, and processes rich 3-D geometric information needed for our bodies to interface with the world."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Transformers and Self-Attention Architecture",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf",
            "page_no": 424,
            "page_oevrview": "The network processes the input sequence X sequentially; each X feeds into the hidden layer, which also has as input the activation vector A\u22121 from the previous element in the sequence, and produces the current activation vector A."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Transformers and Self-Attention Architecture",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 219,
            "page_oevrview": "Each input shown to them is processed independently, with no state kept in between inputs. With such net works, in order to process a sequence or a temporal series of data points, you have to show the entire sequence to the network at once: turn it into a single data point."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Implementing Transformers with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 70,
            "page_oevrview": "Built-in model flavors include: PyTorch and TensorFlow Hugging Face Transformers (relatedly, see Hugging Face Transformers&#39; MLflowCallback) LangChain OpenAI API (See the documentation for a complete list) For other LLM pipelines, MLflow can package the pipelines via the MLflow pyfunc flavor, which can store arbitrary Python code."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Implementing Transformers with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 65,
            "page_oevrview": "MosaicML&#39;s composer is an example of a library that uses PyTorch FSDP with additional optimizations to maximize Model FLOPs Utilization (MFU) and Hardware FLOPs Utilization (HFU) during training. Handling GPU failures: Training large models can run for days or even weeks."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Implementing Transformers with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 76,
            "page_oevrview": "In the RAG with fine-tuned model example we illustrate using an existing base model from the model hub that is then fine-tuned in production. Vector Database Some (but not all) LLM applications use vector databases for fast similarity searches, most often to provide context or domain knowledge in LLM queries."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Implementing Transformers with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 461,
            "page_oevrview": "Each thread in a warp executes the same instruction during each cycle, so if different threads within the same warp need to execute different code paths, these different code paths must be traversed sequentially rather than in parallel."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Implementing Transformers with TensorFlow/PyTorch",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 341,
            "page_oevrview": "The gradient flows through many layers provided that the Jacobian of the linear transformation has reasonable singular values. Moreover, linear functions consistently increase in a single direction, so even if the model&#39;s output is very far from correct, it is clear simply from computing the gradient which direction its output should move to reduce the loss function."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Pre-trained Transformer Models: BERT, GPT, T5",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 64,
            "page_oevrview": "Pre-training Pre-training a model from scratch refers to the process of training a language model on a large corpus of data (eg text, code) without using any prior knowledge or weights from an existing model."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Pre-trained Transformer Models: BERT, GPT, T5",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 62,
            "page_oevrview": "Continued pre-training: This fine-tuning method does not rely on input and output examples but instead uses domain-specific unstructured text to continue the same pre-training process (eg next token prediction, masked language modeling)."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Pre-trained Transformer Models: BERT, GPT, T5",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 4,
            "page_oevrview": "78 CHAPTER 6 CHAPTER 7 Contents BIG BOOK OF MLOPS - 2ND EDITION 4."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Pre-trained Transformer Models: BERT, GPT, T5",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 128,
            "page_oevrview": "The training data was generated synthetically, by randomly sampling x values and choosing y deterministically by evaluating a quadratic function. (Left)A linear function fit to the data suffers from underfitting\u2014it cannot capture the curvature that is present in the data."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Recurrent Neural Networks (RNNs) and Transformers",
            "topic": "Pre-trained Transformer Models: BERT, GPT, T5",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 30,
            "page_oevrview": "This linear model could recognize two different categories of inputs by testing whether f (x, w) is positive or negative. Of course, for the model to correspond to the desired definition of the categories, the weights needed to be set correctly."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Generative Adversarial Networks (GANs)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 328,
            "page_oevrview": "As such, a GAN is made of two parts: \uf0a1 Generator network\u2014Takes as input a random vector (a random point in the latent space), and decodes it into a synthetic image \uf0a1 Discriminator network (or adversary)\u2014Takes as input an image (real or synthetic), and predicts whether the image came from the training set or was created by the generator network."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Generative Adversarial Networks (GANs)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 330,
            "page_oevrview": "The specific implementation is a deep convolutional GAN (DCGAN): a GAN where the generator and discriminator are deep convnets. In particular, it uses a Conv2DTranspose layer for image upsampling in the generator."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Generative Adversarial Networks (GANs)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 333,
            "page_oevrview": "This model turns latent-space points into a classification decision\u2014\u201cfake\u201d or \u201creal\u201d\u2014and it&#39;s meant to be trained with labels that are always \u201cthese are real images.\u201d So, training gan will update the weights of generator in a way that makes discriminator more likely to predict \u201creal\u201d when looking at fake images."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Generative Adversarial Networks (GANs)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 284,
            "page_oevrview": "This can be seen as a way of explicitly introducing a local constancy prior into supervised neural nets. Adversarial training helps to illustrate the power of using a large function family in combination with aggressive regularization."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Generative Adversarial Networks (GANs)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 242,
            "page_oevrview": "Today, gradient-based learning in feedforward networks is used as a tool to develop probabilistic models, such as the variational autoencoder and generative adversarial networks, described in chapter 20."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Variational Autoencoders (VAEs)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 322,
            "page_oevrview": "They have turned out to be a powerful tool for image generation. A VAE, instead of compressing its input image into a fixed code in the latent space, turns the image into the parameters of a statistical distribution: a mean and a vari ance."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Variational Autoencoders (VAEs)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 319,
            "page_oevrview": "GANs and VAEs are two different strategies for learning such latent spaces of image representations, each with its own characteristics. VAEs are great for learning latent spaces that are well structured, where specific directions encode a meaningful axis of variation in the data."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Variational Autoencoders (VAEs)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 321,
            "page_oevrview": "They&#39;re a modern take on autoencoders\u2014a type of network that aims to encode an input to a low-dimensional latent space and then decode it back\u2014that mixes ideas from deep learning with Bayesian inference."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Variational Autoencoders (VAEs)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 19,
            "page_oevrview": "Autoencoders are trained to preserve as much information as possible when an input is run through the encoder and then the decoder, but are also trained to make the new representation have various nice properties."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Variational Autoencoders (VAEs)",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 286,
            "page_oevrview": "They are not able to shrink their derivatives by saturating at a high value with large weights, as sigmoid or tanh units can. Dataset augmentation works well with rectified linear units because different subsets of rectified units can activate for different transformed versions of each original input."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Diffusion Models for Image Generation",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 330,
            "page_oevrview": "This means, at every step, you move the weights of the generator in a direction that makes the discriminator more likely to classify as \u201creal\u201d the images decoded by the generator. In other words, you train the generator to fool the discriminator."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Diffusion Models for Image Generation",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 319,
            "page_oevrview": "8.4.1 Sampling from latent spaces of images The key idea of image generation is to develop a low-dimensional latent space of repre sentations (which naturally is a vector space) where any point can be mapped to a realistic-looking image."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Diffusion Models for Image Generation",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 327,
            "page_oevrview": "By sampling and decod ing points from the latent space, you can generate never-before-seen images. There are two major tools to do this: VAEs and GANs. \uf0a1 VAEs result in highly structured, continuous latent representations."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Diffusion Models for Image Generation",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 374,
            "page_oevrview": "The tensor of convolution kernels U is used on each step to compute the hidden representation given the input image. The kernel tensor V is used to produce an estimate of the labels given the hidden values. On all but the first step, the kernels W are convolved over Y\u02c6 to provide input to the hidden layer."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Diffusion Models for Image Generation",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 375,
            "page_oevrview": "The general idea is to assume that large groups of contiguous pixels tend to be associated with the same label. Graphical models can describe the probabilistic relationships between neighboring pixels."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Text-to-Image Models: DALL-E, Stable Diffusion",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 211,
            "page_oevrview": "This embedding technique is based on factorizing a matrix of word co-occurrence statis tics. Its developers have made available precomputed embeddings for millions of English tokens, obtained from Wikipedia data and Common Crawl data."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Text-to-Image Models: DALL-E, Stable Diffusion",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 295,
            "page_oevrview": "In the example we present in this section, you&#39;ll take a LSTM layer, feed it strings of N characters extracted from a text corpus, and train it to predict character N + 1. The output of the model will be a softmax over all possible characters: a probability distribution for the next character."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Text-to-Image Models: DALL-E, Stable Diffusion",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 346,
            "page_oevrview": "Key concepts in review 323 \uf0a1 Mapping vector data to vector data \u2013 Predictive healthcare\u2014Mapping patient medical records to predictions of patient outcomes \u2013 Behavioral targeting\u2014Mapping a set of website attributes with data on how long a user will spend on the website \u2013 Product quality control\u2014Mapping a set of attributes relative to an instance of a manufactured product with the probability that the product will fail by next year \uf0a1 Mapping image data to vector data \u2013 Doctor assistant\u2014Mapping slides of medical images with a prediction about the presence of a tumor \u2013 Self-driving vehicle\u2014Mapping car dash-cam video frames to steering wheel angle commands \u2013 Board game AI\u2014Mapping Go and chess boards to the next player move \u2013 Diet helper\u2014Mapping pictures of a dish to its calorie count \u2013 Age prediction\u2014Mapping selfies to the age of the person \uf0a1 Mapping timeseries data to vector data \u2013 Weather prediction\u2014Mapping timeseries of weather data in a grid of locations of weather data the following week at a specific location \u2013 Brain-computer interfaces\u2014Mapping timeseries of magnetoencephalogram (MEG) data to computer commands \u2013 Behavioral targeting\u2014Mapping timeseries of user interactions on a website to the probability that a user will buy something \uf0a1 Mapping text to text \u2013 Smart reply\u2014Mapping emails to possible one-line replies \u2013 Answering questions\u2014Mapping general-knowledge questions to answers \u2013 Summarization\u2014Mapping a long article to a short summary of the article \uf0a1 Mapping images to text \u2013 Captioning\u2014Mapping images to short captions describing the contents of the images \uf0a1 Mapping text to images \u2013 Conditioned image generation\u2014Mapping a short text description to images matching the description \u2013 Logo generation/selection\u2014Mapping the name and description of a company to the company&#39;s logo \uf0a1 Mapping images to images \u2013 Super-resolution\u2014Mapping downsized images to higher-resolution versions of the same images \u2013 Visual depth sensing\u2014Mapping images of indoor environments to maps of depth predictions Licensed to &lt;null&gt;"
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Text-to-Image Models: DALL-E, Stable Diffusion",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 479,
            "page_oevrview": "The word representations embed those points in a feature space of lower dimension. In the original space, every word is represented by a one-hot vector, so every pair of words is at Euclidean distance \u221a 2 from each other."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Text-to-Image Models: DALL-E, Stable Diffusion",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 489,
            "page_oevrview": "This general idea of an encoder-decoder framework for machine translation is illustrated in figure 12.5."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Evaluating Generative Models: Inception Score, FID",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 65,
            "page_oevrview": "With this in mind, it&#39;s important to evaluate a model in context and have some ability to compare it to what existed before the model\u2014whether a previous model or a rules based process\u2014to get an idea of what the outcome would be if the current model or decision process were replaced by the new one."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Evaluating Generative Models: Inception Score, FID",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 69,
            "page_oevrview": "In this example, it would mean that the hiring rate of Togruta should be equal to or larger than 80% of the hiring rate of Weequay. In this example, it means that it would be OK to hire up to 65% Weequay. The point here is that defining these objectives cannot be a decision made by data scientists alone."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Evaluating Generative Models: Inception Score, FID",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 67,
            "page_oevrview": "If one feature value is changed, the model prediction is likely to be wrong if the new feature value has never been seen in the training dataset or if it has never been seen in combination with the values of the other features in this dataset."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Evaluating Generative Models: Inception Score, FID",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 304,
            "page_oevrview": "Builds the Inception V3 network, without its convolutional base. The model will be loaded with pretrained ImageNet weights. Dictionary mapping layer names to a coefficient quantifying how much the layer&#39;s activation contributes to the loss you&#39;ll seek to maximize."
        },
        {
            "domain": "Deep Learning",
            "sub_domain": "Generative Models in Deep Learning",
            "topic": "Evaluating Generative Models: Inception Score, FID",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 265,
            "page_oevrview": "We consider the supervised learning setting, where we obtain pairs (x1, y1), . . . ,(xN , yN ). Given this data, we would like to estimate a predictor f(\u00b7, \u03b8) : RD \u2192 R, parametrized by \u03b8. We hope to be able to find a good parameter \u03b8 \u2217 such that we fit the data well, that is, f(xn, \u03b8 \u2217 ) \u2248 yn for all n = 1, . . . , N ."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Model Deployment using TensorFlow Serving",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 41,
            "page_oevrview": "This involves triggering the model deployment pipeline, which creates a Model Serving endpoint in the staging environment, loads a test model (eg, a model trained on a limited subset of data). To test the endpoint, some of the testing approaches mentioned in the Pre-deployment testing section could be employed."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Model Deployment using TensorFlow Serving",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 14,
            "page_oevrview": "High availability and scalability Built for production use, Model Serving supports very low latency (p50 overhead latency of less than 10ms) and high query volumes (QPS of greater than 25k), and can automatically scale up and down based on demand."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Model Deployment using TensorFlow Serving",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 13,
            "page_oevrview": "Given the complexity that is often involved with deploying a real-time ML model, Model Serving reduces operational costs, streamlines the ML lifecycle, and makes it easier for data science teams to focus on the core task of integrating production-grade real-time ML into their solutions."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Model Deployment using TensorFlow Serving",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 42,
            "page_oevrview": "There are commonly two types of model deployment: Model-as-a-service, or live-scoring model Typically the model is deployed into a simple framework to provide a REST API endpoint (the means from which the API can access the resources it needs to perform the task) that responds to requests in real time."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Model Deployment using TensorFlow Serving",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 74,
            "page_oevrview": "Ideally, models running in the development environment would be validated and sent as is to production; this minimizes the amount of adaptation work and improves the chances that the model in production will behave as it did in development."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Flask and FastAPI for Model Serving",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 14,
            "page_oevrview": "Model Serving facilitates the implementation of such online evaluation approaches through the ability to serve multiple models to a Model Serving endpoint. The design decisions taken around online evaluation will depend on use case requirements."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Flask and FastAPI for Model Serving",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 13,
            "page_oevrview": "Model Serving Databricks Model Serving provides a production-ready, serverless solution to simplify real-time ML model deployment. With Model Serving, it is possible to efficiently deploy models as an API so that you can integrate model predictions with applications or websites."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Flask and FastAPI for Model Serving",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 48,
            "page_oevrview": "If the model version was trained using features from Unity Catalog, the model stores the dependencies to features and functions used. Model Serving will automatically use this dependency graph to look up features from appropriate online stores at inference time."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Flask and FastAPI for Model Serving",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 94,
            "page_oevrview": "It allows an application to be packaged, sent to a server (the Docker host), and run with all its dependencies in isolation from other applications. Building the basis of a model-serving environment that can accommodate many models, each of which may run multiple copies, may require multiple Docker hosts."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Flask and FastAPI for Model Serving",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 42,
            "page_oevrview": "They also enable new models to be seamlessly deployed using the blue-green deploy\u2010 ment technique.1 Compute resources for models can be scaled elastically using multi\u2010 ple containers, too. Orchestrating many containers is the role of technologies such as Kubernetes and can be used both in the cloud and on-premise."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Containerization with Docker for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 42,
            "page_oevrview": "Container technologies such as Docker are light\u2010 weight alternatives to virtual machines, allowing applications to be deployed in independent, self-contained environments, matching the exact requirements of each model."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Containerization with Docker for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 94,
            "page_oevrview": "Unlike virtual machines (VMs), containers do not duplicate the complete operating system; multiple containers share a common operating system and are therefore far more resource efficient. The most well-known containerization technology is the open source platform Docker."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Containerization with Docker for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 95,
            "page_oevrview": "and maintain Kubernetes itself. If an application or a model is packaged as a Docker container, users can directly submit it, and the service will provision the required machines to run one or several instances of the container inside Kubernetes."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Containerization with Docker for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 13,
            "page_oevrview": "Given its close integration with MLflow, Model Serving can automatically build a container from a logged MLflow model and deploy the model as a REST endpoint. This abstracts away what would ordinarily be a notably more complex process for the user."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Containerization with Docker for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 70,
            "page_oevrview": "This pipeline may use a pretrained model or a custom fine-tuned model. An engineered prompt, possibly stored as a template in a tool such as LangChain. Though LLMs add new terminology and tools for composing ML logic, all of the above still constitute models and pipelines."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Kubernetes for Scaling ML Services",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 95,
            "page_oevrview": "For example, a Kubernetes cluster in the cloud can have an auto-scaling capability that automatically adds machines when the cluster usage metrics are high and removes them when they are low."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Kubernetes for Scaling ML Services",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 94,
            "page_oevrview": "Kubernetes, an open source platform that has gained a lot of traction in the past few years and is becoming the standard for container orchestration, greatly simplifies these issues and many others. It provides a powerful declarative API to run applica\u2010 tions in a group of Docker hosts, called a Kubernetes cluster."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Kubernetes for Scaling ML Services",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 96,
            "page_oevrview": "Kubernetes orchestrates containers, but Kubernetes is not aware of what the containers are actually doing; as far as Kubernetes is concerned, they are just containers that run an application on one specific host."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Kubernetes for Scaling ML Services",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 14,
            "page_oevrview": "High availability and scalability Built for production use, Model Serving supports very low latency (p50 overhead latency of less than 10ms) and high query volumes (QPS of greater than 25k), and can automatically scale up and down based on demand. This ensures optimal performance and cost-efficiency."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Kubernetes for Scaling ML Services",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 5,
            "page_oevrview": "The previously complex infrastructure required to serve real-time models can now be replaced and easily scaled with Databricks Model Serving. Long-term efficiency and performance stability of ML in production can be achieved using Databricks Lakehouse Monitoring."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Cloud Deployment for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 42,
            "page_oevrview": "There are commonly two types of model deployment: Model-as-a-service, or live-scoring model Typically the model is deployed into a simple framework to provide a REST API endpoint (the means from which the API can access the resources it needs to perform the task) that responds to requests in real time."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Cloud Deployment for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 95,
            "page_oevrview": "Scaling Deployments As ML adoption grows, organizations face two types of growth challenges: \u2022 The ability to use a model in production with high-scale data \u2022 The ability to train larger and larger numbers of models Handling more data for real-time scoring is made much easier by frameworks such as Kubernetes."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Cloud Deployment for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 43,
            "page_oevrview": "The desire in MLOps, just as in DevOps, is to automate the CI/CD pipeline as far as possi\u2010 ble. This not only speeds up the deployment process, but it enables more extensive regression testing and reduces the likelihood of errors in the deployment."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Cloud Deployment for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 8,
            "page_oevrview": "ML deployment patterns Code and models often progress asynchronously through these stages. Thus, it becomes crucial to leverage a solution that allows for the management of model artifacts independently of code, making it possible to update a production model without necessarily making a code change."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Cloud Deployment for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 70,
            "page_oevrview": "Packaging models or pipelines for deployment In traditional ML, there are generally two types of ML logic to package for deployment: models and pipelines. These artifacts are generally managed toward production via a model registry and Git version control, respectively."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Edge Deployment for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 8,
            "page_oevrview": "ML deployment patterns Code and models often progress asynchronously through these stages. Thus, it becomes crucial to leverage a solution that allows for the management of model artifacts independently of code, making it possible to update a production model without necessarily making a code change."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Edge Deployment for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 41,
            "page_oevrview": "This involves triggering the model deployment pipeline, which creates a Model Serving endpoint in the staging environment, loads a test model (eg, a model trained on a limited subset of data). To test the endpoint, some of the testing approaches mentioned in the Pre-deployment testing section could be employed."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Edge Deployment for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 14,
            "page_oevrview": "Simplified deployment Data scientists or ML engineers can easily create a Model Serving endpoint from a model version without requiring extensive infrastructure knowledge or experience. Thus, creating or updating a Model Serving endpoint becomes a trivial additional step in the model deployment pipeline."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Edge Deployment for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 42,
            "page_oevrview": "There are commonly two types of model deployment: Model-as-a-service, or live-scoring model Typically the model is deployed into a simple framework to provide a REST API endpoint (the means from which the API can access the resources it needs to perform the task) that responds to requests in real time."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Deployment and Serving",
            "topic": "Edge Deployment for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 175,
            "page_oevrview": "example causes of, 93 data engineers, 19 data exploration, 46 data governance, 36 questions for ML model data sources, 25 data pipeline structure for recommendation engine project, 141 data privacy, 35, 108 GDPR and CCPA regulations on, 35, 110 data scientists, 17-19 collaboration with SMEs in ML model life cycle, 16 concerns in ML model monitoring, 30 ground truth, 30 input drift, 31 role in and needs from MLOps, 18 role in machine learning model life cycle, 17 data sources for machine learning models, 24 DataOps, 7 decision modeling (business), 16 decision-making processes, statistically driven, 105 deep learning, 45, 48, 54 degradation of model performance common approaches to discovering, 30 understanding, 89-92 delivery, 77 (see also CI/CD pipelines) continuous delivery versus deployment, 77 dependencies partial dependency plots, 27 on production environment, reducing, 28 deployment strategies, 77-79 categories of model deployment, 77 concepts and terminology, 77 considerations in sending models to pro\u2010 duction, 78 maintenance of models in production, 79 deployments broader model deployment, greater risks from, 69 consumption forecast models, 155 deploying to production, 73-84 building ML artifacts, 75-76 CI/CD pipelines, 73 consumer credit risk management model, 132 containerization, 79-81 scaling deployments, 81-83 strategies for, 77-79 deployment, defined, 77 marketing recommendation engine model, 138-141 model deployment types and contents, 28 model deploymnt requirements, 29 development environments, adaptation to pro\u2010 duction environments, 60-62 DevOps, 20 concerns in ML model monitoring, 30 MLOps and, 6 monitoring of ML models, 86 role in and needs from MLOps, 21 role in machine learning model life cycle, 20 DI (Data Integrity), 109 dimensionality, curse of, 71 dimensioning constraints on model develop\u2010 ment, 54 disaggregations of data, 153, 156 distances between probability distributions, 132 distillation (model), 61 distributed computation, 82 distribution of data, 92 divergence between training and testing phases, 92 Docker, 80 deployment of models through, 132 using Kubernetes with, 80 documentation of model development, 26 domain knowledge, importance in data explo\u2010 ration for models, 46 domains domain classsifier, 94 model retraining frequency and, 86 drift, 91 (see also input drift) detection in practice, 92-95 example causes of data drift, 93 input drift detection techniques, 93 measuring for consumer credit risk assess\u2010 ment model, 132 monitoring, 156 monitoring and mitigation measures, 103 E EDA (exploratory data analysis), 24, 25, 46 effienciency, data scientists&#39; need for from MLOps, 19 elastic systems, 81 Index | 161."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Building ML Pipelines with Airflow",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 42,
            "page_oevrview": "These pipelines trigger model training, validate and deploy new model versions, publish predictions to downstream tables or applications, and monitor the entire process to avoid performance degradation and instability."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Building ML Pipelines with Airflow",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 70,
            "page_oevrview": "Packaging models or pipelines for deployment In traditional ML, there are generally two types of ML logic to package for deployment: models and pipelines. These artifacts are generally managed toward production via a model registry and Git version control, respectively."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Building ML Pipelines with Airflow",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 33,
            "page_oevrview": "4 Release code: The release branch is cut from the main branch, and doing so deploys the project ML pipelines to the production environment. 5 Model training and validation: The model training pipeline ingests data from the prod catalog. Upon validating, the resulting model artifact is registered to the prod catalog."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Building ML Pipelines with Airflow",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 33,
            "page_oevrview": "Given data engineers&#39; central role in the ML model life cycle, underpinning both the building and monitoring portions, MLOps can bring significant efficiency gains."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Building ML Pipelines with Airflow",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 175,
            "page_oevrview": "example causes of, 93 data engineers, 19 data exploration, 46 data governance, 36 questions for ML model data sources, 25 data pipeline structure for recommendation engine project, 141 data privacy, 35, 108 GDPR and CCPA regulations on, 35, 110 data scientists, 17-19 collaboration with SMEs in ML model life cycle, 16 concerns in ML model monitoring, 30 ground truth, 30 input drift, 31 role in and needs from MLOps, 18 role in machine learning model life cycle, 17 data sources for machine learning models, 24 DataOps, 7 decision modeling (business), 16 decision-making processes, statistically driven, 105 deep learning, 45, 48, 54 degradation of model performance common approaches to discovering, 30 understanding, 89-92 delivery, 77 (see also CI/CD pipelines) continuous delivery versus deployment, 77 dependencies partial dependency plots, 27 on production environment, reducing, 28 deployment strategies, 77-79 categories of model deployment, 77 concepts and terminology, 77 considerations in sending models to pro\u2010 duction, 78 maintenance of models in production, 79 deployments broader model deployment, greater risks from, 69 consumption forecast models, 155 deploying to production, 73-84 building ML artifacts, 75-76 CI/CD pipelines, 73 consumer credit risk management model, 132 containerization, 79-81 scaling deployments, 81-83 strategies for, 77-79 deployment, defined, 77 marketing recommendation engine model, 138-141 model deployment types and contents, 28 model deploymnt requirements, 29 development environments, adaptation to pro\u2010 duction environments, 60-62 DevOps, 20 concerns in ML model monitoring, 30 MLOps and, 6 monitoring of ML models, 86 role in and needs from MLOps, 21 role in machine learning model life cycle, 20 DI (Data Integrity), 109 dimensionality, curse of, 71 dimensioning constraints on model develop\u2010 ment, 54 disaggregations of data, 153, 156 distances between probability distributions, 132 distillation (model), 61 distributed computation, 82 distribution of data, 92 divergence between training and testing phases, 92 Docker, 80 deployment of models through, 132 using Kubernetes with, 80 documentation of model development, 26 domain knowledge, importance in data explo\u2010 ration for models, 46 domains domain classsifier, 94 model retraining frequency and, 86 drift, 91 (see also input drift) detection in practice, 92-95 example causes of data drift, 93 input drift detection techniques, 93 measuring for consumer credit risk assess\u2010 ment model, 132 monitoring, 156 monitoring and mitigation measures, 103 E EDA (exploratory data analysis), 24, 25, 46 effienciency, data scientists&#39; need for from MLOps, 19 elastic systems, 81 Index | 161."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Kubeflow for Kubernetes-based ML Workflows",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 95,
            "page_oevrview": "For example, a Kubernetes cluster in the cloud can have an auto-scaling capability that automatically adds machines when the cluster usage metrics are high and removes them when they are low."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Kubeflow for Kubernetes-based ML Workflows",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 94,
            "page_oevrview": "Released in 2014, it has become the de facto standard. It allows an application to be packaged, sent to a server (the Docker host), and run with all its dependencies in isolation from other applications."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Kubeflow for Kubernetes-based ML Workflows",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 120,
            "page_oevrview": "The parallels with ML are clear: ML offers ben\u2010 efits to all and yet brings risks that need to be managed if the public is to trust it. Without public trust, the benefits will not fully materialize."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Kubeflow for Kubernetes-based ML Workflows",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 44,
            "page_oevrview": "The first task in this workflow, the model training task, loads tables and/or feature tables from the prod catalog and performs the following steps: TRAINING AND TUNING During the training process, logs are recorded to the production environment MLflow Tracking server."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Kubeflow for Kubernetes-based ML Workflows",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 50,
            "page_oevrview": "This can be achieved through a SQL alert to check whether a metric is anomalous (eg, check drift or model quality against a threshold). The alert can be configured to use a webhook destination, which can subsequently trigger the training workflow."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Data Validation and Model Monitoring",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 120,
            "page_oevrview": "You train on the training data and evaluate your model on the validation data. Once your model is ready for prime time, you test it one final time on the test data. You may ask, why not have two sets: a training set and a test set? You&#39;d train on the training data and evaluate on the test data. Much simpler!"
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Data Validation and Model Monitoring",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 122,
            "page_oevrview": "K-FOLD VALIDATION With this approach, you split your data into K partitions of equal size. For each parti tion i, train a model on the remaining K \u2013 1 partitions, and evaluate it on partition i. Your final score is then the averages of the K scores obtained."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Data Validation and Model Monitoring",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 139,
            "page_oevrview": "K-fold valida tion? Which portion of the data should you use for validation? \uf0a1 Develop a first model that does better than a basic baseline: a model with statistical power. \uf0a1 Develop a model that overfits. \uf0a1 Regularize your model and tune its hyperparameters, based on perfor mance on the validation data."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Data Validation and Model Monitoring",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 45,
            "page_oevrview": "Model validation As outlined in the \u201cDevelopment\u201d section above, the model validation pipeline uses the model URI from the preceding model training pipeline, and loads the model from Unity Catalog. The model artifact then undergoes a series of validation checks, adjusted to fit the specific context of the use case."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Data Validation and Model Monitoring",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 49,
            "page_oevrview": "TRIGGER MODEL TRAINING When the model monitoring metrics indicate performance issues, or when a model inevitably becomes out of date, the data scientist may need to return to the development environment and develop a new model version. SQL alerts can be used to notify data scientists when this happens."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Feature Stores for ML",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 63,
            "page_oevrview": "Feature Stores Feature factories, or feature stores, are repositories of different features associated with business entities that are created and stored in a central location for easier reuse."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Feature Stores for ML",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 112,
            "page_oevrview": "Model evaluation store As a reminder, model evaluation stores are structures that centralize the data related to model life cycles to allow comparisons. The two main tasks of a model evaluation store are: 98 | Chapter 7: Monitoring and Feedback Loop."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Feature Stores for ML",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 62,
            "page_oevrview": "Adding more features may produce a more accurate model, achieve more fairness when splitting into more precise groups, or compensate for some other useful missing information."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Feature Stores for ML",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 10,
            "page_oevrview": "In addition to managing ML models, feature tables are also a part of Unity Catalog. With Feature Engineering in Unity Catalog, any Delta table in Unity Catalog that has been assigned a primary key (and additionally a timestamp key) can be used as a source of features to train and serve models."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Pipelines and Orchestration",
            "topic": "Feature Stores for ML",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 44,
            "page_oevrview": "These include model metrics, parameters, tags and the model itself. If using feature tables, the model will be logged to MLflow using the Databricks Feature Store client. This will result in the logged model being packaged with feature lookup information, which can be used at inference time."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "ML Experiment Tracking with MLflow",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 36,
            "page_oevrview": "EVALUATION Model quality is evaluated by testing on held-out data. The results of these tests are logged to the MLflow Tracking server. At this point it can be determined if a newly developed model outperforms that of the current model in production."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "ML Experiment Tracking with MLflow",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 37,
            "page_oevrview": "MODEL TRAINING OUTPUT The model training pipeline produces an ML model artifact stored in the MLflow Tracking server. At this point the model artifact is tracked to the development environment MLflow Tracking server."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "ML Experiment Tracking with MLflow",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 45,
            "page_oevrview": "EVALUATION Model quality is evaluated by testing on held-out production data. The results of these tests are logged to the MLflow Tracking server. During development, data scientists select meaningful evaluation metrics for the use case, and those metrics or their custom logic are used in this step."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "ML Experiment Tracking with MLflow",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 40,
            "page_oevrview": "The challenge here is reproducibility, which is an important concept in experimental science in general. The aim in ML is to save enough information about the environment the model was developed in so that the model can be reproduced with the same results from scratch."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "ML Experiment Tracking with MLflow",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 99,
            "page_oevrview": "Monitoring and feedback loop highlighted in the larger context of the ML project life cycle Machine learning models need to be monitored at two levels: \u2022 At the resource level, including ensuring the model is running correctly in the production environment. Key questions include: Is the system alive?"
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "Model Registry and Versioning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 20,
            "page_oevrview": "Model version A version of a registered model. When a new model is added to the Model Registry, it is added as Version 1. Each model registered to the same model name increments the version number. A specific model version can be loaded using the model URI models:/&lt;catalog&gt;.&lt;schema&gt;.&lt;model&gt;/&lt;model_version&gt;."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "Model Registry and Versioning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 45,
            "page_oevrview": "REGISTER MODEL Upon completion of model training, the model artifact is registered to the prod catalog. The model appears as a newly registered model version under the model path in Unity Catalog."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "Model Registry and Versioning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 70,
            "page_oevrview": "Packaging prompts and pipelines as MLflow models simplifies versioning. Just as a newly retrained model can be tracked as a new model version in the MLflow model Registry, a newly updated prompt can be tracked as a new model version."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "Model Registry and Versioning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 112,
            "page_oevrview": "A logical model is a collection of model templates and their versions that aims to solve a business problem. A model version is obtained by training a model template on a given dataset."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "Model Registry and Versioning",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 113,
            "page_oevrview": "Each logged version of the logical model must come with all the essential information concerning its training phase, including: \u2014 The list of features used \u2014 The preprocessing techniques that are applied to each feature \u2014 The algorithm used, along with the chosen hyperparameters \u2014 The training dataset \u2014 The test dataset used to evaluate the trained model (this is necessary for the version comparison phase) \u2014 Evaluation metrics \u2022 Comparing the performance between different versions of a logical model."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "Model Metadata Management",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 110,
            "page_oevrview": "Model metadata Identification of the model and the version. Model inputs Feature values of new observations, which allow for verification of whether the new incoming data is what the model was expecting and thus allowing for detec\u2010 tion of data drift (as explained in the previous section)."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "Model Metadata Management",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 41,
            "page_oevrview": "They offer a way to mitigate uncertainty and help prevent unintended consequences. The techniques most commonly used today include: \u2022 Partial dependence plots, which look at the marginal impact of features on the predicted outcome \u2022 Subpopulation analyses, which look at how the model treats specific subpopula\u2010 tions and that are the basis of many fairness analyses \u2022 Individual model predictions, such as Shapley values, which explain how the value of each feature contributes to a specific prediction \u2022 What-if analysis, which helps the ML model user to understand the sensitivity of the prediction to its inputs As we&#39;ve seen in this section, even though model development happens very early on, it&#39;s still an important place to incorporate MLOps practices."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "Model Metadata Management",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 30,
            "page_oevrview": "This allows the models to be integrated with the business rules, as well as helps the SMEs to fully understand decision contexts and the potential impact of model changes."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "Model Metadata Management",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 11,
            "page_oevrview": "From an architecture design standpoint, this means the flexibility to govern both data and AI assets under the same namespace, enabling management at the environment, project or team level."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "Model Metadata Management",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 45,
            "page_oevrview": "If the model successfully passes all validation checks, the \u201cChallenger\u201d alias is assigned to the model version in Unity Catalog. In the event that the newly trained model does not pass all validation checks, the process will exit and users can be notified on failure of the task to investigate further."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "Model Lineage Tracking",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 12,
            "page_oevrview": "Lineage With Unity Catalog, a robust link between data and AI assets can natively be recorded. Lineage can be traced from a model version in Unity Catalog back to the data used for training. Additionally, downstream lineage records consumers of assets in Unity Catalog."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "Model Lineage Tracking",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 45,
            "page_oevrview": "If the model successfully passes all validation checks, the \u201cChallenger\u201d alias is assigned to the model version in Unity Catalog. In the event that the newly trained model does not pass all validation checks, the process will exit and users can be notified on failure of the task to investigate further."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "Model Lineage Tracking",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 37,
            "page_oevrview": "When executed in the development environment, this model is registered to the dev catalog. Pipeline code is typically parameterized in such a manner that the model will be registered to the catalog corresponding to the environment the model training pipeline is executed in."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "Model Lineage Tracking",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 111,
            "page_oevrview": "It is a structure that allows data scien\u2010 tists to: \u2022 Compare multiple, newly trained model versions against existing deployed versions \u2022 Compare completely new models against versions of other models on labeled data \u2022 Track model performance over time Formally, the model evaluation store serves as a structure that centralizes the data related to model life cycle to allow comparisons (though note that comparing models makes sense only if they address the same problem)."
        },
        {
            "domain": "MLOps",
            "sub_domain": "Model Management and Versioning",
            "topic": "Model Lineage Tracking",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 110,
            "page_oevrview": "Model metadata Identification of the model and the version. Model inputs Feature values of new observations, which allow for verification of whether the new incoming data is what the model was expecting and thus allowing for detec\u2010 tion of data drift (as explained in the previous section)."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "CI/CD Pipelines for ML Projects",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 88,
            "page_oevrview": "CI/CD concepts apply to traditional software engineering, but they apply just as well to machine learning systems and are a critical part of MLOps strategy. After success\u2010 fully developing a model, a data scientist should push the code, metadata, and docu\u2010 mentation to a central repository and trigger a CI/CD pipeline."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "CI/CD Pipelines for ML Projects",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 87,
            "page_oevrview": "Figure 6-1. Deployment to production highlighted in the larger context of the ML project life cycle CI/CD Pipelines CI/CD is a common acronym for continuous integration and continuous delivery (or put more simply, deployment)."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "CI/CD Pipelines for ML Projects",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 89,
            "page_oevrview": "Building ML Artifacts The goal of a continuous integration pipeline is to avoid unnecessary effort in merg\u2010 ing the work from several contributors as well as to detect bugs or development con\u2010 flicts as soon as possible."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "CI/CD Pipelines for ML Projects",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 50,
            "page_oevrview": "These pipelines are deployed to specified Databricks workspaces as Databricks Workflows using the Databricks CLI with Databricks asset bundles. Databricks asset bundles in particular enable the ability to programmatically validate, deploy and run Databricks Workflows such as Databricks jobs, and Delta Live Tables."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "CI/CD Pipelines for ML Projects",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 33,
            "page_oevrview": "2 Testing: Upon making a pull request from the dev branch to the main branch, a CI trigger runs unit tests on the CI runner and integration tests in the staging environment. 3 Merge code: After successfully passing these tests, changes are merged from the dev branch to the main branch."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "Automated Testing for ML Systems",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 35,
            "page_oevrview": "Automated reporting adds an extra layer of efficiency for MRM and audit teams in MLOps systems and processes. Machine Learning Architect Traditional data architects are responsible for understanding the overall enterprise architecture and ensuring that it meets the requirements for data needs from across the business."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "Automated Testing for ML Systems",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 90,
            "page_oevrview": "This category of tests should be performed and automati\u2010 cally analyzed when the model is already deployed to production. Chapter 7 pro\u2010 vides more specific details on how to do that. Automating these tests as much as possible is essential and, indeed, is a key compo\u2010 nent of efficient MLOps."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "Automated Testing for ML Systems",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 116,
            "page_oevrview": "\u2022 The objective to optimize is only indirectly related to the performance of the pre\u2010 diction. Imagine an ad engine based on an ML model that predicts if a user will click on the ad. Now imagine that it is evaluated on the buy rate, ie, whether the user bought the product or service."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "Automated Testing for ML Systems",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 28,
            "page_oevrview": "A machine-learning system is trained rather than explicitly programmed. It&#39;s presented with many examples relevant to a task, and it finds statistical structure in these exam ples that eventually allows the system to come up with rules for automating the task."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "Automated Testing for ML Systems",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning with Python by Fran\u00e7ois Chollet.pdf",
            "page_no": 120,
            "page_oevrview": "You train on the training data and evaluate your model on the validation data. Once your model is ready for prime time, you test it one final time on the test data. You may ask, why not have two sets: a training set and a test set? You&#39;d train on the training data and evaluate on the test data. Much simpler!"
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "Continuous Training and Deployment",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 91,
            "page_oevrview": "Fully automated deployment is not always practical or desirable and is a business deci\u2010 sion as much as a technical decision, whereas continuous delivery is a tool for the development team to improve productivity and quality as well as measure pro\u2010 gress more reliably."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "Continuous Training and Deployment",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 87,
            "page_oevrview": "The two form a modern philosophy of agile software development and a set of practices and tools to release applications more often and faster, while also better controlling quality and risk. 73."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "Continuous Training and Deployment",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 90,
            "page_oevrview": "A lack of automation or speed wastes time, but, more importantly, it discourages the development team from testing and deploying often, which can delay the discovery of bugs or design choices that make it impossible to deploy to production."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "Continuous Training and Deployment",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 29,
            "page_oevrview": "Some common deployment patterns include: A/B testing Deploy multiple model versions concurrently and distribute the traffic among them to test and evaluate their performance. Based on predefined success criteria, such as accuracy or conversion rates, the best-performing model is then selected to handle all traffic."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "Continuous Training and Deployment",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 9,
            "page_oevrview": "This pattern is typically used when deploying a one-off model, or when model training is expensive and read-access to production data from the development environment is possible As in our prior paper, we recommend a deploy code approach for the majority of use cases, and the reference architecture presented in this update continues to follow this recommendation. dev staging prod DEPLOY MODELS BIG BOOK OF MLOPS - 2ND EDITION 9."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "Infrastructure as Code for ML Systems",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 34,
            "page_oevrview": "MLOps is a way for data scientists and software engineers to speak the same language and have the same baseline understanding of how different models deployed across the silos of the enterprise are working together in production."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "Infrastructure as Code for ML Systems",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 88,
            "page_oevrview": "CI/CD concepts apply to traditional software engineering, but they apply just as well to machine learning systems and are a critical part of MLOps strategy. After success\u2010 fully developing a model, a data scientist should push the code, metadata, and docu\u2010 mentation to a central repository and trigger a CI/CD pipeline."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "Infrastructure as Code for ML Systems",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 21,
            "page_oevrview": "While software code is relatively static (\u201crelatively\u201d because many modern software-as-a-service [SaaS] companies do have DevOps teams that can iterate quite quickly and deploy in production multiple times per day), data is always changing, which means machine learning models are constantly learning and adapting\u2014or not, as the case may be\u2014to new inputs."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "Infrastructure as Code for ML Systems",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 5,
            "page_oevrview": "The previously complex infrastructure required to serve real-time models can now be replaced and easily scaled with Databricks Model Serving. Long-term efficiency and performance stability of ML in production can be achieved using Databricks Lakehouse Monitoring."
        },
        {
            "domain": "MLOps",
            "sub_domain": "CI/CD for Machine Learning",
            "topic": "Infrastructure as Code for ML Systems",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 76,
            "page_oevrview": "MLflow AI Gateway In LLM-based applications where a third-party LLM API is used, the MLflow AI Gateway can be used as a standardized interface to route requests from vendors such as OpenAI and Anthropic."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "Monitoring ML Model Performance",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 44,
            "page_oevrview": "Data Scientist Concerns The data scientist is interested in monitoring ML models for a new, more challenging reason: they can degrade over time, since ML models are effectively models of the data they were trained on. This is not a problem faced by traditional software, but it is inherent to machine learning."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "Monitoring ML Model Performance",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 117,
            "page_oevrview": "We have seen that performance monitoring based on the ground truth is the cornerstone, while drift monitoring can provide early warning signals. Among possible drift mitigation measures, the workhorse is definitely retraining on new data, while model modifica\u2010 tion remains an option."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "Monitoring ML Model Performance",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 110,
            "page_oevrview": "Data from these environments needs to be centralized to be analyzed and monitored, either automatically or manually. This will enable continuous improvement of the ML system. An event log of a machine learning system is a record with a timestamp and the following information."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "Monitoring ML Model Performance",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 49,
            "page_oevrview": "TRIGGER MODEL TRAINING When the model monitoring metrics indicate performance issues, or when a model inevitably becomes out of date, the data scientist may need to return to the development environment and develop a new model version. SQL alerts can be used to notify data scientists when this happens."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "Monitoring ML Model Performance",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 16,
            "page_oevrview": "Users can also add their own charts to the dashboard that join the monitoring metrics with external business data. Alerts can be defined against metrics in the generated metrics tables. A natural evolution to this is to set up alerts when quality or performance indicators deviate from expectations."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "Detecting Data and Concept Drift",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 106,
            "page_oevrview": "Drift Detection in Practice As explained previously, to be able to react in a timely manner, model behavior should be monitored solely based on the feature values of the incoming data, without waiting for the ground truth to be available."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "Detecting Data and Concept Drift",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 107,
            "page_oevrview": "drifted dataset will not be an option, but it can be part of mitigation measures (eg, reverting to a simpler model, reweighting). Example Causes of Data Drift There are two frequent root causes of data drift: \u2022 Sample selection bias, where the training sample is not representative of the pop\u2010 ulation."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "Detecting Data and Concept Drift",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 105,
            "page_oevrview": "Once this problem is solved for retraining, the solution (reweighting, random sam\u2010 pling) can be used for monitoring. Input drift detection complements this approach, as it is needed to make sure that ground truth covering new, unexplored domains is made available to retrain the model."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "Detecting Data and Concept Drift",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 20,
            "page_oevrview": "They may also exist as constructs in the human mind that provide useful simplifying explanations or inferred causes of the observed data. They can be thought of as concepts or abstractions that help us make sense of the rich variability in the data."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "Detecting Data and Concept Drift",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 126,
            "page_oevrview": "We typically make a set of assumptions known collectively as the iid assumptions. These assumptions are that the examples in each dataset are independent from each other, and that the train set and test set are identically distributed, drawn from the same probability distribution as each other. This assumption allows us to describe the data gen erating process with a probability distribution over a single example."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "A/B Testing for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 116,
            "page_oevrview": "During the A/B test It is important not to stop the experiment before the test duration is over, even if the statistical test starts to return a significant metric difference. This practice (also called p-hacking) produces unreliable and biased results due to cherry-picking the desired outcome."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "A/B Testing for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 115,
            "page_oevrview": "A/B testing A/B testing (a randomized experiment testing two variants, A and B) is a widely used technique in website optimization. For ML models, it should be used only when champion/challenger is not possible. This might happen when: \u2022 The ground truth cannot be evaluated for both models."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "A/B Testing for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 48,
            "page_oevrview": "Multi-armed bandit testing is adaptive: the algorithm that decides the split between models adapts according to live results and reduces the workload of underperforming models. While multi-armed bandit testing is more complex, it can reduce the business cost of sending traffic to a poorly performing model."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "A/B Testing for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 29,
            "page_oevrview": "Based on predefined success criteria, such as accuracy or conversion rates, the best-performing model is then selected to handle all traffic. RELATION TO OTHER PATTERNS: A/B testing usually involves splitting traffic evenly or in some predefined ratio between model versions."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "A/B Testing for ML Models",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 30,
            "page_oevrview": "For instance, during A/B testing, you can set specific traffic percentages for each model version. Similarly, in rolling deployments, adjust these percentages as you phase in the new model."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "Model Explainability and Interpretability",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 41,
            "page_oevrview": "This is the field of explainability, which is connected to Responsible AI as discussed in Chapter 1 and which will be discussed in further detail in Chapter 4. Explainability techniques are becoming increasingly important as global concerns grow about the impact of unbridled AI."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "Model Explainability and Interpretability",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 68,
            "page_oevrview": "Some legal frameworks mandate some kind of explainability for decisions made by a model that have consequences on humans, like recommending a loan to be denied. \u201cElement 2: Bias\u201d on page 114 discusses this topic in detail. Note that the notion of explainability has several dimensions."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "Model Explainability and Interpretability",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 67,
            "page_oevrview": "They do not necessarily imply a specific causal relationship between some vari\u2010 ables and an outcome; they merely show how the model sees that relationship. In other words, the model should be used with care for what-if analysis."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "Model Explainability and Interpretability",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 291,
            "page_oevrview": "We assume that simpler models are less prone to overfitting than complex models, and hence the objective of model selection is to find the simplest model that explains the data reasonably well. This concept is also known as Occam&#39;s razor. Occam&#39;s razor Remark."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML System Monitoring",
            "topic": "Model Explainability and Interpretability",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 289,
            "page_oevrview": "More complex models are more flexible in A polynomial y = a0 +a1x+a2x 2 can also describe linear functions by setting a2 = 0, ie, it is strictly more expressive than a first-order polynomial. the sense that they can be used to describe more datasets."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "ML System Design Patterns",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 8,
            "page_oevrview": "ML deployment patterns Code and models often progress asynchronously through these stages. Thus, it becomes crucial to leverage a solution that allows for the management of model artifacts independently of code, making it possible to update a production model without necessarily making a code change."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "ML System Design Patterns",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 5,
            "page_oevrview": "Long-term efficiency and performance stability of ML in production can be achieved using Databricks Lakehouse Monitoring. These components collectively form the data pipelines of an ML solution, all of which can be orchestrated using Databricks Workflows."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "ML System Design Patterns",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 2,
            "page_oevrview": "30 CHAPTER 1 CHAPTER 2 CHAPTER 3 CHAPTER 4 Contents BIG BOOK OF MLOPS - 2ND EDITION 2."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "ML System Design Patterns",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 17,
            "page_oevrview": "Since machine learning is inherently data driven, data is at the core data of machine learning. The goal of machine learning is to design general purpose methodologies to extract valuable patterns from data, ideally without much domain-specific expertise."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "ML System Design Patterns",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf",
            "page_no": 18,
            "page_oevrview": "In the second sense, we use the exact same phrase \u201cmachine learning algorithm\u201d to mean a system that adapts some internal parameters of the predictor so that it performs well on future unseen input data. Here we refer to this adapta training tion as training a system."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "Scalable ML System Architecture",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 36,
            "page_oevrview": "Machine learning architects play a critical role in the ML model life cycle, ensuring a scalable and flexible environment for model pipelines. In addition, data teams need their expertise to introduce new technologies (when appropriate) that improve ML model performance in production."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "Scalable ML System Architecture",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 95,
            "page_oevrview": "Scalable and Elastic Systems A computational system is said to be horizontally scalable (or just scalable) if it is pos\u2010 sible to incrementally add more computers to expand its processing power. For exam\u2010 ple, a Kubernetes cluster can be expanded to hundreds of machines."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "Scalable ML System Architecture",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 11,
            "page_oevrview": "While this shift is exciting, it&#39;s also challenging, as it combines the complexities of machine learning models with the complexities of the modern organization. One difficulty, as organizations move from experimenting with machine learning to scaling it in production environments, is maintenance."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "Scalable ML System Architecture",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 17,
            "page_oevrview": "Having a well-defined structure to store AI assets further facilitates the addition of new data and ML models without disrupting existing use cases in production Further, a well-defined MLOps workflow enforces a standardized approach to deploying AI assets to production using Unity Catalog."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "Scalable ML System Architecture",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 5,
            "page_oevrview": "The previously complex infrastructure required to serve real-time models can now be replaced and easily scaled with Databricks Model Serving. Long-term efficiency and performance stability of ML in production can be achieved using Databricks Lakehouse Monitoring."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "Cost Optimization for ML Systems",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 290,
            "page_oevrview": "Machine learning usually acts indirectly. In most machine learning scenarios, we care about some performance measure P, that is defined with respect to the test set and may also be intractable. We therefore optimize P only indirectly. We reduce a different cost function J(\u03b8) in the hope that doing so will improve P ."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "Cost Optimization for ML Systems",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 169,
            "page_oevrview": "The most common cost function is the negative log-likelihood, so that minimizing the cost function causes maximum likelihood estimation. The cost function may also include additional terms, such as regularization terms."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "Cost Optimization for ML Systems",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf",
            "page_no": 168,
            "page_oevrview": "Today, we know that the machine learning models described in part II work very well when trained with gradient descent. The optimization algorithm may not be guaranteed to arrive at even a local minimum in a reasonable amount of time, but it often finds a very low value of the cost function quickly enough to be useful."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "Cost Optimization for ML Systems",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 72,
            "page_oevrview": "3 Reduce costs by tweaking LLMs and queries: There are many LLM-specific techniques for reducing computation and costs. These include shortening queries, tweaking inference configurations, and using smaller versions of models."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "Cost Optimization for ML Systems",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 73,
            "page_oevrview": "REDUCE COMPUTATION FOR A GIVEN MODEL. Shorten queries and responses. Computation scales with input and output sizes, reducing costs by using more concise queries and responses. Tweak inference configurations. Some types of inference, such as beam search, require more computation. OTHER Split traffic."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "ML Technical Debt Management",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 63,
            "page_oevrview": "It can also reduce modeling debt, allowing data scientists to understand the main prediction drivers and ensure that they are robust. Of course, there are trade-offs to consider between the cost of time spent to understand the model and the expected value, as well as risks associated with the model&#39;s use."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "ML Technical Debt Management",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 30,
            "page_oevrview": "This allows the models to be integrated with the business rules, as well as helps the SMEs to fully understand decision contexts and the potential impact of model changes."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "ML Technical Debt Management",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 35,
            "page_oevrview": "When it comes to the ML model life cycle, model risk managers play the critical role of analyzing not just model outcomes, but the initial goal and business questions ML models seek to resolve to minimize overall risk to the company."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "ML Technical Debt Management",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 52,
            "page_oevrview": "KEY PROPERTIES OF LLMS IMPLICATIONS FOR MLOPS LLMs are available in many forms: Very general proprietary models behind paid APIs Open source models that vary from general to specific applications Custom models fine-tuned for specific applications Development process: Projects often develop incrementally, starting from existing, third-party or open source models and ending with custom fine-tuned models."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "ML Technical Debt Management",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 36,
            "page_oevrview": "EVALUATION Model quality is evaluated by testing on held-out data. The results of these tests are logged to the MLflow Tracking server. At this point it can be determined if a newly developed model outperforms that of the current model in production."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "ML System Security and Compliance",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 83,
            "page_oevrview": "Machine learning security shares many common traits with general computer system security, one of the main ideas being that security is not an additional independent feature of the system; that is, generally you cannot secure a system that is not designed to be secure, and the organization processes must take into account the nature of the threat from the beginning."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "ML System Security and Compliance",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 81,
            "page_oevrview": "As a result, full auditability comes at a cost that should be balanced with the criticality of the model itself. Machine Learning Security As a piece of software, a deployed model running in its serving framework can present multiple security issues that range from low-level glitches to social engineer\u2010 ing."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "ML System Security and Compliance",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf",
            "page_no": 120,
            "page_oevrview": "For businesses, this means developing strong governance of their MLOps process. They must assess the risks, determine their own set of fairness values, and then implement the necessary process to manage them."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "ML System Security and Compliance",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 18,
            "page_oevrview": "In many cases this is required for compliance with data protection regulations, and for maintaining data privacy and security. For the purposes of auditability, Unity Catalog captures a log of actions performed against the metastore, and these logs are delivered as part of Databricks audit logs. Collaboration In many organizations, multiple teams or individuals will be working with the same AI assets \u2014 sharing features and models across different use cases Consolidating both data and AI assets into a centralized location, and clearly delineating where specific assets are located and how they should be used, facilitates collaboration CONCEPTS Unity Catalog has a number of core concepts that we should define before unpacking the considerations when organizing AI assets along with data in Unity Catalog."
        },
        {
            "domain": "MLOps",
            "sub_domain": "ML Engineering Best Practices",
            "topic": "ML System Security and Compliance",
            "resource_type": "book",
            "gcs_uri": "gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf",
            "page_no": 5,
            "page_oevrview": "Long-term efficiency and performance stability of ML in production can be achieved using Databricks Lakehouse Monitoring. These components collectively form the data pipelines of an ML solution, all of which can be orchestrated using Databricks Workflows."
        }
    ]
}