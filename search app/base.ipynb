{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07a8b583-ee8c-46ed-96af-8a7323647621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "# !pip install PyMuPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08bcead3-9a76-4776-bd96-b6e306115dfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_pdfs(bucket_name = 'lossless-learning', prefix=\"books\"):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    return ['gs://lossless-learning/'+blob.name for blob in bucket.list_blobs(prefix=prefix) if blob.name.endswith(\".pdf\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b279379b-eaef-4f21-903a-c47476216a98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_list = list_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b05c08b-9f7b-4205-a148-5d13cbdaa897",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://lossless-learning/books/A Modern Introduction to Probability and Statistics by FM Dekking.pdf',\n",
       " 'gs://lossless-learning/books/Advanced Calculus by Lynn H Loomis.pdf',\n",
       " 'gs://lossless-learning/books/An Introduction to Statistical Learning by Gareth James.pdf',\n",
       " 'gs://lossless-learning/books/Calculus by Gilbert Strang.pdf',\n",
       " 'gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf',\n",
       " 'gs://lossless-learning/books/Deep Learning with Python by François Chollet.pdf',\n",
       " 'gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf',\n",
       " 'gs://lossless-learning/books/Introduction to Applied Linear Algebra by Stephen Boyd.pdf',\n",
       " 'gs://lossless-learning/books/Linear Algebra Done Right by Sheldon Axler.pdf',\n",
       " 'gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf',\n",
       " 'gs://lossless-learning/books/Probability and Statistics The Science of Uncertainity by Michael J. Evans.pdf',\n",
       " 'gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf',\n",
       " 'gs://lossless-learning/books/The Elements of Statistical Learning by Trevor Hastie.pdf']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f213579d-779b-43a8-96d8-9ea4ffa512ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from google.cloud import storage\n",
    "import io\n",
    "\n",
    "def read_pdf_from_gcs(bucket_name, blob_name):\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    pdf_bytes = blob.download_as_bytes()\n",
    "\n",
    "    doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "    pages = [page.get_text().lower() for page in doc]\n",
    "    doc.close()\n",
    "    return pages  # List of page texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c1cc2da5-ca6a-4577-8b82-ea25f01a5fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "books_pages = []\n",
    "for pdf in pdf_list:\n",
    "    pages = read_pdf_from_gcs('lossless-learning',pdf )\n",
    "    books_pages.append(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d87c968a-b90b-40b5-ade1-6785019b1420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def slice_books_from_chapter_1(books_pages):\n",
    "    sliced_books = []\n",
    "\n",
    "    for book_pages in books_pages:\n",
    "        chapter_start_idx = 0\n",
    "\n",
    "        for i, page in enumerate(book_pages):\n",
    "            if re.search(r\"chapter\\s*[\\/\\-:]?\\s*(ONE|1)\", page):\n",
    "                chapter_start_idx = i\n",
    "                break\n",
    "\n",
    "        sliced_books.append(book_pages[chapter_start_idx:])\n",
    "\n",
    "    return sliced_books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "89604944-959f-4112-b63f-d5380c4a971a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "req_book_pages =  slice_books_from_chapter_1(books_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "82fa16e2-4c1a-4a7a-9548-004c873b31d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxii\\ncontents\\n18.2\\ndiagonal linear discriminant analysis\\nand nearest shrunken centroids . . . . . . . . . . . . . .\\n651\\n18.3\\nlinear classiﬁers with quadratic regularization . . . . .\\n654\\n18.3.1\\nregularized discriminant analysis . . . . . . . .\\n656\\n18.3.2\\nlogistic regression\\nwith quadratic regularization . . . . . . . . . .\\n657\\n18.3.3\\nthe support vector classiﬁer\\n. . . . . . . . . .\\n657\\n18.3.4\\nfeature selection . . . . . . . . . . . . . . . . . .\\n658\\n18.3.5\\ncomputational shortcuts when p ≫n . . . . .\\n659\\n18.4\\nlinear classiﬁers with l1 regularization\\n. . . . . . . . .\\n661\\n18.4.1\\napplication of lasso\\nto protein mass spectroscopy\\n. . . . . . . . . .\\n664\\n18.4.2\\nthe fused lasso for functional data\\n. . . . . .\\n666\\n18.5\\nclassiﬁcation when features are unavailable . . . . . . .\\n668\\n18.5.1\\nexample: string kernels\\nand protein classiﬁcation . . . . . . . . . . . . .\\n668\\n18.5.2\\nclassiﬁcation and other models using\\ninner-product kernels and pairwise distances .\\n670\\n18.5.3\\nexample: abstracts classiﬁcation\\n. . . . . . . .\\n672\\n18.6\\nhigh-dimensional regression:\\nsupervised principal components\\n. . . . . . . . . . . . .\\n674\\n18.6.1\\nconnection to latent-variable modeling\\n. . . .\\n678\\n18.6.2\\nrelationship with partial least squares . . . . .\\n680\\n18.6.3\\npre-conditioning for feature selection\\n. . . . .\\n681\\n18.7\\nfeature assessment and the multiple-testing problem . .\\n683\\n18.7.1\\nthe false discovery rate . . . . . . . . . . . . .\\n687\\n18.7.2\\nasymmetric cutpoints and the sam procedure\\n690\\n18.7.3\\na bayesian interpretation of the fdr . . . . . .\\n692\\n18.8\\nbibliographic notes\\n. . . . . . . . . . . . . . . . . . . . .\\n693\\nexercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n694\\nreferences\\n699\\nauthor index\\n729\\nindex\\n737\\n'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req_book_pages[12][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97611c4-7cbc-422f-a18a-8a4d554daef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3603a0f7-6245-49d6-acaa-8ddf76ab56e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import discoveryengine\n",
    "\n",
    "# TODO(developer): Uncomment these variables before running the sample.\n",
    "project_id = \"ardent-sun-453501-d5\"\n",
    "location = \"global\" # Values: \"global\"\n",
    "data_store_id = \"book_store\"\n",
    "\n",
    "\n",
    "def create_data_store_sample(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    data_store_id: str,\n",
    ") -> str:\n",
    "    #  For more information, refer to:\n",
    "    # https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store\n",
    "    client_options = (\n",
    "        ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
    "        if location != \"global\"\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Create a client\n",
    "    client = discoveryengine.DataStoreServiceClient(client_options=client_options)\n",
    "\n",
    "    # The full resource name of the collection\n",
    "    # e.g. projects/{project}/locations/{location}/collections/default_collection\n",
    "    parent = client.collection_path(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        collection=\"default_collection\",\n",
    "    )\n",
    "\n",
    "    data_store = discoveryengine.DataStore(\n",
    "        display_name=\"datastore books\",\n",
    "        # Options: GENERIC, MEDIA, HEALTHCARE_FHIR\n",
    "        industry_vertical=discoveryengine.IndustryVertical.GENERIC,\n",
    "        # Options: SOLUTION_TYPE_RECOMMENDATION, SOLUTION_TYPE_SEARCH, SOLUTION_TYPE_CHAT, SOLUTION_TYPE_GENERATIVE_CHAT\n",
    "        solution_types=[discoveryengine.SolutionType.SOLUTION_TYPE_SEARCH],\n",
    "        # TODO(developer): Update content_config based on data store type.\n",
    "        # Options: NO_CONTENT, CONTENT_REQUIRED, PUBLIC_WEBSITE\n",
    "        content_config=discoveryengine.DataStore.ContentConfig.CONTENT_REQUIRED,\n",
    "    )\n",
    "\n",
    "    request = discoveryengine.CreateDataStoreRequest(\n",
    "        parent=parent,\n",
    "        data_store_id=data_store_id,\n",
    "        data_store=data_store,\n",
    "        # Optional: For Advanced Site Search Only\n",
    "        # create_advanced_site_search=True,\n",
    "    )\n",
    "\n",
    "    # Make the request\n",
    "    operation = client.create_data_store(request=request)\n",
    "\n",
    "    print(f\"Waiting for operation to complete: {operation.operation.name}\")\n",
    "    response = operation.result()\n",
    "\n",
    "    # After the operation is complete,\n",
    "    # get information from operation metadata\n",
    "    metadata = discoveryengine.CreateDataStoreMetadata(operation.metadata)\n",
    "\n",
    "    # Handle the response\n",
    "    print(response)\n",
    "    print(metadata)\n",
    "\n",
    "    return operation.operation.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7f76b6e-2246-4772-9669-5f2d0842ff13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation to complete: projects/203101603788/locations/global/collections/default_collection/operations/create-data-store-5470281226982137829\n",
      "name: \"projects/203101603788/locations/global/collections/default_collection/dataStores/book_store\"\n",
      "display_name: \"datastore books\"\n",
      "industry_vertical: GENERIC\n",
      "solution_types: SOLUTION_TYPE_SEARCH\n",
      "content_config: CONTENT_REQUIRED\n",
      "default_schema_id: \"default_schema\"\n",
      "document_processing_config {\n",
      "  name: \"projects/203101603788/locations/global/collections/default_collection/dataStores/book_store/documentProcessingConfig\"\n",
      "  default_parsing_config {\n",
      "    digital_parsing_config {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "serving_config_data_store {\n",
      "}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'projects/203101603788/locations/global/collections/default_collection/operations/create-data-store-5470281226982137829'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_data_store_sample(  project_id, location, data_store_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "572c7fba-0c51-4892-af54-7d53157f324a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for operation to complete: projects/203101603788/locations/global/collections/default_collection/dataStores/book_store/branches/0/operations/import-documents-13123355099857093690\n",
      "error_config {\n",
      "  gcs_prefix: \"gs://203101603788_us_central1_import_content/errors13123355099857096525\"\n",
      "}\n",
      "\n",
      "create_time {\n",
      "  seconds: 1744483317\n",
      "  nanos: 62505000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1744483998\n",
      "  nanos: 953814000\n",
      "}\n",
      "success_count: 13\n",
      "total_count: 13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import discoveryengine\n",
    "\n",
    "# TODO(developer): Uncomment these variables before running the sample.\n",
    "# project_id = \"YOUR_PROJECT_ID\"\n",
    "# location = \"YOUR_LOCATION\" # Values: \"global\"\n",
    "# data_store_id = \"YOUR_DATA_STORE_ID\"\n",
    "\n",
    "# Examples:\n",
    "# - Unstructured documents\n",
    "#   - `gs://bucket/directory/file.pdf`\n",
    "#   - `gs://bucket/directory/*.pdf`\n",
    "# - Unstructured documents with JSONL Metadata\n",
    "#   - `gs://bucket/directory/file.json`\n",
    "# - Unstructured documents with CSV Metadata\n",
    "#   - `gs://bucket/directory/file.csv`\n",
    "gcs_uri = pdf_list\n",
    "\n",
    "#  For more information, refer to:\n",
    "# https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store\n",
    "client_options = (\n",
    "    ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\n",
    "    if location != \"global\"\n",
    "    else None\n",
    ")\n",
    "\n",
    "# Create a client\n",
    "client = discoveryengine.DocumentServiceClient(client_options=client_options)\n",
    "\n",
    "# The full resource name of the search engine branch.\n",
    "# e.g. projects/{project}/locations/{location}/dataStores/{data_store_id}/branches/{branch}\n",
    "parent = client.branch_path(\n",
    "    project=project_id,\n",
    "    location=location,\n",
    "    data_store=data_store_id,\n",
    "    branch=\"default_branch\",\n",
    ")\n",
    "\n",
    "request = discoveryengine.ImportDocumentsRequest(\n",
    "    parent=parent,\n",
    "    gcs_source=discoveryengine.GcsSource(\n",
    "        # Multiple URIs are supported\n",
    "        input_uris=gcs_uri,\n",
    "        # Options:\n",
    "        # - `content` - Unstructured documents (PDF, HTML, DOC, TXT, PPTX)\n",
    "        # - `custom` - Unstructured documents with custom JSONL metadata\n",
    "        # - `document` - Structured documents in the discoveryengine.Document format.\n",
    "        # - `csv` - Unstructured documents with CSV metadata\n",
    "        data_schema=\"content\",\n",
    "    ),\n",
    "    # Options: `FULL`, `INCREMENTAL`\n",
    "    reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL,\n",
    ")\n",
    "\n",
    "operation = client.import_documents(request=request)\n",
    "\n",
    "print(f\"Waiting for operation to complete: {operation.operation.name}\")\n",
    "response = operation.result()\n",
    "\n",
    "metadata = discoveryengine.ImportDocumentsMetadata(operation.metadata)\n",
    "\n",
    "# Handle the response\n",
    "print(response)\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23bf3f96-caf2-4645-83f8-e29caade3a81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"ardent-sun-453501-d5\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "DATA_STORE_ID = \"book_store\"  # @param {type:\"string\"}\n",
    "DATA_STORE_LOCATION = \"global\"  # @param {type:\"string\"}\n",
    "\n",
    "MODEL = \"gemini-2.0-flash\"  # @param {type:\"string\"}\n",
    "\n",
    "# if PROJECT_ID == \"ardent-sun-453501-d5\" or DATA_STORE_ID == \"book_store\":\n",
    "#     raise ValueError(\n",
    "#         \"Please set the PROJECT_ID, DATA_STORE_ID constants to reflect your environment.\"\n",
    "#     )\n",
    "     \n",
    "    \n",
    "from langchain.chains import (\n",
    "    ConversationalRetrievalChain,\n",
    "    RetrievalQA,\n",
    "    RetrievalQAWithSourcesChain,\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_community import (\n",
    "    VertexAIMultiTurnSearchRetriever,\n",
    "    VertexAISearchRetriever,\n",
    ")\n",
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "llm = VertexAI(model_name=MODEL)\n",
    "\n",
    "retriever = VertexAISearchRetriever(\n",
    "    project_id=PROJECT_ID,\n",
    "    location_id=DATA_STORE_LOCATION,\n",
    "    data_store_id=DATA_STORE_ID,\n",
    "    get_extractive_answers=True,\n",
    "    max_documents=4,\n",
    "    max_extractive_segment_count=1,\n",
    "    max_extractive_answer_count=5,\n",
    "    beta=True\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "search_query = \"What is MLOps\"  # @param {type:\"string\"}\n",
    "\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n",
    ")\n",
    "\n",
    "ans  = retrieval_qa.invoke(search_query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "21410175-2dcb-497f-91e4-f12622f61f3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is MLOps',\n",
       " 'result': \"MLOps (Machine Learning Operations) is a rapidly evolving field that combines DataOps, DevOps, and ModelOps to build and maintain robust, flexible, and efficient workflows for developing, deploying, and maintaining AI models at scale. It's becoming critical for successful data science project deployment in the enterprise.\\n\",\n",
       " 'source_documents': [Document(metadata={'id': 'c95500c41a9d900b798c436f0368e892', 'source': 'gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf17'}, page_content='CHAPTER 1 Why Now and Challenges Machine learning operations (MLOps) is quickly becoming a critical component of successful data science project deployment in the enterprise (Figure 1-1).'),\n",
       "  Document(metadata={'id': 'c95500c41a9d900b798c436f0368e892', 'source': 'gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf21'}, page_content='The complexity of this environment, including the fact that machine learning models are made up of both code and data, is what makes MLOps a new and unique discipline. What About DataOps?'),\n",
       "  Document(metadata={'id': 'c95500c41a9d900b798c436f0368e892', 'source': 'gs://lossless-learning/books/Introducing MLOps by Mark Treveil.pdf15'}, page_content='PART I MLOps: What and Why.'),\n",
       "  Document(metadata={'id': '521794879f38dbe8de076e53be59e9ad', 'source': 'gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf5'}, page_content='MLOps = DataOps + DevOps + ModelOps <b>Machine learning operations</b> (MLOps) is a rapidly evolving field where building and maintaining robust, flexible and efficient workflows is critical.'),\n",
       "  Document(metadata={'id': '521794879f38dbe8de076e53be59e9ad', 'source': 'gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf78'}, page_content='CHAPTER 7 Conclusion In an era defined by data-driven decision making and intelligent automation, the importance of MLOps cannot be overstated. MLOps provides the essential scaffolding for developing, deploying, and maintaining AI models at scale, ensuring they remain accurate and continue to deliver business value.'),\n",
       "  Document(metadata={'id': '521794879f38dbe8de076e53be59e9ad', 'source': 'gs://lossless-learning/books/The Big Book of MLOps by Databricks.pdf51'}, page_content='In short, LLMs are a new class of natural language processing (NLP) models that have significantly surpassed their predecessors in size and performance across a variety of tasks, such as open-ended question answering, summarization and execution of near-arbitrary instructions.'),\n",
       "  Document(metadata={'id': '6cb8f71c301d6c26000e590287b0e48a', 'source': 'gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf17'}, page_content='1 Introduction and Motivation Machine learning is about designing algorithms that automatically extract valuable information from data.'),\n",
       "  Document(metadata={'id': '6cb8f71c301d6c26000e590287b0e48a', 'source': 'gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf401'}, page_content='References Abel, Niels H. 1826. D´emonstration de l&#39;Impossibilit´e de la R´esolution Alg´ebrique des Equations G ´ ´en´erales qui Passent le Quatri`eme Degr´e.'),\n",
       "  Document(metadata={'id': '6cb8f71c301d6c26000e590287b0e48a', 'source': 'gs://lossless-learning/books/Mathematics for Machine Learning by Marc Peter Deisenroth.pdf104'}, page_content='We will first cover a square-root-like operation for symmetric, positive definite matrices, the Cholesky decomposition (Section 4.3).'),\n",
       "  Document(metadata={'id': 'b24ca645ba3f5836188f90908cf4ad26', 'source': 'gs://lossless-learning/books/Deep Learning by Ian Goodfellow.pdf154'}, page_content='Many regularized estimation strategies, such as maximum likelihood learning regularized with weight decay, can be interpreted as making the MAP approxima tion to Bayesian inference. This view applies when the regularization consists of adding an extra term to the objective function that corresponds to log p(θ ).')]}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd19a49b-8a75-4b2c-b5df-dcae79a57200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu124.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu124:m129"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
